{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:20:24.945729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 14:20:26.183771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "d63bda04-9b34-4f59-c696-954296a31a7e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elJ5NA1ZbQS2"
   },
   "outputs": [],
   "source": [
    "# many more here - https://archive.ics.uci.edu/\n",
    "\n",
    "# iris dataset - very small (150 records)\n",
    "# flower classification dataset\n",
    "\n",
    "# for external dataset - leave one out class\n",
    "# ds = tfds.load('iris', split='train', shuffle_files=True, as_supervised=True)\n",
    "# ood_class = np.random.randint(0, 3)\n",
    "# allY, allX = np.array([y for x, y in ds]), np.array([x for x, y in ds])\n",
    "# binY = label_binarize(np.array(allY), classes=[0, 1, 2])\n",
    "# dataY, dataX = binY[allY!=ood_class], allX[allY!=ood_class]\n",
    "# dataX, dataX_tst, dataY, dataY_tst = train_test_split(dataX, dataY, test_size=0.10, random_state=42)\n",
    "\n",
    "# external_Y, external_X = binY[allY==ood_class], allX[allY==ood_class]\n",
    "# print(dataX.shape, dataX_tst.shape, dataY.shape, dataY_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyjFibMvEdsJ",
    "outputId": "b206648d-1aad-428e-f039-b1e709b8e257"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8034, 8) (3444, 8) (8034, 6) (3444, 6)\n"
     ]
    }
   ],
   "source": [
    "# # Shuttle datsaset - large (58k records)\n",
    "# # class 1 consumes 80% dataset hence can be inlier dataset\n",
    "# url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/shuttle/shuttle.tst\"\n",
    "# df = pd.read_csv(url, header=None, delimiter=' ')\n",
    "# df = df.loc[df[9] != 4]\n",
    "\n",
    "# y = label_binarize(np.array(df[9]), classes=[1,2,3,5,6,7])\n",
    "# not_ood_class = 1\n",
    "\n",
    "# dataY, dataX = y[np.array(df[9])==not_ood_class], df[df[9]==not_ood_class].iloc[:,0:8]\n",
    "# dataX, dataX_tst, dataY, dataY_tst = train_test_split(dataX, dataY, test_size=0.3, random_state=42)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# dataX = scaler.fit_transform(dataX)\n",
    "# dataX_tst = scaler.transform(dataX_tst)\n",
    "\n",
    "# external_Y, external_X = y[np.array(df[9])!=not_ood_class], df[df[9]!=not_ood_class].iloc[:,0:8]\n",
    "# external_X = scaler.transform(external_X)\n",
    "# print(dataX.shape, dataX_tst.shape, dataY.shape, dataY_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SQPwn-_G_Q3E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10869, 128) (1208, 128) (10869, 6) (1208, 6)\n"
     ]
    }
   ],
   "source": [
    "# Gas sensor datsaset - medium (13k records)\n",
    "# data drift detection + classification task\n",
    "gas_data = pd.concat([pd.read_csv(f'./datasets-ood/Dataset/batch{i+1}.dat', sep=' ', header=None) for i in range(10)])\n",
    "\n",
    "def transform(r):\n",
    "    d = []\n",
    "    d.append(r[0])\n",
    "    for e in r[1:]:\n",
    "        d.append(e[e.find(':') +1:])\n",
    "    return pd.Series(data=d, index=r.index)\n",
    "\n",
    "df = gas_data.apply(transform, axis=1)\n",
    "feature_data = df[np.arange(1,129)]\n",
    "scaler = StandardScaler().fit(feature_data) # should we use a different scaler for id and ood data?\n",
    "scaled_feature_data = scaler.transform(feature_data)\n",
    "target_data = df[np.array(0)]\n",
    "binY = label_binarize(np.array(target_data), classes=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "ood_class = np.random.randint(0, 6)+1\n",
    "dataY, dataX = binY[target_data!=ood_class], scaled_feature_data[target_data!=ood_class]\n",
    "dataX, dataX_tst, dataY, dataY_tst = train_test_split(dataX, dataY, test_size=0.10, random_state=42)\n",
    "\n",
    "external_Y, external_X = binY[target_data==ood_class], scaled_feature_data[target_data==ood_class]\n",
    "\n",
    "print(dataX.shape, dataX_tst.shape, dataY.shape, dataY_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "hzTMrTPaESW-",
    "outputId": "3ec68142-b61d-4ad9-b430-8603a2f613d0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8f003a027063>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drive Diagnosis datsaset - large (58k records)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/datasets-ood/Sensorless_drive_diagnosis.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mood_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# # Drive Diagnosis datsaset - large (58k records)\n",
    "# dataset = pd.read_csv('/content/drive/My Drive/datasets-ood/Sensorless_drive_diagnosis.txt', sep=\" \", header=None,names=[i for i in range(48)]+[\"label\"])\n",
    "# y = label_binarize(np.array(dataset[\"label\"]), classes=[1, 2,3,4,5,6,7,8,9,10,11])\n",
    "# ood_class = np.random.randint(0, 11)+1\n",
    "\n",
    "# dataY, dataX = y[np.array(dataset[\"label\"])!=ood_class], dataset[dataset[\"label\"]!=ood_class].iloc[:,0:48]\n",
    "# dataX, dataX_tst, dataY, dataY_tst = train_test_split(dataX, dataY, test_size=0.3, random_state=42)\n",
    "# scaler = StandardScaler()\n",
    "# dataX = scaler.fit_transform(dataX)\n",
    "# dataX_tst = scaler.transform(dataX_tst)\n",
    "\n",
    "# external_Y, external_X = y[np.array(dataset[\"label\"])==ood_class], dataset[dataset[\"label\"]==ood_class].iloc[:,0:48]\n",
    "# external_X = scaler.transform(external_X)\n",
    "# print(dataX.shape, dataX_tst.shape, dataY.shape, dataY_tst.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sucs4AA_q09l"
   },
   "outputs": [],
   "source": [
    "# Now you can use train_features in place of dataX and train_labels in place of dataY\n",
    "# X = tf.cast(train_features, tf.float32)\n",
    "# Y = tf.cast(train_labels, tf.float32)\n",
    "\n",
    "# pperm = np.random.permutation(len(X))\n",
    "\n",
    "# X = tf.constant(np.array(X)[pperm])\n",
    "# Y = tf.constant(np.array(Y)[pperm])\n",
    "\n",
    "# X = X[:len(X)//2]\n",
    "# Y = Y[:len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "#sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "0dfcbb26-6d3d-4223-8f94-78c1bbdd5a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12699, 2048)\n",
      "(12699,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  def __init__(self, sigma_rot, d, D=160):\n",
    "\n",
    "    self.sigmas = [sigma_rot/8, sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4, sigma_rot/8]\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(np.array(X)[rprm[:100], :], np.array(X)[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1])\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  # start = time.time()\n",
    "  clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  # end = time.time()\n",
    "  # print(end-start)\n",
    "  # clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "  # print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print(np.mean(prd[:, 0]==Y))\n",
    "  return wgts\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    beta = linear_coefs(get_rand_feats(X_sub, model), Y_sub)\n",
    "\n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "e9f47524-f11d-4cd9-9db1-de9eb10f0b29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Step 25\n",
      "Step 50\n",
      "Step 75\n",
      "Step 100\n",
      "Step 125\n",
      "Step 150\n",
      "Step 175\n",
      "Step 200\n",
      "Step 225\n",
      "Step 250\n",
      "Step 275\n",
      "Step 300\n",
      "Step 325\n",
      "Step 350\n",
      "Step 375\n",
      "Step 400\n",
      "Step 425\n",
      "Step 450\n",
      "Step 475\n",
      "Step 500\n",
      "Step 525\n",
      "Step 550\n",
      "Step 575\n",
      "Step 600\n",
      "Step 625\n",
      "Step 650\n",
      "Step 675\n",
      "Step 700\n",
      "Step 725\n",
      "Step 750\n",
      "Step 775\n",
      "Step 800\n",
      "Step 825\n",
      "Step 850\n",
      "Step 875\n",
      "Step 900\n",
      "Step 925\n",
      "Step 950\n",
      "Step 975\n",
      "Step 1000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "model = define_rand_feats(X)\n",
    "\n",
    "N = 2**10    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "# Some other strategies:\n",
    "### Completely random\n",
    "### Maybe draw random directions, but also some one hot directions (essentially taking the highest/lowest values of certain features as a subset)\n",
    "### Second idea in theory would increase variance between subsets - thinking similar instances have similar feature values\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas = get_models(X, Y, random_dirs, model, percentile=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "np.save('random_dirs.npy', random_dirs)\n",
    "np.save('betas.npy', betas)\n",
    "np.save('Ws.npy', model.Ws)\n",
    "\n",
    "# random_dirs = np.load('random_dirs.npy')\n",
    "# betas = np.load('betas.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYKgRDoVX-qO"
   },
   "outputs": [],
   "source": [
    "random_dirs1 = tf.constant(np.load('random_dirs.npy'))\n",
    "betas1 = tf.squeeze(tf.constant(np.load('betas.npy')))\n",
    "model1 = define_rand_feats(X)\n",
    "model1.Ws = tf.constant(np.load('Ws.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "6a74610c-2b52-4351-8e22-982a404874c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 2241)\n",
      "(1024, 2048)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "4a7e7449-d9f9-40a7-a1b1-64f4c28f5d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.08072856204520046, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "18b27514-bdd1-423f-b39d-48220eeec813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.749895002099958"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 75)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub, model), betas[sample])\n",
    "\n",
    "np.mean(prd == Y_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "beta_input = layers.Input(shape=(beta_dim,))\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([beta_input, dir_input])\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(32, activation=tf.nn.elu)(x)\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# x = layers.Dense(32, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([beta_input, dir_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs, decoder_dir_input], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([beta_input, dir_input])[2], dir_input])\n",
    "vae = models.Model([beta_input, dir_input], outputs, name=\"vae\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, z_mean, z_log_var, reg=1.0):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs, dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs, axis=-1)[0], axis=-1))\n",
    "  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = recon_loss + 0.001 * kl_loss\n",
    "  return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "def train_step(model, inputs, dir_inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z_mean, z_log_var, z = model.encoder([inputs, dir_inputs])\n",
    "    outputs = model.decoder([z, dir_inputs])\n",
    "    total_loss, recon_loss, kl_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(betas, dirs, batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield betas[i:i+batch_size], dirs[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "8dd8c722-dc02-4ed6-f74c-cd21085979b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 1.0103455781936646, recon_loss = 1.0067553520202637, kl_loss = 3.590211868286133\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.2799820005893707, recon_loss = 0.27367377281188965, kl_loss = 6.308230400085449\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.27651458978652954, recon_loss = 0.2670864760875702, kl_loss = 9.428110122680664\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.2727864384651184, recon_loss = 0.25954699516296387, kl_loss = 13.239457130432129\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.2744542062282562, recon_loss = 0.2694692313671112, kl_loss = 4.984969615936279\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.27791842818260193, recon_loss = 0.26833489537239075, kl_loss = 9.583529472351074\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.27114659547805786, recon_loss = 0.2649970054626465, kl_loss = 6.149577617645264\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.2724546790122986, recon_loss = 0.26969853043556213, kl_loss = 2.756157875061035\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.2909242808818817, recon_loss = 0.26863303780555725, kl_loss = 22.291244506835938\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.28022387623786926, recon_loss = 0.27266252040863037, kl_loss = 7.561341762542725\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.27152660489082336, recon_loss = 0.2612072825431824, kl_loss = 10.319314956665039\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.28654658794403076, recon_loss = 0.2668033242225647, kl_loss = 19.743274688720703\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.27295684814453125, recon_loss = 0.2671903371810913, kl_loss = 5.766496658325195\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.2658936679363251, recon_loss = 0.2616302967071533, kl_loss = 4.263363838195801\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.282732218503952, recon_loss = 0.2643449902534485, kl_loss = 18.387229919433594\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.27286216616630554, recon_loss = 0.266465961933136, kl_loss = 6.396197319030762\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.2809516489505768, recon_loss = 0.2683638632297516, kl_loss = 12.587797164916992\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.2750331163406372, recon_loss = 0.2631780505180359, kl_loss = 11.85505485534668\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.27528202533721924, recon_loss = 0.27069857716560364, kl_loss = 4.583457946777344\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.27621299028396606, recon_loss = 0.27135413885116577, kl_loss = 4.858847618103027\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.2641749382019043, recon_loss = 0.26144030690193176, kl_loss = 2.73462176322937\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.2744951844215393, recon_loss = 0.26448720693588257, kl_loss = 10.007978439331055\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.266950398683548, recon_loss = 0.2631751298904419, kl_loss = 3.775275707244873\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.2706637382507324, recon_loss = 0.2665858566761017, kl_loss = 4.077895641326904\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.26614636182785034, recon_loss = 0.26277533173561096, kl_loss = 3.3710179328918457\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.2673017978668213, recon_loss = 0.2654355764389038, kl_loss = 1.8662182092666626\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.28300395607948303, recon_loss = 0.26343733072280884, kl_loss = 19.56662368774414\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.27412500977516174, recon_loss = 0.26682597398757935, kl_loss = 7.299034595489502\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.27380067110061646, recon_loss = 0.2651142477989197, kl_loss = 8.686431884765625\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.26693546772003174, recon_loss = 0.2605418264865875, kl_loss = 6.393650054931641\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.2688608765602112, recon_loss = 0.2633616328239441, kl_loss = 5.499238967895508\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.25881335139274597, recon_loss = 0.25573626160621643, kl_loss = 3.077085018157959\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.26011499762535095, recon_loss = 0.2582705616950989, kl_loss = 1.844425916671753\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.24801917374134064, recon_loss = 0.2465238869190216, kl_loss = 1.4952795505523682\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.23556728661060333, recon_loss = 0.23450088500976562, kl_loss = 1.0663952827453613\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.22029215097427368, recon_loss = 0.219486266374588, kl_loss = 0.8058913946151733\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.2051297426223755, recon_loss = 0.20430949330329895, kl_loss = 0.8202507495880127\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.19999384880065918, recon_loss = 0.19913722574710846, kl_loss = 0.8566261529922485\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.18686267733573914, recon_loss = 0.18592923879623413, kl_loss = 0.9334368705749512\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.17422565817832947, recon_loss = 0.17325527966022491, kl_loss = 0.9703753590583801\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.1662321537733078, recon_loss = 0.16522759199142456, kl_loss = 1.0045636892318726\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.1536797285079956, recon_loss = 0.1527235358953476, kl_loss = 0.9561982154846191\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.14564266800880432, recon_loss = 0.14469699561595917, kl_loss = 0.9456695318222046\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.13440567255020142, recon_loss = 0.13352841138839722, kl_loss = 0.8772561550140381\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.12669703364372253, recon_loss = 0.12592124938964844, kl_loss = 0.7757821679115295\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.1222204640507698, recon_loss = 0.12149088084697723, kl_loss = 0.7295861840248108\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.11392363160848618, recon_loss = 0.11326316744089127, kl_loss = 0.6604652404785156\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.10612833499908447, recon_loss = 0.10547986626625061, kl_loss = 0.6484707593917847\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.10262138396501541, recon_loss = 0.10202737152576447, kl_loss = 0.5940151214599609\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.09885280579328537, recon_loss = 0.09830950200557709, kl_loss = 0.5433042049407959\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.09394554048776627, recon_loss = 0.09348427504301071, kl_loss = 0.4612647294998169\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.0902438685297966, recon_loss = 0.08981230854988098, kl_loss = 0.4315580725669861\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.08607352524995804, recon_loss = 0.08566594868898392, kl_loss = 0.40757328271865845\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.08281981945037842, recon_loss = 0.08247090876102448, kl_loss = 0.34891098737716675\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.07850625365972519, recon_loss = 0.07818043231964111, kl_loss = 0.32582157850265503\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.07907641679048538, recon_loss = 0.07877053320407867, kl_loss = 0.3058866262435913\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.07465376704931259, recon_loss = 0.07431255280971527, kl_loss = 0.34121227264404297\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.07775591313838959, recon_loss = 0.07684947550296783, kl_loss = 0.906435489654541\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.07214796543121338, recon_loss = 0.07168138772249222, kl_loss = 0.4665788412094116\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.06660756468772888, recon_loss = 0.06631438434123993, kl_loss = 0.2931820750236511\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.06770844757556915, recon_loss = 0.06750485301017761, kl_loss = 0.203597754240036\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.06649170815944672, recon_loss = 0.06633313000202179, kl_loss = 0.15858051180839539\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.06743215769529343, recon_loss = 0.06729806214570999, kl_loss = 0.13409894704818726\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.06434440612792969, recon_loss = 0.06414864212274551, kl_loss = 0.19576144218444824\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.0627441480755806, recon_loss = 0.06259434670209885, kl_loss = 0.14980018138885498\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.05944039672613144, recon_loss = 0.05933833867311478, kl_loss = 0.10205711424350739\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.05908508598804474, recon_loss = 0.05899364501237869, kl_loss = 0.0914427638053894\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.05961001291871071, recon_loss = 0.05953329801559448, kl_loss = 0.07671639323234558\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.0564541257917881, recon_loss = 0.05636800080537796, kl_loss = 0.08612525463104248\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.0561220720410347, recon_loss = 0.05603907257318497, kl_loss = 0.08299792557954788\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.05545802786946297, recon_loss = 0.05540072172880173, kl_loss = 0.05730697140097618\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.05387255549430847, recon_loss = 0.053824979811906815, kl_loss = 0.0475764125585556\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.05188316851854324, recon_loss = 0.05183866620063782, kl_loss = 0.04450143501162529\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.0531100332736969, recon_loss = 0.05306996777653694, kl_loss = 0.040066614747047424\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.05270485207438469, recon_loss = 0.052662819623947144, kl_loss = 0.042033400386571884\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.051780518144369125, recon_loss = 0.051746826618909836, kl_loss = 0.033693037927150726\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.0522625632584095, recon_loss = 0.052229106426239014, kl_loss = 0.033455610275268555\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.05014009028673172, recon_loss = 0.05010846257209778, kl_loss = 0.031627144664525986\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.05123911052942276, recon_loss = 0.05120829492807388, kl_loss = 0.030817102640867233\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.04974094033241272, recon_loss = 0.04971085488796234, kl_loss = 0.030085841193795204\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.051025789231061935, recon_loss = 0.050996650010347366, kl_loss = 0.029137905687093735\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.04911745712161064, recon_loss = 0.04909161478281021, kl_loss = 0.025842294096946716\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.047557491809129715, recon_loss = 0.04753292724490166, kl_loss = 0.024564314633607864\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.0476696714758873, recon_loss = 0.04764449596405029, kl_loss = 0.025174111127853394\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.04830784723162651, recon_loss = 0.04828730970621109, kl_loss = 0.020536094903945923\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.046312734484672546, recon_loss = 0.04629352316260338, kl_loss = 0.01921001449227333\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.046530403196811676, recon_loss = 0.04650866985321045, kl_loss = 0.021734587848186493\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.04876688867807388, recon_loss = 0.048747703433036804, kl_loss = 0.019185736775398254\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.045458756387233734, recon_loss = 0.04543975368142128, kl_loss = 0.019004350528120995\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.04620877280831337, recon_loss = 0.0461910218000412, kl_loss = 0.017750848084688187\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.04459008201956749, recon_loss = 0.04457434266805649, kl_loss = 0.015739552676677704\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.0446581169962883, recon_loss = 0.04464257135987282, kl_loss = 0.01554728951305151\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.046479929238557816, recon_loss = 0.04646351933479309, kl_loss = 0.01640915311872959\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.045249372720718384, recon_loss = 0.04522791504859924, kl_loss = 0.02145681157708168\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.04472227022051811, recon_loss = 0.04468630254268646, kl_loss = 0.0359685942530632\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.045226894319057465, recon_loss = 0.04519882798194885, kl_loss = 0.028067830950021744\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.04501847177743912, recon_loss = 0.04498681053519249, kl_loss = 0.031661927700042725\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.04359525814652443, recon_loss = 0.0435699000954628, kl_loss = 0.025356952100992203\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.044529858976602554, recon_loss = 0.04451080411672592, kl_loss = 0.019054222851991653\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.04279831424355507, recon_loss = 0.04278150573372841, kl_loss = 0.016808921471238136\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.04400178790092468, recon_loss = 0.04398602992296219, kl_loss = 0.01575738750398159\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.04195158928632736, recon_loss = 0.0419381819665432, kl_loss = 0.013405694626271725\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.04277510941028595, recon_loss = 0.042762916535139084, kl_loss = 0.01219343300908804\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.041469771414995193, recon_loss = 0.04145577922463417, kl_loss = 0.013990637846291065\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.04176376014947891, recon_loss = 0.04175190255045891, kl_loss = 0.011859269812703133\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.041868943721055984, recon_loss = 0.041854798793792725, kl_loss = 0.014143133535981178\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.04184580594301224, recon_loss = 0.041830986738204956, kl_loss = 0.014820517040789127\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.041747868061065674, recon_loss = 0.04173649847507477, kl_loss = 0.011371031403541565\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.041862960904836655, recon_loss = 0.04184846580028534, kl_loss = 0.014494173228740692\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.040144991129636765, recon_loss = 0.04013434797525406, kl_loss = 0.0106436088681221\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.03957878425717354, recon_loss = 0.03956742584705353, kl_loss = 0.011358327232301235\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.04253271222114563, recon_loss = 0.04252238571643829, kl_loss = 0.010325281880795956\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.040425918996334076, recon_loss = 0.04041524976491928, kl_loss = 0.010667393915355206\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.04077817499637604, recon_loss = 0.04076572507619858, kl_loss = 0.012449439615011215\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.0403418205678463, recon_loss = 0.040329448878765106, kl_loss = 0.012370046228170395\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.04077809676527977, recon_loss = 0.04076589643955231, kl_loss = 0.012198615819215775\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.040518343448638916, recon_loss = 0.040505871176719666, kl_loss = 0.012471930123865604\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.04000251367688179, recon_loss = 0.03999203070998192, kl_loss = 0.010481610894203186\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.039695512503385544, recon_loss = 0.03968324884772301, kl_loss = 0.012261939235031605\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.0396539568901062, recon_loss = 0.03964323550462723, kl_loss = 0.01071952749043703\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.03904241323471069, recon_loss = 0.039031099528074265, kl_loss = 0.01131252758204937\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.03962928429245949, recon_loss = 0.03961842507123947, kl_loss = 0.010859614238142967\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.03872936591506004, recon_loss = 0.03871864452958107, kl_loss = 0.010721513070166111\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.03827715292572975, recon_loss = 0.03826804831624031, kl_loss = 0.00910310260951519\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.03831859305500984, recon_loss = 0.038308076560497284, kl_loss = 0.010516051203012466\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.039230845868587494, recon_loss = 0.039222292602062225, kl_loss = 0.008553584106266499\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.038922157138586044, recon_loss = 0.03891272842884064, kl_loss = 0.009430399164557457\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.03812594711780548, recon_loss = 0.03811677545309067, kl_loss = 0.009172213263809681\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.038008078932762146, recon_loss = 0.03799699991941452, kl_loss = 0.01107820775359869\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.0380643866956234, recon_loss = 0.038055289536714554, kl_loss = 0.009096224792301655\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.03820604458451271, recon_loss = 0.038198210299015045, kl_loss = 0.007832495495676994\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.03796514868736267, recon_loss = 0.037957288324832916, kl_loss = 0.007862006314098835\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.0382002592086792, recon_loss = 0.03819127753376961, kl_loss = 0.008980401791632175\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.03866352140903473, recon_loss = 0.03865561634302139, kl_loss = 0.007906594313681126\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.039239753037691116, recon_loss = 0.039230212569236755, kl_loss = 0.009542075917124748\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.03856329619884491, recon_loss = 0.03855236619710922, kl_loss = 0.010930575430393219\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.037269819527864456, recon_loss = 0.03725961223244667, kl_loss = 0.010205893777310848\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.03752410039305687, recon_loss = 0.03751567006111145, kl_loss = 0.008428588509559631\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.03747715428471565, recon_loss = 0.037468791007995605, kl_loss = 0.008365055546164513\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.03759901225566864, recon_loss = 0.03759053349494934, kl_loss = 0.008479640819132328\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.03772420063614845, recon_loss = 0.03771631792187691, kl_loss = 0.007883782498538494\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.03672466427087784, recon_loss = 0.03671630471944809, kl_loss = 0.008360199630260468\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.03764045238494873, recon_loss = 0.03763160854578018, kl_loss = 0.0088427122682333\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.036626044660806656, recon_loss = 0.0366177037358284, kl_loss = 0.008340958505868912\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.03696873411536217, recon_loss = 0.03696131333708763, kl_loss = 0.007419781759381294\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.03701358288526535, recon_loss = 0.03700554370880127, kl_loss = 0.008038919419050217\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.0374591164290905, recon_loss = 0.03745199367403984, kl_loss = 0.00712144561111927\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.036603040993213654, recon_loss = 0.03659464418888092, kl_loss = 0.008396994322538376\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.03703147917985916, recon_loss = 0.037022970616817474, kl_loss = 0.008507280610501766\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.03683045506477356, recon_loss = 0.03682134672999382, kl_loss = 0.009107029065489769\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.03728136420249939, recon_loss = 0.03727206587791443, kl_loss = 0.009297998622059822\n",
      "\n",
      "Epoch 151\n",
      "Step 0: loss = 0.037057310342788696, recon_loss = 0.03704793378710747, kl_loss = 0.009377785958349705\n",
      "\n",
      "Epoch 152\n",
      "Step 0: loss = 0.03773884475231171, recon_loss = 0.03772949427366257, kl_loss = 0.009352272376418114\n",
      "\n",
      "Epoch 153\n",
      "Step 0: loss = 0.036752767860889435, recon_loss = 0.03674276918172836, kl_loss = 0.009999053552746773\n",
      "\n",
      "Epoch 154\n",
      "Step 0: loss = 0.0376371294260025, recon_loss = 0.0376272089779377, kl_loss = 0.009921198710799217\n",
      "\n",
      "Epoch 155\n",
      "Step 0: loss = 0.036311812698841095, recon_loss = 0.03630082309246063, kl_loss = 0.010990733280777931\n",
      "\n",
      "Epoch 156\n",
      "Step 0: loss = 0.036137621849775314, recon_loss = 0.03612414002418518, kl_loss = 0.013481409288942814\n",
      "\n",
      "Epoch 157\n",
      "Step 0: loss = 0.0373615100979805, recon_loss = 0.03734957054257393, kl_loss = 0.011938588693737984\n",
      "\n",
      "Epoch 158\n",
      "Step 0: loss = 0.035969801247119904, recon_loss = 0.03595739230513573, kl_loss = 0.01241000834852457\n",
      "\n",
      "Epoch 159\n",
      "Step 0: loss = 0.03689144179224968, recon_loss = 0.03688158094882965, kl_loss = 0.009860065765678883\n",
      "\n",
      "Epoch 160\n",
      "Step 0: loss = 0.03653459995985031, recon_loss = 0.03652399405837059, kl_loss = 0.010606112889945507\n",
      "\n",
      "Epoch 161\n",
      "Step 0: loss = 0.03643285855650902, recon_loss = 0.03642342984676361, kl_loss = 0.009429313242435455\n",
      "\n",
      "Epoch 162\n",
      "Step 0: loss = 0.03623788058757782, recon_loss = 0.03622975945472717, kl_loss = 0.008121462538838387\n",
      "\n",
      "Epoch 163\n",
      "Step 0: loss = 0.0359405055642128, recon_loss = 0.03593312203884125, kl_loss = 0.007382872514426708\n",
      "\n",
      "Epoch 164\n",
      "Step 0: loss = 0.03580354526638985, recon_loss = 0.03579564392566681, kl_loss = 0.007900621742010117\n",
      "\n",
      "Epoch 165\n",
      "Step 0: loss = 0.0363590233027935, recon_loss = 0.03635212779045105, kl_loss = 0.006893770769238472\n",
      "\n",
      "Epoch 166\n",
      "Step 0: loss = 0.035332441329956055, recon_loss = 0.03532688692212105, kl_loss = 0.005552716553211212\n",
      "\n",
      "Epoch 167\n",
      "Step 0: loss = 0.03597415238618851, recon_loss = 0.0359681099653244, kl_loss = 0.006043615750968456\n",
      "\n",
      "Epoch 168\n",
      "Step 0: loss = 0.03664195537567139, recon_loss = 0.03663621470332146, kl_loss = 0.005738927982747555\n",
      "\n",
      "Epoch 169\n",
      "Step 0: loss = 0.03634999692440033, recon_loss = 0.03634477034211159, kl_loss = 0.005224863067269325\n",
      "\n",
      "Epoch 170\n",
      "Step 0: loss = 0.0351773276925087, recon_loss = 0.03517257422208786, kl_loss = 0.004753268323838711\n",
      "\n",
      "Epoch 171\n",
      "Step 0: loss = 0.03489834442734718, recon_loss = 0.034893520176410675, kl_loss = 0.004825618118047714\n",
      "\n",
      "Epoch 172\n",
      "Step 0: loss = 0.035092759877443314, recon_loss = 0.03508615121245384, kl_loss = 0.006607038900256157\n",
      "\n",
      "Epoch 173\n",
      "Step 0: loss = 0.0344461053609848, recon_loss = 0.03444160521030426, kl_loss = 0.004498424008488655\n",
      "\n",
      "Epoch 174\n",
      "Step 0: loss = 0.036100469529628754, recon_loss = 0.036096036434173584, kl_loss = 0.0044344766065478325\n",
      "\n",
      "Epoch 175\n",
      "Step 0: loss = 0.03535185754299164, recon_loss = 0.03534603863954544, kl_loss = 0.0058180708438158035\n",
      "\n",
      "Epoch 176\n",
      "Step 0: loss = 0.03484165668487549, recon_loss = 0.034836284816265106, kl_loss = 0.005371333099901676\n",
      "\n",
      "Epoch 177\n",
      "Step 0: loss = 0.0351300910115242, recon_loss = 0.03512394055724144, kl_loss = 0.006149438209831715\n",
      "\n",
      "Epoch 178\n",
      "Step 0: loss = 0.03530440852046013, recon_loss = 0.035298869013786316, kl_loss = 0.005538387224078178\n",
      "\n",
      "Epoch 179\n",
      "Step 0: loss = 0.03477351367473602, recon_loss = 0.03476724773645401, kl_loss = 0.006267018616199493\n",
      "\n",
      "Epoch 180\n",
      "Step 0: loss = 0.033977966755628586, recon_loss = 0.03397107869386673, kl_loss = 0.006889156065881252\n",
      "\n",
      "Epoch 181\n",
      "Step 0: loss = 0.03449207916855812, recon_loss = 0.034484460949897766, kl_loss = 0.007619898766279221\n",
      "\n",
      "Epoch 182\n",
      "Step 0: loss = 0.03485799580812454, recon_loss = 0.034851692616939545, kl_loss = 0.0063041336834430695\n",
      "\n",
      "Epoch 183\n",
      "Step 0: loss = 0.0341462641954422, recon_loss = 0.03413970768451691, kl_loss = 0.006557098589837551\n",
      "\n",
      "Epoch 184\n",
      "Step 0: loss = 0.03547985479235649, recon_loss = 0.03547287732362747, kl_loss = 0.00697791762650013\n",
      "\n",
      "Epoch 185\n",
      "Step 0: loss = 0.03387727960944176, recon_loss = 0.03387005627155304, kl_loss = 0.007221478037536144\n",
      "\n",
      "Epoch 186\n",
      "Step 0: loss = 0.034791719168424606, recon_loss = 0.034784723073244095, kl_loss = 0.0069960737600922585\n",
      "\n",
      "Epoch 187\n",
      "Step 0: loss = 0.0346873477101326, recon_loss = 0.03468027338385582, kl_loss = 0.007072632201015949\n",
      "\n",
      "Epoch 188\n",
      "Step 0: loss = 0.0340731181204319, recon_loss = 0.034066639840602875, kl_loss = 0.006476948969066143\n",
      "\n",
      "Epoch 189\n",
      "Step 0: loss = 0.03421863541007042, recon_loss = 0.034212641417980194, kl_loss = 0.005993412807583809\n",
      "\n",
      "Epoch 190\n",
      "Step 0: loss = 0.03312644734978676, recon_loss = 0.03311813250184059, kl_loss = 0.008315273560583591\n",
      "\n",
      "Epoch 191\n",
      "Step 0: loss = 0.03442736715078354, recon_loss = 0.03442027419805527, kl_loss = 0.0070927925407886505\n",
      "\n",
      "Epoch 192\n",
      "Step 0: loss = 0.03371728956699371, recon_loss = 0.03371132165193558, kl_loss = 0.005966959521174431\n",
      "\n",
      "Epoch 193\n",
      "Step 0: loss = 0.03279240056872368, recon_loss = 0.03278511390089989, kl_loss = 0.007287619635462761\n",
      "\n",
      "Epoch 194\n",
      "Step 0: loss = 0.03245328739285469, recon_loss = 0.03244638442993164, kl_loss = 0.006904801353812218\n",
      "\n",
      "Epoch 195\n",
      "Step 0: loss = 0.03264567628502846, recon_loss = 0.03263818472623825, kl_loss = 0.007492111995816231\n",
      "\n",
      "Epoch 196\n",
      "Step 0: loss = 0.03336251527070999, recon_loss = 0.03335518389940262, kl_loss = 0.007332304492592812\n",
      "\n",
      "Epoch 197\n",
      "Step 0: loss = 0.03322736173868179, recon_loss = 0.03321848809719086, kl_loss = 0.008873038925230503\n",
      "\n",
      "Epoch 198\n",
      "Step 0: loss = 0.032274894416332245, recon_loss = 0.032267600297927856, kl_loss = 0.007294997572898865\n",
      "\n",
      "Epoch 199\n",
      "Step 0: loss = 0.03190714120864868, recon_loss = 0.03190065547823906, kl_loss = 0.006484867073595524\n",
      "\n",
      "Epoch 200\n",
      "Step 0: loss = 0.03285975754261017, recon_loss = 0.032853398472070694, kl_loss = 0.006358928047120571\n",
      "\n",
      "Epoch 201\n",
      "Step 0: loss = 0.03252121061086655, recon_loss = 0.03251456469297409, kl_loss = 0.006647568196058273\n",
      "\n",
      "Epoch 202\n",
      "Step 0: loss = 0.03130105137825012, recon_loss = 0.0312935970723629, kl_loss = 0.007454383186995983\n",
      "\n",
      "Epoch 203\n",
      "Step 0: loss = 0.033051203936338425, recon_loss = 0.03304523974657059, kl_loss = 0.005963574163615704\n",
      "\n",
      "Epoch 204\n",
      "Step 0: loss = 0.03177932649850845, recon_loss = 0.03177326172590256, kl_loss = 0.006065744906663895\n",
      "\n",
      "Epoch 205\n",
      "Step 0: loss = 0.03237620368599892, recon_loss = 0.032368242740631104, kl_loss = 0.007962454110383987\n",
      "\n",
      "Epoch 206\n",
      "Step 0: loss = 0.03145848959684372, recon_loss = 0.03145146369934082, kl_loss = 0.007026582024991512\n",
      "\n",
      "Epoch 207\n",
      "Step 0: loss = 0.032006412744522095, recon_loss = 0.031999945640563965, kl_loss = 0.006465933285653591\n",
      "\n",
      "Epoch 208\n",
      "Step 0: loss = 0.03124254010617733, recon_loss = 0.031236140057444572, kl_loss = 0.006400406360626221\n",
      "\n",
      "Epoch 209\n",
      "Step 0: loss = 0.03175890073180199, recon_loss = 0.031752489507198334, kl_loss = 0.0064130015671253204\n",
      "\n",
      "Epoch 210\n",
      "Step 0: loss = 0.030830351635813713, recon_loss = 0.03082362562417984, kl_loss = 0.0067251939326524734\n",
      "\n",
      "Epoch 211\n",
      "Step 0: loss = 0.030519427731633186, recon_loss = 0.030512545257806778, kl_loss = 0.006882661022245884\n",
      "\n",
      "Epoch 212\n",
      "Step 0: loss = 0.029855763539671898, recon_loss = 0.029849037528038025, kl_loss = 0.006725529208779335\n",
      "\n",
      "Epoch 213\n",
      "Step 0: loss = 0.030814746394753456, recon_loss = 0.030808625742793083, kl_loss = 0.006121104583144188\n",
      "\n",
      "Epoch 214\n",
      "Step 0: loss = 0.030323702841997147, recon_loss = 0.030316146090626717, kl_loss = 0.00755713414400816\n",
      "\n",
      "Epoch 215\n",
      "Step 0: loss = 0.02993389032781124, recon_loss = 0.029926728457212448, kl_loss = 0.007161717861890793\n",
      "\n",
      "Epoch 216\n",
      "Step 0: loss = 0.030487699434161186, recon_loss = 0.03048016130924225, kl_loss = 0.007538387551903725\n",
      "\n",
      "Epoch 217\n",
      "Step 0: loss = 0.02885231375694275, recon_loss = 0.02884580008685589, kl_loss = 0.006513887085020542\n",
      "\n",
      "Epoch 218\n",
      "Step 0: loss = 0.02985897660255432, recon_loss = 0.02985217608511448, kl_loss = 0.00680135004222393\n",
      "\n",
      "Epoch 219\n",
      "Step 0: loss = 0.030392181128263474, recon_loss = 0.030385643243789673, kl_loss = 0.006538498215377331\n",
      "\n",
      "Epoch 220\n",
      "Step 0: loss = 0.029746877029538155, recon_loss = 0.02974010817706585, kl_loss = 0.006769225001335144\n",
      "\n",
      "Epoch 221\n",
      "Step 0: loss = 0.029548363760113716, recon_loss = 0.029541397467255592, kl_loss = 0.006966456770896912\n",
      "\n",
      "Epoch 222\n",
      "Step 0: loss = 0.02996273897588253, recon_loss = 0.029956504702568054, kl_loss = 0.006234342232346535\n",
      "\n",
      "Epoch 223\n",
      "Step 0: loss = 0.029190050438046455, recon_loss = 0.029182961210608482, kl_loss = 0.00708949938416481\n",
      "\n",
      "Epoch 224\n",
      "Step 0: loss = 0.028594963252544403, recon_loss = 0.028587814420461655, kl_loss = 0.007148943841457367\n",
      "\n",
      "Epoch 225\n",
      "Step 0: loss = 0.029601870104670525, recon_loss = 0.029594002291560173, kl_loss = 0.00786755234003067\n",
      "\n",
      "Epoch 226\n",
      "Step 0: loss = 0.027653591707348824, recon_loss = 0.027645964175462723, kl_loss = 0.007627896033227444\n",
      "\n",
      "Epoch 227\n",
      "Step 0: loss = 0.027179766446352005, recon_loss = 0.02717367745935917, kl_loss = 0.006089141592383385\n",
      "\n",
      "Epoch 228\n",
      "Step 0: loss = 0.02894957922399044, recon_loss = 0.028943106532096863, kl_loss = 0.006472276523709297\n",
      "\n",
      "Epoch 229\n",
      "Step 0: loss = 0.02743917517364025, recon_loss = 0.027433428913354874, kl_loss = 0.005745416507124901\n",
      "\n",
      "Epoch 230\n",
      "Step 0: loss = 0.028889309614896774, recon_loss = 0.028881656005978584, kl_loss = 0.007653042674064636\n",
      "\n",
      "Epoch 231\n",
      "Step 0: loss = 0.027882186695933342, recon_loss = 0.027874162420630455, kl_loss = 0.008024217560887337\n",
      "\n",
      "Epoch 232\n",
      "Step 0: loss = 0.02922154776751995, recon_loss = 0.029208093881607056, kl_loss = 0.013454475440084934\n",
      "\n",
      "Epoch 233\n",
      "Step 0: loss = 0.028585391119122505, recon_loss = 0.028575817123055458, kl_loss = 0.009573185816407204\n",
      "\n",
      "Epoch 234\n",
      "Step 0: loss = 0.028157759457826614, recon_loss = 0.028146469965577126, kl_loss = 0.01128905825316906\n",
      "\n",
      "Epoch 235\n",
      "Step 0: loss = 0.026326274499297142, recon_loss = 0.026313981041312218, kl_loss = 0.012292840518057346\n",
      "\n",
      "Epoch 236\n",
      "Step 0: loss = 0.028409263119101524, recon_loss = 0.028399815782904625, kl_loss = 0.00944693572819233\n",
      "\n",
      "Epoch 237\n",
      "Step 0: loss = 0.026351673528552055, recon_loss = 0.026343917474150658, kl_loss = 0.0077559808269143105\n",
      "\n",
      "Epoch 238\n",
      "Step 0: loss = 0.029017066583037376, recon_loss = 0.029008831828832626, kl_loss = 0.008235501125454903\n",
      "\n",
      "Epoch 239\n",
      "Step 0: loss = 0.025382308289408684, recon_loss = 0.025374623015522957, kl_loss = 0.007686051540076733\n",
      "\n",
      "Epoch 240\n",
      "Step 0: loss = 0.02544109895825386, recon_loss = 0.025433620437979698, kl_loss = 0.007478728890419006\n",
      "\n",
      "Epoch 241\n",
      "Step 0: loss = 0.024859033524990082, recon_loss = 0.024851778522133827, kl_loss = 0.007255281321704388\n",
      "\n",
      "Epoch 242\n",
      "Step 0: loss = 0.024020332843065262, recon_loss = 0.024012230336666107, kl_loss = 0.008102812804281712\n",
      "\n",
      "Epoch 243\n",
      "Step 0: loss = 0.02414768934249878, recon_loss = 0.024143213406205177, kl_loss = 0.004476307891309261\n",
      "\n",
      "Epoch 244\n",
      "Step 0: loss = 0.024004921317100525, recon_loss = 0.023999527096748352, kl_loss = 0.005394258536398411\n",
      "\n",
      "Epoch 245\n",
      "Step 0: loss = 0.025795644149184227, recon_loss = 0.025789696723222733, kl_loss = 0.005946778692305088\n",
      "\n",
      "Epoch 246\n",
      "Step 0: loss = 0.025737417861819267, recon_loss = 0.02573242038488388, kl_loss = 0.00499703548848629\n",
      "\n",
      "Epoch 247\n",
      "Step 0: loss = 0.02349071018397808, recon_loss = 0.023485887795686722, kl_loss = 0.004823308438062668\n",
      "\n",
      "Epoch 248\n",
      "Step 0: loss = 0.025322861969470978, recon_loss = 0.025317294523119926, kl_loss = 0.005568185821175575\n",
      "\n",
      "Epoch 249\n",
      "Step 0: loss = 0.02470163069665432, recon_loss = 0.024697136133909225, kl_loss = 0.004494565539062023\n",
      "\n",
      "Epoch 250\n",
      "Step 0: loss = 0.025033418089151382, recon_loss = 0.02502865344285965, kl_loss = 0.0047639403492212296\n",
      "\n",
      "Epoch 251\n",
      "Step 0: loss = 0.024328146129846573, recon_loss = 0.02432219684123993, kl_loss = 0.00594840943813324\n",
      "\n",
      "Epoch 252\n",
      "Step 0: loss = 0.024186326190829277, recon_loss = 0.024180112406611443, kl_loss = 0.006214529275894165\n",
      "\n",
      "Epoch 253\n",
      "Step 0: loss = 0.02374260686337948, recon_loss = 0.023736394941806793, kl_loss = 0.006211843341588974\n",
      "\n",
      "Epoch 254\n",
      "Step 0: loss = 0.023053066805005074, recon_loss = 0.02304767444729805, kl_loss = 0.005393225699663162\n",
      "\n",
      "Epoch 255\n",
      "Step 0: loss = 0.0231846384704113, recon_loss = 0.02317831665277481, kl_loss = 0.006321784108877182\n",
      "\n",
      "Epoch 256\n",
      "Step 0: loss = 0.023129235953092575, recon_loss = 0.02312314510345459, kl_loss = 0.0060900356620550156\n",
      "\n",
      "Epoch 257\n",
      "Step 0: loss = 0.02307804673910141, recon_loss = 0.023071186617016792, kl_loss = 0.006859361194074154\n",
      "\n",
      "Epoch 258\n",
      "Step 0: loss = 0.023835979402065277, recon_loss = 0.02382870763540268, kl_loss = 0.00727105513215065\n",
      "\n",
      "Epoch 259\n",
      "Step 0: loss = 0.022987058386206627, recon_loss = 0.022977929562330246, kl_loss = 0.009128501638770103\n",
      "\n",
      "Epoch 260\n",
      "Step 0: loss = 0.022978952154517174, recon_loss = 0.022971149533987045, kl_loss = 0.007803329266607761\n",
      "\n",
      "Epoch 261\n",
      "Step 0: loss = 0.024024343118071556, recon_loss = 0.024016477167606354, kl_loss = 0.00786542147397995\n",
      "\n",
      "Epoch 262\n",
      "Step 0: loss = 0.022178974002599716, recon_loss = 0.02216973528265953, kl_loss = 0.009237795136868954\n",
      "\n",
      "Epoch 263\n",
      "Step 0: loss = 0.022999970242381096, recon_loss = 0.022989002987742424, kl_loss = 0.010967512615025043\n",
      "\n",
      "Epoch 264\n",
      "Step 0: loss = 0.022656086832284927, recon_loss = 0.022647133097052574, kl_loss = 0.008954206481575966\n",
      "\n",
      "Epoch 265\n",
      "Step 0: loss = 0.024302242323756218, recon_loss = 0.02429051883518696, kl_loss = 0.011724370531737804\n",
      "\n",
      "Epoch 266\n",
      "Step 0: loss = 0.02289563976228237, recon_loss = 0.02288712188601494, kl_loss = 0.00851763691753149\n",
      "\n",
      "Epoch 267\n",
      "Step 0: loss = 0.021421225741505623, recon_loss = 0.021411847323179245, kl_loss = 0.009378485381603241\n",
      "\n",
      "Epoch 268\n",
      "Step 0: loss = 0.02201428823173046, recon_loss = 0.022005397826433182, kl_loss = 0.008890781551599503\n",
      "\n",
      "Epoch 269\n",
      "Step 0: loss = 0.022748472169041634, recon_loss = 0.022739790380001068, kl_loss = 0.008682520128786564\n",
      "\n",
      "Epoch 270\n",
      "Step 0: loss = 0.021669749170541763, recon_loss = 0.021661097183823586, kl_loss = 0.00865111593157053\n",
      "\n",
      "Epoch 271\n",
      "Step 0: loss = 0.02224244922399521, recon_loss = 0.022235242649912834, kl_loss = 0.007206358015537262\n",
      "\n",
      "Epoch 272\n",
      "Step 0: loss = 0.02138146571815014, recon_loss = 0.02137439139187336, kl_loss = 0.0070736221969127655\n",
      "\n",
      "Epoch 273\n",
      "Step 0: loss = 0.021533885970711708, recon_loss = 0.021526997908949852, kl_loss = 0.006888928823173046\n",
      "\n",
      "Epoch 274\n",
      "Step 0: loss = 0.022257018834352493, recon_loss = 0.022251153364777565, kl_loss = 0.005865019746124744\n",
      "\n",
      "Epoch 275\n",
      "Step 0: loss = 0.020132575184106827, recon_loss = 0.02012748457491398, kl_loss = 0.005090547725558281\n",
      "\n",
      "Epoch 276\n",
      "Step 0: loss = 0.0212230384349823, recon_loss = 0.021216290071606636, kl_loss = 0.0067485542967915535\n",
      "\n",
      "Epoch 277\n",
      "Step 0: loss = 0.020874284207820892, recon_loss = 0.020867856219410896, kl_loss = 0.006428820081055164\n",
      "\n",
      "Epoch 278\n",
      "Step 0: loss = 0.02123066782951355, recon_loss = 0.02122494950890541, kl_loss = 0.005718781612813473\n",
      "\n",
      "Epoch 279\n",
      "Step 0: loss = 0.020420605316758156, recon_loss = 0.020414896309375763, kl_loss = 0.00570924486964941\n",
      "\n",
      "Epoch 280\n",
      "Step 0: loss = 0.020593905821442604, recon_loss = 0.020587299019098282, kl_loss = 0.00660718884319067\n",
      "\n",
      "Epoch 281\n",
      "Step 0: loss = 0.020478922873735428, recon_loss = 0.020470768213272095, kl_loss = 0.008155389688909054\n",
      "\n",
      "Epoch 282\n",
      "Step 0: loss = 0.019826402887701988, recon_loss = 0.019816741347312927, kl_loss = 0.00966202188283205\n",
      "\n",
      "Epoch 283\n",
      "Step 0: loss = 0.020564671605825424, recon_loss = 0.02055572159588337, kl_loss = 0.008949772454798222\n",
      "\n",
      "Epoch 284\n",
      "Step 0: loss = 0.019959555938839912, recon_loss = 0.019952718168497086, kl_loss = 0.006837148219347\n",
      "\n",
      "Epoch 285\n",
      "Step 0: loss = 0.020041676238179207, recon_loss = 0.020033422857522964, kl_loss = 0.0082536730915308\n",
      "\n",
      "Epoch 286\n",
      "Step 0: loss = 0.01890292949974537, recon_loss = 0.018895160406827927, kl_loss = 0.007769361138343811\n",
      "\n",
      "Epoch 287\n",
      "Step 0: loss = 0.019273430109024048, recon_loss = 0.019265973940491676, kl_loss = 0.007456318475306034\n",
      "\n",
      "Epoch 288\n",
      "Step 0: loss = 0.018595663830637932, recon_loss = 0.018588081002235413, kl_loss = 0.007582573220133781\n",
      "\n",
      "Epoch 289\n",
      "Step 0: loss = 0.019174078479409218, recon_loss = 0.019167905673384666, kl_loss = 0.006172587163746357\n",
      "\n",
      "Epoch 290\n",
      "Step 0: loss = 0.019675960764288902, recon_loss = 0.019666370004415512, kl_loss = 0.00959060899913311\n",
      "\n",
      "Epoch 291\n",
      "Step 0: loss = 0.018531635403633118, recon_loss = 0.018523504957556725, kl_loss = 0.008129844442009926\n",
      "\n",
      "Epoch 292\n",
      "Step 0: loss = 0.018706046044826508, recon_loss = 0.018699176609516144, kl_loss = 0.006869712844491005\n",
      "\n",
      "Epoch 293\n",
      "Step 0: loss = 0.020087383687496185, recon_loss = 0.02007918432354927, kl_loss = 0.008199235424399376\n",
      "\n",
      "Epoch 294\n",
      "Step 0: loss = 0.019441423937678337, recon_loss = 0.019435256719589233, kl_loss = 0.006167312152683735\n",
      "\n",
      "Epoch 295\n",
      "Step 0: loss = 0.01843787357211113, recon_loss = 0.018429363146424294, kl_loss = 0.008509932085871696\n",
      "\n",
      "Epoch 296\n",
      "Step 0: loss = 0.018853504210710526, recon_loss = 0.018845941871404648, kl_loss = 0.007561878301203251\n",
      "\n",
      "Epoch 297\n",
      "Step 0: loss = 0.018627798184752464, recon_loss = 0.018617967143654823, kl_loss = 0.009830275550484657\n",
      "\n",
      "Epoch 298\n",
      "Step 0: loss = 0.019438590854406357, recon_loss = 0.019430389627814293, kl_loss = 0.008200627751648426\n",
      "\n",
      "Epoch 299\n",
      "Step 0: loss = 0.01929391361773014, recon_loss = 0.019285546615719795, kl_loss = 0.008366850204765797\n",
      "\n",
      "Epoch 300\n",
      "Step 0: loss = 0.017438260838389397, recon_loss = 0.017428014427423477, kl_loss = 0.01024667825549841\n",
      "\n",
      "Epoch 301\n",
      "Step 0: loss = 0.019510634243488312, recon_loss = 0.019502202048897743, kl_loss = 0.008431326597929\n",
      "\n",
      "Epoch 302\n",
      "Step 0: loss = 0.018268700689077377, recon_loss = 0.01826063171029091, kl_loss = 0.00806970614939928\n",
      "\n",
      "Epoch 303\n",
      "Step 0: loss = 0.01868603006005287, recon_loss = 0.018677406013011932, kl_loss = 0.008624622598290443\n",
      "\n",
      "Epoch 304\n",
      "Step 0: loss = 0.01848730631172657, recon_loss = 0.018477115780115128, kl_loss = 0.010190783068537712\n",
      "\n",
      "Epoch 305\n",
      "Step 0: loss = 0.01793617010116577, recon_loss = 0.017922163009643555, kl_loss = 0.014006608165800571\n",
      "\n",
      "Epoch 306\n",
      "Step 0: loss = 0.018284548074007034, recon_loss = 0.018272729590535164, kl_loss = 0.011818154715001583\n",
      "\n",
      "Epoch 307\n",
      "Step 0: loss = 0.018953684717416763, recon_loss = 0.018943268805742264, kl_loss = 0.010416348464787006\n",
      "\n",
      "Epoch 308\n",
      "Step 0: loss = 0.01809665560722351, recon_loss = 0.01808732934296131, kl_loss = 0.009326910600066185\n",
      "\n",
      "Epoch 309\n",
      "Step 0: loss = 0.01808498054742813, recon_loss = 0.018074341118335724, kl_loss = 0.010639811865985394\n",
      "\n",
      "Epoch 310\n",
      "Step 0: loss = 0.018165597692131996, recon_loss = 0.018156124278903008, kl_loss = 0.009473806247115135\n",
      "\n",
      "Epoch 311\n",
      "Step 0: loss = 0.017416443675756454, recon_loss = 0.017405647784471512, kl_loss = 0.01079681795090437\n",
      "\n",
      "Epoch 312\n",
      "Step 0: loss = 0.01810407266020775, recon_loss = 0.01809399388730526, kl_loss = 0.01007834728807211\n",
      "\n",
      "Epoch 313\n",
      "Step 0: loss = 0.018201565369963646, recon_loss = 0.01819220930337906, kl_loss = 0.009356790222227573\n",
      "\n",
      "Epoch 314\n",
      "Step 0: loss = 0.01782345212996006, recon_loss = 0.017813678830862045, kl_loss = 0.00977341365069151\n",
      "\n",
      "Epoch 315\n",
      "Step 0: loss = 0.018734125420451164, recon_loss = 0.018727347254753113, kl_loss = 0.006778601557016373\n",
      "\n",
      "Epoch 316\n",
      "Step 0: loss = 0.017155377194285393, recon_loss = 0.017146529629826546, kl_loss = 0.008848115801811218\n",
      "\n",
      "Epoch 317\n",
      "Step 0: loss = 0.017077172175049782, recon_loss = 0.01706940121948719, kl_loss = 0.007771424017846584\n",
      "\n",
      "Epoch 318\n",
      "Step 0: loss = 0.017140891402959824, recon_loss = 0.01713198609650135, kl_loss = 0.008904836140573025\n",
      "\n",
      "Epoch 319\n",
      "Step 0: loss = 0.016690118238329887, recon_loss = 0.01668345183134079, kl_loss = 0.00666623841971159\n",
      "\n",
      "Epoch 320\n",
      "Step 0: loss = 0.016356907784938812, recon_loss = 0.016350725665688515, kl_loss = 0.006182349286973476\n",
      "\n",
      "Epoch 321\n",
      "Step 0: loss = 0.017353950068354607, recon_loss = 0.01734563522040844, kl_loss = 0.00831405445933342\n",
      "\n",
      "Epoch 322\n",
      "Step 0: loss = 0.01705685444176197, recon_loss = 0.017049161717295647, kl_loss = 0.007693011313676834\n",
      "\n",
      "Epoch 323\n",
      "Step 0: loss = 0.01680278778076172, recon_loss = 0.016795579344034195, kl_loss = 0.007208094000816345\n",
      "\n",
      "Epoch 324\n",
      "Step 0: loss = 0.015363442711532116, recon_loss = 0.01535545103251934, kl_loss = 0.00799200776964426\n",
      "\n",
      "Epoch 325\n",
      "Step 0: loss = 0.016761016100645065, recon_loss = 0.01675240881741047, kl_loss = 0.008607447147369385\n",
      "\n",
      "Epoch 326\n",
      "Step 0: loss = 0.016682889312505722, recon_loss = 0.016674254089593887, kl_loss = 0.008634383790194988\n",
      "\n",
      "Epoch 327\n",
      "Step 0: loss = 0.016493448987603188, recon_loss = 0.016485702246427536, kl_loss = 0.007747017778456211\n",
      "\n",
      "Epoch 328\n",
      "Step 0: loss = 0.016249455511569977, recon_loss = 0.01624131202697754, kl_loss = 0.00814440194517374\n",
      "\n",
      "Epoch 329\n",
      "Step 0: loss = 0.016558922827243805, recon_loss = 0.016549132764339447, kl_loss = 0.009789989329874516\n",
      "\n",
      "Epoch 330\n",
      "Step 0: loss = 0.01717076078057289, recon_loss = 0.017161916941404343, kl_loss = 0.008843556046485901\n",
      "\n",
      "Epoch 331\n",
      "Step 0: loss = 0.016609150916337967, recon_loss = 0.01660163886845112, kl_loss = 0.007512107491493225\n",
      "\n",
      "Epoch 332\n",
      "Step 0: loss = 0.01642458885908127, recon_loss = 0.016416557133197784, kl_loss = 0.008032077923417091\n",
      "\n",
      "Epoch 333\n",
      "Step 0: loss = 0.015954146161675453, recon_loss = 0.01594688929617405, kl_loss = 0.007257693447172642\n",
      "\n",
      "Epoch 334\n",
      "Step 0: loss = 0.016448283568024635, recon_loss = 0.01643921434879303, kl_loss = 0.009069915860891342\n",
      "\n",
      "Epoch 335\n",
      "Step 0: loss = 0.015586983412504196, recon_loss = 0.015573844313621521, kl_loss = 0.013138663955032825\n",
      "\n",
      "Epoch 336\n",
      "Step 0: loss = 0.015783101320266724, recon_loss = 0.015772104263305664, kl_loss = 0.010997987352311611\n",
      "\n",
      "Epoch 337\n",
      "Step 0: loss = 0.015303447842597961, recon_loss = 0.015292903408408165, kl_loss = 0.010544246062636375\n",
      "\n",
      "Epoch 338\n",
      "Step 0: loss = 0.015989605337381363, recon_loss = 0.015979299321770668, kl_loss = 0.010305323638021946\n",
      "\n",
      "Epoch 339\n",
      "Step 0: loss = 0.016145335510373116, recon_loss = 0.016133233904838562, kl_loss = 0.012101986445486546\n",
      "\n",
      "Epoch 340\n",
      "Step 0: loss = 0.016257625073194504, recon_loss = 0.016243677586317062, kl_loss = 0.01394826639443636\n",
      "\n",
      "Epoch 341\n",
      "Step 0: loss = 0.01607700251042843, recon_loss = 0.016068749129772186, kl_loss = 0.00825279951095581\n",
      "\n",
      "Epoch 342\n",
      "Step 0: loss = 0.01646990329027176, recon_loss = 0.01645517908036709, kl_loss = 0.01472495123744011\n",
      "\n",
      "Epoch 343\n",
      "Step 0: loss = 0.016108857467770576, recon_loss = 0.01609635353088379, kl_loss = 0.012504687532782555\n",
      "\n",
      "Epoch 344\n",
      "Step 0: loss = 0.01582091674208641, recon_loss = 0.015810489654541016, kl_loss = 0.01042641419917345\n",
      "\n",
      "Epoch 345\n",
      "Step 0: loss = 0.01571359671652317, recon_loss = 0.015699226409196854, kl_loss = 0.014369932003319263\n",
      "\n",
      "Epoch 346\n",
      "Step 0: loss = 0.01689022220671177, recon_loss = 0.01688227243721485, kl_loss = 0.007950244471430779\n",
      "\n",
      "Epoch 347\n",
      "Step 0: loss = 0.016212742775678635, recon_loss = 0.016201762482523918, kl_loss = 0.010979627259075642\n",
      "\n",
      "Epoch 348\n",
      "Step 0: loss = 0.01561553031206131, recon_loss = 0.015606256201863289, kl_loss = 0.009274517185986042\n",
      "\n",
      "Epoch 349\n",
      "Step 0: loss = 0.01572679914534092, recon_loss = 0.015717944130301476, kl_loss = 0.008855857886373997\n",
      "\n",
      "Epoch 350\n",
      "Step 0: loss = 0.016396967694163322, recon_loss = 0.0163880605250597, kl_loss = 0.008907386101782322\n",
      "\n",
      "Epoch 351\n",
      "Step 0: loss = 0.01531186979264021, recon_loss = 0.015303084626793861, kl_loss = 0.008784802630543709\n",
      "\n",
      "Epoch 352\n",
      "Step 0: loss = 0.015516580082476139, recon_loss = 0.015508396551012993, kl_loss = 0.008183184079825878\n",
      "\n",
      "Epoch 353\n",
      "Step 0: loss = 0.015135081484913826, recon_loss = 0.015125345438718796, kl_loss = 0.009735731407999992\n",
      "\n",
      "Epoch 354\n",
      "Step 0: loss = 0.014803886413574219, recon_loss = 0.01479574479162693, kl_loss = 0.008142001926898956\n",
      "\n",
      "Epoch 355\n",
      "Step 0: loss = 0.01508207619190216, recon_loss = 0.015072006732225418, kl_loss = 0.010069038718938828\n",
      "\n",
      "Epoch 356\n",
      "Step 0: loss = 0.01571391336619854, recon_loss = 0.015705520287156105, kl_loss = 0.008392496034502983\n",
      "\n",
      "Epoch 357\n",
      "Step 0: loss = 0.015278326347470284, recon_loss = 0.01527077704668045, kl_loss = 0.007548898458480835\n",
      "\n",
      "Epoch 358\n",
      "Step 0: loss = 0.015606088563799858, recon_loss = 0.01559584029018879, kl_loss = 0.010248704813420773\n",
      "\n",
      "Epoch 359\n",
      "Step 0: loss = 0.016335107386112213, recon_loss = 0.016323557123541832, kl_loss = 0.011550068855285645\n",
      "\n",
      "Epoch 360\n",
      "Step 0: loss = 0.0149848572909832, recon_loss = 0.014975007623434067, kl_loss = 0.00984939094632864\n",
      "\n",
      "Epoch 361\n",
      "Step 0: loss = 0.015140006318688393, recon_loss = 0.01513049565255642, kl_loss = 0.009510471485555172\n",
      "\n",
      "Epoch 362\n",
      "Step 0: loss = 0.01452535018324852, recon_loss = 0.014515481889247894, kl_loss = 0.009868417866528034\n",
      "\n",
      "Epoch 363\n",
      "Step 0: loss = 0.015830745920538902, recon_loss = 0.01582125760614872, kl_loss = 0.009487797506153584\n",
      "\n",
      "Epoch 364\n",
      "Step 0: loss = 0.015554315410554409, recon_loss = 0.01554408110678196, kl_loss = 0.010234755463898182\n",
      "\n",
      "Epoch 365\n",
      "Step 0: loss = 0.015615472570061684, recon_loss = 0.01560693047940731, kl_loss = 0.008542544208467007\n",
      "\n",
      "Epoch 366\n",
      "Step 0: loss = 0.015140103176236153, recon_loss = 0.015130562707781792, kl_loss = 0.00954036321491003\n",
      "\n",
      "Epoch 367\n",
      "Step 0: loss = 0.014508555643260479, recon_loss = 0.014494603499770164, kl_loss = 0.013951700180768967\n",
      "\n",
      "Epoch 368\n",
      "Step 0: loss = 0.015496059320867062, recon_loss = 0.015484197065234184, kl_loss = 0.011861845850944519\n",
      "\n",
      "Epoch 369\n",
      "Step 0: loss = 0.014595221728086472, recon_loss = 0.014584260061383247, kl_loss = 0.010961553081870079\n",
      "\n",
      "Epoch 370\n",
      "Step 0: loss = 0.014749742113053799, recon_loss = 0.014738650992512703, kl_loss = 0.011090782471001148\n",
      "\n",
      "Epoch 371\n",
      "Step 0: loss = 0.01536288671195507, recon_loss = 0.015347747132182121, kl_loss = 0.015139782801270485\n",
      "\n",
      "Epoch 372\n",
      "Step 0: loss = 0.014031304977834225, recon_loss = 0.014019640162587166, kl_loss = 0.011664393357932568\n",
      "\n",
      "Epoch 373\n",
      "Step 0: loss = 0.014471190981566906, recon_loss = 0.01445995643734932, kl_loss = 0.011234268546104431\n",
      "\n",
      "Epoch 374\n",
      "Step 0: loss = 0.015015419572591782, recon_loss = 0.015004269778728485, kl_loss = 0.011149339377880096\n",
      "\n",
      "Epoch 375\n",
      "Step 0: loss = 0.01405882928520441, recon_loss = 0.014050014317035675, kl_loss = 0.008814625442028046\n",
      "\n",
      "Epoch 376\n",
      "Step 0: loss = 0.01471253577619791, recon_loss = 0.014704179018735886, kl_loss = 0.008357089012861252\n",
      "\n",
      "Epoch 377\n",
      "Step 0: loss = 0.014534220099449158, recon_loss = 0.01452556811273098, kl_loss = 0.00865230057388544\n",
      "\n",
      "Epoch 378\n",
      "Step 0: loss = 0.014212492853403091, recon_loss = 0.014206087216734886, kl_loss = 0.006405545398592949\n",
      "\n",
      "Epoch 379\n",
      "Step 0: loss = 0.014240921474993229, recon_loss = 0.014231614768505096, kl_loss = 0.009306726977229118\n",
      "\n",
      "Epoch 380\n",
      "Step 0: loss = 0.014258832670748234, recon_loss = 0.014251740649342537, kl_loss = 0.007092408835887909\n",
      "\n",
      "Epoch 381\n",
      "Step 0: loss = 0.01388096809387207, recon_loss = 0.01387074962258339, kl_loss = 0.010218682698905468\n",
      "\n",
      "Epoch 382\n",
      "Step 0: loss = 0.015347011387348175, recon_loss = 0.015338318422436714, kl_loss = 0.008692607283592224\n",
      "\n",
      "Epoch 383\n",
      "Step 0: loss = 0.013679330237209797, recon_loss = 0.013670019805431366, kl_loss = 0.00931034330278635\n",
      "\n",
      "Epoch 384\n",
      "Step 0: loss = 0.014280670322477818, recon_loss = 0.014270942658185959, kl_loss = 0.009727456606924534\n",
      "\n",
      "Epoch 385\n",
      "Step 0: loss = 0.013843356631696224, recon_loss = 0.01383507251739502, kl_loss = 0.00828403141349554\n",
      "\n",
      "Epoch 386\n",
      "Step 0: loss = 0.014505486935377121, recon_loss = 0.014494525268673897, kl_loss = 0.010961523279547691\n",
      "\n",
      "Epoch 387\n",
      "Step 0: loss = 0.01392508577555418, recon_loss = 0.01391615904867649, kl_loss = 0.008926428854465485\n",
      "\n",
      "Epoch 388\n",
      "Step 0: loss = 0.014272780157625675, recon_loss = 0.014262767508625984, kl_loss = 0.010013021528720856\n",
      "\n",
      "Epoch 389\n",
      "Step 0: loss = 0.013550825417041779, recon_loss = 0.013538120314478874, kl_loss = 0.012705174274742603\n",
      "\n",
      "Epoch 390\n",
      "Step 0: loss = 0.01407713070511818, recon_loss = 0.014066901057958603, kl_loss = 0.010229852050542831\n",
      "\n",
      "Epoch 391\n",
      "Step 0: loss = 0.013885141350328922, recon_loss = 0.013872114941477776, kl_loss = 0.013026025146245956\n",
      "\n",
      "Epoch 392\n",
      "Step 0: loss = 0.014417286030948162, recon_loss = 0.014402985572814941, kl_loss = 0.014300264418125153\n",
      "\n",
      "Epoch 393\n",
      "Step 0: loss = 0.014556022360920906, recon_loss = 0.014544384554028511, kl_loss = 0.011637619696557522\n",
      "\n",
      "Epoch 394\n",
      "Step 0: loss = 0.013969996012747288, recon_loss = 0.013958608731627464, kl_loss = 0.011387231759727001\n",
      "\n",
      "Epoch 395\n",
      "Step 0: loss = 0.013934202492237091, recon_loss = 0.013924013823270798, kl_loss = 0.010188774205744267\n",
      "\n",
      "Epoch 396\n",
      "Step 0: loss = 0.014534072019159794, recon_loss = 0.014521250501275063, kl_loss = 0.012821962125599384\n",
      "\n",
      "Epoch 397\n",
      "Step 0: loss = 0.013840646483004093, recon_loss = 0.01383253000676632, kl_loss = 0.008116519078612328\n",
      "\n",
      "Epoch 398\n",
      "Step 0: loss = 0.014264995232224464, recon_loss = 0.014255579560995102, kl_loss = 0.009415442124009132\n",
      "\n",
      "Epoch 399\n",
      "Step 0: loss = 0.013241387903690338, recon_loss = 0.01323322206735611, kl_loss = 0.008165409788489342\n",
      "\n",
      "Epoch 400\n",
      "Step 0: loss = 0.014190852642059326, recon_loss = 0.014180352911353111, kl_loss = 0.010499547235667706\n",
      "\n",
      "Epoch 401\n",
      "Step 0: loss = 0.014095527119934559, recon_loss = 0.0140862837433815, kl_loss = 0.009243626147508621\n",
      "\n",
      "Epoch 402\n",
      "Step 0: loss = 0.013963215984404087, recon_loss = 0.013955416157841682, kl_loss = 0.007800290361046791\n",
      "\n",
      "Epoch 403\n",
      "Step 0: loss = 0.01381734013557434, recon_loss = 0.013810429722070694, kl_loss = 0.006910345517098904\n",
      "\n",
      "Epoch 404\n",
      "Step 0: loss = 0.014076673425734043, recon_loss = 0.014068445190787315, kl_loss = 0.008228661492466927\n",
      "\n",
      "Epoch 405\n",
      "Step 0: loss = 0.012608850374817848, recon_loss = 0.012599712237715721, kl_loss = 0.009137687273323536\n",
      "\n",
      "Epoch 406\n",
      "Step 0: loss = 0.013271830044686794, recon_loss = 0.013265695422887802, kl_loss = 0.00613508652895689\n",
      "\n",
      "Epoch 407\n",
      "Step 0: loss = 0.013347936794161797, recon_loss = 0.013338826596736908, kl_loss = 0.009109782986342907\n",
      "\n",
      "Epoch 408\n",
      "Step 0: loss = 0.013740572147071362, recon_loss = 0.013732099905610085, kl_loss = 0.008472169749438763\n",
      "\n",
      "Epoch 409\n",
      "Step 0: loss = 0.013323736377060413, recon_loss = 0.013315688818693161, kl_loss = 0.008047884330153465\n",
      "\n",
      "Epoch 410\n",
      "Step 0: loss = 0.01302375365048647, recon_loss = 0.013015268370509148, kl_loss = 0.008484936319291592\n",
      "\n",
      "Epoch 411\n",
      "Step 0: loss = 0.013234305195510387, recon_loss = 0.013224614784121513, kl_loss = 0.009690260514616966\n",
      "\n",
      "Epoch 412\n",
      "Step 0: loss = 0.013099579140543938, recon_loss = 0.013085655868053436, kl_loss = 0.013923658058047295\n",
      "\n",
      "Epoch 413\n",
      "Step 0: loss = 0.013923771679401398, recon_loss = 0.013910377398133278, kl_loss = 0.01339468453079462\n",
      "\n",
      "Epoch 414\n",
      "Step 0: loss = 0.01361830998212099, recon_loss = 0.013605929911136627, kl_loss = 0.012379661202430725\n",
      "\n",
      "Epoch 415\n",
      "Step 0: loss = 0.012995166704058647, recon_loss = 0.012984173372387886, kl_loss = 0.01099323108792305\n",
      "\n",
      "Epoch 416\n",
      "Step 0: loss = 0.013197977095842361, recon_loss = 0.01318802684545517, kl_loss = 0.009950694628059864\n",
      "\n",
      "Epoch 417\n",
      "Step 0: loss = 0.013738136738538742, recon_loss = 0.013724766671657562, kl_loss = 0.013370531611144543\n",
      "\n",
      "Epoch 418\n",
      "Step 0: loss = 0.013086922466754913, recon_loss = 0.01307661458849907, kl_loss = 0.010307474061846733\n",
      "\n",
      "Epoch 419\n",
      "Step 0: loss = 0.012867437675595284, recon_loss = 0.012856930494308472, kl_loss = 0.010506791062653065\n",
      "\n",
      "Epoch 420\n",
      "Step 0: loss = 0.013493561185896397, recon_loss = 0.013483380898833275, kl_loss = 0.01018067542463541\n",
      "\n",
      "Epoch 421\n",
      "Step 0: loss = 0.013304130174219608, recon_loss = 0.013293104246258736, kl_loss = 0.011026146821677685\n",
      "\n",
      "Epoch 422\n",
      "Step 0: loss = 0.013003509491682053, recon_loss = 0.012994971126317978, kl_loss = 0.008538475260138512\n",
      "\n",
      "Epoch 423\n",
      "Step 0: loss = 0.01315174251794815, recon_loss = 0.013142570853233337, kl_loss = 0.009171846322715282\n",
      "\n",
      "Epoch 424\n",
      "Step 0: loss = 0.013106132857501507, recon_loss = 0.013097289949655533, kl_loss = 0.008842973969876766\n",
      "\n",
      "Epoch 425\n",
      "Step 0: loss = 0.013158265501260757, recon_loss = 0.013149799779057503, kl_loss = 0.00846558716148138\n",
      "\n",
      "Epoch 426\n",
      "Step 0: loss = 0.013194569386541843, recon_loss = 0.013182355090975761, kl_loss = 0.012214182876050472\n",
      "\n",
      "Epoch 427\n",
      "Step 0: loss = 0.013233721256256104, recon_loss = 0.013224974274635315, kl_loss = 0.008747421205043793\n",
      "\n",
      "Epoch 428\n",
      "Step 0: loss = 0.013132485561072826, recon_loss = 0.01312245987355709, kl_loss = 0.010025781579315662\n",
      "\n",
      "Epoch 429\n",
      "Step 0: loss = 0.012998916208744049, recon_loss = 0.012988116592168808, kl_loss = 0.010800071991980076\n",
      "\n",
      "Epoch 430\n",
      "Step 0: loss = 0.013451794162392616, recon_loss = 0.013439016416668892, kl_loss = 0.01277805957943201\n",
      "\n",
      "Epoch 431\n",
      "Step 0: loss = 0.013239141553640366, recon_loss = 0.013228662312030792, kl_loss = 0.010478866286575794\n",
      "\n",
      "Epoch 432\n",
      "Step 0: loss = 0.012788144871592522, recon_loss = 0.01277850940823555, kl_loss = 0.009635500609874725\n",
      "\n",
      "Epoch 433\n",
      "Step 0: loss = 0.012515867128968239, recon_loss = 0.012507565319538116, kl_loss = 0.008301622234284878\n",
      "\n",
      "Epoch 434\n",
      "Step 0: loss = 0.012875662185251713, recon_loss = 0.01286519318819046, kl_loss = 0.010468823835253716\n",
      "\n",
      "Epoch 435\n",
      "Step 0: loss = 0.013413162901997566, recon_loss = 0.013403167948126793, kl_loss = 0.00999534223228693\n",
      "\n",
      "Epoch 436\n",
      "Step 0: loss = 0.012067245319485664, recon_loss = 0.012059759348630905, kl_loss = 0.007486247457563877\n",
      "\n",
      "Epoch 437\n",
      "Step 0: loss = 0.012780474498867989, recon_loss = 0.012772362679243088, kl_loss = 0.008111444301903248\n",
      "\n",
      "Epoch 438\n",
      "Step 0: loss = 0.012523855082690716, recon_loss = 0.01251787506043911, kl_loss = 0.005980024114251137\n",
      "\n",
      "Epoch 439\n",
      "Step 0: loss = 0.01219743862748146, recon_loss = 0.012190442532300949, kl_loss = 0.006995784118771553\n",
      "\n",
      "Epoch 440\n",
      "Step 0: loss = 0.012153107672929764, recon_loss = 0.012147190049290657, kl_loss = 0.005917889066040516\n",
      "\n",
      "Epoch 441\n",
      "Step 0: loss = 0.012865706346929073, recon_loss = 0.012857850641012192, kl_loss = 0.007855528965592384\n",
      "\n",
      "Epoch 442\n",
      "Step 0: loss = 0.012206199578940868, recon_loss = 0.012201016768813133, kl_loss = 0.005183025263249874\n",
      "\n",
      "Epoch 443\n",
      "Step 0: loss = 0.012803863734006882, recon_loss = 0.012793703004717827, kl_loss = 0.010160678997635841\n",
      "\n",
      "Epoch 444\n",
      "Step 0: loss = 0.012374717742204666, recon_loss = 0.012367967516183853, kl_loss = 0.006749965250492096\n",
      "\n",
      "Epoch 445\n",
      "Step 0: loss = 0.012668532319366932, recon_loss = 0.012660864740610123, kl_loss = 0.007667566649615765\n",
      "\n",
      "Epoch 446\n",
      "Step 0: loss = 0.013276459649205208, recon_loss = 0.013268984854221344, kl_loss = 0.007475094869732857\n",
      "\n",
      "Epoch 447\n",
      "Step 0: loss = 0.012679126113653183, recon_loss = 0.012672163546085358, kl_loss = 0.006962315179407597\n",
      "\n",
      "Epoch 448\n",
      "Step 0: loss = 0.012515549547970295, recon_loss = 0.01250687800347805, kl_loss = 0.008671839721500874\n",
      "\n",
      "Epoch 449\n",
      "Step 0: loss = 0.012965520843863487, recon_loss = 0.012953819707036018, kl_loss = 0.011700911447405815\n",
      "\n",
      "Epoch 450\n",
      "Step 0: loss = 0.012430530041456223, recon_loss = 0.012419220060110092, kl_loss = 0.01130971685051918\n",
      "\n",
      "Epoch 451\n",
      "Step 0: loss = 0.012675940990447998, recon_loss = 0.012665888294577599, kl_loss = 0.01005230937153101\n",
      "\n",
      "Epoch 452\n",
      "Step 0: loss = 0.012549784034490585, recon_loss = 0.012539364397525787, kl_loss = 0.010419276542961597\n",
      "\n",
      "Epoch 453\n",
      "Step 0: loss = 0.012303871102631092, recon_loss = 0.012294825166463852, kl_loss = 0.009046152234077454\n",
      "\n",
      "Epoch 454\n",
      "Step 0: loss = 0.01245117373764515, recon_loss = 0.012438679113984108, kl_loss = 0.012494541704654694\n",
      "\n",
      "Epoch 455\n",
      "Step 0: loss = 0.013270213268697262, recon_loss = 0.013258380815386772, kl_loss = 0.011832351796329021\n",
      "\n",
      "Epoch 456\n",
      "Step 0: loss = 0.012802598997950554, recon_loss = 0.012791085988283157, kl_loss = 0.011513256467878819\n",
      "\n",
      "Epoch 457\n",
      "Step 0: loss = 0.012932302430272102, recon_loss = 0.012921996414661407, kl_loss = 0.010305940173566341\n",
      "\n",
      "Epoch 458\n",
      "Step 0: loss = 0.012259461916983128, recon_loss = 0.012248309329152107, kl_loss = 0.01115250401198864\n",
      "\n",
      "Epoch 459\n",
      "Step 0: loss = 0.012384282425045967, recon_loss = 0.012374931946396828, kl_loss = 0.009350026026368141\n",
      "\n",
      "Epoch 460\n",
      "Step 0: loss = 0.012696541845798492, recon_loss = 0.012687180191278458, kl_loss = 0.00936194509267807\n",
      "\n",
      "Epoch 461\n",
      "Step 0: loss = 0.012273081578314304, recon_loss = 0.0122635867446661, kl_loss = 0.009495027363300323\n",
      "\n",
      "Epoch 462\n",
      "Step 0: loss = 0.01209618616849184, recon_loss = 0.012087756767868996, kl_loss = 0.008429608307778835\n",
      "\n",
      "Epoch 463\n",
      "Step 0: loss = 0.012345635332167149, recon_loss = 0.012336358428001404, kl_loss = 0.009277163073420525\n",
      "\n",
      "Epoch 464\n",
      "Step 0: loss = 0.012040301226079464, recon_loss = 0.012030595913529396, kl_loss = 0.009705442935228348\n",
      "\n",
      "Epoch 465\n",
      "Step 0: loss = 0.012396865524351597, recon_loss = 0.012389713898301125, kl_loss = 0.007151627913117409\n",
      "\n",
      "Epoch 466\n",
      "Step 0: loss = 0.012296070344746113, recon_loss = 0.012287633493542671, kl_loss = 0.008437266573309898\n",
      "\n",
      "Epoch 467\n",
      "Step 0: loss = 0.0128649165853858, recon_loss = 0.012857010588049889, kl_loss = 0.007905570790171623\n",
      "\n",
      "Epoch 468\n",
      "Step 0: loss = 0.012383818626403809, recon_loss = 0.012375663965940475, kl_loss = 0.008154604583978653\n",
      "\n",
      "Epoch 469\n",
      "Step 0: loss = 0.01227883156388998, recon_loss = 0.012270445004105568, kl_loss = 0.008386703208088875\n",
      "\n",
      "Epoch 470\n",
      "Step 0: loss = 0.012174785137176514, recon_loss = 0.012165259569883347, kl_loss = 0.009526032023131847\n",
      "\n",
      "Epoch 471\n",
      "Step 0: loss = 0.011563471518456936, recon_loss = 0.01155707985162735, kl_loss = 0.006391783244907856\n",
      "\n",
      "Epoch 472\n",
      "Step 0: loss = 0.012210332788527012, recon_loss = 0.012201441451907158, kl_loss = 0.00889125932008028\n",
      "\n",
      "Epoch 473\n",
      "Step 0: loss = 0.012276548892259598, recon_loss = 0.012266207486391068, kl_loss = 0.010341672226786613\n",
      "\n",
      "Epoch 474\n",
      "Step 0: loss = 0.012301255017518997, recon_loss = 0.012291789054870605, kl_loss = 0.009466376155614853\n",
      "\n",
      "Epoch 475\n",
      "Step 0: loss = 0.0125475088134408, recon_loss = 0.012537777423858643, kl_loss = 0.009731000289320946\n",
      "\n",
      "Epoch 476\n",
      "Step 0: loss = 0.012118824757635593, recon_loss = 0.012110045179724693, kl_loss = 0.008779616095125675\n",
      "\n",
      "Epoch 477\n",
      "Step 0: loss = 0.012368278577923775, recon_loss = 0.012359881773591042, kl_loss = 0.00839697290211916\n",
      "\n",
      "Epoch 478\n",
      "Step 0: loss = 0.012472614645957947, recon_loss = 0.012464810162782669, kl_loss = 0.0078044068068265915\n",
      "\n",
      "Epoch 479\n",
      "Step 0: loss = 0.012005946598947048, recon_loss = 0.011997101828455925, kl_loss = 0.008844785392284393\n",
      "\n",
      "Epoch 480\n",
      "Step 0: loss = 0.012111933901906013, recon_loss = 0.012103047221899033, kl_loss = 0.008886783383786678\n",
      "\n",
      "Epoch 481\n",
      "Step 0: loss = 0.012234468013048172, recon_loss = 0.012224191799759865, kl_loss = 0.01027610246092081\n",
      "\n",
      "Epoch 482\n",
      "Step 0: loss = 0.012258012779057026, recon_loss = 0.01224697194993496, kl_loss = 0.011041202582418919\n",
      "\n",
      "Epoch 483\n",
      "Step 0: loss = 0.01185902114957571, recon_loss = 0.011848142370581627, kl_loss = 0.010878611356019974\n",
      "\n",
      "Epoch 484\n",
      "Step 0: loss = 0.012130646966397762, recon_loss = 0.012120701372623444, kl_loss = 0.009945979341864586\n",
      "\n",
      "Epoch 485\n",
      "Step 0: loss = 0.012132100760936737, recon_loss = 0.012121709063649178, kl_loss = 0.010391416028141975\n",
      "\n",
      "Epoch 486\n",
      "Step 0: loss = 0.01203009020537138, recon_loss = 0.0120200514793396, kl_loss = 0.010038577020168304\n",
      "\n",
      "Epoch 487\n",
      "Step 0: loss = 0.012261208146810532, recon_loss = 0.012252531945705414, kl_loss = 0.008676034398376942\n",
      "\n",
      "Epoch 488\n",
      "Step 0: loss = 0.011500884778797626, recon_loss = 0.011489182710647583, kl_loss = 0.011702105402946472\n",
      "\n",
      "Epoch 489\n",
      "Step 0: loss = 0.012114998884499073, recon_loss = 0.012106586247682571, kl_loss = 0.008412583731114864\n",
      "\n",
      "Epoch 490\n",
      "Step 0: loss = 0.012324988842010498, recon_loss = 0.012317713350057602, kl_loss = 0.007275762967765331\n",
      "\n",
      "Epoch 491\n",
      "Step 0: loss = 0.011738904751837254, recon_loss = 0.011731423437595367, kl_loss = 0.007480977103114128\n",
      "\n",
      "Epoch 492\n",
      "Step 0: loss = 0.011601080186665058, recon_loss = 0.0115949846804142, kl_loss = 0.006095320917665958\n",
      "\n",
      "Epoch 493\n",
      "Step 0: loss = 0.011698654852807522, recon_loss = 0.011692557483911514, kl_loss = 0.006097317673265934\n",
      "\n",
      "Epoch 494\n",
      "Step 0: loss = 0.01184825785458088, recon_loss = 0.011841613799333572, kl_loss = 0.006644324399530888\n",
      "\n",
      "Epoch 495\n",
      "Step 0: loss = 0.011499735526740551, recon_loss = 0.011493107303977013, kl_loss = 0.006627887487411499\n",
      "\n",
      "Epoch 496\n",
      "Step 0: loss = 0.011823929846286774, recon_loss = 0.011818315833806992, kl_loss = 0.005613931454718113\n",
      "\n",
      "Epoch 497\n",
      "Step 0: loss = 0.011721507646143436, recon_loss = 0.011715095490217209, kl_loss = 0.006411758251488209\n",
      "\n",
      "Epoch 498\n",
      "Step 0: loss = 0.01149068120867014, recon_loss = 0.011483849957585335, kl_loss = 0.0068316031247377396\n",
      "\n",
      "Epoch 499\n",
      "Step 0: loss = 0.011740362271666527, recon_loss = 0.011733854189515114, kl_loss = 0.006507654674351215\n",
      "\n",
      "Epoch 500\n",
      "Step 0: loss = 0.011549380607903004, recon_loss = 0.011541837826371193, kl_loss = 0.007542602717876434\n",
      "\n",
      "Epoch 501\n",
      "Step 0: loss = 0.011631584726274014, recon_loss = 0.011624259874224663, kl_loss = 0.007325280457735062\n",
      "\n",
      "Epoch 502\n",
      "Step 0: loss = 0.012123906053602695, recon_loss = 0.012118449434638023, kl_loss = 0.005456865765154362\n",
      "\n",
      "Epoch 503\n",
      "Step 0: loss = 0.011529411189258099, recon_loss = 0.0115229282528162, kl_loss = 0.006483274511992931\n",
      "\n",
      "Epoch 504\n",
      "Step 0: loss = 0.012146185152232647, recon_loss = 0.012136530131101608, kl_loss = 0.00965517945587635\n",
      "\n",
      "Epoch 505\n",
      "Step 0: loss = 0.011935929767787457, recon_loss = 0.011927606537938118, kl_loss = 0.00832295697182417\n",
      "\n",
      "Epoch 506\n",
      "Step 0: loss = 0.01166618149727583, recon_loss = 0.011657100170850754, kl_loss = 0.009081761352717876\n",
      "\n",
      "Epoch 507\n",
      "Step 0: loss = 0.011303388513624668, recon_loss = 0.011294180527329445, kl_loss = 0.009208209812641144\n",
      "\n",
      "Epoch 508\n",
      "Step 0: loss = 0.011281442828476429, recon_loss = 0.011271849274635315, kl_loss = 0.009593215771019459\n",
      "\n",
      "Epoch 509\n",
      "Step 0: loss = 0.011844072490930557, recon_loss = 0.011835046112537384, kl_loss = 0.009026529267430305\n",
      "\n",
      "Epoch 510\n",
      "Step 0: loss = 0.012009890750050545, recon_loss = 0.011999957263469696, kl_loss = 0.009933791123330593\n",
      "\n",
      "Epoch 511\n",
      "Step 0: loss = 0.011953706853091717, recon_loss = 0.011943582445383072, kl_loss = 0.010124540887773037\n",
      "\n",
      "Epoch 512\n",
      "Step 0: loss = 0.011590328998863697, recon_loss = 0.01157875545322895, kl_loss = 0.011573105119168758\n",
      "\n",
      "Epoch 513\n",
      "Step 0: loss = 0.011515209451317787, recon_loss = 0.011504899710416794, kl_loss = 0.010309386998414993\n",
      "\n",
      "Epoch 514\n",
      "Step 0: loss = 0.011863715946674347, recon_loss = 0.01185501366853714, kl_loss = 0.008702628314495087\n",
      "\n",
      "Epoch 515\n",
      "Step 0: loss = 0.012055905535817146, recon_loss = 0.012048069387674332, kl_loss = 0.007836605422198772\n",
      "\n",
      "Epoch 516\n",
      "Step 0: loss = 0.011718138121068478, recon_loss = 0.011709535494446754, kl_loss = 0.008602824993431568\n",
      "\n",
      "Epoch 517\n",
      "Step 0: loss = 0.01184375211596489, recon_loss = 0.01183340884745121, kl_loss = 0.010343536734580994\n",
      "\n",
      "Epoch 518\n",
      "Step 0: loss = 0.012333117425441742, recon_loss = 0.012322358787059784, kl_loss = 0.010759077034890652\n",
      "\n",
      "Epoch 519\n",
      "Step 0: loss = 0.011999950744211674, recon_loss = 0.011986872181296349, kl_loss = 0.013078441843390465\n",
      "\n",
      "Epoch 520\n",
      "Step 0: loss = 0.011610370129346848, recon_loss = 0.01159825548529625, kl_loss = 0.012114235199987888\n",
      "\n",
      "Epoch 521\n",
      "Step 0: loss = 0.011509002186357975, recon_loss = 0.011497488245368004, kl_loss = 0.011513913050293922\n",
      "\n",
      "Epoch 522\n",
      "Step 0: loss = 0.011433389037847519, recon_loss = 0.011422352865338326, kl_loss = 0.011036133393645287\n",
      "\n",
      "Epoch 523\n",
      "Step 0: loss = 0.011288606561720371, recon_loss = 0.011275531724095345, kl_loss = 0.01307469978928566\n",
      "\n",
      "Epoch 524\n",
      "Step 0: loss = 0.011946525424718857, recon_loss = 0.011937052011489868, kl_loss = 0.009473638609051704\n",
      "\n",
      "Epoch 525\n",
      "Step 0: loss = 0.011737089604139328, recon_loss = 0.011728700250387192, kl_loss = 0.008389700204133987\n",
      "\n",
      "Epoch 526\n",
      "Step 0: loss = 0.011668479070067406, recon_loss = 0.011660557240247726, kl_loss = 0.00792164821177721\n",
      "\n",
      "Epoch 527\n",
      "Step 0: loss = 0.011956942267715931, recon_loss = 0.011947952210903168, kl_loss = 0.008989986032247543\n",
      "\n",
      "Epoch 528\n",
      "Step 0: loss = 0.011316884309053421, recon_loss = 0.011310597881674767, kl_loss = 0.006286825984716415\n",
      "\n",
      "Epoch 529\n",
      "Step 0: loss = 0.011314274743199348, recon_loss = 0.011308850720524788, kl_loss = 0.005423972383141518\n",
      "\n",
      "Epoch 530\n",
      "Step 0: loss = 0.011351337656378746, recon_loss = 0.011345859616994858, kl_loss = 0.005478120408952236\n",
      "\n",
      "Epoch 531\n",
      "Step 0: loss = 0.01115422323346138, recon_loss = 0.01114710047841072, kl_loss = 0.007122788578271866\n",
      "\n",
      "Epoch 532\n",
      "Step 0: loss = 0.01104242354631424, recon_loss = 0.011036360636353493, kl_loss = 0.006062909960746765\n",
      "\n",
      "Epoch 533\n",
      "Step 0: loss = 0.010905279777944088, recon_loss = 0.010900495573878288, kl_loss = 0.004783915355801582\n",
      "\n",
      "Epoch 534\n",
      "Step 0: loss = 0.01116591040045023, recon_loss = 0.011161211878061295, kl_loss = 0.004698863252997398\n",
      "\n",
      "Epoch 535\n",
      "Step 0: loss = 0.011131374165415764, recon_loss = 0.011126905679702759, kl_loss = 0.004468790255486965\n",
      "\n",
      "Epoch 536\n",
      "Step 0: loss = 0.01143821980804205, recon_loss = 0.011433126404881477, kl_loss = 0.005093446001410484\n",
      "\n",
      "Epoch 537\n",
      "Step 0: loss = 0.011171751655638218, recon_loss = 0.011166106909513474, kl_loss = 0.005644305609166622\n",
      "\n",
      "Epoch 538\n",
      "Step 0: loss = 0.011319708079099655, recon_loss = 0.011313997209072113, kl_loss = 0.005710438825190067\n",
      "\n",
      "Epoch 539\n",
      "Step 0: loss = 0.011156660504639149, recon_loss = 0.011150529608130455, kl_loss = 0.006130722351372242\n",
      "\n",
      "Epoch 540\n",
      "Step 0: loss = 0.01157552283257246, recon_loss = 0.011566365137696266, kl_loss = 0.009157613851130009\n",
      "\n",
      "Epoch 541\n",
      "Step 0: loss = 0.011052246205508709, recon_loss = 0.011041266843676567, kl_loss = 0.010978932492434978\n",
      "\n",
      "Epoch 542\n",
      "Step 0: loss = 0.011685386300086975, recon_loss = 0.011673612520098686, kl_loss = 0.011773336678743362\n",
      "\n",
      "Epoch 543\n",
      "Step 0: loss = 0.011094910092651844, recon_loss = 0.01108236238360405, kl_loss = 0.012547680176794529\n",
      "\n",
      "Epoch 544\n",
      "Step 0: loss = 0.011640079319477081, recon_loss = 0.011627169325947762, kl_loss = 0.012909665703773499\n",
      "\n",
      "Epoch 545\n",
      "Step 0: loss = 0.011627130210399628, recon_loss = 0.011614879593253136, kl_loss = 0.012250176630914211\n",
      "\n",
      "Epoch 546\n",
      "Step 0: loss = 0.011671051383018494, recon_loss = 0.011658450588583946, kl_loss = 0.012601171620190144\n",
      "\n",
      "Epoch 547\n",
      "Step 0: loss = 0.01162527222186327, recon_loss = 0.011612646281719208, kl_loss = 0.012625573202967644\n",
      "\n",
      "Epoch 548\n",
      "Step 0: loss = 0.011808731593191624, recon_loss = 0.011797891929745674, kl_loss = 0.010839727707207203\n",
      "\n",
      "Epoch 549\n",
      "Step 0: loss = 0.011340026743710041, recon_loss = 0.011328913271427155, kl_loss = 0.011113674379885197\n",
      "\n",
      "Epoch 550\n",
      "Step 0: loss = 0.011508766561746597, recon_loss = 0.011500488966703415, kl_loss = 0.00827725138515234\n",
      "\n",
      "Epoch 551\n",
      "Step 0: loss = 0.011885061860084534, recon_loss = 0.011876117438077927, kl_loss = 0.00894408579915762\n",
      "\n",
      "Epoch 552\n",
      "Step 0: loss = 0.011779122985899448, recon_loss = 0.011772429570555687, kl_loss = 0.006693708710372448\n",
      "\n",
      "Epoch 553\n",
      "Step 0: loss = 0.01131241675466299, recon_loss = 0.011302545666694641, kl_loss = 0.009871263056993484\n",
      "\n",
      "Epoch 554\n",
      "Step 0: loss = 0.011160836555063725, recon_loss = 0.011152422055602074, kl_loss = 0.008414097130298615\n",
      "\n",
      "Epoch 555\n",
      "Step 0: loss = 0.010983595624566078, recon_loss = 0.010974438861012459, kl_loss = 0.009156392887234688\n",
      "\n",
      "Epoch 556\n",
      "Step 0: loss = 0.011310746893286705, recon_loss = 0.011303646489977837, kl_loss = 0.007100232876837254\n",
      "\n",
      "Epoch 557\n",
      "Step 0: loss = 0.011046633124351501, recon_loss = 0.01104068011045456, kl_loss = 0.0059528229758143425\n",
      "\n",
      "Epoch 558\n",
      "Step 0: loss = 0.011100925505161285, recon_loss = 0.011094894260168076, kl_loss = 0.006031567230820656\n",
      "\n",
      "Epoch 559\n",
      "Step 0: loss = 0.011118358932435513, recon_loss = 0.011113347485661507, kl_loss = 0.005011902190744877\n",
      "\n",
      "Epoch 560\n",
      "Step 0: loss = 0.011270763352513313, recon_loss = 0.011265801265835762, kl_loss = 0.004961738362908363\n",
      "\n",
      "Epoch 561\n",
      "Step 0: loss = 0.011152785271406174, recon_loss = 0.011146780103445053, kl_loss = 0.006005526520311832\n",
      "\n",
      "Epoch 562\n",
      "Step 0: loss = 0.011020059697329998, recon_loss = 0.011012904345989227, kl_loss = 0.007155285216867924\n",
      "\n",
      "Epoch 563\n",
      "Step 0: loss = 0.011084468103945255, recon_loss = 0.011078516021370888, kl_loss = 0.005952469073235989\n",
      "\n",
      "Epoch 564\n",
      "Step 0: loss = 0.010933355428278446, recon_loss = 0.010927662253379822, kl_loss = 0.005693571642041206\n",
      "\n",
      "Epoch 565\n",
      "Step 0: loss = 0.010702232830226421, recon_loss = 0.010695599019527435, kl_loss = 0.006633451208472252\n",
      "\n",
      "Epoch 566\n",
      "Step 0: loss = 0.0111875394359231, recon_loss = 0.011180296540260315, kl_loss = 0.007242996245622635\n",
      "\n",
      "Epoch 567\n",
      "Step 0: loss = 0.01097854133695364, recon_loss = 0.010969307273626328, kl_loss = 0.009233670309185982\n",
      "\n",
      "Epoch 568\n",
      "Step 0: loss = 0.01146943960338831, recon_loss = 0.011461645364761353, kl_loss = 0.00779450498521328\n",
      "\n",
      "Epoch 569\n",
      "Step 0: loss = 0.010995730757713318, recon_loss = 0.010986996814608574, kl_loss = 0.00873355008661747\n",
      "\n",
      "Epoch 570\n",
      "Step 0: loss = 0.011198272928595543, recon_loss = 0.011189792305231094, kl_loss = 0.008480280637741089\n",
      "\n",
      "Epoch 571\n",
      "Step 0: loss = 0.01100210566073656, recon_loss = 0.010991105809807777, kl_loss = 0.010999966412782669\n",
      "\n",
      "Epoch 572\n",
      "Step 0: loss = 0.011029927991330624, recon_loss = 0.011021863669157028, kl_loss = 0.008064678870141506\n",
      "\n",
      "Epoch 573\n",
      "Step 0: loss = 0.01120490487664938, recon_loss = 0.011198826134204865, kl_loss = 0.006078328937292099\n",
      "\n",
      "Epoch 574\n",
      "Step 0: loss = 0.011216048151254654, recon_loss = 0.011206835508346558, kl_loss = 0.009212280623614788\n",
      "\n",
      "Epoch 575\n",
      "Step 0: loss = 0.011440112255513668, recon_loss = 0.011431140825152397, kl_loss = 0.008971016854047775\n",
      "\n",
      "Epoch 576\n",
      "Step 0: loss = 0.011016884818673134, recon_loss = 0.011006858199834824, kl_loss = 0.010026929900050163\n",
      "\n",
      "Epoch 577\n",
      "Step 0: loss = 0.011621350422501564, recon_loss = 0.011612992733716965, kl_loss = 0.008357541635632515\n",
      "\n",
      "Epoch 578\n",
      "Step 0: loss = 0.01092193927615881, recon_loss = 0.010911917313933372, kl_loss = 0.010021761991083622\n",
      "\n",
      "Epoch 579\n",
      "Step 0: loss = 0.0108870230615139, recon_loss = 0.01087908260524273, kl_loss = 0.007940219715237617\n",
      "\n",
      "Epoch 580\n",
      "Step 0: loss = 0.011069467291235924, recon_loss = 0.01106029562652111, kl_loss = 0.009172122925519943\n",
      "\n",
      "Epoch 581\n",
      "Step 0: loss = 0.011300845071673393, recon_loss = 0.01129148155450821, kl_loss = 0.009363236837089062\n",
      "\n",
      "Epoch 582\n",
      "Step 0: loss = 0.011187274008989334, recon_loss = 0.011177275329828262, kl_loss = 0.009998618625104427\n",
      "\n",
      "Epoch 583\n",
      "Step 0: loss = 0.011025714688003063, recon_loss = 0.011017711833119392, kl_loss = 0.00800297874957323\n",
      "\n",
      "Epoch 584\n",
      "Step 0: loss = 0.011317812837660313, recon_loss = 0.011309979483485222, kl_loss = 0.007833772338926792\n",
      "\n",
      "Epoch 585\n",
      "Step 0: loss = 0.010929770767688751, recon_loss = 0.010922005400061607, kl_loss = 0.007765189744532108\n",
      "\n",
      "Epoch 586\n",
      "Step 0: loss = 0.011135099455714226, recon_loss = 0.011126376688480377, kl_loss = 0.008723122999072075\n",
      "\n",
      "Epoch 587\n",
      "Step 0: loss = 0.010930407792329788, recon_loss = 0.010922404006123543, kl_loss = 0.008003521710634232\n",
      "\n",
      "Epoch 588\n",
      "Step 0: loss = 0.010691825300455093, recon_loss = 0.010684305801987648, kl_loss = 0.007519694045186043\n",
      "\n",
      "Epoch 589\n",
      "Step 0: loss = 0.011093929409980774, recon_loss = 0.011084310710430145, kl_loss = 0.009618987329304218\n",
      "\n",
      "Epoch 590\n",
      "Step 0: loss = 0.01095576398074627, recon_loss = 0.010948551818728447, kl_loss = 0.007212432101368904\n",
      "\n",
      "Epoch 591\n",
      "Step 0: loss = 0.011111208237707615, recon_loss = 0.011104216799139977, kl_loss = 0.0069918567314744\n",
      "\n",
      "Epoch 592\n",
      "Step 0: loss = 0.010789183899760246, recon_loss = 0.010782284662127495, kl_loss = 0.006899564526975155\n",
      "\n",
      "Epoch 593\n",
      "Step 0: loss = 0.01103017944842577, recon_loss = 0.011024920269846916, kl_loss = 0.005259186029434204\n",
      "\n",
      "Epoch 594\n",
      "Step 0: loss = 0.010897019878029823, recon_loss = 0.010890461504459381, kl_loss = 0.006558014079928398\n",
      "\n",
      "Epoch 595\n",
      "Step 0: loss = 0.010635631158947945, recon_loss = 0.010629255324602127, kl_loss = 0.0063758185133337975\n",
      "\n",
      "Epoch 596\n",
      "Step 0: loss = 0.011074100621044636, recon_loss = 0.011067850515246391, kl_loss = 0.006250297650694847\n",
      "\n",
      "Epoch 597\n",
      "Step 0: loss = 0.011115331202745438, recon_loss = 0.01110798493027687, kl_loss = 0.007346128113567829\n",
      "\n",
      "Epoch 598\n",
      "Step 0: loss = 0.01129218190908432, recon_loss = 0.01128406636416912, kl_loss = 0.008115669712424278\n",
      "\n",
      "Epoch 599\n",
      "Step 0: loss = 0.011146346107125282, recon_loss = 0.011138826608657837, kl_loss = 0.00751916691660881\n",
      "\n",
      "Epoch 600\n",
      "Step 0: loss = 0.011130254715681076, recon_loss = 0.011122852563858032, kl_loss = 0.007402544841170311\n",
      "\n",
      "Epoch 601\n",
      "Step 0: loss = 0.010754325427114964, recon_loss = 0.01074628159403801, kl_loss = 0.008043626323342323\n",
      "\n",
      "Epoch 602\n",
      "Step 0: loss = 0.010890540666878223, recon_loss = 0.010882513597607613, kl_loss = 0.008027350530028343\n",
      "\n",
      "Epoch 603\n",
      "Step 0: loss = 0.010675081983208656, recon_loss = 0.01066729985177517, kl_loss = 0.007782315835356712\n",
      "\n",
      "Epoch 604\n",
      "Step 0: loss = 0.010623657144606113, recon_loss = 0.01061665453016758, kl_loss = 0.007002430967986584\n",
      "\n",
      "Epoch 605\n",
      "Step 0: loss = 0.010626588948071003, recon_loss = 0.010620156303048134, kl_loss = 0.006432399153709412\n",
      "\n",
      "Epoch 606\n",
      "Step 0: loss = 0.010791395790874958, recon_loss = 0.010785005986690521, kl_loss = 0.006389625370502472\n",
      "\n",
      "Epoch 607\n",
      "Step 0: loss = 0.011016178876161575, recon_loss = 0.01100909523665905, kl_loss = 0.007083701901137829\n",
      "\n",
      "Epoch 608\n",
      "Step 0: loss = 0.011035365983843803, recon_loss = 0.01102844625711441, kl_loss = 0.006919262930750847\n",
      "\n",
      "Epoch 609\n",
      "Step 0: loss = 0.010794375091791153, recon_loss = 0.010786615312099457, kl_loss = 0.007760172709822655\n",
      "\n",
      "Epoch 610\n",
      "Step 0: loss = 0.010823122225701809, recon_loss = 0.010816676542162895, kl_loss = 0.006445466540753841\n",
      "\n",
      "Epoch 611\n",
      "Step 0: loss = 0.010357050225138664, recon_loss = 0.010348893702030182, kl_loss = 0.008156392723321915\n",
      "\n",
      "Epoch 612\n",
      "Step 0: loss = 0.010939325205981731, recon_loss = 0.01093214750289917, kl_loss = 0.007178119383752346\n",
      "\n",
      "Epoch 613\n",
      "Step 0: loss = 0.010952586308121681, recon_loss = 0.010946039110422134, kl_loss = 0.0065468670800328255\n",
      "\n",
      "Epoch 614\n",
      "Step 0: loss = 0.010856612585484982, recon_loss = 0.010848840698599815, kl_loss = 0.007771738804876804\n",
      "\n",
      "Epoch 615\n",
      "Step 0: loss = 0.010864436626434326, recon_loss = 0.010856520384550095, kl_loss = 0.007916058413684368\n",
      "\n",
      "Epoch 616\n",
      "Step 0: loss = 0.010923881083726883, recon_loss = 0.01091637834906578, kl_loss = 0.0075024086982011795\n",
      "\n",
      "Epoch 617\n",
      "Step 0: loss = 0.01109282299876213, recon_loss = 0.01108488067984581, kl_loss = 0.007942300289869308\n",
      "\n",
      "Epoch 618\n",
      "Step 0: loss = 0.010854529216885567, recon_loss = 0.010845934972167015, kl_loss = 0.008594412356615067\n",
      "\n",
      "Epoch 619\n",
      "Step 0: loss = 0.011004607193171978, recon_loss = 0.010994262993335724, kl_loss = 0.01034396979957819\n",
      "\n",
      "Epoch 620\n",
      "Step 0: loss = 0.010509700514376163, recon_loss = 0.010501861572265625, kl_loss = 0.00783857237547636\n",
      "\n",
      "Epoch 621\n",
      "Step 0: loss = 0.011029996909201145, recon_loss = 0.011021804064512253, kl_loss = 0.008192604407668114\n",
      "\n",
      "Epoch 622\n",
      "Step 0: loss = 0.010697171092033386, recon_loss = 0.0106887836009264, kl_loss = 0.008387085050344467\n",
      "\n",
      "Epoch 623\n",
      "Step 0: loss = 0.011069407686591148, recon_loss = 0.011061511933803558, kl_loss = 0.007895976305007935\n",
      "\n",
      "Epoch 624\n",
      "Step 0: loss = 0.01106911152601242, recon_loss = 0.011060737073421478, kl_loss = 0.008374557830393314\n",
      "\n",
      "Epoch 625\n",
      "Step 0: loss = 0.01061149500310421, recon_loss = 0.010604150593280792, kl_loss = 0.007344038225710392\n",
      "\n",
      "Epoch 626\n",
      "Step 0: loss = 0.01053780410438776, recon_loss = 0.010530892759561539, kl_loss = 0.006911466829478741\n",
      "\n",
      "Epoch 627\n",
      "Step 0: loss = 0.010979412123560905, recon_loss = 0.010970575734972954, kl_loss = 0.008836501277983189\n",
      "\n",
      "Epoch 628\n",
      "Step 0: loss = 0.010572591796517372, recon_loss = 0.01056348904967308, kl_loss = 0.00910248328000307\n",
      "\n",
      "Epoch 629\n",
      "Step 0: loss = 0.011084293946623802, recon_loss = 0.01107698306441307, kl_loss = 0.007310950197279453\n",
      "\n",
      "Epoch 630\n",
      "Step 0: loss = 0.011058133095502853, recon_loss = 0.011050283908843994, kl_loss = 0.007849585264921188\n",
      "\n",
      "Epoch 631\n",
      "Step 0: loss = 0.010834522545337677, recon_loss = 0.010826103389263153, kl_loss = 0.008419192396104336\n",
      "\n",
      "Epoch 632\n",
      "Step 0: loss = 0.01061872486025095, recon_loss = 0.010612092912197113, kl_loss = 0.006631624884903431\n",
      "\n",
      "Epoch 633\n",
      "Step 0: loss = 0.010604999028146267, recon_loss = 0.010596321895718575, kl_loss = 0.00867688562721014\n",
      "\n",
      "Epoch 634\n",
      "Step 0: loss = 0.01090809516608715, recon_loss = 0.010899042710661888, kl_loss = 0.00905265100300312\n",
      "\n",
      "Epoch 635\n",
      "Step 0: loss = 0.01041150838136673, recon_loss = 0.01040462777018547, kl_loss = 0.006880686618387699\n",
      "\n",
      "Epoch 636\n",
      "Step 0: loss = 0.010820507071912289, recon_loss = 0.010814065113663673, kl_loss = 0.006441582925617695\n",
      "\n",
      "Epoch 637\n",
      "Step 0: loss = 0.010831523686647415, recon_loss = 0.010824186727404594, kl_loss = 0.007337185554206371\n",
      "\n",
      "Epoch 638\n",
      "Step 0: loss = 0.011040674522519112, recon_loss = 0.011032413691282272, kl_loss = 0.008260705508291721\n",
      "\n",
      "Epoch 639\n",
      "Step 0: loss = 0.010644099675118923, recon_loss = 0.010634591802954674, kl_loss = 0.009507829323410988\n",
      "\n",
      "Epoch 640\n",
      "Step 0: loss = 0.01059319730848074, recon_loss = 0.010584449395537376, kl_loss = 0.008748280815780163\n",
      "\n",
      "Epoch 641\n",
      "Step 0: loss = 0.010906225070357323, recon_loss = 0.010898247361183167, kl_loss = 0.007977580651640892\n",
      "\n",
      "Epoch 642\n",
      "Step 0: loss = 0.011032569222152233, recon_loss = 0.011024700477719307, kl_loss = 0.007868796586990356\n",
      "\n",
      "Epoch 643\n",
      "Step 0: loss = 0.010999112389981747, recon_loss = 0.010991506278514862, kl_loss = 0.0076061394065618515\n",
      "\n",
      "Epoch 644\n",
      "Step 0: loss = 0.01057497039437294, recon_loss = 0.010565049946308136, kl_loss = 0.009920010343194008\n",
      "\n",
      "Epoch 645\n",
      "Step 0: loss = 0.010522423312067986, recon_loss = 0.01051146537065506, kl_loss = 0.010958126746118069\n",
      "\n",
      "Epoch 646\n",
      "Step 0: loss = 0.010731770657002926, recon_loss = 0.010722341015934944, kl_loss = 0.009429512545466423\n",
      "\n",
      "Epoch 647\n",
      "Step 0: loss = 0.01072722114622593, recon_loss = 0.010718191042542458, kl_loss = 0.009029783308506012\n",
      "\n",
      "Epoch 648\n",
      "Step 0: loss = 0.011034084483981133, recon_loss = 0.011026179417967796, kl_loss = 0.007905482314527035\n",
      "\n",
      "Epoch 649\n",
      "Step 0: loss = 0.010923749767243862, recon_loss = 0.010916078463196754, kl_loss = 0.007671418599784374\n",
      "\n",
      "Epoch 650\n",
      "Step 0: loss = 0.010027686133980751, recon_loss = 0.010021381080150604, kl_loss = 0.006305375136435032\n",
      "\n",
      "Epoch 651\n",
      "Step 0: loss = 0.010906373150646687, recon_loss = 0.010899337008595467, kl_loss = 0.007035869173705578\n",
      "\n",
      "Epoch 652\n",
      "Step 0: loss = 0.010510744526982307, recon_loss = 0.010502329096198082, kl_loss = 0.008415170945227146\n",
      "\n",
      "Epoch 653\n",
      "Step 0: loss = 0.010662872344255447, recon_loss = 0.010655535385012627, kl_loss = 0.007337374612689018\n",
      "\n",
      "Epoch 654\n",
      "Step 0: loss = 0.010956576094031334, recon_loss = 0.010951269418001175, kl_loss = 0.005306856706738472\n",
      "\n",
      "Epoch 655\n",
      "Step 0: loss = 0.010688242502510548, recon_loss = 0.010681165382266045, kl_loss = 0.007077285088598728\n",
      "\n",
      "Epoch 656\n",
      "Step 0: loss = 0.010324536822736263, recon_loss = 0.010317949578166008, kl_loss = 0.006587035953998566\n",
      "\n",
      "Epoch 657\n",
      "Step 0: loss = 0.01065430324524641, recon_loss = 0.010649256408214569, kl_loss = 0.005046858452260494\n",
      "\n",
      "Epoch 658\n",
      "Step 0: loss = 0.010911896824836731, recon_loss = 0.010906891897320747, kl_loss = 0.00500459223985672\n",
      "\n",
      "Epoch 659\n",
      "Step 0: loss = 0.010168333537876606, recon_loss = 0.01016213558614254, kl_loss = 0.00619827676564455\n",
      "\n",
      "Epoch 660\n",
      "Step 0: loss = 0.010491875931620598, recon_loss = 0.010486526414752007, kl_loss = 0.005349826067686081\n",
      "\n",
      "Epoch 661\n",
      "Step 0: loss = 0.01011835690587759, recon_loss = 0.010113293305039406, kl_loss = 0.00506365392357111\n",
      "\n",
      "Epoch 662\n",
      "Step 0: loss = 0.010311231017112732, recon_loss = 0.010305818170309067, kl_loss = 0.005413135513663292\n",
      "\n",
      "Epoch 663\n",
      "Step 0: loss = 0.01068821270018816, recon_loss = 0.01068343035876751, kl_loss = 0.0047825053334236145\n",
      "\n",
      "Epoch 664\n",
      "Step 0: loss = 0.010013889521360397, recon_loss = 0.01000816561281681, kl_loss = 0.005724256858229637\n",
      "\n",
      "Epoch 665\n",
      "Step 0: loss = 0.010326930321753025, recon_loss = 0.010323353111743927, kl_loss = 0.0035771075636148453\n",
      "\n",
      "Epoch 666\n",
      "Step 0: loss = 0.010562216863036156, recon_loss = 0.010556502267718315, kl_loss = 0.005714249797165394\n",
      "\n",
      "Epoch 667\n",
      "Step 0: loss = 0.010302938520908356, recon_loss = 0.010296937078237534, kl_loss = 0.006001541391015053\n",
      "\n",
      "Epoch 668\n",
      "Step 0: loss = 0.01059860922396183, recon_loss = 0.010593494400382042, kl_loss = 0.005114929750561714\n",
      "\n",
      "Epoch 669\n",
      "Step 0: loss = 0.010617777705192566, recon_loss = 0.010611768811941147, kl_loss = 0.006008855998516083\n",
      "\n",
      "Epoch 670\n",
      "Step 0: loss = 0.01047282200306654, recon_loss = 0.010465843603014946, kl_loss = 0.00697858352214098\n",
      "\n",
      "Epoch 671\n",
      "Step 0: loss = 0.010522929020226002, recon_loss = 0.010515036061406136, kl_loss = 0.007893017493188381\n",
      "\n",
      "Epoch 672\n",
      "Step 0: loss = 0.010448252782225609, recon_loss = 0.010438278317451477, kl_loss = 0.009974551387131214\n",
      "\n",
      "Epoch 673\n",
      "Step 0: loss = 0.010709816589951515, recon_loss = 0.010703792795538902, kl_loss = 0.006023729220032692\n",
      "\n",
      "Epoch 674\n",
      "Step 0: loss = 0.010377992875874043, recon_loss = 0.010371282696723938, kl_loss = 0.006710277870297432\n",
      "\n",
      "Epoch 675\n",
      "Step 0: loss = 0.010578971356153488, recon_loss = 0.010573137551546097, kl_loss = 0.005834200419485569\n",
      "\n",
      "Epoch 676\n",
      "Step 0: loss = 0.010736051015555859, recon_loss = 0.0107301976531744, kl_loss = 0.0058529311791062355\n",
      "\n",
      "Epoch 677\n",
      "Step 0: loss = 0.010648442432284355, recon_loss = 0.010639483109116554, kl_loss = 0.00895907822996378\n",
      "\n",
      "Epoch 678\n",
      "Step 0: loss = 0.010417098179459572, recon_loss = 0.010408077389001846, kl_loss = 0.009020697325468063\n",
      "\n",
      "Epoch 679\n",
      "Step 0: loss = 0.01056909654289484, recon_loss = 0.010562097653746605, kl_loss = 0.006999313831329346\n",
      "\n",
      "Epoch 680\n",
      "Step 0: loss = 0.010668914765119553, recon_loss = 0.010659977793693542, kl_loss = 0.00893655326217413\n",
      "\n",
      "Epoch 681\n",
      "Step 0: loss = 0.010725150816142559, recon_loss = 0.010717162862420082, kl_loss = 0.007988326251506805\n",
      "\n",
      "Epoch 682\n",
      "Step 0: loss = 0.010388333350419998, recon_loss = 0.010381273925304413, kl_loss = 0.007059556432068348\n",
      "\n",
      "Epoch 683\n",
      "Step 0: loss = 0.01075451448559761, recon_loss = 0.010747043415904045, kl_loss = 0.007470935583114624\n",
      "\n",
      "Epoch 684\n",
      "Step 0: loss = 0.010784141719341278, recon_loss = 0.010777218267321587, kl_loss = 0.006923196837306023\n",
      "\n",
      "Epoch 685\n",
      "Step 0: loss = 0.01020043808966875, recon_loss = 0.010193318128585815, kl_loss = 0.007119698449969292\n",
      "\n",
      "Epoch 686\n",
      "Step 0: loss = 0.010564584285020828, recon_loss = 0.010556995868682861, kl_loss = 0.0075884126126766205\n",
      "\n",
      "Epoch 687\n",
      "Step 0: loss = 0.010661279782652855, recon_loss = 0.01065361499786377, kl_loss = 0.007664669305086136\n",
      "\n",
      "Epoch 688\n",
      "Step 0: loss = 0.010696666315197945, recon_loss = 0.010686838999390602, kl_loss = 0.009827135130763054\n",
      "\n",
      "Epoch 689\n",
      "Step 0: loss = 0.010644898749887943, recon_loss = 0.010636130347847939, kl_loss = 0.008768538013100624\n",
      "\n",
      "Epoch 690\n",
      "Step 0: loss = 0.010567894205451012, recon_loss = 0.0105598084628582, kl_loss = 0.008085611276328564\n",
      "\n",
      "Epoch 691\n",
      "Step 0: loss = 0.01097176130861044, recon_loss = 0.010963225737214088, kl_loss = 0.00853532925248146\n",
      "\n",
      "Epoch 692\n",
      "Step 0: loss = 0.01077008806169033, recon_loss = 0.010759273543953896, kl_loss = 0.010814094915986061\n",
      "\n",
      "Epoch 693\n",
      "Step 0: loss = 0.01075464766472578, recon_loss = 0.010745100677013397, kl_loss = 0.00954713486135006\n",
      "\n",
      "Epoch 694\n",
      "Step 0: loss = 0.010432958602905273, recon_loss = 0.010423760861158371, kl_loss = 0.009197762235999107\n",
      "\n",
      "Epoch 695\n",
      "Step 0: loss = 0.010960234329104424, recon_loss = 0.010951625183224678, kl_loss = 0.008609138429164886\n",
      "\n",
      "Epoch 696\n",
      "Step 0: loss = 0.010618302039802074, recon_loss = 0.010609989985823631, kl_loss = 0.008311985991895199\n",
      "\n",
      "Epoch 697\n",
      "Step 0: loss = 0.01045201625674963, recon_loss = 0.010443411767482758, kl_loss = 0.00860444363206625\n",
      "\n",
      "Epoch 698\n",
      "Step 0: loss = 0.010297675617039204, recon_loss = 0.010290788486599922, kl_loss = 0.006887356750667095\n",
      "\n",
      "Epoch 699\n",
      "Step 0: loss = 0.010586980730295181, recon_loss = 0.010580122470855713, kl_loss = 0.0068585919216275215\n",
      "\n",
      "Epoch 700\n",
      "Step 0: loss = 0.011066856794059277, recon_loss = 0.011059051379561424, kl_loss = 0.007805006578564644\n",
      "\n",
      "Epoch 701\n",
      "Step 0: loss = 0.010601304471492767, recon_loss = 0.010593665763735771, kl_loss = 0.007638639770448208\n",
      "\n",
      "Epoch 702\n",
      "Step 0: loss = 0.010411386378109455, recon_loss = 0.010403066873550415, kl_loss = 0.008319118991494179\n",
      "\n",
      "Epoch 703\n",
      "Step 0: loss = 0.010558148846030235, recon_loss = 0.010551996529102325, kl_loss = 0.0061525609344244\n",
      "\n",
      "Epoch 704\n",
      "Step 0: loss = 0.010204360820353031, recon_loss = 0.010197808966040611, kl_loss = 0.006552203558385372\n",
      "\n",
      "Epoch 705\n",
      "Step 0: loss = 0.010428260080516338, recon_loss = 0.010423101484775543, kl_loss = 0.00515847560018301\n",
      "\n",
      "Epoch 706\n",
      "Step 0: loss = 0.010360945016145706, recon_loss = 0.01035560853779316, kl_loss = 0.0053368546068668365\n",
      "\n",
      "Epoch 707\n",
      "Step 0: loss = 0.010429504327476025, recon_loss = 0.010422764346003532, kl_loss = 0.006739916279911995\n",
      "\n",
      "Epoch 708\n",
      "Step 0: loss = 0.010584836825728416, recon_loss = 0.010579610243439674, kl_loss = 0.005227028392255306\n",
      "\n",
      "Epoch 709\n",
      "Step 0: loss = 0.010497815907001495, recon_loss = 0.010492630302906036, kl_loss = 0.0051857829093933105\n",
      "\n",
      "Epoch 710\n",
      "Step 0: loss = 0.0099537568166852, recon_loss = 0.009949380531907082, kl_loss = 0.004376422613859177\n",
      "\n",
      "Epoch 711\n",
      "Step 0: loss = 0.010244661942124367, recon_loss = 0.010240212082862854, kl_loss = 0.0044494327157735825\n",
      "\n",
      "Epoch 712\n",
      "Step 0: loss = 0.010269456543028355, recon_loss = 0.010263534262776375, kl_loss = 0.005921884439885616\n",
      "\n",
      "Epoch 713\n",
      "Step 0: loss = 0.010026250965893269, recon_loss = 0.010020803660154343, kl_loss = 0.005447201430797577\n",
      "\n",
      "Epoch 714\n",
      "Step 0: loss = 0.010312126949429512, recon_loss = 0.010308099910616875, kl_loss = 0.004027052782475948\n",
      "\n",
      "Epoch 715\n",
      "Step 0: loss = 0.010224351659417152, recon_loss = 0.010219110175967216, kl_loss = 0.005241622216999531\n",
      "\n",
      "Epoch 716\n",
      "Step 0: loss = 0.010107387788593769, recon_loss = 0.010101398453116417, kl_loss = 0.005989662371575832\n",
      "\n",
      "Epoch 717\n",
      "Step 0: loss = 0.010581915266811848, recon_loss = 0.010577794164419174, kl_loss = 0.0041212039068341255\n",
      "\n",
      "Epoch 718\n",
      "Step 0: loss = 0.010123027488589287, recon_loss = 0.010117167606949806, kl_loss = 0.005859664641320705\n",
      "\n",
      "Epoch 719\n",
      "Step 0: loss = 0.01018466241657734, recon_loss = 0.010178348049521446, kl_loss = 0.006314665079116821\n",
      "\n",
      "Epoch 720\n",
      "Step 0: loss = 0.010276482440531254, recon_loss = 0.010271649807691574, kl_loss = 0.004832981154322624\n",
      "\n",
      "Epoch 721\n",
      "Step 0: loss = 0.010339767672121525, recon_loss = 0.010333601385354996, kl_loss = 0.0061666034162044525\n",
      "\n",
      "Epoch 722\n",
      "Step 0: loss = 0.010036205872893333, recon_loss = 0.010028902441263199, kl_loss = 0.007303682155907154\n",
      "\n",
      "Epoch 723\n",
      "Step 0: loss = 0.010037015192210674, recon_loss = 0.010030921548604965, kl_loss = 0.006093661300837994\n",
      "\n",
      "Epoch 724\n",
      "Step 0: loss = 0.010282362811267376, recon_loss = 0.010275408625602722, kl_loss = 0.006954362615942955\n",
      "\n",
      "Epoch 725\n",
      "Step 0: loss = 0.010318279266357422, recon_loss = 0.010311927646398544, kl_loss = 0.006351848132908344\n",
      "\n",
      "Epoch 726\n",
      "Step 0: loss = 0.010547713376581669, recon_loss = 0.01054023765027523, kl_loss = 0.007475709542632103\n",
      "\n",
      "Epoch 727\n",
      "Step 0: loss = 0.010309526696801186, recon_loss = 0.010301182046532631, kl_loss = 0.008344346657395363\n",
      "\n",
      "Epoch 728\n",
      "Step 0: loss = 0.010672529228031635, recon_loss = 0.010665742680430412, kl_loss = 0.006786218844354153\n",
      "\n",
      "Epoch 729\n",
      "Step 0: loss = 0.010274635627865791, recon_loss = 0.010267863050103188, kl_loss = 0.006772552616894245\n",
      "\n",
      "Epoch 730\n",
      "Step 0: loss = 0.010264760814607143, recon_loss = 0.010258324444293976, kl_loss = 0.00643675122410059\n",
      "\n",
      "Epoch 731\n",
      "Step 0: loss = 0.010340120643377304, recon_loss = 0.010332269594073296, kl_loss = 0.00785150472074747\n",
      "\n",
      "Epoch 732\n",
      "Step 0: loss = 0.010223214514553547, recon_loss = 0.010216675698757172, kl_loss = 0.006538571789860725\n",
      "\n",
      "Epoch 733\n",
      "Step 0: loss = 0.010585562326014042, recon_loss = 0.010576263070106506, kl_loss = 0.00929903332144022\n",
      "\n",
      "Epoch 734\n",
      "Step 0: loss = 0.01022949069738388, recon_loss = 0.010221496224403381, kl_loss = 0.007994144223630428\n",
      "\n",
      "Epoch 735\n",
      "Step 0: loss = 0.010511771775782108, recon_loss = 0.01050386019051075, kl_loss = 0.00791184976696968\n",
      "\n",
      "Epoch 736\n",
      "Step 0: loss = 0.01046217791736126, recon_loss = 0.010454289615154266, kl_loss = 0.007888450287282467\n",
      "\n",
      "Epoch 737\n",
      "Step 0: loss = 0.010437083430588245, recon_loss = 0.010428572073578835, kl_loss = 0.008511432446539402\n",
      "\n",
      "Epoch 738\n",
      "Step 0: loss = 0.010288597084581852, recon_loss = 0.010279230773448944, kl_loss = 0.00936664268374443\n",
      "\n",
      "Epoch 739\n",
      "Step 0: loss = 0.010441245511174202, recon_loss = 0.010433288291096687, kl_loss = 0.00795720610767603\n",
      "\n",
      "Epoch 740\n",
      "Step 0: loss = 0.010423501953482628, recon_loss = 0.010416531935334206, kl_loss = 0.006969945505261421\n",
      "\n",
      "Epoch 741\n",
      "Step 0: loss = 0.01059782411903143, recon_loss = 0.010591838508844376, kl_loss = 0.005985786207020283\n",
      "\n",
      "Epoch 742\n",
      "Step 0: loss = 0.010294503532350063, recon_loss = 0.010288262739777565, kl_loss = 0.0062410058453679085\n",
      "\n",
      "Epoch 743\n",
      "Step 0: loss = 0.010194465517997742, recon_loss = 0.010188108310103416, kl_loss = 0.006357207894325256\n",
      "\n",
      "Epoch 744\n",
      "Step 0: loss = 0.010242527350783348, recon_loss = 0.010235870257019997, kl_loss = 0.006657063961029053\n",
      "\n",
      "Epoch 745\n",
      "Step 0: loss = 0.01012569759041071, recon_loss = 0.010120823979377747, kl_loss = 0.004873679019510746\n",
      "\n",
      "Epoch 746\n",
      "Step 0: loss = 0.01014306303113699, recon_loss = 0.01013713888823986, kl_loss = 0.005923750810325146\n",
      "\n",
      "Epoch 747\n",
      "Step 0: loss = 0.010253753513097763, recon_loss = 0.010246790945529938, kl_loss = 0.006962415762245655\n",
      "\n",
      "Epoch 748\n",
      "Step 0: loss = 0.010142514482140541, recon_loss = 0.010134659707546234, kl_loss = 0.007854715920984745\n",
      "\n",
      "Epoch 749\n",
      "Step 0: loss = 0.010297037661075592, recon_loss = 0.010292289778590202, kl_loss = 0.0047481972724199295\n",
      "\n",
      "Epoch 750\n",
      "Step 0: loss = 0.010280665010213852, recon_loss = 0.010274708271026611, kl_loss = 0.005956544540822506\n",
      "\n",
      "Epoch 751\n",
      "Step 0: loss = 0.01050505880266428, recon_loss = 0.010500174015760422, kl_loss = 0.004885169677436352\n",
      "\n",
      "Epoch 752\n",
      "Step 0: loss = 0.010290036909282207, recon_loss = 0.01028507761657238, kl_loss = 0.004959181882441044\n",
      "\n",
      "Epoch 753\n",
      "Step 0: loss = 0.010183954611420631, recon_loss = 0.010178711265325546, kl_loss = 0.0052436599507927895\n",
      "\n",
      "Epoch 754\n",
      "Step 0: loss = 0.010195841081440449, recon_loss = 0.010191991925239563, kl_loss = 0.003849613480269909\n",
      "\n",
      "Epoch 755\n",
      "Step 0: loss = 0.010144255124032497, recon_loss = 0.010138589888811111, kl_loss = 0.005664864555001259\n",
      "\n",
      "Epoch 756\n",
      "Step 0: loss = 0.009756123647093773, recon_loss = 0.009751399978995323, kl_loss = 0.004723508842289448\n",
      "\n",
      "Epoch 757\n",
      "Step 0: loss = 0.010151855647563934, recon_loss = 0.010146545246243477, kl_loss = 0.005310209468007088\n",
      "\n",
      "Epoch 758\n",
      "Step 0: loss = 0.00989752821624279, recon_loss = 0.009892908856272697, kl_loss = 0.00461927056312561\n",
      "\n",
      "Epoch 759\n",
      "Step 0: loss = 0.01046291645616293, recon_loss = 0.010458193719387054, kl_loss = 0.004722923971712589\n",
      "\n",
      "Epoch 760\n",
      "Step 0: loss = 0.010576815344393253, recon_loss = 0.010571552440524101, kl_loss = 0.005262452177703381\n",
      "\n",
      "Epoch 761\n",
      "Step 0: loss = 0.010048714466392994, recon_loss = 0.010043453425168991, kl_loss = 0.005260721780359745\n",
      "\n",
      "Epoch 762\n",
      "Step 0: loss = 0.010235190391540527, recon_loss = 0.010229228064417839, kl_loss = 0.005962072871625423\n",
      "\n",
      "Epoch 763\n",
      "Step 0: loss = 0.010003605857491493, recon_loss = 0.009997386485338211, kl_loss = 0.006218969821929932\n",
      "\n",
      "Epoch 764\n",
      "Step 0: loss = 0.01005833875387907, recon_loss = 0.010053282603621483, kl_loss = 0.005055980756878853\n",
      "\n",
      "Epoch 765\n",
      "Step 0: loss = 0.010425319895148277, recon_loss = 0.010418621823191643, kl_loss = 0.006697646342217922\n",
      "\n",
      "Epoch 766\n",
      "Step 0: loss = 0.010360383428633213, recon_loss = 0.0103533323854208, kl_loss = 0.00705107394605875\n",
      "\n",
      "Epoch 767\n",
      "Step 0: loss = 0.010290971025824547, recon_loss = 0.010284768417477608, kl_loss = 0.006202535703778267\n",
      "\n",
      "Epoch 768\n",
      "Step 0: loss = 0.010085032321512699, recon_loss = 0.010076556354761124, kl_loss = 0.008475711569190025\n",
      "\n",
      "Epoch 769\n",
      "Step 0: loss = 0.010389079339802265, recon_loss = 0.010380851104855537, kl_loss = 0.008228185586631298\n",
      "\n",
      "Epoch 770\n",
      "Step 0: loss = 0.010386183857917786, recon_loss = 0.010378366336226463, kl_loss = 0.007817360572516918\n",
      "\n",
      "Epoch 771\n",
      "Step 0: loss = 0.010244598612189293, recon_loss = 0.0102361049503088, kl_loss = 0.008493702858686447\n",
      "\n",
      "Epoch 772\n",
      "Step 0: loss = 0.010251181200146675, recon_loss = 0.010240532457828522, kl_loss = 0.010649151168763638\n",
      "\n",
      "Epoch 773\n",
      "Step 0: loss = 0.010443486273288727, recon_loss = 0.010434448719024658, kl_loss = 0.009037836454808712\n",
      "\n",
      "Epoch 774\n",
      "Step 0: loss = 0.010199585929512978, recon_loss = 0.010189959779381752, kl_loss = 0.009626219049096107\n",
      "\n",
      "Epoch 775\n",
      "Step 0: loss = 0.010507632978260517, recon_loss = 0.010499255731701851, kl_loss = 0.008377603255212307\n",
      "\n",
      "Epoch 776\n",
      "Step 0: loss = 0.010575209744274616, recon_loss = 0.01056625321507454, kl_loss = 0.008956379257142544\n",
      "\n",
      "Epoch 777\n",
      "Step 0: loss = 0.010445188730955124, recon_loss = 0.010436318814754486, kl_loss = 0.008869887329638004\n",
      "\n",
      "Epoch 778\n",
      "Step 0: loss = 0.010114836506545544, recon_loss = 0.010105088353157043, kl_loss = 0.009748502634465694\n",
      "\n",
      "Epoch 779\n",
      "Step 0: loss = 0.010017592459917068, recon_loss = 0.010010173544287682, kl_loss = 0.007419245317578316\n",
      "\n",
      "Epoch 780\n",
      "Step 0: loss = 0.010355527512729168, recon_loss = 0.010349534451961517, kl_loss = 0.0059927087277174\n",
      "\n",
      "Epoch 781\n",
      "Step 0: loss = 0.010027170181274414, recon_loss = 0.010020080953836441, kl_loss = 0.0070891547948122025\n",
      "\n",
      "Epoch 782\n",
      "Step 0: loss = 0.00998156052082777, recon_loss = 0.009974852204322815, kl_loss = 0.006708703003823757\n",
      "\n",
      "Epoch 783\n",
      "Step 0: loss = 0.01000162959098816, recon_loss = 0.009996514767408371, kl_loss = 0.005114930681884289\n",
      "\n",
      "Epoch 784\n",
      "Step 0: loss = 0.010056865401566029, recon_loss = 0.01005232147872448, kl_loss = 0.004543628543615341\n",
      "\n",
      "Epoch 785\n",
      "Step 0: loss = 0.009882688522338867, recon_loss = 0.00987929292023182, kl_loss = 0.0033959057182073593\n",
      "\n",
      "Epoch 786\n",
      "Step 0: loss = 0.010018938221037388, recon_loss = 0.010015591979026794, kl_loss = 0.003346536308526993\n",
      "\n",
      "Epoch 787\n",
      "Step 0: loss = 0.009713459759950638, recon_loss = 0.009709903970360756, kl_loss = 0.0035557420924305916\n",
      "\n",
      "Epoch 788\n",
      "Step 0: loss = 0.009711689315736294, recon_loss = 0.009708598256111145, kl_loss = 0.0030906423926353455\n",
      "\n",
      "Epoch 789\n",
      "Step 0: loss = 0.009979032911360264, recon_loss = 0.009975342079997063, kl_loss = 0.003690853714942932\n",
      "\n",
      "Epoch 790\n",
      "Step 0: loss = 0.009412712417542934, recon_loss = 0.009409544989466667, kl_loss = 0.003167569637298584\n",
      "\n",
      "Epoch 791\n",
      "Step 0: loss = 0.009615880437195301, recon_loss = 0.009612305089831352, kl_loss = 0.003575628623366356\n",
      "\n",
      "Epoch 792\n",
      "Step 0: loss = 0.010143602266907692, recon_loss = 0.010139675810933113, kl_loss = 0.003926161676645279\n",
      "\n",
      "Epoch 793\n",
      "Step 0: loss = 0.009824172593653202, recon_loss = 0.009820707142353058, kl_loss = 0.0034657148644328117\n",
      "\n",
      "Epoch 794\n",
      "Step 0: loss = 0.009789390489459038, recon_loss = 0.009783444926142693, kl_loss = 0.005945410579442978\n",
      "\n",
      "Epoch 795\n",
      "Step 0: loss = 0.01007182989269495, recon_loss = 0.010066881775856018, kl_loss = 0.0049478719010949135\n",
      "\n",
      "Epoch 796\n",
      "Step 0: loss = 0.010037709027528763, recon_loss = 0.010031551122665405, kl_loss = 0.006157871335744858\n",
      "\n",
      "Epoch 797\n",
      "Step 0: loss = 0.009989934042096138, recon_loss = 0.009983867406845093, kl_loss = 0.006066307425498962\n",
      "\n",
      "Epoch 798\n",
      "Step 0: loss = 0.009942258708178997, recon_loss = 0.009934598580002785, kl_loss = 0.00765985157340765\n",
      "\n",
      "Epoch 799\n",
      "Step 0: loss = 0.010590975172817707, recon_loss = 0.010582325980067253, kl_loss = 0.008648781105875969\n",
      "\n",
      "Epoch 800\n",
      "Step 0: loss = 0.010306356474757195, recon_loss = 0.010299181565642357, kl_loss = 0.00717475451529026\n",
      "\n",
      "Epoch 801\n",
      "Step 0: loss = 0.009927215054631233, recon_loss = 0.009919613599777222, kl_loss = 0.007601103745400906\n",
      "\n",
      "Epoch 802\n",
      "Step 0: loss = 0.010168557986617088, recon_loss = 0.01015898771584034, kl_loss = 0.009570100344717503\n",
      "\n",
      "Epoch 803\n",
      "Step 0: loss = 0.010213772766292095, recon_loss = 0.010206835344433784, kl_loss = 0.006936967372894287\n",
      "\n",
      "Epoch 804\n",
      "Step 0: loss = 0.010224391706287861, recon_loss = 0.010217452421784401, kl_loss = 0.006939024664461613\n",
      "\n",
      "Epoch 805\n",
      "Step 0: loss = 0.010180969722568989, recon_loss = 0.010175172239542007, kl_loss = 0.00579735916107893\n",
      "\n",
      "Epoch 806\n",
      "Step 0: loss = 0.010295553132891655, recon_loss = 0.010287944227457047, kl_loss = 0.007608725689351559\n",
      "\n",
      "Epoch 807\n",
      "Step 0: loss = 0.010192248038947582, recon_loss = 0.0101857241243124, kl_loss = 0.006523512303829193\n",
      "\n",
      "Epoch 808\n",
      "Step 0: loss = 0.010133234784007072, recon_loss = 0.010125590488314629, kl_loss = 0.0076445117592811584\n",
      "\n",
      "Epoch 809\n",
      "Step 0: loss = 0.01017236802726984, recon_loss = 0.010165359824895859, kl_loss = 0.007008055225014687\n",
      "\n",
      "Epoch 810\n",
      "Step 0: loss = 0.009700595401227474, recon_loss = 0.009695520624518394, kl_loss = 0.005074905231595039\n",
      "\n",
      "Epoch 811\n",
      "Step 0: loss = 0.009780202992260456, recon_loss = 0.009774785488843918, kl_loss = 0.005417541600763798\n",
      "\n",
      "Epoch 812\n",
      "Step 0: loss = 0.010338181629776955, recon_loss = 0.010332562029361725, kl_loss = 0.005619850941002369\n",
      "\n",
      "Epoch 813\n",
      "Step 0: loss = 0.009894567541778088, recon_loss = 0.009887760505080223, kl_loss = 0.0068068793043494225\n",
      "\n",
      "Epoch 814\n",
      "Step 0: loss = 0.010023873299360275, recon_loss = 0.010016929358243942, kl_loss = 0.006943575106561184\n",
      "\n",
      "Epoch 815\n",
      "Step 0: loss = 0.010059653781354427, recon_loss = 0.010054148733615875, kl_loss = 0.005504610016942024\n",
      "\n",
      "Epoch 816\n",
      "Step 0: loss = 0.0100929019972682, recon_loss = 0.01008780300617218, kl_loss = 0.005099219270050526\n",
      "\n",
      "Epoch 817\n",
      "Step 0: loss = 0.00980070699006319, recon_loss = 0.009795067831873894, kl_loss = 0.00563961174339056\n",
      "\n",
      "Epoch 818\n",
      "Step 0: loss = 0.009970089420676231, recon_loss = 0.009964905679225922, kl_loss = 0.00518377497792244\n",
      "\n",
      "Epoch 819\n",
      "Step 0: loss = 0.010038585402071476, recon_loss = 0.010034482926130295, kl_loss = 0.004102244041860104\n",
      "\n",
      "Epoch 820\n",
      "Step 0: loss = 0.010144490748643875, recon_loss = 0.010138977319002151, kl_loss = 0.0055132582783699036\n",
      "\n",
      "Epoch 821\n",
      "Step 0: loss = 0.009949293918907642, recon_loss = 0.009944524616003036, kl_loss = 0.004769527353346348\n",
      "\n",
      "Epoch 822\n",
      "Step 0: loss = 0.010157032869756222, recon_loss = 0.010151620954275131, kl_loss = 0.005411761812865734\n",
      "\n",
      "Epoch 823\n",
      "Step 0: loss = 0.010043389163911343, recon_loss = 0.010037785395979881, kl_loss = 0.005603383295238018\n",
      "\n",
      "Epoch 824\n",
      "Step 0: loss = 0.00990095641463995, recon_loss = 0.009893946349620819, kl_loss = 0.007009737193584442\n",
      "\n",
      "Epoch 825\n",
      "Step 0: loss = 0.009907468222081661, recon_loss = 0.009902315214276314, kl_loss = 0.005152622237801552\n",
      "\n",
      "Epoch 826\n",
      "Step 0: loss = 0.010239308699965477, recon_loss = 0.010232418775558472, kl_loss = 0.0068902308121323586\n",
      "\n",
      "Epoch 827\n",
      "Step 0: loss = 0.009946156293153763, recon_loss = 0.009940752759575844, kl_loss = 0.0054032690823078156\n",
      "\n",
      "Epoch 828\n",
      "Step 0: loss = 0.010169382207095623, recon_loss = 0.010163778439164162, kl_loss = 0.005603305995464325\n",
      "\n",
      "Epoch 829\n",
      "Step 0: loss = 0.009959889575839043, recon_loss = 0.009952716529369354, kl_loss = 0.007173256948590279\n",
      "\n",
      "Epoch 830\n",
      "Step 0: loss = 0.00995748769491911, recon_loss = 0.009950846433639526, kl_loss = 0.006641576997935772\n",
      "\n",
      "Epoch 831\n",
      "Step 0: loss = 0.009872786700725555, recon_loss = 0.00986667163670063, kl_loss = 0.006114807911217213\n",
      "\n",
      "Epoch 832\n",
      "Step 0: loss = 0.010018914006650448, recon_loss = 0.010013209655880928, kl_loss = 0.0057040024548769\n",
      "\n",
      "Epoch 833\n",
      "Step 0: loss = 0.009941177442669868, recon_loss = 0.009935889393091202, kl_loss = 0.005288350395858288\n",
      "\n",
      "Epoch 834\n",
      "Step 0: loss = 0.009793911129236221, recon_loss = 0.009788908064365387, kl_loss = 0.00500263087451458\n",
      "\n",
      "Epoch 835\n",
      "Step 0: loss = 0.00982423685491085, recon_loss = 0.009819038212299347, kl_loss = 0.005198350176215172\n",
      "\n",
      "Epoch 836\n",
      "Step 0: loss = 0.009912700392305851, recon_loss = 0.009905684739351273, kl_loss = 0.007015891373157501\n",
      "\n",
      "Epoch 837\n",
      "Step 0: loss = 0.010150947608053684, recon_loss = 0.010146742686629295, kl_loss = 0.0042050499469041824\n",
      "\n",
      "Epoch 838\n",
      "Step 0: loss = 0.009835583157837391, recon_loss = 0.009830474853515625, kl_loss = 0.005107969045639038\n",
      "\n",
      "Epoch 839\n",
      "Step 0: loss = 0.009966607205569744, recon_loss = 0.009961443021893501, kl_loss = 0.005164586938917637\n",
      "\n",
      "Epoch 840\n",
      "Step 0: loss = 0.00965986866503954, recon_loss = 0.009654533118009567, kl_loss = 0.005335649475455284\n",
      "\n",
      "Epoch 841\n",
      "Step 0: loss = 0.009610322304069996, recon_loss = 0.009604260325431824, kl_loss = 0.006061587482690811\n",
      "\n",
      "Epoch 842\n",
      "Step 0: loss = 0.01007845439016819, recon_loss = 0.010074369609355927, kl_loss = 0.004084601067006588\n",
      "\n",
      "Epoch 843\n",
      "Step 0: loss = 0.009823672473430634, recon_loss = 0.00981868989765644, kl_loss = 0.004982171580195427\n",
      "\n",
      "Epoch 844\n",
      "Step 0: loss = 0.010000137612223625, recon_loss = 0.009994205087423325, kl_loss = 0.005932926200330257\n",
      "\n",
      "Epoch 845\n",
      "Step 0: loss = 0.009839082136750221, recon_loss = 0.009833671152591705, kl_loss = 0.005410702899098396\n",
      "\n",
      "Epoch 846\n",
      "Step 0: loss = 0.009937037713825703, recon_loss = 0.00993126817047596, kl_loss = 0.005769999697804451\n",
      "\n",
      "Epoch 847\n",
      "Step 0: loss = 0.010142610408365726, recon_loss = 0.010136665776371956, kl_loss = 0.005944409407675266\n",
      "\n",
      "Epoch 848\n",
      "Step 0: loss = 0.00983812939375639, recon_loss = 0.009831678122282028, kl_loss = 0.006451229564845562\n",
      "\n",
      "Epoch 849\n",
      "Step 0: loss = 0.010070889256894588, recon_loss = 0.01006428711116314, kl_loss = 0.006602457724511623\n",
      "\n",
      "Epoch 850\n",
      "Step 0: loss = 0.010084932669997215, recon_loss = 0.010078798979520798, kl_loss = 0.006134055554866791\n",
      "\n",
      "Epoch 851\n",
      "Step 0: loss = 0.010079638101160526, recon_loss = 0.010073378682136536, kl_loss = 0.006259649060666561\n",
      "\n",
      "Epoch 852\n",
      "Step 0: loss = 0.010194777511060238, recon_loss = 0.010186338797211647, kl_loss = 0.008438361808657646\n",
      "\n",
      "Epoch 853\n",
      "Step 0: loss = 0.00996006466448307, recon_loss = 0.009951084852218628, kl_loss = 0.008979969657957554\n",
      "\n",
      "Epoch 854\n",
      "Step 0: loss = 0.010547536425292492, recon_loss = 0.01053994707763195, kl_loss = 0.007589318789541721\n",
      "\n",
      "Epoch 855\n",
      "Step 0: loss = 0.010019611567258835, recon_loss = 0.010011650621891022, kl_loss = 0.00796122383326292\n",
      "\n",
      "Epoch 856\n",
      "Step 0: loss = 0.010060766711831093, recon_loss = 0.01005222462117672, kl_loss = 0.008541991002857685\n",
      "\n",
      "Epoch 857\n",
      "Step 0: loss = 0.010141166858375072, recon_loss = 0.010134927928447723, kl_loss = 0.006238970905542374\n",
      "\n",
      "Epoch 858\n",
      "Step 0: loss = 0.009951790794730186, recon_loss = 0.009946642443537712, kl_loss = 0.005148664116859436\n",
      "\n",
      "Epoch 859\n",
      "Step 0: loss = 0.009786083362996578, recon_loss = 0.009780269116163254, kl_loss = 0.005813799798488617\n",
      "\n",
      "Epoch 860\n",
      "Step 0: loss = 0.010081498883664608, recon_loss = 0.010076990351080894, kl_loss = 0.004508913494646549\n",
      "\n",
      "Epoch 861\n",
      "Step 0: loss = 0.009805552661418915, recon_loss = 0.009799875319004059, kl_loss = 0.005677517503499985\n",
      "\n",
      "Epoch 862\n",
      "Step 0: loss = 0.00957733578979969, recon_loss = 0.009572498500347137, kl_loss = 0.004836981184780598\n",
      "\n",
      "Epoch 863\n",
      "Step 0: loss = 0.009952355176210403, recon_loss = 0.009947706013917923, kl_loss = 0.004649128764867783\n",
      "\n",
      "Epoch 864\n",
      "Step 0: loss = 0.009799894876778126, recon_loss = 0.009796515107154846, kl_loss = 0.0033801551908254623\n",
      "\n",
      "Epoch 865\n",
      "Step 0: loss = 0.009752998128533363, recon_loss = 0.009748464450240135, kl_loss = 0.004533299244940281\n",
      "\n",
      "Epoch 866\n",
      "Step 0: loss = 0.009907477535307407, recon_loss = 0.00990305282175541, kl_loss = 0.004424399696290493\n",
      "\n",
      "Epoch 867\n",
      "Step 0: loss = 0.010081796906888485, recon_loss = 0.010077282786369324, kl_loss = 0.004514207132160664\n",
      "\n",
      "Epoch 868\n",
      "Step 0: loss = 0.009962078183889389, recon_loss = 0.009956378489732742, kl_loss = 0.005699637345969677\n",
      "\n",
      "Epoch 869\n",
      "Step 0: loss = 0.009835336357355118, recon_loss = 0.009830757975578308, kl_loss = 0.0045782336965203285\n",
      "\n",
      "Epoch 870\n",
      "Step 0: loss = 0.00983724556863308, recon_loss = 0.009832121431827545, kl_loss = 0.005124490708112717\n",
      "\n",
      "Epoch 871\n",
      "Step 0: loss = 0.010073820129036903, recon_loss = 0.010068690404295921, kl_loss = 0.005129305645823479\n",
      "\n",
      "Epoch 872\n",
      "Step 0: loss = 0.010007777251303196, recon_loss = 0.010002950206398964, kl_loss = 0.004826623015105724\n",
      "\n",
      "Epoch 873\n",
      "Step 0: loss = 0.009562639519572258, recon_loss = 0.00955704040825367, kl_loss = 0.00559886172413826\n",
      "\n",
      "Epoch 874\n",
      "Step 0: loss = 0.009914267808198929, recon_loss = 0.009906159713864326, kl_loss = 0.008107714354991913\n",
      "\n",
      "Epoch 875\n",
      "Step 0: loss = 0.010083047673106194, recon_loss = 0.0100757647305727, kl_loss = 0.007282947190105915\n",
      "\n",
      "Epoch 876\n",
      "Step 0: loss = 0.00966868456453085, recon_loss = 0.009662579745054245, kl_loss = 0.00610481109470129\n",
      "\n",
      "Epoch 877\n",
      "Step 0: loss = 0.009697987698018551, recon_loss = 0.009692477062344551, kl_loss = 0.0055110864341259\n",
      "\n",
      "Epoch 878\n",
      "Step 0: loss = 0.010174726136028767, recon_loss = 0.010169414803385735, kl_loss = 0.005311440676450729\n",
      "\n",
      "Epoch 879\n",
      "Step 0: loss = 0.010064239613711834, recon_loss = 0.010057197883725166, kl_loss = 0.00704184640198946\n",
      "\n",
      "Epoch 880\n",
      "Step 0: loss = 0.010145479813218117, recon_loss = 0.010139727964997292, kl_loss = 0.005751810036599636\n",
      "\n",
      "Epoch 881\n",
      "Step 0: loss = 0.009973282925784588, recon_loss = 0.009966379031538963, kl_loss = 0.0069035813212394714\n",
      "\n",
      "Epoch 882\n",
      "Step 0: loss = 0.01016742642968893, recon_loss = 0.010161485522985458, kl_loss = 0.005940694361925125\n",
      "\n",
      "Epoch 883\n",
      "Step 0: loss = 0.010061653330922127, recon_loss = 0.010055102407932281, kl_loss = 0.006550712510943413\n",
      "\n",
      "Epoch 884\n",
      "Step 0: loss = 0.009885919280350208, recon_loss = 0.009880311787128448, kl_loss = 0.005607433617115021\n",
      "\n",
      "Epoch 885\n",
      "Step 0: loss = 0.009843572974205017, recon_loss = 0.009837474673986435, kl_loss = 0.006097962148487568\n",
      "\n",
      "Epoch 886\n",
      "Step 0: loss = 0.010107660666108131, recon_loss = 0.010101716965436935, kl_loss = 0.005944069474935532\n",
      "\n",
      "Epoch 887\n",
      "Step 0: loss = 0.009866506792604923, recon_loss = 0.00985790230333805, kl_loss = 0.008604525588452816\n",
      "\n",
      "Epoch 888\n",
      "Step 0: loss = 0.010014208033680916, recon_loss = 0.010008096694946289, kl_loss = 0.006111409515142441\n",
      "\n",
      "Epoch 889\n",
      "Step 0: loss = 0.009839103557169437, recon_loss = 0.009833808988332748, kl_loss = 0.0052947793155908585\n",
      "\n",
      "Epoch 890\n",
      "Step 0: loss = 0.009953971020877361, recon_loss = 0.009949306026101112, kl_loss = 0.0046650078147649765\n",
      "\n",
      "Epoch 891\n",
      "Step 0: loss = 0.010091480799019337, recon_loss = 0.010085808113217354, kl_loss = 0.0056731440126895905\n",
      "\n",
      "Epoch 892\n",
      "Step 0: loss = 0.00959906168282032, recon_loss = 0.009593725204467773, kl_loss = 0.005336103029549122\n",
      "\n",
      "Epoch 893\n",
      "Step 0: loss = 0.009887990541756153, recon_loss = 0.00988377258181572, kl_loss = 0.004217580892145634\n",
      "\n",
      "Epoch 894\n",
      "Step 0: loss = 0.010077213868498802, recon_loss = 0.010071923956274986, kl_loss = 0.005290346220135689\n",
      "\n",
      "Epoch 895\n",
      "Step 0: loss = 0.009696603752672672, recon_loss = 0.009690921753644943, kl_loss = 0.005682071670889854\n",
      "\n",
      "Epoch 896\n",
      "Step 0: loss = 0.009911002591252327, recon_loss = 0.009905990213155746, kl_loss = 0.005012398585677147\n",
      "\n",
      "Epoch 897\n",
      "Step 0: loss = 0.009407158941030502, recon_loss = 0.009402455762028694, kl_loss = 0.004703089594841003\n",
      "\n",
      "Epoch 898\n",
      "Step 0: loss = 0.009803242050111294, recon_loss = 0.009797852486371994, kl_loss = 0.005389966070652008\n",
      "\n",
      "Epoch 899\n",
      "Step 0: loss = 0.009894313290715218, recon_loss = 0.009889693930745125, kl_loss = 0.004619118757545948\n",
      "\n",
      "Epoch 900\n",
      "Step 0: loss = 0.009812301956117153, recon_loss = 0.0098078902810812, kl_loss = 0.004411671310663223\n",
      "\n",
      "Epoch 901\n",
      "Step 0: loss = 0.009683228097856045, recon_loss = 0.009678930044174194, kl_loss = 0.004297805018723011\n",
      "\n",
      "Epoch 902\n",
      "Step 0: loss = 0.01001423504203558, recon_loss = 0.01000944897532463, kl_loss = 0.004786090925335884\n",
      "\n",
      "Epoch 903\n",
      "Step 0: loss = 0.009695496410131454, recon_loss = 0.009690113365650177, kl_loss = 0.005382921546697617\n",
      "\n",
      "Epoch 904\n",
      "Step 0: loss = 0.009718903340399265, recon_loss = 0.009713998064398766, kl_loss = 0.0049056559801101685\n",
      "\n",
      "Epoch 905\n",
      "Step 0: loss = 0.009820220991969109, recon_loss = 0.009814547374844551, kl_loss = 0.005673972889780998\n",
      "\n",
      "Epoch 906\n",
      "Step 0: loss = 0.009733028709888458, recon_loss = 0.00972784124314785, kl_loss = 0.005187325179576874\n",
      "\n",
      "Epoch 907\n",
      "Step 0: loss = 0.009776906110346317, recon_loss = 0.009772267192602158, kl_loss = 0.0046392763033509254\n",
      "\n",
      "Epoch 908\n",
      "Step 0: loss = 0.009837408550083637, recon_loss = 0.009830843657255173, kl_loss = 0.006564941257238388\n",
      "\n",
      "Epoch 909\n",
      "Step 0: loss = 0.00974293239414692, recon_loss = 0.009737201035022736, kl_loss = 0.005731538869440556\n",
      "\n",
      "Epoch 910\n",
      "Step 0: loss = 0.00993289239704609, recon_loss = 0.009926730766892433, kl_loss = 0.006161470897495747\n",
      "\n",
      "Epoch 911\n",
      "Step 0: loss = 0.010108515620231628, recon_loss = 0.010103361681103706, kl_loss = 0.005154035985469818\n",
      "\n",
      "Epoch 912\n",
      "Step 0: loss = 0.00977704394608736, recon_loss = 0.009770270437002182, kl_loss = 0.0067732073366642\n",
      "\n",
      "Epoch 913\n",
      "Step 0: loss = 0.010101217776536942, recon_loss = 0.010096117854118347, kl_loss = 0.005100306123495102\n",
      "\n",
      "Epoch 914\n",
      "Step 0: loss = 0.009601016528904438, recon_loss = 0.009594403207302094, kl_loss = 0.006613101810216904\n",
      "\n",
      "Epoch 915\n",
      "Step 0: loss = 0.00969225075095892, recon_loss = 0.0096871517598629, kl_loss = 0.005098707042634487\n",
      "\n",
      "Epoch 916\n",
      "Step 0: loss = 0.009736943058669567, recon_loss = 0.009731393307447433, kl_loss = 0.005549756810069084\n",
      "\n",
      "Epoch 917\n",
      "Step 0: loss = 0.009943091310560703, recon_loss = 0.009936710819602013, kl_loss = 0.006380592472851276\n",
      "\n",
      "Epoch 918\n",
      "Step 0: loss = 0.00981542095541954, recon_loss = 0.009810272604227066, kl_loss = 0.0051481230184435844\n",
      "\n",
      "Epoch 919\n",
      "Step 0: loss = 0.009716232307255268, recon_loss = 0.009711679071187973, kl_loss = 0.0045531028881669044\n",
      "\n",
      "Epoch 920\n",
      "Step 0: loss = 0.009841942228376865, recon_loss = 0.009835857897996902, kl_loss = 0.006084509193897247\n",
      "\n",
      "Epoch 921\n",
      "Step 0: loss = 0.009816503152251244, recon_loss = 0.009811237454414368, kl_loss = 0.005265507847070694\n",
      "\n",
      "Epoch 922\n",
      "Step 0: loss = 0.010088429786264896, recon_loss = 0.010083567351102829, kl_loss = 0.004862823523581028\n",
      "\n",
      "Epoch 923\n",
      "Step 0: loss = 0.009825898334383965, recon_loss = 0.009818043559789658, kl_loss = 0.007855155505239964\n",
      "\n",
      "Epoch 924\n",
      "Step 0: loss = 0.00995640642940998, recon_loss = 0.009950635954737663, kl_loss = 0.005770698189735413\n",
      "\n",
      "Epoch 925\n",
      "Step 0: loss = 0.010115021839737892, recon_loss = 0.010109225288033485, kl_loss = 0.005796902813017368\n",
      "\n",
      "Epoch 926\n",
      "Step 0: loss = 0.010234988294541836, recon_loss = 0.010229218751192093, kl_loss = 0.005769271403551102\n",
      "\n",
      "Epoch 927\n",
      "Step 0: loss = 0.01002084743231535, recon_loss = 0.010016590356826782, kl_loss = 0.004256831482052803\n",
      "\n",
      "Epoch 928\n",
      "Step 0: loss = 0.009768814779818058, recon_loss = 0.009763246402144432, kl_loss = 0.005568061955273151\n",
      "\n",
      "Epoch 929\n",
      "Step 0: loss = 0.009687861427664757, recon_loss = 0.009682673960924149, kl_loss = 0.00518717709928751\n",
      "\n",
      "Epoch 930\n",
      "Step 0: loss = 0.010000979527831078, recon_loss = 0.00999593548476696, kl_loss = 0.005043713375926018\n",
      "\n",
      "Epoch 931\n",
      "Step 0: loss = 0.009861608035862446, recon_loss = 0.00985655002295971, kl_loss = 0.005057810805737972\n",
      "\n",
      "Epoch 932\n",
      "Step 0: loss = 0.010026328265666962, recon_loss = 0.01002114824950695, kl_loss = 0.005179946310818195\n",
      "\n",
      "Epoch 933\n",
      "Step 0: loss = 0.009537524543702602, recon_loss = 0.00953204371035099, kl_loss = 0.0054804617539048195\n",
      "\n",
      "Epoch 934\n",
      "Step 0: loss = 0.009943713434040546, recon_loss = 0.009939007461071014, kl_loss = 0.0047056591138243675\n",
      "\n",
      "Epoch 935\n",
      "Step 0: loss = 0.009869853965938091, recon_loss = 0.009863410145044327, kl_loss = 0.006444137543439865\n",
      "\n",
      "Epoch 936\n",
      "Step 0: loss = 0.009896522387862206, recon_loss = 0.009890241548418999, kl_loss = 0.006280587054789066\n",
      "\n",
      "Epoch 937\n",
      "Step 0: loss = 0.009963586926460266, recon_loss = 0.009956538677215576, kl_loss = 0.007048297673463821\n",
      "\n",
      "Epoch 938\n",
      "Step 0: loss = 0.009794052690267563, recon_loss = 0.009787686169147491, kl_loss = 0.006366947665810585\n",
      "\n",
      "Epoch 939\n",
      "Step 0: loss = 0.009908919222652912, recon_loss = 0.009903497993946075, kl_loss = 0.005421374924480915\n",
      "\n",
      "Epoch 940\n",
      "Step 0: loss = 0.009687619283795357, recon_loss = 0.009682530537247658, kl_loss = 0.005088701844215393\n",
      "\n",
      "Epoch 941\n",
      "Step 0: loss = 0.009607803076505661, recon_loss = 0.009602388367056847, kl_loss = 0.005415142513811588\n",
      "\n",
      "Epoch 942\n",
      "Step 0: loss = 0.009673379361629486, recon_loss = 0.009669123217463493, kl_loss = 0.004256280139088631\n",
      "\n",
      "Epoch 943\n",
      "Step 0: loss = 0.009566227905452251, recon_loss = 0.009561711922287941, kl_loss = 0.004515547305345535\n",
      "\n",
      "Epoch 944\n",
      "Step 0: loss = 0.00954289361834526, recon_loss = 0.009538395330309868, kl_loss = 0.004498609341681004\n",
      "\n",
      "Epoch 945\n",
      "Step 0: loss = 0.009458056651055813, recon_loss = 0.009453268721699715, kl_loss = 0.004787965677678585\n",
      "\n",
      "Epoch 946\n",
      "Step 0: loss = 0.009772269986569881, recon_loss = 0.00976967066526413, kl_loss = 0.002599448896944523\n",
      "\n",
      "Epoch 947\n",
      "Step 0: loss = 0.009944424033164978, recon_loss = 0.009940609335899353, kl_loss = 0.0038150865584611893\n",
      "\n",
      "Epoch 948\n",
      "Step 0: loss = 0.00985391903668642, recon_loss = 0.00985044240951538, kl_loss = 0.003477005288004875\n",
      "\n",
      "Epoch 949\n",
      "Step 0: loss = 0.009673220105469227, recon_loss = 0.009668253362178802, kl_loss = 0.004967180080711842\n",
      "\n",
      "Epoch 950\n",
      "Step 0: loss = 0.009602737613022327, recon_loss = 0.009598735719919205, kl_loss = 0.004001804627478123\n",
      "\n",
      "Epoch 951\n",
      "Step 0: loss = 0.009828041307628155, recon_loss = 0.009823737666010857, kl_loss = 0.004303538240492344\n",
      "\n",
      "Epoch 952\n",
      "Step 0: loss = 0.009798388928174973, recon_loss = 0.00979214534163475, kl_loss = 0.006243279203772545\n",
      "\n",
      "Epoch 953\n",
      "Step 0: loss = 0.009764212183654308, recon_loss = 0.009759536013007164, kl_loss = 0.004676308482885361\n",
      "\n",
      "Epoch 954\n",
      "Step 0: loss = 0.009574801661074162, recon_loss = 0.009569458663463593, kl_loss = 0.005342809483408928\n",
      "\n",
      "Epoch 955\n",
      "Step 0: loss = 0.009444866329431534, recon_loss = 0.009439688175916672, kl_loss = 0.005177697166800499\n",
      "\n",
      "Epoch 956\n",
      "Step 0: loss = 0.00990232452750206, recon_loss = 0.009896859526634216, kl_loss = 0.005465046502649784\n",
      "\n",
      "Epoch 957\n",
      "Step 0: loss = 0.009912462905049324, recon_loss = 0.009907415136694908, kl_loss = 0.005047844722867012\n",
      "\n",
      "Epoch 958\n",
      "Step 0: loss = 0.010128425434231758, recon_loss = 0.010121887549757957, kl_loss = 0.006537744775414467\n",
      "\n",
      "Epoch 959\n",
      "Step 0: loss = 0.009926780126988888, recon_loss = 0.009920137003064156, kl_loss = 0.006642939522862434\n",
      "\n",
      "Epoch 960\n",
      "Step 0: loss = 0.01010129228234291, recon_loss = 0.010093124583363533, kl_loss = 0.008167292922735214\n",
      "\n",
      "Epoch 961\n",
      "Step 0: loss = 0.009934451431035995, recon_loss = 0.009927300736308098, kl_loss = 0.00715028028935194\n",
      "\n",
      "Epoch 962\n",
      "Step 0: loss = 0.009864709340035915, recon_loss = 0.009857399389147758, kl_loss = 0.007310193032026291\n",
      "\n",
      "Epoch 963\n",
      "Step 0: loss = 0.009963217191398144, recon_loss = 0.009957674890756607, kl_loss = 0.005542438477277756\n",
      "\n",
      "Epoch 964\n",
      "Step 0: loss = 0.010163752362132072, recon_loss = 0.010155802592635155, kl_loss = 0.007950134575366974\n",
      "\n",
      "Epoch 965\n",
      "Step 0: loss = 0.009843841195106506, recon_loss = 0.009836699813604355, kl_loss = 0.007140974514186382\n",
      "\n",
      "Epoch 966\n",
      "Step 0: loss = 0.00989573635160923, recon_loss = 0.009890148416161537, kl_loss = 0.005587507970631123\n",
      "\n",
      "Epoch 967\n",
      "Step 0: loss = 0.0099732531234622, recon_loss = 0.009965194389224052, kl_loss = 0.008058825507760048\n",
      "\n",
      "Epoch 968\n",
      "Step 0: loss = 0.009769919328391552, recon_loss = 0.009764369577169418, kl_loss = 0.0055498480796813965\n",
      "\n",
      "Epoch 969\n",
      "Step 0: loss = 0.009883084334433079, recon_loss = 0.009878318756818771, kl_loss = 0.004765350371599197\n",
      "\n",
      "Epoch 970\n",
      "Step 0: loss = 0.009870316833257675, recon_loss = 0.009863326326012611, kl_loss = 0.006990138441324234\n",
      "\n",
      "Epoch 971\n",
      "Step 0: loss = 0.009602008387446404, recon_loss = 0.009596550837159157, kl_loss = 0.005457655526697636\n",
      "\n",
      "Epoch 972\n",
      "Step 0: loss = 0.009871309623122215, recon_loss = 0.009867008775472641, kl_loss = 0.00430062972009182\n",
      "\n",
      "Epoch 973\n",
      "Step 0: loss = 0.009572170674800873, recon_loss = 0.009567171335220337, kl_loss = 0.004999752156436443\n",
      "\n",
      "Epoch 974\n",
      "Step 0: loss = 0.009664536453783512, recon_loss = 0.009660499170422554, kl_loss = 0.004037306644022465\n",
      "\n",
      "Epoch 975\n",
      "Step 0: loss = 0.00917357113212347, recon_loss = 0.009169671684503555, kl_loss = 0.00389973446726799\n",
      "\n",
      "Epoch 976\n",
      "Step 0: loss = 0.009427604265511036, recon_loss = 0.009424181655049324, kl_loss = 0.0034225815907120705\n",
      "\n",
      "Epoch 977\n",
      "Step 0: loss = 0.009778284467756748, recon_loss = 0.009775200858712196, kl_loss = 0.003083593212068081\n",
      "\n",
      "Epoch 978\n",
      "Step 0: loss = 0.009659003466367722, recon_loss = 0.009655015543103218, kl_loss = 0.00398760661482811\n",
      "\n",
      "Epoch 979\n",
      "Step 0: loss = 0.009620822966098785, recon_loss = 0.009618226438760757, kl_loss = 0.0025961874052882195\n",
      "\n",
      "Epoch 980\n",
      "Step 0: loss = 0.009571896865963936, recon_loss = 0.009568288922309875, kl_loss = 0.0036078384146094322\n",
      "\n",
      "Epoch 981\n",
      "Step 0: loss = 0.00947900116443634, recon_loss = 0.009475333616137505, kl_loss = 0.0036673881113529205\n",
      "\n",
      "Epoch 982\n",
      "Step 0: loss = 0.009497696533799171, recon_loss = 0.009494319558143616, kl_loss = 0.003377082757651806\n",
      "\n",
      "Epoch 983\n",
      "Step 0: loss = 0.00945370364934206, recon_loss = 0.00945020280778408, kl_loss = 0.0035007940605282784\n",
      "\n",
      "Epoch 984\n",
      "Step 0: loss = 0.009576631709933281, recon_loss = 0.009572966024279594, kl_loss = 0.003665960393846035\n",
      "\n",
      "Epoch 985\n",
      "Step 0: loss = 0.009639007970690727, recon_loss = 0.009634889662265778, kl_loss = 0.004117877222597599\n",
      "\n",
      "Epoch 986\n",
      "Step 0: loss = 0.0094827301800251, recon_loss = 0.009478496387600899, kl_loss = 0.0042339228093624115\n",
      "\n",
      "Epoch 987\n",
      "Step 0: loss = 0.009415185078978539, recon_loss = 0.009411614388227463, kl_loss = 0.0035710036754608154\n",
      "\n",
      "Epoch 988\n",
      "Step 0: loss = 0.00978927407413721, recon_loss = 0.009785644710063934, kl_loss = 0.003629472106695175\n",
      "\n",
      "Epoch 989\n",
      "Step 0: loss = 0.009702691808342934, recon_loss = 0.00969821959733963, kl_loss = 0.004472676664590836\n",
      "\n",
      "Epoch 990\n",
      "Step 0: loss = 0.00970836728811264, recon_loss = 0.009703850373625755, kl_loss = 0.004517269320785999\n",
      "\n",
      "Epoch 991\n",
      "Step 0: loss = 0.010172301903367043, recon_loss = 0.01016545481979847, kl_loss = 0.0068473294377326965\n",
      "\n",
      "Epoch 992\n",
      "Step 0: loss = 0.00946445669978857, recon_loss = 0.009457707405090332, kl_loss = 0.0067496513947844505\n",
      "\n",
      "Epoch 993\n",
      "Step 0: loss = 0.010156678035855293, recon_loss = 0.010147759690880775, kl_loss = 0.008917995728552341\n",
      "\n",
      "Epoch 994\n",
      "Step 0: loss = 0.009745964780449867, recon_loss = 0.009736064821481705, kl_loss = 0.009899544529616833\n",
      "\n",
      "Epoch 995\n",
      "Step 0: loss = 0.009972607716917992, recon_loss = 0.009964153170585632, kl_loss = 0.008454682305455208\n",
      "\n",
      "Epoch 996\n",
      "Step 0: loss = 0.009746966883540154, recon_loss = 0.009737558662891388, kl_loss = 0.009408154524862766\n",
      "\n",
      "Epoch 997\n",
      "Step 0: loss = 0.009973308071494102, recon_loss = 0.009965887293219566, kl_loss = 0.007421107031404972\n",
      "\n",
      "Epoch 998\n",
      "Step 0: loss = 0.009993510320782661, recon_loss = 0.009986776858568192, kl_loss = 0.006733681075274944\n",
      "\n",
      "Epoch 999\n",
      "Step 0: loss = 0.009821579791605473, recon_loss = 0.00981508381664753, kl_loss = 0.006495553068816662\n",
      "\n",
      "Epoch 1000\n",
      "Step 0: loss = 0.009687851183116436, recon_loss = 0.009683024138212204, kl_loss = 0.004826768301427364\n",
      "\n",
      "Epoch 1001\n",
      "Step 0: loss = 0.009680059738457203, recon_loss = 0.009674273431301117, kl_loss = 0.005785980261862278\n",
      "\n",
      "Epoch 1002\n",
      "Step 0: loss = 0.009733282029628754, recon_loss = 0.009727120399475098, kl_loss = 0.00616180244833231\n",
      "\n",
      "Epoch 1003\n",
      "Step 0: loss = 0.009945867583155632, recon_loss = 0.009942090138792992, kl_loss = 0.0037778038531541824\n",
      "\n",
      "Epoch 1004\n",
      "Step 0: loss = 0.009903297759592533, recon_loss = 0.009898440912365913, kl_loss = 0.00485718809068203\n",
      "\n",
      "Epoch 1005\n",
      "Step 0: loss = 0.009661546908318996, recon_loss = 0.009655335918068886, kl_loss = 0.0062106577679514885\n",
      "\n",
      "Epoch 1006\n",
      "Step 0: loss = 0.009897757321596146, recon_loss = 0.009892964735627174, kl_loss = 0.004792683757841587\n",
      "\n",
      "Epoch 1007\n",
      "Step 0: loss = 0.009871255606412888, recon_loss = 0.009865526109933853, kl_loss = 0.005729668773710728\n",
      "\n",
      "Epoch 1008\n",
      "Step 0: loss = 0.009808513335883617, recon_loss = 0.00980445183813572, kl_loss = 0.004061250947415829\n",
      "\n",
      "Epoch 1009\n",
      "Step 0: loss = 0.009541800245642662, recon_loss = 0.0095371063798666, kl_loss = 0.004693853668868542\n",
      "\n",
      "Epoch 1010\n",
      "Step 0: loss = 0.0095281433314085, recon_loss = 0.009523406624794006, kl_loss = 0.0047370195388793945\n",
      "\n",
      "Epoch 1011\n",
      "Step 0: loss = 0.00969777349382639, recon_loss = 0.009694863110780716, kl_loss = 0.002910827286541462\n",
      "\n",
      "Epoch 1012\n",
      "Step 0: loss = 0.009368757717311382, recon_loss = 0.00936521403491497, kl_loss = 0.0035436833277344704\n",
      "\n",
      "Epoch 1013\n",
      "Step 0: loss = 0.0097359549254179, recon_loss = 0.009733231738209724, kl_loss = 0.0027229078114032745\n",
      "\n",
      "Epoch 1014\n",
      "Step 0: loss = 0.009215857833623886, recon_loss = 0.009212533012032509, kl_loss = 0.003325250931084156\n",
      "\n",
      "Epoch 1015\n",
      "Step 0: loss = 0.009617581032216549, recon_loss = 0.009614236652851105, kl_loss = 0.003344811499118805\n",
      "\n",
      "Epoch 1016\n",
      "Step 0: loss = 0.009632198140025139, recon_loss = 0.00962885096669197, kl_loss = 0.0033470643684267998\n",
      "\n",
      "Epoch 1017\n",
      "Step 0: loss = 0.009294059127569199, recon_loss = 0.009290730580687523, kl_loss = 0.0033286353573203087\n",
      "\n",
      "Epoch 1018\n",
      "Step 0: loss = 0.009403932839632034, recon_loss = 0.009400423616170883, kl_loss = 0.003509044647216797\n",
      "\n",
      "Epoch 1019\n",
      "Step 0: loss = 0.00970378890633583, recon_loss = 0.009700078517198563, kl_loss = 0.003709939308464527\n",
      "\n",
      "Epoch 1020\n",
      "Step 0: loss = 0.009609817527234554, recon_loss = 0.009606607258319855, kl_loss = 0.003209933638572693\n",
      "\n",
      "Epoch 1021\n",
      "Step 0: loss = 0.009558415971696377, recon_loss = 0.00955534540116787, kl_loss = 0.0030709635466337204\n",
      "\n",
      "Epoch 1022\n",
      "Step 0: loss = 0.009426984004676342, recon_loss = 0.009423552080988884, kl_loss = 0.003432345576584339\n",
      "\n",
      "Epoch 1023\n",
      "Step 0: loss = 0.009475206024944782, recon_loss = 0.009471770375967026, kl_loss = 0.0034360093995928764\n",
      "\n",
      "Epoch 1024\n",
      "Step 0: loss = 0.009315408766269684, recon_loss = 0.00931151956319809, kl_loss = 0.0038888761773705482\n",
      "\n",
      "Epoch 1025\n",
      "Step 0: loss = 0.009477131068706512, recon_loss = 0.00947297178208828, kl_loss = 0.00415901280939579\n",
      "\n",
      "Epoch 1026\n",
      "Step 0: loss = 0.009571071714162827, recon_loss = 0.009566381573677063, kl_loss = 0.004690008237957954\n",
      "\n",
      "Epoch 1027\n",
      "Step 0: loss = 0.009647058323025703, recon_loss = 0.009642273187637329, kl_loss = 0.0047851139679551125\n",
      "\n",
      "Epoch 1028\n",
      "Step 0: loss = 0.009587532840669155, recon_loss = 0.009583396837115288, kl_loss = 0.004136214964091778\n",
      "\n",
      "Epoch 1029\n",
      "Step 0: loss = 0.009732585400342941, recon_loss = 0.009727273136377335, kl_loss = 0.005312556400895119\n",
      "\n",
      "Epoch 1030\n",
      "Step 0: loss = 0.00971292331814766, recon_loss = 0.00970563106238842, kl_loss = 0.007291826419532299\n",
      "\n",
      "Epoch 1031\n",
      "Step 0: loss = 0.009854978881776333, recon_loss = 0.009848620742559433, kl_loss = 0.006358484737575054\n",
      "\n",
      "Epoch 1032\n",
      "Step 0: loss = 0.009714959189295769, recon_loss = 0.009707752615213394, kl_loss = 0.007206389680504799\n",
      "\n",
      "Epoch 1033\n",
      "Step 0: loss = 0.010131197050213814, recon_loss = 0.010123355314135551, kl_loss = 0.007841696962714195\n",
      "\n",
      "Epoch 1034\n",
      "Step 0: loss = 0.009946238249540329, recon_loss = 0.009939206764101982, kl_loss = 0.007031207904219627\n",
      "\n",
      "Epoch 1035\n",
      "Step 0: loss = 0.010026143863797188, recon_loss = 0.010017773136496544, kl_loss = 0.008370826952159405\n",
      "\n",
      "Epoch 1036\n",
      "Step 0: loss = 0.00999016035348177, recon_loss = 0.009982885792851448, kl_loss = 0.00727479811757803\n",
      "\n",
      "Epoch 1037\n",
      "Step 0: loss = 0.009922693483531475, recon_loss = 0.009914316236972809, kl_loss = 0.008377467282116413\n",
      "\n",
      "Epoch 1038\n",
      "Step 0: loss = 0.00987662561237812, recon_loss = 0.009869279339909554, kl_loss = 0.007346024736762047\n",
      "\n",
      "Epoch 1039\n",
      "Step 0: loss = 0.009953044354915619, recon_loss = 0.00994713045656681, kl_loss = 0.005914101377129555\n",
      "\n",
      "Epoch 1040\n",
      "Step 0: loss = 0.00979862455278635, recon_loss = 0.009792344644665718, kl_loss = 0.006279772147536278\n",
      "\n",
      "Epoch 1041\n",
      "Step 0: loss = 0.009805985726416111, recon_loss = 0.009799500927329063, kl_loss = 0.006484968587756157\n",
      "\n",
      "Epoch 1042\n",
      "Step 0: loss = 0.009644866921007633, recon_loss = 0.009640442207455635, kl_loss = 0.004424590617418289\n",
      "\n",
      "Epoch 1043\n",
      "Step 0: loss = 0.009641584008932114, recon_loss = 0.009637417271733284, kl_loss = 0.004166655242443085\n",
      "\n",
      "Epoch 1044\n",
      "Step 0: loss = 0.009638233110308647, recon_loss = 0.00963449478149414, kl_loss = 0.0037386473268270493\n",
      "\n",
      "Epoch 1045\n",
      "Step 0: loss = 0.00950962770730257, recon_loss = 0.009505515918135643, kl_loss = 0.004111625254154205\n",
      "\n",
      "Epoch 1046\n",
      "Step 0: loss = 0.009591133333742619, recon_loss = 0.009587021544575691, kl_loss = 0.0041121831163764\n",
      "\n",
      "Epoch 1047\n",
      "Step 0: loss = 0.009510730393230915, recon_loss = 0.009506432339549065, kl_loss = 0.004298165440559387\n",
      "\n",
      "Epoch 1048\n",
      "Step 0: loss = 0.009548437781631947, recon_loss = 0.009544750675559044, kl_loss = 0.0036875633522868156\n",
      "\n",
      "Epoch 1049\n",
      "Step 0: loss = 0.009185806848108768, recon_loss = 0.009181806817650795, kl_loss = 0.003999777138233185\n",
      "\n",
      "Epoch 1050\n",
      "Step 0: loss = 0.009427637793123722, recon_loss = 0.009424686431884766, kl_loss = 0.0029517170041799545\n",
      "\n",
      "Epoch 1051\n",
      "Step 0: loss = 0.009291931986808777, recon_loss = 0.009288590401411057, kl_loss = 0.003341684117913246\n",
      "\n",
      "Epoch 1052\n",
      "Step 0: loss = 0.00967409461736679, recon_loss = 0.009670069441199303, kl_loss = 0.004024732857942581\n",
      "\n",
      "Epoch 1053\n",
      "Step 0: loss = 0.009527595713734627, recon_loss = 0.009523997083306313, kl_loss = 0.0035983212292194366\n",
      "\n",
      "Epoch 1054\n",
      "Step 0: loss = 0.009486249648034573, recon_loss = 0.00948304869234562, kl_loss = 0.0032009854912757874\n",
      "\n",
      "Epoch 1055\n",
      "Step 0: loss = 0.009550157003104687, recon_loss = 0.009545234963297844, kl_loss = 0.004921945743262768\n",
      "\n",
      "Epoch 1056\n",
      "Step 0: loss = 0.00972035713493824, recon_loss = 0.009714778512716293, kl_loss = 0.0055784303694963455\n",
      "\n",
      "Epoch 1057\n",
      "Step 0: loss = 0.009619977325201035, recon_loss = 0.009616032242774963, kl_loss = 0.00394524447619915\n",
      "\n",
      "Epoch 1058\n",
      "Step 0: loss = 0.00959791336208582, recon_loss = 0.009593481197953224, kl_loss = 0.004432192072272301\n",
      "\n",
      "Epoch 1059\n",
      "Step 0: loss = 0.009451281279325485, recon_loss = 0.00944732129573822, kl_loss = 0.003959844820201397\n",
      "\n",
      "Epoch 1060\n",
      "Step 0: loss = 0.009349413216114044, recon_loss = 0.00934518314898014, kl_loss = 0.004230343736708164\n",
      "\n",
      "Epoch 1061\n",
      "Step 0: loss = 0.009680256247520447, recon_loss = 0.00967506505548954, kl_loss = 0.005191534757614136\n",
      "\n",
      "Epoch 1062\n",
      "Step 0: loss = 0.009747832082211971, recon_loss = 0.009744178503751755, kl_loss = 0.0036531537771224976\n",
      "\n",
      "Epoch 1063\n",
      "Step 0: loss = 0.009757524356245995, recon_loss = 0.009753447026014328, kl_loss = 0.004076915793120861\n",
      "\n",
      "Epoch 1064\n",
      "Step 0: loss = 0.009550310671329498, recon_loss = 0.009544527158141136, kl_loss = 0.005783135071396828\n",
      "\n",
      "Epoch 1065\n",
      "Step 0: loss = 0.009591343812644482, recon_loss = 0.009585967287421227, kl_loss = 0.005376491695642471\n",
      "\n",
      "Epoch 1066\n",
      "Step 0: loss = 0.00979550275951624, recon_loss = 0.009788932278752327, kl_loss = 0.006570769473910332\n",
      "\n",
      "Epoch 1067\n",
      "Step 0: loss = 0.0093753756955266, recon_loss = 0.009370652958750725, kl_loss = 0.004722881130874157\n",
      "\n",
      "Epoch 1068\n",
      "Step 0: loss = 0.009563169442117214, recon_loss = 0.009558459743857384, kl_loss = 0.004710007458925247\n",
      "\n",
      "Epoch 1069\n",
      "Step 0: loss = 0.009703127667307854, recon_loss = 0.00969802588224411, kl_loss = 0.0051019079983234406\n",
      "\n",
      "Epoch 1070\n",
      "Step 0: loss = 0.009655877947807312, recon_loss = 0.009651066735386848, kl_loss = 0.00481075793504715\n",
      "\n",
      "Epoch 1071\n",
      "Step 0: loss = 0.009594178758561611, recon_loss = 0.009589998051524162, kl_loss = 0.004180652089416981\n",
      "\n",
      "Epoch 1072\n",
      "Step 0: loss = 0.009492048062384129, recon_loss = 0.009488284587860107, kl_loss = 0.0037635071203112602\n",
      "\n",
      "Epoch 1073\n",
      "Step 0: loss = 0.009350249543786049, recon_loss = 0.009345818310976028, kl_loss = 0.004431557841598988\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon_loss = {loss_vals[1].numpy()}, kl_loss = {loss_vals[2].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs = np.random.randn(num_samples, d)\n",
    "  random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "  random_dirs = tf.constant(random_dirs)\n",
    "  latent_samples = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples, random_dirs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [],
   "source": [
    "drawn_betas = generate_new_betas(vae, 50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfAej5fqsfHh"
   },
   "outputs": [],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "# ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "# ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U07oDyUdsl1u"
   },
   "outputs": [],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "# external_X = tf.cast(ood_test_features, tf.float32)\n",
    "external_Y = ood_val_labels\n",
    "# external_Y = ood_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6UwhrytprJ7"
   },
   "outputs": [],
   "source": [
    "pperm = np.random.permutation(len(external_X))\n",
    "\n",
    "external_X = tf.constant(np.array(external_X)[pperm])\n",
    "external_Y = external_Y[pperm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/My Drive/ml-ood/chem/val_ood.csv', 'r') as f:\n",
    "#   external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "# with open('/content/drive/My Drive/ml-ood/chem/val_ood.csv', 'r') as f:\n",
    "#   external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "# external_X = (external_X-mu_x)/sigma_x\n",
    "external_randfeats_X = get_rand_feats(external_X, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "0bf991ee-3291-4511-829c-846c40205d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(2048,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.9500462  -0.9970952  -0.7351471   0.25615945 -0.09993453  0.05183483\n",
      " -0.5821794   0.11442215 -0.07140958  0.35098624], shape=(10,), dtype=float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "bfcd92a0-6374-45ae-8850-46540e1e8f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24599, 2048)\n",
      "(24599, 1000)\n",
      "(24599,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "0a3ffc4e-705f-447c-f5fc-3185929e811e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(10, 2048), dtype=float32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W32O3S_gKnEp"
   },
   "outputs": [],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred > 0.5), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return aggregate_preds(preds)\n",
    "\n",
    "\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "6915fcf3-4622-436b-863d-57f520e7bfe7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00610113,  0.09620966,  0.01917842, ...,  0.06529432,\n",
       "        -0.15893549, -0.9960615 ], dtype=float32),\n",
       " array([-0.04719334,  0.16918047, -0.17358743, ...,  0.02874647,\n",
       "         0.22791718, -1.57171667]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "6cb5d323-4178-435a-fd68-0c0159e117bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "9ca75ef9-3c3a-4d03-e527-7ae816f0b812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [1. 1. 1. 1. 1. 0. 1. 1. 1. 1.]\n",
      "Total Positive Preds:  2958.0\n",
      "Total Preds:  3544\n",
      "% Positive Preds:  0.8346501128668171\n",
      "\n",
      "First 10 Ground Truth:  [1 1 1 1 1 1 1 1 1 1]\n",
      "Total Positive Ground Truth:  3288\n",
      "Total Ground Truth:  3544\n",
      "% Positive Ground Truth:  0.927765237020316\n",
      "\n",
      "Accuracy:  0.7990970654627539\n"
     ]
    }
   ],
   "source": [
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", external_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(external_Y))\n",
    "print(\"Total Ground Truth: \", len(external_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(external_Y) / len(external_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == external_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "L7kl6J3wLped",
    "outputId": "d7a361d9-892f-481e-868d-9ef8e9059946"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f8c64d902b0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.6, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.5, 0.4738372093023256, 0.4956921309592188, 0.49409681227863045, 0.49305069493050696, 0.4944530357022793, 0.49494787489975944, 0.49523989958793163, 0.49640065362363645, 0.4956647398843931, 0.4954388303411505, 0.4954494914137462, 0.49574293900941463, 0.4954910841800302, 0.4957665065537735, 0.49576926206167116, 0.49581164606376055, 0.4957713263397577, 0.49579251189072726]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
