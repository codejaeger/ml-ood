{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 02:49:27.704455: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 02:49:28.679200: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-17 02:49:28.679307: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-17 02:49:28.679321: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "20637ffb-f819-4f0a-b926-d24784746a9e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkF1N4olXTtZ",
    "outputId": "de7467ba-69c5-42b4-a918-755fe87765d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "weKR7jCuMvQl"
   },
   "outputs": [],
   "source": [
    "with open('../datasets-ood/chem/train.csv', 'r') as f:\n",
    "  dataX = np.float32(np.array([line.strip().split(',')[2:] for line in f])[1:])\n",
    "\n",
    "with open('../datasets-ood/chem/train.csv', 'r') as f:\n",
    "  dataY = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "X = dataX\n",
    "Y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets-ood/chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('../datasets-ood/chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "2948628d-0085-48ba-8e9b-4bbe93f27e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5997, 1024)\n",
      "(5997,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, d, D=128):\n",
    "\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot, sigma_rot/4]\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot/2, sigma_rot/4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, X, d, D=128):\n",
    "\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot/2, sigma_rot/4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "    self.Ws = self.sample_features(X)\n",
    "    \n",
    "  def sample_features(self, X, ):\n",
    "    L = int(0.3 * len(X))\n",
    "    M = self.Ws.shape[0] * self.Ws.shape[2]\n",
    "    # N = np.random.choice(M, int(M/100), replace=False)\n",
    "    N = int(M/100)+10\n",
    "    phi_Xt = tf.transpose(self.get_features(X[np.random.choice(len(X), L)])) / np.sqrt(L)\n",
    "    phi_phi_T = phi_Xt @ tf.transpose(phi_Xt)\n",
    "    mu = np.power(10, 0)\n",
    "    diag = np.diag(phi_phi_T @ np.linalg.inv(phi_phi_T + mu))\n",
    "    diag = diag / np.sum(diag)\n",
    "    print(M, len(diag)//2, phi_Xt.shape, self.Ws.shape)\n",
    "    print(\"Diag\", M, diag, np.argsort(diag)[-N])\n",
    "    k, N, D = self.Ws.shape\n",
    "    diag = diag[:len(diag)//2] + diag[len(diag)//2:]\n",
    "    w_indices = np.unique(np.argsort(diag)[-N:])\n",
    "    _Ws = np.reshape(np.transpose(self.Ws, [1, 2, 0]), (N, -1))[:, w_indices]\n",
    "    return np.transpose(np.reshape(_Ws, (N, -1, 1)), [2, 0, 1])\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    # phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.transpose(phis, [1, 2, 0])[:, None, :, :]  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3S8unT73bEtM"
   },
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, X_ids, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  # clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  clf = SVC(random_state=0, tol=1e-5, kernel='linear').fit(X, Y)\n",
    "  # clf = LinearSVC(random_state=0, tol=1e-5).fit(X, Y)\n",
    "  support = (clf.support_, clf.n_support_)\n",
    "\n",
    "  def get_supp(support):\n",
    "      supps_, n_supps_ = support\n",
    "      supps_0 = supps_[:n_supps_[0]]\n",
    "      supps_1 = supps_[n_supps_[0]:]\n",
    "      return X_ids[supps_0], X_ids[supps_1]\n",
    "\n",
    "  support = get_supp(support)\n",
    "    \n",
    "  # clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  print(wgts.shape)\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print(np.mean(prd[:, 0]==Y))\n",
    "  return wgts, support\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  supps = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    print(X_sub.shape, X_ids.shape)\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    beta, supp = linear_coefs(get_rand_feats(X_sub@pca_projs, model), np.argwhere(X_ids), Y_sub)\n",
    "    # beta = linear_coefs(X_sub, Y_sub)\n",
    "\n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    supps.append(supp)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas, supps\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997, 1024)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-17 02:50:19.369005: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-17 02:50:19.547947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22994 MB memory:  -> device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:05:00.0, compute capability: 7.5\n",
      "2024-06-17 02:50:20.000173: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x82faa00\n"
     ]
    }
   ],
   "source": [
    "s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(8.793969, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[318.07037   232.08453   170.68028   ...   5.900153    5.7814674\n",
      "   4.8108892], shape=(1024,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(s[int(X.shape[-1]*dims[4])-1])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims = [0.05, 0.25, 0.3, 0.4, 0.8]\n",
    "dims = [0.05, 0.2, 0.35, 0.4, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5997, 1024), TensorShape([1024, 358]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, pca_projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "f3d09ad5-4e02-44a5-e001-fe83e3d99938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2860 2860 (5720, 1799) (5, 358, 572)\n",
      "Diag 2860 [ 0.00037612  0.00302426 -0.00062038 ...  0.00060356  0.00212593\n",
      " -0.00028237] 4784\n",
      "Step 0\n",
      "(3000, 1024) (5997,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m random_dirs \u001b[38;5;241m=\u001b[39m random_dirs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(random_dirs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# the percentile data increases diversity without reducing accuracy since the same max margin svm classifier is learnt for the subset data \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# as well; the subset data leads to better local generalisation\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m betas, supps \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_projs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 14\u001b[0m, in \u001b[0;36mget_models\u001b[0;34m(X, Y, pca_projs, dirs, model, percentile)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_sub\u001b[38;5;241m.\u001b[39mshape, X_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print((X_sub@pca_projs).shape)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m beta, supp \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_coefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_rand_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;129;43m@pca_projs\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# beta = linear_coefs(X_sub, Y_sub)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#X_subsets.append(X_sub)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#data_ids.append(X_ids)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Y_subsets.append(Y_sub)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m betas\u001b[38;5;241m.\u001b[39mappend(beta)\n",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m, in \u001b[0;36mlinear_coefs\u001b[0;34m(X, X_ids, Y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  X: N x d matrix of input features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m  Beta: d x 1 matrix of linear coefficients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mSVC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# clf = LinearSVC(random_state=0, tol=1e-5).fit(X, Y)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m support \u001b[38;5;241m=\u001b[39m (clf\u001b[38;5;241m.\u001b[39msupport_, clf\u001b[38;5;241m.\u001b[39mn_support_)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    319\u001b[0m (\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 1.6)\n",
    "\n",
    "N = 50    # ~ 8k\n",
    "# N = 16    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "\n",
    "# the percentile data increases diversity without reducing accuracy since the same max margin svm classifier is learnt for the subset data \n",
    "# as well; the subset data leads to better local generalisation\n",
    "betas, supps = get_models(X, Y, pca_projs, random_dirs, model, percentile=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional sampling\n",
    "# pick direction and find beta\n",
    "# find poor performing points' clusters means say at x+10, x+30 ... % data (x is what was trained on)\n",
    "# add all those cluster mean directions to the random_dir set\n",
    "# repeat \n",
    "# should improve accuracy since poor performing points get classified and also reduce number of betas required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dirs:  1\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  1\n",
      "New dirs:  10\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  11\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9916666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.997504159733777\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Total dirs:  61\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.9941763727121464\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Total dirs:  111\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9916666666666667\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Total dirs:  161\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9950041631973355\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Total dirs:  211\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9958368026644463\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  261\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9925062447960034\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.9966722129783694\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  311\n"
     ]
    }
   ],
   "source": [
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "nclusters = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "random_dirs = []\n",
    "betas, supps = [], []\n",
    "iters = 100\n",
    "max_rand_dirs = 300\n",
    "new_rand_dirs = np.random.randn(1, d)\n",
    "num_rand_dirs = 0\n",
    "for i in range(iters):\n",
    "    if num_rand_dirs > max_rand_dirs:\n",
    "        break\n",
    "    new_rand_dirs = new_rand_dirs / np.linalg.norm(new_rand_dirs, axis=1, keepdims=True)\n",
    "    new_rand_dirs = new_rand_dirs[np.random.choice(len(new_rand_dirs), size=min(len(new_rand_dirs), 50), replace=False, p=None)]\n",
    "    random_dirs.append(new_rand_dirs)\n",
    "    num_rand_dirs += len(new_rand_dirs)\n",
    "    #print(new_rand_dirs.shape)\n",
    "    print(\"New dirs: \", len(new_rand_dirs))\n",
    "    \n",
    "    _betas, _supps = get_models(X, Y, pca_projs, new_rand_dirs, model, percentile=20)\n",
    "    betas.append(_betas)\n",
    "    supps += _supps\n",
    "    _new_rand_dirs = []\n",
    "    for _dir, _beta in zip(new_rand_dirs, _betas):\n",
    "        for ix, prcnt in enumerate(range(25, 100, 20)):\n",
    "            X_sub, X_ids = project_and_filter(X, _dir, prcnt)\n",
    "            Y_sub = Y[X_ids]\n",
    "            _beta = tf.squeeze(_beta)\n",
    "            _dir = tf.constant(_dir)\n",
    "            prd = softmax(get_rand_feats(X_sub@pca_projs, model), _beta)\n",
    "\n",
    "            incrct = np.where(prd != Y_sub)\n",
    "            # print(len(prd), len(Y_sub), get_rand_feats(X_sub@pca_projs, model).shape, _beta.shape, prd)\n",
    "            incrct_X = get_rand_feats(X_sub[incrct]@pca_projs, model)\n",
    "            if len(incrct_X) < 20:\n",
    "                continue\n",
    "            kmeans = KMeans(n_clusters=nclusters[ix], random_state=0, n_init=\"auto\").fit(incrct_X)\n",
    "            mp = kmeans.cluster_centers_\n",
    "            neigh = NearestNeighbors(n_neighbors=5)\n",
    "            neigh.fit(incrct_X)\n",
    "\n",
    "            n_ind = neigh.kneighbors(mp, 3, return_distance=False)\n",
    "            nngr_id = [_n_ind[np.random.choice(3, 1)[0]] for _n_ind in n_ind]\n",
    "\n",
    "            _new_rand_dirs.append(X_sub[incrct][nngr_id] + np.random.randn(*X_sub[incrct][nngr_id].shape)*0.1)\n",
    "    new_rand_dirs = np.concatenate(_new_rand_dirs, axis=0)\n",
    "    print(\"Total dirs: \", num_rand_dirs)\n",
    "\n",
    "betas = np.concatenate(betas, axis=0)\n",
    "random_dirs = np.concatenate(random_dirs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5697, 1024)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-0.1706338411346348>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_simi(a, b):\n",
    "    return tf.losses.CosineSimilarity()(a, b)    \n",
    "\n",
    "check_simi(betas[2], betas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'betas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# np.save('random_dirs-chem2.npy', random_dirs)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# np.save('betas-chem2.npy', betas)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# np.save('Ws-chem2.npy', model.Ws)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchem-random_dirs-svm.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, random_dirs)\n\u001b[0;32m----> 6\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchem-betas-svm.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mbetas\u001b[49m)\n\u001b[1;32m      7\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchem-Ws-svm.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m.\u001b[39mWs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'betas' is not defined"
     ]
    }
   ],
   "source": [
    "# np.save('random_dirs-chem2.npy', random_dirs)\n",
    "# np.save('betas-chem2.npy', betas)\n",
    "# np.save('Ws-chem2.npy', model.Ws)\n",
    "\n",
    "np.save('chem-random_dirs-svm_large_3841f.npy', random_dirs)\n",
    "np.save('chem-betas-svm_large_3841f.npy', betas)\n",
    "np.save('chem-Ws-svm_large_3841f.npy', model.Ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open(\"chem-supps_large_3841f.npy\", \"wb\") as fp:\n",
    "#    pickle.dump(supps, fp)\n",
    "\n",
    "with open(\"chem-supps_large.npy\", \"rb\") as fp:\n",
    "    supps_b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-chem2.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./betas-chem2.npy')))\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "model.Ws = tf.constant(np.load('./Ws-chem2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "CnxEkcWwWlwn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "random_dirs = tf.constant(np.load('./chem-random_dirs-svm_large.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./chem-betas-svm_large.npy')))\n",
    "# model = define_rand_feats(X, 1.5)\n",
    "model = define_rand_feats(X, 2)\n",
    "model.Ws = tf.constant(np.load('./chem-Ws-svm_large.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BCHvO4qeupNQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mrandom_dirs1\u001b[49m, random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.allclose(random_dirs1, random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mrandom_dirs1\u001b[49m \u001b[38;5;241m-\u001b[39m random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(random_dirs1 - random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "264b9e19-74f8-4d13-ee50-bb4796f0d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3433)\n",
      "(50, 1024)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "15119544-cde9-4462-b565-7deb6dd61daf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.67359007 0.07284031 0.06704679 0.11871486 0.08134009 0.07223204\n",
      " 0.06578048 0.09957357 0.06538183 0.11517227 0.06808361 0.07230289\n",
      " 0.08044564 0.06715028 0.07314073 0.10923933 0.0676486  0.07192759\n",
      " 0.07831634 0.10833581 0.07824756 0.11328711 0.08205179 0.07256502\n",
      " 0.06170149 0.1004982  0.08952043 0.06356012 0.10328347 0.12419592\n",
      " 0.08537051 0.0694497  0.07996752 0.06575992 0.0812195  0.06487855\n",
      " 0.07011374 0.0720129  0.07330054 0.06410784 0.06708717 0.06468315\n",
      " 0.07939502 0.06358906 0.0804094  0.0927862  0.06151002 0.1032934\n",
      " 0.06179255 0.07593843 0.11523812 0.05969495 0.09308085 0.06923283\n",
      " 0.06824253 0.11062312 0.1151929  0.06778733 0.06730518 0.07203664\n",
      " 0.07125041 0.06838243 0.06803359 0.07240119 0.06192699 0.06835125\n",
      " 0.07637007 0.06941051 0.07204794 0.07405202 0.07071403 0.0716552\n",
      " 0.06128182 0.06445627 0.06350282 0.09007524 0.06571353 0.06488772\n",
      " 0.09448381 0.10456254 0.07256466 0.0826147  0.11120242 0.06792213\n",
      " 0.07659697 0.10339679 0.10981638 0.07132365 0.06307783 0.06444445\n",
      " 0.06684251 0.07105874 0.07350659 0.07283988 0.06555721 0.06875365\n",
      " 0.07771249 0.08177914 0.08985236 0.07188106 0.07675703 0.10150176\n",
      " 0.07951672 0.07137738 0.0899919  0.12333257 0.08871342 0.07753638\n",
      " 0.06669545 0.06940231 0.10745739 0.08085401 0.0738521  0.07647884\n",
      " 0.07877275 0.11535464 0.08160566 0.07936491 0.06190091 0.11871917\n",
      " 0.06864454 0.07614553 0.10851702 0.06271755 0.07709637 0.10149163\n",
      " 0.07770493 0.11035173 0.06979246 0.10631867 0.07888436 0.06787953\n",
      " 0.08423547 0.10515728 0.07582477 0.0734661  0.10395725 0.06035749\n",
      " 0.0631407  0.0806462  0.08260861 0.09184647 0.068541   0.12568466\n",
      " 0.10442394 0.07659739 0.05975658 0.07882387 0.07214825 0.07896739\n",
      " 0.06872708 0.06486798 0.07122052 0.07464958 0.06453745 0.0634111\n",
      " 0.0662826  0.06654381 0.10670793 0.06650188 0.07053742 0.06895638\n",
      " 0.08531659 0.0623953  0.0719764  0.12199535 0.07293387 0.07820626\n",
      " 0.06657765 0.06642344 0.06700614 0.07489021 0.10503389 0.08345477\n",
      " 0.07488761 0.09799307 0.07678562 0.07001266 0.08343272 0.08036478\n",
      " 0.07365696 0.11470803 0.07338435 0.06677797 0.06311477 0.11898425\n",
      " 0.06795641 0.07696223 0.06905686 0.06900609 0.10380625 0.11290169\n",
      " 0.11155094 0.1112395  0.06454098 0.11166833 0.1094071  0.12826356\n",
      " 0.06993088 0.06590311 0.06874009 0.07766671 0.07133087 0.11792594\n",
      " 0.06525787 0.08324586 0.09629141 0.06194838 0.07124203 0.11783804\n",
      " 0.06815657 0.0607546  0.06693446 0.10850154 0.07398581 0.07738857\n",
      " 0.07726125 0.07287352 0.1134871  0.07371912 0.10460267 0.07542637\n",
      " 0.06546611 0.07119475 0.07628828 0.07204596 0.07614054 0.08029243\n",
      " 0.06610101 0.08319586 0.07345918 0.06535736 0.08412251 0.10606798\n",
      " 0.08054666 0.06559355 0.06910889 0.07583357 0.07010034 0.08182172\n",
      " 0.07706366 0.10390144 0.10820366 0.06846014 0.1101813  0.06299624\n",
      " 0.0593843  0.11353021 0.06856831 0.07802539 0.07272289 0.06296969\n",
      " 0.0726189  0.07849582 0.06299832 0.11008786 0.08136675 0.07709117\n",
      " 0.0652505  0.1325067  0.08348418 0.07170477 0.07714294 0.11151411\n",
      " 0.06134501 0.12567265 0.06549819 0.08394777 0.07784765 0.06696031\n",
      " 0.07159942 0.11327072 0.07226371 0.06664225 0.07598495 0.12594664\n",
      " 0.07987921 0.12448053 0.07726267 0.06369147 0.06952551 0.10776697\n",
      " 0.07965835 0.06599504 0.12093706 0.11748523 0.07339495 0.0670207\n",
      " 0.08325976 0.06529279 0.07796387 0.05881728 0.06088162 0.06621873\n",
      " 0.07956693 0.07103588 0.0721318  0.06403643 0.09237153 0.07068583\n",
      " 0.0730793  0.11418394 0.06575713 0.09560428 0.06499788 0.08190548\n",
      " 0.11165975 0.06522763 0.08388134 0.06802737 0.06461838 0.13378638\n",
      " 0.11121587 0.06667375 0.06494507 0.07570385 0.08829263 0.06605769\n",
      " 0.06974071 0.06723919 0.06552938 0.06718214 0.08325711 0.06901438\n",
      " 0.06773014 0.07984122 0.06940914 0.06924522 0.06451015 0.0679001\n",
      " 0.06648319 0.08186894 0.06209855 0.0611996  0.08036507 0.12240631\n",
      " 0.06568438 0.08648885 0.10786134 0.06835054 0.08056951 0.10800634\n",
      " 0.11687485 0.06881811 0.06355119 0.05349949 0.06612925 0.06838357\n",
      " 0.06119686 0.07697211 0.06508836 0.0705764  0.0841587  0.07956836\n",
      " 0.08247901 0.05739133 0.08251776 0.1256077  0.07631475 0.064527\n",
      " 0.08337335 0.1601182  0.07372621 0.08418385 0.07237256 0.0672489\n",
      " 0.101327   0.06642675 0.06340661 0.07676449 0.07973438 0.11528165\n",
      " 0.07960648 0.0759026  0.06853811 0.11570762 0.06938733 0.07765978\n",
      " 0.12561249 0.0650887  0.07456394 0.09193828 0.0852762  0.10506755\n",
      " 0.06640076 0.10230254 0.0771688  0.06197063 0.08613533 0.13525301\n",
      " 0.0834248  0.07319377 0.11439297 0.0665981  0.06586734 0.07459015\n",
      " 0.07707511 0.10632927 0.06062746 0.12512862 0.10379305 0.06430525\n",
      " 0.07034892 0.08056908 0.06561404 0.08310136 0.06709763 0.06463852\n",
      " 0.06691782 0.07592535 0.06327734 0.07283729 0.06605232 0.07031086\n",
      " 0.12875961 0.06591877 0.07229704 0.07054235 0.07773231 0.06906528\n",
      " 0.07265849 0.13278246 0.07460847 0.08035734 0.06098541 0.06707917\n",
      " 0.06598712 0.06078946 0.11787558 0.08832482 0.07531717 0.13841577\n",
      " 0.07556599 0.07146349 0.08449668 0.07844524 0.07132198 0.09771668\n",
      " 0.06839068 0.06377992 0.06898036 0.10430917 0.07472968 0.05980775\n",
      " 0.06359695 0.06405768 0.10160754 0.12855025 0.11988558 0.11682202\n",
      " 0.06542892 0.10991988 0.11954437 0.11098318 0.06431843 0.06077834\n",
      " 0.0654115  0.07250371 0.07422685 0.0952617  0.06418537 0.07490838\n",
      " 0.10698372 0.06941697 0.06732365 0.13738661 0.06083894 0.07078123\n",
      " 0.07261508 0.11990347 0.07567515 0.07530812 0.06997038 0.0759028\n",
      " 0.10168026 0.07081962 0.14240684 0.07333767 0.07139324 0.0664926\n",
      " 0.0957106  0.0634167  0.06718275 0.07696012 0.06479369 0.08271429\n",
      " 0.08189362 0.06581419 0.07375805 0.13133612 0.07252054 0.06524863\n",
      " 0.06175089 0.06471248 0.07419543 0.08081585 0.08248397 0.12651499\n",
      " 0.13342273 0.07025605 0.11654938 0.0629159  0.06546607 0.12357783\n",
      " 0.07535722 0.07790079 0.07652992 0.06643886 0.07553796 0.08494396\n",
      " 0.06636504 0.11303028 0.07421495], shape=(513,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "b44e65f3-b44b-40fd-e564-63e35d43997c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.996"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "def softmax_proba(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) ) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 25)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd1 = prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0NZNorQW0vg",
    "outputId": "afcb4822-0cda-4c13-ea68-8454473bb529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5234042553191489"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[10], 25)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "perds = np.zeros((ex_X.shape[0],))\n",
    "cnt = np.zeros((ex_X.shape[0],))\n",
    "print(perds.shape)\n",
    "for sm in range(512):\n",
    "    X_sub, X_ids = project_and_filter(ex_X, random_dirs[sm], 15)\n",
    "    Y_sub = ex_Y[X_ids]\n",
    "    prd = softmax_prob(get_rand_feats(X_sub@pca_projs, model), betas[sm])\n",
    "    # print(prd.shape, X_ids, ex_X.shape)\n",
    "    perds[X_ids] += prd\n",
    "    cnt[X_ids] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.501992552056132"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zer_ids = np.argwhere(cnt!=0)\n",
    "veX = perds[zer_ids]\n",
    "veX /= cnt[zer_ids]\n",
    "((veX>0.5)==Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print((np.mean(tf.stack(perds, axis=0), axis=0)==Y_sub).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381, 1),\n",
       " array([[  19],\n",
       "        [  63],\n",
       "        [  68],\n",
       "        [ 115],\n",
       "        [ 116],\n",
       "        [ 120],\n",
       "        [ 123],\n",
       "        [ 126],\n",
       "        [ 149],\n",
       "        [ 152],\n",
       "        [ 155],\n",
       "        [ 162],\n",
       "        [ 166],\n",
       "        [ 167],\n",
       "        [ 180],\n",
       "        [ 181],\n",
       "        [ 202],\n",
       "        [ 221],\n",
       "        [ 222],\n",
       "        [ 224],\n",
       "        [ 226],\n",
       "        [ 230],\n",
       "        [ 257],\n",
       "        [ 263],\n",
       "        [ 275],\n",
       "        [ 278],\n",
       "        [ 287],\n",
       "        [ 320],\n",
       "        [ 337],\n",
       "        [ 357],\n",
       "        [ 374],\n",
       "        [ 382],\n",
       "        [ 491],\n",
       "        [ 508],\n",
       "        [ 540],\n",
       "        [ 576],\n",
       "        [ 579],\n",
       "        [ 650],\n",
       "        [ 652],\n",
       "        [ 653],\n",
       "        [ 676],\n",
       "        [ 696],\n",
       "        [ 704],\n",
       "        [ 733],\n",
       "        [ 734],\n",
       "        [ 751],\n",
       "        [ 754],\n",
       "        [ 768],\n",
       "        [ 778],\n",
       "        [ 780],\n",
       "        [ 930],\n",
       "        [ 931],\n",
       "        [ 936],\n",
       "        [ 942],\n",
       "        [ 943],\n",
       "        [ 953],\n",
       "        [ 969],\n",
       "        [ 996],\n",
       "        [1023],\n",
       "        [1041],\n",
       "        [1042],\n",
       "        [1072],\n",
       "        [1116],\n",
       "        [1122],\n",
       "        [1125],\n",
       "        [1127],\n",
       "        [1135],\n",
       "        [1137],\n",
       "        [1149],\n",
       "        [1153],\n",
       "        [1156],\n",
       "        [1159],\n",
       "        [1160],\n",
       "        [1164],\n",
       "        [1169],\n",
       "        [1170],\n",
       "        [1181],\n",
       "        [1195],\n",
       "        [1196],\n",
       "        [1220],\n",
       "        [1238],\n",
       "        [1245],\n",
       "        [1268],\n",
       "        [1269],\n",
       "        [1270],\n",
       "        [1293],\n",
       "        [1298],\n",
       "        [1328],\n",
       "        [1333],\n",
       "        [1338],\n",
       "        [1347],\n",
       "        [1348],\n",
       "        [1390],\n",
       "        [1442],\n",
       "        [1451],\n",
       "        [1456],\n",
       "        [1466],\n",
       "        [1470],\n",
       "        [1503],\n",
       "        [1523],\n",
       "        [1528],\n",
       "        [1569],\n",
       "        [1591],\n",
       "        [1605],\n",
       "        [1626],\n",
       "        [1646],\n",
       "        [1648],\n",
       "        [1649],\n",
       "        [1651],\n",
       "        [1656],\n",
       "        [1657],\n",
       "        [1674],\n",
       "        [1738],\n",
       "        [1761],\n",
       "        [1787],\n",
       "        [1805],\n",
       "        [1812],\n",
       "        [1821],\n",
       "        [1845],\n",
       "        [1846],\n",
       "        [1848],\n",
       "        [1853],\n",
       "        [1874],\n",
       "        [1879],\n",
       "        [1882],\n",
       "        [1908],\n",
       "        [1932],\n",
       "        [1958],\n",
       "        [1962],\n",
       "        [1982],\n",
       "        [1983],\n",
       "        [2012],\n",
       "        [2026],\n",
       "        [2031],\n",
       "        [2034],\n",
       "        [2071],\n",
       "        [2073],\n",
       "        [2075],\n",
       "        [2086],\n",
       "        [2100],\n",
       "        [2101],\n",
       "        [2154],\n",
       "        [2178],\n",
       "        [2183],\n",
       "        [2186],\n",
       "        [2224],\n",
       "        [2228],\n",
       "        [2236],\n",
       "        [2322],\n",
       "        [2329],\n",
       "        [2340],\n",
       "        [2343],\n",
       "        [2403],\n",
       "        [2406],\n",
       "        [2427],\n",
       "        [2448],\n",
       "        [2463],\n",
       "        [2485],\n",
       "        [2540],\n",
       "        [2541],\n",
       "        [2544],\n",
       "        [2548],\n",
       "        [2579],\n",
       "        [2609],\n",
       "        [2611],\n",
       "        [2613],\n",
       "        [2634],\n",
       "        [2636],\n",
       "        [2642],\n",
       "        [2645],\n",
       "        [2647],\n",
       "        [2649],\n",
       "        [2673],\n",
       "        [2680],\n",
       "        [2703],\n",
       "        [2756],\n",
       "        [2758],\n",
       "        [2775],\n",
       "        [2784],\n",
       "        [2785],\n",
       "        [2794],\n",
       "        [2803],\n",
       "        [2804],\n",
       "        [2814],\n",
       "        [2837],\n",
       "        [2847],\n",
       "        [2848],\n",
       "        [2849],\n",
       "        [2863],\n",
       "        [2866],\n",
       "        [2868],\n",
       "        [2881],\n",
       "        [2888],\n",
       "        [2889],\n",
       "        [2890],\n",
       "        [2904],\n",
       "        [2908],\n",
       "        [2913],\n",
       "        [2935],\n",
       "        [2936],\n",
       "        [2937],\n",
       "        [2938],\n",
       "        [2939],\n",
       "        [2944],\n",
       "        [2945],\n",
       "        [2995],\n",
       "        [2997],\n",
       "        [3008],\n",
       "        [3017],\n",
       "        [3028],\n",
       "        [3032],\n",
       "        [3038],\n",
       "        [3042],\n",
       "        [3043],\n",
       "        [3053],\n",
       "        [3057],\n",
       "        [3079],\n",
       "        [3108],\n",
       "        [3111],\n",
       "        [3115],\n",
       "        [3116],\n",
       "        [3117],\n",
       "        [3129],\n",
       "        [3131],\n",
       "        [3133],\n",
       "        [3147],\n",
       "        [3153],\n",
       "        [3154],\n",
       "        [3156],\n",
       "        [3157],\n",
       "        [3158],\n",
       "        [3165],\n",
       "        [3166],\n",
       "        [3170],\n",
       "        [3171],\n",
       "        [3172],\n",
       "        [3174],\n",
       "        [3175],\n",
       "        [3177],\n",
       "        [3184],\n",
       "        [3197],\n",
       "        [3203],\n",
       "        [3249],\n",
       "        [3266],\n",
       "        [3270],\n",
       "        [3311],\n",
       "        [3312],\n",
       "        [3313],\n",
       "        [3318],\n",
       "        [3336],\n",
       "        [3338],\n",
       "        [3361],\n",
       "        [3368],\n",
       "        [3411],\n",
       "        [3412],\n",
       "        [3416],\n",
       "        [3445],\n",
       "        [3449],\n",
       "        [3451],\n",
       "        [3462],\n",
       "        [3471],\n",
       "        [3488],\n",
       "        [3518],\n",
       "        [3554],\n",
       "        [3561],\n",
       "        [3572],\n",
       "        [3574],\n",
       "        [3576],\n",
       "        [3591],\n",
       "        [3611],\n",
       "        [3654],\n",
       "        [3658],\n",
       "        [3660],\n",
       "        [3692],\n",
       "        [3693],\n",
       "        [3700],\n",
       "        [3710],\n",
       "        [3719],\n",
       "        [3720],\n",
       "        [3721],\n",
       "        [3722],\n",
       "        [3731],\n",
       "        [3732],\n",
       "        [3757],\n",
       "        [3761],\n",
       "        [3762],\n",
       "        [3773],\n",
       "        [3796],\n",
       "        [3848],\n",
       "        [3854],\n",
       "        [3855],\n",
       "        [3858],\n",
       "        [3859],\n",
       "        [3863],\n",
       "        [3880],\n",
       "        [3883],\n",
       "        [3890],\n",
       "        [3958],\n",
       "        [3960],\n",
       "        [3987],\n",
       "        [4010],\n",
       "        [4013],\n",
       "        [4014],\n",
       "        [4028],\n",
       "        [4064],\n",
       "        [4071],\n",
       "        [4072],\n",
       "        [4073],\n",
       "        [4075],\n",
       "        [4077],\n",
       "        [4078],\n",
       "        [4079],\n",
       "        [4080],\n",
       "        [4082],\n",
       "        [4125],\n",
       "        [4142],\n",
       "        [4143],\n",
       "        [4152],\n",
       "        [4155],\n",
       "        [4197],\n",
       "        [4198],\n",
       "        [4199],\n",
       "        [4200],\n",
       "        [4255],\n",
       "        [4315],\n",
       "        [4317],\n",
       "        [4322],\n",
       "        [4324],\n",
       "        [4328],\n",
       "        [4336],\n",
       "        [4367],\n",
       "        [4369],\n",
       "        [4370],\n",
       "        [4371],\n",
       "        [4372],\n",
       "        [4384],\n",
       "        [4399],\n",
       "        [4438],\n",
       "        [4439],\n",
       "        [4455],\n",
       "        [4486],\n",
       "        [4489],\n",
       "        [4490],\n",
       "        [4493],\n",
       "        [4494],\n",
       "        [4495],\n",
       "        [4499],\n",
       "        [4522],\n",
       "        [4540],\n",
       "        [4574],\n",
       "        [4584],\n",
       "        [4588],\n",
       "        [4596],\n",
       "        [4597],\n",
       "        [4616],\n",
       "        [4635],\n",
       "        [4642],\n",
       "        [4647],\n",
       "        [4657],\n",
       "        [4659],\n",
       "        [4672],\n",
       "        [4676],\n",
       "        [4691],\n",
       "        [4705],\n",
       "        [4764],\n",
       "        [4769],\n",
       "        [4781],\n",
       "        [4800],\n",
       "        [4805],\n",
       "        [4838],\n",
       "        [4849],\n",
       "        [4855],\n",
       "        [4878],\n",
       "        [4891],\n",
       "        [4894],\n",
       "        [4896],\n",
       "        [4899],\n",
       "        [4915],\n",
       "        [4947],\n",
       "        [4951],\n",
       "        [4955],\n",
       "        [4976],\n",
       "        [5009],\n",
       "        [5010],\n",
       "        [5013],\n",
       "        [5014],\n",
       "        [5041],\n",
       "        [5055],\n",
       "        [5061],\n",
       "        [5064],\n",
       "        [5066],\n",
       "        [5067],\n",
       "        [5069],\n",
       "        [5071],\n",
       "        [5072],\n",
       "        [5085],\n",
       "        [5096],\n",
       "        [5103],\n",
       "        [5104],\n",
       "        [5111],\n",
       "        [5113],\n",
       "        [5114],\n",
       "        [5117],\n",
       "        [5121],\n",
       "        [5134],\n",
       "        [5142],\n",
       "        [5149],\n",
       "        [5161],\n",
       "        [5163],\n",
       "        [5174],\n",
       "        [5176],\n",
       "        [5191],\n",
       "        [5198],\n",
       "        [5200],\n",
       "        [5226],\n",
       "        [5238],\n",
       "        [5252],\n",
       "        [5310],\n",
       "        [5311],\n",
       "        [5314],\n",
       "        [5332],\n",
       "        [5333],\n",
       "        [5335],\n",
       "        [5345],\n",
       "        [5353],\n",
       "        [5360],\n",
       "        [5361],\n",
       "        [5363],\n",
       "        [5366],\n",
       "        [5368],\n",
       "        [5375],\n",
       "        [5378],\n",
       "        [5390],\n",
       "        [5408],\n",
       "        [5423],\n",
       "        [5436],\n",
       "        [5437],\n",
       "        [5438],\n",
       "        [5445],\n",
       "        [5446],\n",
       "        [5487],\n",
       "        [5488],\n",
       "        [5491],\n",
       "        [5505],\n",
       "        [5533],\n",
       "        [5537],\n",
       "        [5549],\n",
       "        [5553],\n",
       "        [5570],\n",
       "        [5575],\n",
       "        [5581],\n",
       "        [5583],\n",
       "        [5598],\n",
       "        [5638],\n",
       "        [5641],\n",
       "        [5646],\n",
       "        [5653],\n",
       "        [5657],\n",
       "        [5672],\n",
       "        [5753],\n",
       "        [5755],\n",
       "        [5766],\n",
       "        [5777],\n",
       "        [5778],\n",
       "        [5781],\n",
       "        [5783],\n",
       "        [5788],\n",
       "        [5791],\n",
       "        [5794],\n",
       "        [5808],\n",
       "        [5811],\n",
       "        [5836],\n",
       "        [5897],\n",
       "        [5909],\n",
       "        [5923]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supps_0, supps_1 = supps_b[sample]\n",
    "supps_0.shape, supps_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026,  0.7023512 , -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       ...,\n",
       "       [-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882,  0.91829246, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882,  0.91829246, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppvs_0 = X[supps_0.reshape((-1,))]\n",
    "suppvs_1 = X[supps_1.reshape((-1,))]\n",
    "y_supp_0 = Y[supps_0.reshape((-1,))]\n",
    "y_supp_1 = Y[supps_1.reshape((-1,))]\n",
    "suppvs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 1024)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppvs_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99999557 -0.99999864 -1.00000015 -1.00000274 -0.99999844 -1.00000071\n",
      " -1.00000048 -0.99999712 -0.99999474 -1.00000096 -0.99999895 -1.0000031\n",
      " -1.00000271 -0.99999792 -0.99999714 -0.99999876 -0.99999873 -0.99999996\n",
      " -1.00000344 -0.99999897 -0.9999988  -1.00000246 -0.99999731 -0.99999866\n",
      " -0.99999845 -0.99999905 -1.00000161 -0.9999993  -0.99999992 -1.00000107\n",
      " -1.0000001  -0.99999536 -0.99999485 -0.99999945 -1.00000243 -0.99999581\n",
      " -0.99999901 -1.00000531 -1.         -0.9999974  -0.99999841 -0.99999726\n",
      " -1.0000001  -1.00000406 -1.00000235 -1.00000085 -1.00000044 -1.00000429\n",
      " -1.00000434 -1.00000024 -1.00000118 -0.999996   -0.99999972 -0.9999999\n",
      " -1.00000058 -0.99999397 -0.99999636 -1.00000286 -1.00000797 -1.00000088\n",
      " -1.00000028 -1.00000142 -1.0000043  -0.99999972 -0.99999621 -1.00000086\n",
      " -1.00000302 -1.00000322 -1.00000138 -1.00000425 -0.99999989 -1.0000025\n",
      " -1.00000054 -0.99999991 -0.9999996  -0.99999743 -0.99999847 -1.00000061\n",
      " -1.00000114 -1.00000165 -1.00000441 -0.99999933 -0.99999731 -0.99999625\n",
      " -1.0000037  -1.00000287 -0.99999944 -0.99999899 -1.0000044  -1.0000004\n",
      " -0.99999581 -1.00000248 -1.0000064  -0.99999798 -0.99999999 -0.99999881\n",
      " -0.99999808 -1.00000072 -1.00000203 -0.99999953 -0.99999995 -0.99999593\n",
      " -1.00000357 -1.00000529 -0.99999768 -1.00000481 -1.00000473 -0.99999755\n",
      " -0.99999789  0.89253063 -0.99999876 -1.00000252 -0.99999586 -1.00000442\n",
      " -0.99999629 -1.00000115 -0.9999997  -0.99999353 -1.00000014 -0.99999954\n",
      " -1.00000217 -0.9999988  -0.99999832 -0.99999735 -1.00000248 -0.99999779\n",
      " -1.00000045 -0.99999884 -1.00000063 -0.99999683 -0.9999978  -0.99999795\n",
      " -0.99999801 -1.00000365 -0.99999959 -0.99999692 -0.99999894 -1.00000214\n",
      " -0.99999843 -0.99999089 -0.99999655 -0.9999992  -1.00000184 -0.99999908\n",
      " -1.00000461 -0.99999957 -1.00000653 -1.00000206 -1.00000463 -0.99999801\n",
      " -0.99999928 -1.00000223 -0.9999973  -0.99999701 -0.9999971  -0.99999629\n",
      " -1.00000057 -1.00000184 -1.00000487 -0.99999784 -1.00000173 -1.00000592\n",
      " -1.00000484 -0.99999972 -0.9999973  -0.99999867 -0.99999415 -0.99999746\n",
      " -0.99999313 -1.0000024  -0.99999341 -0.99999853 -0.99999876 -0.99999852\n",
      " -1.00000494 -0.99999825 -1.00000011 -0.99999596 -0.9999956  -1.00000199\n",
      " -0.99999946 -0.99999383 -1.0000001  -0.99999952 -0.9999991  -1.00000252\n",
      " -0.999998   -1.00000384 -1.00000118 -0.99999517 -0.99999922 -1.0000009\n",
      " -0.99999826 -0.99999685 -0.99999451 -1.00000002 -0.99999946 -0.99999747\n",
      " -0.99999298 -0.99999739 -1.00000293 -1.00000114 -1.00000154 -0.99999784\n",
      " -0.99999468 -0.99999999 -0.99999775 -1.00000061 -1.00000053 -1.00000053\n",
      " -0.99999498 -0.99999889 -1.00000104 -0.99999952 -0.99999912 -1.0000007\n",
      " -1.00000199 -0.99999876 -1.00000152 -1.0000017  -0.99999833 -1.0000039\n",
      " -0.99999812 -1.00000013 -1.00000347 -0.99999859 -0.99999663 -0.99999868\n",
      " -1.0000014  -1.00000149 -1.00000252 -0.99999527 -0.99999682 -1.00000403\n",
      " -1.00000005 -0.9999948  -0.99999384 -0.99999698 -1.00000233 -0.99999743\n",
      " -0.99999908 -1.00000696 -0.99999999 -0.99999722 -0.9999993  -1.00000078\n",
      " -1.00000247 -0.99999992 -0.99999872  0.36856059 -0.99999907 -0.9999998\n",
      " -0.99999839 -0.99999575 -0.99999872 -1.00000171 -0.99999992 -0.99999876\n",
      " -1.00000106 -0.99999893 -0.99999619 -1.00000254 -0.17124262 -0.99999992\n",
      " -0.99999897 -1.00000023 -1.00000021 -0.99999993 -1.00000322 -0.99999314\n",
      " -1.00000368 -1.00000043 -0.99999718 -1.00000063 -0.99999859 -0.99999798\n",
      " -0.99999768 -0.9999984  -1.00000031 -0.99999497 -1.00000271 -1.\n",
      " -0.9999989  -1.00000092 -1.00000034 -0.99999778 -0.99999774 -0.99999749\n",
      " -1.00000044 -0.99999659 -0.99999723 -1.00000046 -1.00000401 -1.00000262\n",
      " -0.99999467 -0.99999473 -1.00000239 -0.99999617 -0.99999812 -1.00000387\n",
      " -1.00000201 -1.00000031 -0.99999892 -0.99999793 -1.00000155 -0.99999784\n",
      " -1.00000221 -0.99999727 -1.00000276 -1.00000142 -0.9999993  -0.9707472\n",
      " -1.00000239 -1.00000272 -0.9999995  -1.00000308 -0.99999798 -1.00000052\n",
      " -1.00000039 -0.99999548 -0.9999959  -1.00000357 -1.00000262 -0.9999942\n",
      " -0.99999939 -0.99999865 -0.99999932 -1.00000143 -0.99999898 -0.99999929\n",
      " -1.00000118 -1.00000508 -0.99999844 -0.99999526 -0.9999998  -0.9999979\n",
      " -0.99999888 -1.00000198 -1.00000279 -1.00000012 -1.00000345 -1.00000023\n",
      " -0.9999979  -1.00000409 -1.00000257 -0.99999924 -0.99999843 -0.99999737\n",
      " -0.99999582 -0.99999584 -0.99999923 -0.99999941 -1.00000319 -0.99999846\n",
      " -1.00000199 -0.99999927 -0.99999965 -1.00000136 -1.00000332 -0.99999471\n",
      " -1.00000336 -0.99999931 -1.00000327 -1.00000506 -0.15631103 -1.00000319\n",
      " -1.00000222 -0.99999952 -0.99999703 -0.99999816 -0.99999448 -0.99999711\n",
      " -1.0000037  -0.99999945  0.47949874 -1.000002   -1.00000096 -1.00000417\n",
      " -1.00000333 -0.99999964 -1.0000007 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "tf.Tensor(0.013705432773427331, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def get_margin(X, wgts):\n",
    "  sd = (np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T) * 1.0\n",
    "  return sd[:]\n",
    "\n",
    "def loss_2(labels, margins):\n",
    "    l = 2*labels - 1\n",
    "    return tf.reduce_sum(tf.reduce_mean(tf.nn.relu(-margins * l), axis=0))\n",
    "\n",
    "get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model).shape, betas[sample].shape\n",
    "print(get_margin(get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model), betas[sample]), y_supp_0)\n",
    "# loss_2 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "print(loss_2(y_supp_0[:len(suppvs_0)], get_margin(get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model), betas[sample]*3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "# beta_input = layers.Input(shape=(beta_dim,))\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([dir_input])\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(latent_dim, activation=tf.nn.elu)(x)\n",
    "# z_mean = layers.Dense(latent_dim)(x)\n",
    "# z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "# z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "z = layers.Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "# decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs])\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([dir_input], z, name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([dir_input])])\n",
    "vae = models.Model([dir_input], outputs, name=\"autoenc\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "beta_input = layers.Input(shape=(beta_dim,))\n",
    "beta_x = layers.Dense(256, activation=tf.nn.elu)(beta_input)\n",
    "\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([dir_input])\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(latent_dim, activation=tf.nn.elu)(x)\n",
    "# z_mean = layers.Dense(latent_dim)(x)\n",
    "# z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "# z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "z = layers.Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "# decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs])\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([dir_input], z, name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([dir_input])])\n",
    "vae = models.Model([dir_input], outputs, name=\"autoenc\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin_batched(supp_vs, wgts):\n",
    "  sd = (tf.concat([tf.ones((*supp_vs.shape[:-1], 1)), supp_vs], axis=-1) @ wgts[:, :, None]) * 1.0\n",
    "  return sd[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = vae((betas[:1], random_dirs[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "#def vae_loss(inputs, outputs, z_mean, z_log_var, reg=0.002):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "#  intercp_loss = tf.reduce_mean(tf.abs(tf.cast(inputs[:, :1], dtype=tf.float32) - outputs[:, :1]))\n",
    "#  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs[:, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs[:, 1:], axis=-1)[0], axis=-1))\n",
    "#  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "#  total_loss = recon_loss + intercp_loss + reg * kl_loss\n",
    "#  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, supp_vs, supp_ys, reg=1.0, regr=1.0):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs, dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs, axis=-1)[0], axis=-1))\n",
    "  # print(supp_vs.shape, outputs.shape)\n",
    "  supp_margins = get_margin_batched(supp_vs, outputs)\n",
    "  supp_loss = loss_2(supp_ys, supp_margins)\n",
    "  # print(supp_ys[0][30:], supp_margins[0][30:])\n",
    "  # print(supp_margins.shape, supp_ys.shape)\n",
    "  # kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = reg * recon_loss + regr * supp_loss\n",
    "  return total_loss, reg*recon_loss, regr*supp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.1964941>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0429045>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09860455>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=27.492508>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_loss(betas[:1], ots[:1], tf.ones((1, 32)), tf.ones((1, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def train_step(model, inputs, dir_inputs, supp_vs, supp_ys, alpha, ralpha=1.0):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z = model.encoder([dir_inputs])\n",
    "    outputs = model.decoder([z])\n",
    "    total_loss, recon_loss, supp_loss = vae_loss(inputs, outputs, supp_vs, supp_ys, alpha, ralpha)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, supp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:41:59.480502: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1576b9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-29 19:41:59.480542: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-04-29 19:41:59.486897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-29 19:41:59.594610: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-29 19:41:59.674717: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fb840184ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fb840184ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0526553>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.999847>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.051767856>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.52023363>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(vae, betas[:32], random_dirs[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(381, 204), dtype=float32, numpy=\n",
       "array([[-9.73291   , -0.08359981,  4.8285213 , ..., -0.418212  ,\n",
       "         0.04979858, -0.38527712],\n",
       "       [-1.4835713 ,  1.0568206 ,  2.5018466 , ..., -0.06100272,\n",
       "         0.1693849 , -0.38147366],\n",
       "       [-7.0123835 , -9.252638  , -1.9807861 , ...,  0.136215  ,\n",
       "         0.12370304,  0.3105353 ],\n",
       "       ...,\n",
       "       [-4.699366  ,  2.6286366 , -1.7926993 , ..., -0.3052967 ,\n",
       "         0.18089612, -0.36616707],\n",
       "       [-1.7197244 ,  1.9622226 ,  2.9822083 , ..., -0.33606544,\n",
       "        -0.01879999,  0.30114657],\n",
       "       [-1.7413275 ,  1.9585967 ,  3.080233  , ..., -0.47907415,\n",
       "         0.07559836,  0.45271814]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[supps_0.reshape((-1,))] @ pca_projs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(X, betas, dirs, supps, batch_size, support_batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  supps_ = [supps[i] for i in indices]\n",
    "  supps_ = np.array([np.stack([i[np.random.choice(len(i), support_batch_size)],\n",
    "                     j[np.random.choice(len(j), support_batch_size)]], axis=0) for i, j in supps_])\n",
    "  def supp_get_vs(_supps):\n",
    "      supps_0, supps_1 = np.transpose(_supps, (1, 0, 2, 3))\n",
    "      suppvs_0 = X[supps_0.reshape((-1,))] @ pca_projs\n",
    "      suppvs_1 = X[supps_1.reshape((-1,))] @ pca_projs\n",
    "      suppvs_0 = tf.reshape(get_rand_feats(tf.cast(suppvs_0, dtype=tf.float32), model), (batch_size, support_batch_size, -1))\n",
    "      suppvs_1 = tf.reshape(get_rand_feats(tf.cast(suppvs_1, dtype=tf.float32), model), (batch_size, support_batch_size, -1))\n",
    "      y_supp_0 = Y[supps_0.reshape((-1,))].reshape((batch_size, support_batch_size, -1))\n",
    "      y_supp_1 = Y[supps_1.reshape((-1,))].reshape((batch_size, support_batch_size, -1))\n",
    "      return np.concatenate([suppvs_0, suppvs_1], axis=1), np.concatenate([y_supp_0, y_supp_1], axis=1)\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield betas[i:i+batch_size], dirs[i:i+batch_size], *supp_get_vs(supps_[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch at 0x7fb77c7be890>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch(betas, random_dirs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "a6e1ccc1-2cd0-4549-e61a-6dae8aa1062a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 1.0042555332183838, recon loss = 1.0042555332183838, supp loss = 0.0\n",
      "Step 0: loss = 0.6852933168411255, recon loss = 0.6852933168411255, supp loss = 0.0\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.5890307426452637, recon loss = 0.5890307426452637, supp loss = 0.0\n",
      "Step 0: loss = 0.5855118036270142, recon loss = 0.5855118036270142, supp loss = 0.0\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.5907216668128967, recon loss = 0.5907216668128967, supp loss = 0.0\n",
      "Step 0: loss = 0.5872777700424194, recon loss = 0.5872777700424194, supp loss = 0.0\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.5776413083076477, recon loss = 0.5776413083076477, supp loss = 0.0\n",
      "Step 0: loss = 0.5796323418617249, recon loss = 0.5796323418617249, supp loss = 0.0\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.5833013653755188, recon loss = 0.5833013653755188, supp loss = 0.0\n",
      "Step 0: loss = 0.5857218503952026, recon loss = 0.5857218503952026, supp loss = 0.0\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.578827440738678, recon loss = 0.578827440738678, supp loss = 0.0\n",
      "Step 0: loss = 0.5807657241821289, recon loss = 0.5807657241821289, supp loss = 0.0\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.5597935318946838, recon loss = 0.5597935318946838, supp loss = 0.0\n",
      "Step 0: loss = 0.5642254948616028, recon loss = 0.5642254948616028, supp loss = 0.0\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.5692940950393677, recon loss = 0.5692940950393677, supp loss = 0.0\n",
      "Step 0: loss = 0.5721639394760132, recon loss = 0.5721639394760132, supp loss = 0.0\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.5468140244483948, recon loss = 0.5468140244483948, supp loss = 0.0\n",
      "Step 0: loss = 0.5705466270446777, recon loss = 0.5705466270446777, supp loss = 0.0\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.5546731352806091, recon loss = 0.5546731352806091, supp loss = 0.0\n",
      "Step 0: loss = 0.5715173482894897, recon loss = 0.5715173482894897, supp loss = 0.0\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.5451689958572388, recon loss = 0.5451689958572388, supp loss = 0.0\n",
      "Step 0: loss = 0.5563899278640747, recon loss = 0.5563899278640747, supp loss = 0.0\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.5485641956329346, recon loss = 0.5485641956329346, supp loss = 0.0\n",
      "Step 0: loss = 0.5772020816802979, recon loss = 0.5772020816802979, supp loss = 0.0\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.5570698380470276, recon loss = 0.5570698380470276, supp loss = 0.0\n",
      "Step 0: loss = 0.5731750726699829, recon loss = 0.5731750726699829, supp loss = 0.0\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.5487866401672363, recon loss = 0.5487866401672363, supp loss = 0.0\n",
      "Step 0: loss = 0.5671803951263428, recon loss = 0.5671803951263428, supp loss = 0.0\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.5453540086746216, recon loss = 0.5453540086746216, supp loss = 0.0\n",
      "Step 0: loss = 0.5705669522285461, recon loss = 0.5705669522285461, supp loss = 0.0\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.5264188051223755, recon loss = 0.5264188051223755, supp loss = 0.0\n",
      "Step 0: loss = 0.557187557220459, recon loss = 0.557187557220459, supp loss = 0.0\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.5639516711235046, recon loss = 0.5639516711235046, supp loss = 0.0\n",
      "Step 0: loss = 0.5667285919189453, recon loss = 0.5667285919189453, supp loss = 0.0\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.5454390048980713, recon loss = 0.5454390048980713, supp loss = 0.0\n",
      "Step 0: loss = 0.5714215636253357, recon loss = 0.5714215636253357, supp loss = 0.0\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.5128263235092163, recon loss = 0.5128263235092163, supp loss = 0.0\n",
      "Step 0: loss = 0.5413665771484375, recon loss = 0.5413665771484375, supp loss = 0.0\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.517399787902832, recon loss = 0.517399787902832, supp loss = 0.0\n",
      "Step 0: loss = 0.5611250400543213, recon loss = 0.5611250400543213, supp loss = 0.0\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.521978497505188, recon loss = 0.521978497505188, supp loss = 0.0\n",
      "Step 0: loss = 0.5696579217910767, recon loss = 0.5696579217910767, supp loss = 0.0\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.5147208571434021, recon loss = 0.5147208571434021, supp loss = 0.0\n",
      "Step 0: loss = 0.5676119327545166, recon loss = 0.5676119327545166, supp loss = 0.0\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.5065177083015442, recon loss = 0.5065177083015442, supp loss = 0.0\n",
      "Step 0: loss = 0.5380655527114868, recon loss = 0.5380655527114868, supp loss = 0.0\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.4950907230377197, recon loss = 0.4950907230377197, supp loss = 0.0\n",
      "Step 0: loss = 0.5465813279151917, recon loss = 0.5465813279151917, supp loss = 0.0\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.5096533298492432, recon loss = 0.5096533298492432, supp loss = 0.0\n",
      "Step 0: loss = 0.5566259622573853, recon loss = 0.5566259622573853, supp loss = 0.0\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.5155072212219238, recon loss = 0.5155072212219238, supp loss = 0.0\n",
      "Step 0: loss = 0.5622957944869995, recon loss = 0.5622957944869995, supp loss = 0.0\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.48227232694625854, recon loss = 0.48227232694625854, supp loss = 0.0\n",
      "Step 0: loss = 0.5682346820831299, recon loss = 0.5682346820831299, supp loss = 0.0\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.48908868432044983, recon loss = 0.48908868432044983, supp loss = 0.0\n",
      "Step 0: loss = 0.5502667427062988, recon loss = 0.5502667427062988, supp loss = 0.0\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.5043078064918518, recon loss = 0.5043078064918518, supp loss = 0.0\n",
      "Step 0: loss = 0.5549970269203186, recon loss = 0.5549970269203186, supp loss = 0.0\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.521740198135376, recon loss = 0.521740198135376, supp loss = 0.0\n",
      "Step 0: loss = 0.5657455325126648, recon loss = 0.5657455325126648, supp loss = 0.0\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.4818391799926758, recon loss = 0.4818391799926758, supp loss = 0.0\n",
      "Step 0: loss = 0.5580413341522217, recon loss = 0.5580413341522217, supp loss = 0.0\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.48943209648132324, recon loss = 0.48943209648132324, supp loss = 0.0\n",
      "Step 0: loss = 0.5493830442428589, recon loss = 0.5493830442428589, supp loss = 0.0\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.4818977117538452, recon loss = 0.4818977117538452, supp loss = 0.0\n",
      "Step 0: loss = 0.5493513345718384, recon loss = 0.5493513345718384, supp loss = 0.0\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.49236005544662476, recon loss = 0.49236005544662476, supp loss = 0.0\n",
      "Step 0: loss = 0.5498393774032593, recon loss = 0.5498393774032593, supp loss = 0.0\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.4638493061065674, recon loss = 0.4638493061065674, supp loss = 0.0\n",
      "Step 0: loss = 0.567531943321228, recon loss = 0.567531943321228, supp loss = 0.0\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.48951321840286255, recon loss = 0.48951321840286255, supp loss = 0.0\n",
      "Step 0: loss = 0.5650511980056763, recon loss = 0.5650511980056763, supp loss = 0.0\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.46689867973327637, recon loss = 0.46689867973327637, supp loss = 0.0\n",
      "Step 0: loss = 0.5684894323348999, recon loss = 0.5684894323348999, supp loss = 0.0\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.47684648633003235, recon loss = 0.47684648633003235, supp loss = 0.0\n",
      "Step 0: loss = 0.5434831976890564, recon loss = 0.5434831976890564, supp loss = 0.0\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.45619937777519226, recon loss = 0.45619937777519226, supp loss = 0.0\n",
      "Step 0: loss = 0.5503052473068237, recon loss = 0.5503052473068237, supp loss = 0.0\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.47440552711486816, recon loss = 0.47440552711486816, supp loss = 0.0\n",
      "Step 0: loss = 0.5584657192230225, recon loss = 0.5584657192230225, supp loss = 0.0\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.4625561237335205, recon loss = 0.4625561237335205, supp loss = 0.0\n",
      "Step 0: loss = 0.5683376789093018, recon loss = 0.5683376789093018, supp loss = 0.0\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.4897136688232422, recon loss = 0.4897136688232422, supp loss = 0.0\n",
      "Step 0: loss = 0.5670008063316345, recon loss = 0.5670008063316345, supp loss = 0.0\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.47720199823379517, recon loss = 0.47720199823379517, supp loss = 0.0\n",
      "Step 0: loss = 0.5529204607009888, recon loss = 0.5529204607009888, supp loss = 0.0\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.4630039930343628, recon loss = 0.4630039930343628, supp loss = 0.0\n",
      "Step 0: loss = 0.5483227968215942, recon loss = 0.5483227968215942, supp loss = 0.0\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.47808122634887695, recon loss = 0.47808122634887695, supp loss = 0.0\n",
      "Step 0: loss = 0.5663935542106628, recon loss = 0.5663935542106628, supp loss = 0.0\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.45988166332244873, recon loss = 0.45988166332244873, supp loss = 0.0\n",
      "Step 0: loss = 0.566714882850647, recon loss = 0.566714882850647, supp loss = 0.0\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.4706369638442993, recon loss = 0.4706369638442993, supp loss = 0.0\n",
      "Step 0: loss = 0.5406227111816406, recon loss = 0.5406227111816406, supp loss = 0.0\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.45513948798179626, recon loss = 0.45513948798179626, supp loss = 0.0\n",
      "Step 0: loss = 0.5411928296089172, recon loss = 0.5411928296089172, supp loss = 0.0\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.46712544560432434, recon loss = 0.46712544560432434, supp loss = 0.0\n",
      "Step 0: loss = 0.5493159294128418, recon loss = 0.5493159294128418, supp loss = 0.0\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.46600422263145447, recon loss = 0.46600422263145447, supp loss = 0.0\n",
      "Step 0: loss = 0.5580888986587524, recon loss = 0.5580888986587524, supp loss = 0.0\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.4446013271808624, recon loss = 0.4446013271808624, supp loss = 0.0\n",
      "Step 0: loss = 0.5464833974838257, recon loss = 0.5464833974838257, supp loss = 0.0\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.44230860471725464, recon loss = 0.44230860471725464, supp loss = 0.0\n",
      "Step 0: loss = 0.5373497605323792, recon loss = 0.5373497605323792, supp loss = 0.0\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.4466401934623718, recon loss = 0.4466401934623718, supp loss = 0.0\n",
      "Step 0: loss = 0.5521135330200195, recon loss = 0.5521135330200195, supp loss = 0.0\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.47584784030914307, recon loss = 0.47584784030914307, supp loss = 0.0\n",
      "Step 0: loss = 0.5655933618545532, recon loss = 0.5655933618545532, supp loss = 0.0\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.46929383277893066, recon loss = 0.46929383277893066, supp loss = 0.0\n",
      "Step 0: loss = 0.5444082021713257, recon loss = 0.5444082021713257, supp loss = 0.0\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.4622483551502228, recon loss = 0.4622483551502228, supp loss = 0.0\n",
      "Step 0: loss = 0.5622776746749878, recon loss = 0.5622776746749878, supp loss = 0.0\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.45162200927734375, recon loss = 0.45162200927734375, supp loss = 0.0\n",
      "Step 0: loss = 0.5461131930351257, recon loss = 0.5461131930351257, supp loss = 0.0\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.46431678533554077, recon loss = 0.46431678533554077, supp loss = 0.0\n",
      "Step 0: loss = 0.5579123497009277, recon loss = 0.5579123497009277, supp loss = 0.0\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.4446917772293091, recon loss = 0.4446917772293091, supp loss = 0.0\n",
      "Step 0: loss = 0.5347268581390381, recon loss = 0.5347268581390381, supp loss = 0.0\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.4457235634326935, recon loss = 0.4457235634326935, supp loss = 0.0\n",
      "Step 0: loss = 0.5408508777618408, recon loss = 0.5408508777618408, supp loss = 0.0\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.46020305156707764, recon loss = 0.46020305156707764, supp loss = 0.0\n",
      "Step 0: loss = 0.548920750617981, recon loss = 0.548920750617981, supp loss = 0.0\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.4542720317840576, recon loss = 0.4542720317840576, supp loss = 0.0\n",
      "Step 0: loss = 0.560981810092926, recon loss = 0.560981810092926, supp loss = 0.0\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.4310118556022644, recon loss = 0.4310118556022644, supp loss = 0.0\n",
      "Step 0: loss = 0.5432827472686768, recon loss = 0.5432827472686768, supp loss = 0.0\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.43623900413513184, recon loss = 0.43623900413513184, supp loss = 0.0\n",
      "Step 0: loss = 0.5384852886199951, recon loss = 0.5384852886199951, supp loss = 0.0\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.4457632601261139, recon loss = 0.4457632601261139, supp loss = 0.0\n",
      "Step 0: loss = 0.5330519080162048, recon loss = 0.5330519080162048, supp loss = 0.0\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.4425084888935089, recon loss = 0.4425084888935089, supp loss = 0.0\n",
      "Step 0: loss = 0.5482263565063477, recon loss = 0.5482263565063477, supp loss = 0.0\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.4707346558570862, recon loss = 0.4707346558570862, supp loss = 0.0\n",
      "Step 0: loss = 0.5734330415725708, recon loss = 0.5734330415725708, supp loss = 0.0\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.47442054748535156, recon loss = 0.47442054748535156, supp loss = 0.0\n",
      "Step 0: loss = 0.558100700378418, recon loss = 0.558100700378418, supp loss = 0.0\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.4619362950325012, recon loss = 0.4619362950325012, supp loss = 0.0\n",
      "Step 0: loss = 0.5564182996749878, recon loss = 0.5564182996749878, supp loss = 0.0\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.44840800762176514, recon loss = 0.44840800762176514, supp loss = 0.0\n",
      "Step 0: loss = 0.5610336065292358, recon loss = 0.5610336065292358, supp loss = 0.0\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.42382922768592834, recon loss = 0.42382922768592834, supp loss = 0.0\n",
      "Step 0: loss = 0.5214217901229858, recon loss = 0.5214217901229858, supp loss = 0.0\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.4437015652656555, recon loss = 0.4437015652656555, supp loss = 0.0\n",
      "Step 0: loss = 0.5528862476348877, recon loss = 0.5528862476348877, supp loss = 0.0\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.4311370849609375, recon loss = 0.4311370849609375, supp loss = 0.0\n",
      "Step 0: loss = 0.5493203401565552, recon loss = 0.5493203401565552, supp loss = 0.0\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.44969213008880615, recon loss = 0.44969213008880615, supp loss = 0.0\n",
      "Step 0: loss = 0.5668150186538696, recon loss = 0.5668150186538696, supp loss = 0.0\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.4622461199760437, recon loss = 0.4622461199760437, supp loss = 0.0\n",
      "Step 0: loss = 0.5587430596351624, recon loss = 0.5587430596351624, supp loss = 0.0\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.4483047127723694, recon loss = 0.4483047127723694, supp loss = 0.0\n",
      "Step 0: loss = 0.5616792440414429, recon loss = 0.5616792440414429, supp loss = 0.0\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.42373940348625183, recon loss = 0.42373940348625183, supp loss = 0.0\n",
      "Step 0: loss = 0.559605598449707, recon loss = 0.559605598449707, supp loss = 0.0\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.437059223651886, recon loss = 0.437059223651886, supp loss = 0.0\n",
      "Step 0: loss = 0.5714961290359497, recon loss = 0.5714961290359497, supp loss = 0.0\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.43524158000946045, recon loss = 0.43524158000946045, supp loss = 0.0\n",
      "Step 0: loss = 0.5425041913986206, recon loss = 0.5425041913986206, supp loss = 0.0\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.44529491662979126, recon loss = 0.44529491662979126, supp loss = 0.0\n",
      "Step 0: loss = 0.5788650512695312, recon loss = 0.5788650512695312, supp loss = 0.0\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.445456862449646, recon loss = 0.445456862449646, supp loss = 0.0\n",
      "Step 0: loss = 0.5644669532775879, recon loss = 0.5644669532775879, supp loss = 0.0\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.4512329697608948, recon loss = 0.4512329697608948, supp loss = 0.0\n",
      "Step 0: loss = 0.5237018465995789, recon loss = 0.5237018465995789, supp loss = 0.0\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.3954017758369446, recon loss = 0.3954017758369446, supp loss = 0.0\n",
      "Step 0: loss = 0.5588990449905396, recon loss = 0.5588990449905396, supp loss = 0.0\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.42990320920944214, recon loss = 0.42990320920944214, supp loss = 0.0\n",
      "Step 0: loss = 0.5325595736503601, recon loss = 0.5325595736503601, supp loss = 0.0\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.4180629849433899, recon loss = 0.4180629849433899, supp loss = 0.0\n",
      "Step 0: loss = 0.5420719385147095, recon loss = 0.5420719385147095, supp loss = 0.0\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.4368266761302948, recon loss = 0.4368266761302948, supp loss = 0.0\n",
      "Step 0: loss = 0.5459205508232117, recon loss = 0.5459205508232117, supp loss = 0.0\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.42999595403671265, recon loss = 0.42999595403671265, supp loss = 0.0\n",
      "Step 0: loss = 0.5386070609092712, recon loss = 0.5386070609092712, supp loss = 0.0\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.4276208281517029, recon loss = 0.4276208281517029, supp loss = 0.0\n",
      "Step 0: loss = 0.5147741436958313, recon loss = 0.5147741436958313, supp loss = 0.0\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.4473734498023987, recon loss = 0.4473734498023987, supp loss = 0.0\n",
      "Step 0: loss = 0.5451198816299438, recon loss = 0.5451198816299438, supp loss = 0.0\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.4064348340034485, recon loss = 0.4064348340034485, supp loss = 0.0\n",
      "Step 0: loss = 0.5701874494552612, recon loss = 0.5701874494552612, supp loss = 0.0\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.4211263656616211, recon loss = 0.4211263656616211, supp loss = 0.0\n",
      "Step 0: loss = 0.5361911654472351, recon loss = 0.5361911654472351, supp loss = 0.0\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.43628165125846863, recon loss = 0.43628165125846863, supp loss = 0.0\n",
      "Step 0: loss = 0.5357764959335327, recon loss = 0.5357764959335327, supp loss = 0.0\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.40461087226867676, recon loss = 0.40461087226867676, supp loss = 0.0\n",
      "Step 0: loss = 0.5469390153884888, recon loss = 0.5469390153884888, supp loss = 0.0\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.42312514781951904, recon loss = 0.42312514781951904, supp loss = 0.0\n",
      "Step 0: loss = 0.5571380853652954, recon loss = 0.5571380853652954, supp loss = 0.0\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.39402514696121216, recon loss = 0.39402514696121216, supp loss = 0.0\n",
      "Step 0: loss = 0.5221155285835266, recon loss = 0.5221155285835266, supp loss = 0.0\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.42049360275268555, recon loss = 0.42049360275268555, supp loss = 0.0\n",
      "Step 0: loss = 0.5354539155960083, recon loss = 0.5354539155960083, supp loss = 0.0\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.42465558648109436, recon loss = 0.42465558648109436, supp loss = 0.0\n",
      "Step 0: loss = 0.5335597395896912, recon loss = 0.5335597395896912, supp loss = 0.0\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.41361671686172485, recon loss = 0.41361671686172485, supp loss = 0.0\n",
      "Step 0: loss = 0.5320206880569458, recon loss = 0.5320206880569458, supp loss = 0.0\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.39464491605758667, recon loss = 0.39464491605758667, supp loss = 0.0\n",
      "Step 0: loss = 0.5679604411125183, recon loss = 0.5679604411125183, supp loss = 0.0\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.3843154013156891, recon loss = 0.3843154013156891, supp loss = 0.0\n",
      "Step 0: loss = 0.54737389087677, recon loss = 0.54737389087677, supp loss = 0.0\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.4179891347885132, recon loss = 0.4179891347885132, supp loss = 0.0\n",
      "Step 0: loss = 0.5484305024147034, recon loss = 0.5484305024147034, supp loss = 0.0\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.408954918384552, recon loss = 0.408954918384552, supp loss = 0.0\n",
      "Step 0: loss = 0.5350269675254822, recon loss = 0.5350269675254822, supp loss = 0.0\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.3750723898410797, recon loss = 0.3750723898410797, supp loss = 0.0\n",
      "Step 0: loss = 0.5464074611663818, recon loss = 0.5464074611663818, supp loss = 0.0\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.4284130334854126, recon loss = 0.4284130334854126, supp loss = 0.0\n",
      "Step 0: loss = 0.5413212776184082, recon loss = 0.5413212776184082, supp loss = 0.0\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.41088318824768066, recon loss = 0.41088318824768066, supp loss = 0.0\n",
      "Step 0: loss = 0.5534234046936035, recon loss = 0.5534234046936035, supp loss = 0.0\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.39110225439071655, recon loss = 0.39110225439071655, supp loss = 0.0\n",
      "Step 0: loss = 0.5382544994354248, recon loss = 0.5382544994354248, supp loss = 0.0\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.4005512595176697, recon loss = 0.4005512595176697, supp loss = 0.0\n",
      "Step 0: loss = 0.5495636463165283, recon loss = 0.5495636463165283, supp loss = 0.0\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.37299755215644836, recon loss = 0.37299755215644836, supp loss = 0.0\n",
      "Step 0: loss = 0.5208603143692017, recon loss = 0.5208603143692017, supp loss = 0.0\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.3801218271255493, recon loss = 0.3801218271255493, supp loss = 0.0\n",
      "Step 0: loss = 0.5440890192985535, recon loss = 0.5440890192985535, supp loss = 0.0\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.38982900977134705, recon loss = 0.38982900977134705, supp loss = 0.0\n",
      "Step 0: loss = 0.5370023250579834, recon loss = 0.5370023250579834, supp loss = 0.0\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.3891181945800781, recon loss = 0.3891181945800781, supp loss = 0.0\n",
      "Step 0: loss = 0.5494783520698547, recon loss = 0.5494783520698547, supp loss = 0.0\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.39468374848365784, recon loss = 0.39468374848365784, supp loss = 0.0\n",
      "Step 0: loss = 0.547770619392395, recon loss = 0.547770619392395, supp loss = 0.0\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.3805350065231323, recon loss = 0.3805350065231323, supp loss = 0.0\n",
      "Step 0: loss = 0.5260554552078247, recon loss = 0.5260554552078247, supp loss = 0.0\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.3726755976676941, recon loss = 0.3726755976676941, supp loss = 0.0\n",
      "Step 0: loss = 0.5291498899459839, recon loss = 0.5291498899459839, supp loss = 0.0\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.3616214692592621, recon loss = 0.3616214692592621, supp loss = 0.0\n",
      "Step 0: loss = 0.5178523063659668, recon loss = 0.5178523063659668, supp loss = 0.0\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.3836846649646759, recon loss = 0.3836846649646759, supp loss = 0.0\n",
      "Step 0: loss = 0.5520641803741455, recon loss = 0.5520641803741455, supp loss = 0.0\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.40408098697662354, recon loss = 0.40408098697662354, supp loss = 0.0\n",
      "Step 0: loss = 0.5383238196372986, recon loss = 0.5383238196372986, supp loss = 0.0\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.38884401321411133, recon loss = 0.38884401321411133, supp loss = 0.0\n",
      "Step 0: loss = 0.5160590410232544, recon loss = 0.5160590410232544, supp loss = 0.0\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.3740857243537903, recon loss = 0.3740857243537903, supp loss = 0.0\n",
      "Step 0: loss = 0.5489023923873901, recon loss = 0.5489023923873901, supp loss = 0.0\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.37671852111816406, recon loss = 0.37671852111816406, supp loss = 0.0\n",
      "Step 0: loss = 0.5391459465026855, recon loss = 0.5391459465026855, supp loss = 0.0\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.40356937050819397, recon loss = 0.40356937050819397, supp loss = 0.0\n",
      "Step 0: loss = 0.5524640083312988, recon loss = 0.5524640083312988, supp loss = 0.0\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.4057019352912903, recon loss = 0.4057019352912903, supp loss = 0.0\n",
      "Step 0: loss = 0.5492218136787415, recon loss = 0.5492218136787415, supp loss = 0.0\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.3739245533943176, recon loss = 0.3739245533943176, supp loss = 0.0\n",
      "Step 0: loss = 0.5648807287216187, recon loss = 0.5648807287216187, supp loss = 0.0\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.3730887770652771, recon loss = 0.3730887770652771, supp loss = 0.0\n",
      "Step 0: loss = 0.52293860912323, recon loss = 0.52293860912323, supp loss = 0.0\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.3932236135005951, recon loss = 0.3932236135005951, supp loss = 0.0\n",
      "Step 0: loss = 0.5362192392349243, recon loss = 0.5362192392349243, supp loss = 0.0\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.3476845324039459, recon loss = 0.3476845324039459, supp loss = 0.0\n",
      "Step 0: loss = 0.5223602652549744, recon loss = 0.5223602652549744, supp loss = 0.0\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.36167609691619873, recon loss = 0.36167609691619873, supp loss = 0.0\n",
      "Step 0: loss = 0.5365090370178223, recon loss = 0.5365090370178223, supp loss = 0.0\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.37149667739868164, recon loss = 0.37149667739868164, supp loss = 0.0\n",
      "Step 0: loss = 0.5329421162605286, recon loss = 0.5329421162605286, supp loss = 0.0\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.3787074089050293, recon loss = 0.3787074089050293, supp loss = 0.0\n",
      "Step 0: loss = 0.5242352485656738, recon loss = 0.5242352485656738, supp loss = 0.0\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.36335036158561707, recon loss = 0.36335036158561707, supp loss = 0.0\n",
      "Step 0: loss = 0.5216166377067566, recon loss = 0.5216166377067566, supp loss = 0.0\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.3692753314971924, recon loss = 0.3692753314971924, supp loss = 0.0\n",
      "Step 0: loss = 0.5166130065917969, recon loss = 0.5166130065917969, supp loss = 0.0\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.37235385179519653, recon loss = 0.37235385179519653, supp loss = 0.0\n",
      "Step 0: loss = 0.5273212194442749, recon loss = 0.5273212194442749, supp loss = 0.0\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.3473671078681946, recon loss = 0.3473671078681946, supp loss = 0.0\n",
      "Step 0: loss = 0.5160890817642212, recon loss = 0.5160890817642212, supp loss = 0.0\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.37658512592315674, recon loss = 0.37658512592315674, supp loss = 0.0\n",
      "Step 0: loss = 0.5534236431121826, recon loss = 0.5534236431121826, supp loss = 0.0\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.3577668368816376, recon loss = 0.3577668368816376, supp loss = 0.0\n",
      "Step 0: loss = 0.5391830205917358, recon loss = 0.5391830205917358, supp loss = 0.0\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.3593124747276306, recon loss = 0.3593124747276306, supp loss = 0.0\n",
      "Step 0: loss = 0.5114463567733765, recon loss = 0.5114463567733765, supp loss = 0.0\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.3905816674232483, recon loss = 0.3905816674232483, supp loss = 0.0\n",
      "Step 0: loss = 0.5276695489883423, recon loss = 0.5276695489883423, supp loss = 0.0\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.34998565912246704, recon loss = 0.34998565912246704, supp loss = 0.0\n",
      "Step 0: loss = 0.536384105682373, recon loss = 0.536384105682373, supp loss = 0.0\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.35049012303352356, recon loss = 0.35049012303352356, supp loss = 0.0\n",
      "Step 0: loss = 0.5454021692276001, recon loss = 0.5454021692276001, supp loss = 0.0\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.35012874007225037, recon loss = 0.35012874007225037, supp loss = 0.0\n",
      "Step 0: loss = 0.5329641103744507, recon loss = 0.5329641103744507, supp loss = 0.0\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.3460300862789154, recon loss = 0.3460300862789154, supp loss = 0.0\n",
      "Step 0: loss = 0.5044606924057007, recon loss = 0.5044606924057007, supp loss = 0.0\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.3274235427379608, recon loss = 0.3274235427379608, supp loss = 0.0\n",
      "Step 0: loss = 0.5690267086029053, recon loss = 0.5690267086029053, supp loss = 0.0\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.34682145714759827, recon loss = 0.34682145714759827, supp loss = 0.0\n",
      "Step 0: loss = 0.5032373666763306, recon loss = 0.5032373666763306, supp loss = 0.0\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.36522388458251953, recon loss = 0.36522388458251953, supp loss = 0.0\n",
      "Step 0: loss = 0.5480072498321533, recon loss = 0.5480072498321533, supp loss = 0.0\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.3301016688346863, recon loss = 0.3301016688346863, supp loss = 0.0\n",
      "Step 0: loss = 0.5188256502151489, recon loss = 0.5188256502151489, supp loss = 0.0\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.32685789465904236, recon loss = 0.32685789465904236, supp loss = 0.0\n",
      "Step 0: loss = 0.5156031250953674, recon loss = 0.5156031250953674, supp loss = 0.0\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.3405306041240692, recon loss = 0.3405306041240692, supp loss = 0.0\n",
      "Step 0: loss = 0.5143288969993591, recon loss = 0.5143288969993591, supp loss = 0.0\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.3404829502105713, recon loss = 0.3404829502105713, supp loss = 0.0\n",
      "Step 0: loss = 0.5337139368057251, recon loss = 0.5337139368057251, supp loss = 0.0\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.3114236891269684, recon loss = 0.3114236891269684, supp loss = 0.0\n",
      "Step 0: loss = 0.507530689239502, recon loss = 0.507530689239502, supp loss = 0.0\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.3116111755371094, recon loss = 0.3116111755371094, supp loss = 0.0\n",
      "Step 0: loss = 0.527229905128479, recon loss = 0.527229905128479, supp loss = 0.0\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.30508682131767273, recon loss = 0.30508682131767273, supp loss = 0.0\n",
      "Step 0: loss = 0.512191891670227, recon loss = 0.512191891670227, supp loss = 0.0\n",
      "\n",
      "Epoch 151\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "fine_tune_epochs = 3\n",
    "batch_size = 32\n",
    "support_batch_size = 300\n",
    "alpha0 = 4\n",
    "nalpha = alpha0\n",
    "for i in range(epochs):\n",
    "  nalpha = min(nalpha * 1.02, 60)\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs, supp_vs, supp_ys) in enumerate(batch(X, betas, random_dirs, supps_b, batch_size, support_batch_size)):\n",
    "    # print(supp_vs.shape, supp_ys.shape)\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs, supp_vs, supp_ys, 1.0, 0.0)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon loss = {loss_vals[1].numpy()}, supp loss = {loss_vals[2].numpy()}\")\n",
    "    for j in range(fine_tune_epochs):\n",
    "        loss_vals = train_step(vae, batch_betas, tf.linalg.normalize(batch_dirs + np.random.randn(*batch_dirs.shape)*0.1, axis=-1)[0], supp_vs, supp_ys, 1.0, 0.0)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon loss = {loss_vals[1].numpy()}, supp loss = {loss_vals[2].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHF0P4eISAH5"
   },
   "outputs": [],
   "source": [
    "vae.save_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JkrFRN7uOlf",
    "outputId": "90f21636-cdec-4d1e-eb4f-405a557a43b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7dfda7a63c70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS8sKCRfgUoH",
    "outputId": "b7f487ba-8083-4757-b9ea-250fd343a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  chem-checkpoint.zip\n",
      "   creating: my_checkpoint/\n",
      "  inflating: my_checkpoint/chekpont.index  \n",
      "  inflating: my_checkpoint/.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/checkpoint  \n",
      "  inflating: my_checkpoint/chekpont.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/.index    \n"
     ]
    }
   ],
   "source": [
    "# !zip -r my_checkpoint.zip my_checkpoint/\n",
    "!unzip chem-checkpoint.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EbHImoN2QaKX"
   },
   "outputs": [],
   "source": [
    "drawn_random_dirs = np.random.randn(50_000, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "\n",
    "def posterior_sampling(model, betas, random_dirs, num_samples=1):\n",
    "  zm, zlv, z = model.encoder((betas, random_dirs))\n",
    "  if num_samples == 1:\n",
    "      return model.decoder((z, random_dirs))\n",
    "  else:\n",
    "      samples = [sampling((zm, zlv)) for _ in range(num_samples)]\n",
    "      return tf.concat([model.decoder((sm, random_dirs))[:, None, :] for sm in samples], axis=1)\n",
    "\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs1 = np.random.randn(num_samples, d)\n",
    "  random_dirs2 = np.random.randn(num_samples, d)\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))    \n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m drawn_betas, dir1, drawn_betas2 \u001b[38;5;241m=\u001b[39m generate_new_betas(\u001b[43mvae\u001b[49m, \u001b[38;5;241m50_000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# generate_new_betas\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "drawn_betas, dir1, drawn_betas2 = generate_new_betas(vae, 50_000)\n",
    "# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\n",
    "# generate_new_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 200\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd1 = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "# prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample][0])\n",
    "prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([-0.00631047,  0.12106761,  0.46212596, ...,  0.47387064,\n",
       "        -0.281489  , -0.05550657], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check agreement between vae and training samples\n",
    "from sklearn.metrics import jaccard_score\n",
    "def agreement(y_pred1, y_pred2, y_true):\n",
    "    tp1 = np.float32(y_pred1==1) * np.float32(y_true==1)\n",
    "    fp1 = np.float32(y_pred1==1) * np.float32(y_true==0)\n",
    "    tn1 = np.float32(y_pred1==0) * np.float32(y_true==0)\n",
    "    fn1 = np.float32(y_pred1==0) * np.float32(y_true==1)\n",
    "\n",
    "    tp2 = np.float32(y_pred2==1) * np.float32(y_true==1)\n",
    "    fp2 = np.float32(y_pred2==1) * np.float32(y_true==0)\n",
    "    tn2 = np.float32(y_pred2==0) * np.float32(y_true==0)\n",
    "    fn2 = np.float32(y_pred2==0) * np.float32(y_true==1)\n",
    "    print(np.sum(tp1)/len(tp1), np.sum(fp1)/len(tp1), np.sum(tn1)/len(tp1), np.sum(fn1)/len(tp1))\n",
    "    print(np.sum(tp2)/len(tp1), np.sum(fp2)/len(tp1), np.sum(tn2)/len(tp1), np.sum(fn2)/len(tp1))\n",
    "    print(np.sum(fp1==fp2)/len(tp1), np.sum(fn1==fn2)/len(tp1))\n",
    "    return jaccard_score(tp1, tp2), jaccard_score(fp1, fp2), jaccard_score(tn1, tn2), jaccard_score(fn1, fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5735723218007504 0.0029178824510212586 0.4210087536473531 0.0025010421008753647\n",
      "0.5731554814506045 0.0025010421008753647 0.42142559399749896 0.0029178824510212586\n",
      "0.9979157982492706 0.9979157982492706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9963715529753265,\n",
       " 0.4444444444444444,\n",
       " 0.9950641658440277,\n",
       " 0.4444444444444444)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement(prd1, prd2, Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 10, 4081])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0054231044>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps= betas\n",
    "oups = drawn_betas\n",
    "tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(tf.expand_dims(inps, axis=1)[:, :, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(oups[:, :, 1:], axis=-1)[0], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.012491584>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.abs(tf.cast(tf.expand_dims(inps, axis=1)[:, :, :1], dtype=tf.float32)-oups[:, :, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([ 0.01558006,  0.08848727,  0.02163708, ...,  0.32644653,\n",
       "        -0.05755342,  0.21104604], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IHicI0RhbDe",
    "outputId": "32426975-705a-4668-f061-08acd65bc48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.99283946>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas1, drawn_betas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOFI9xq2b7_3",
    "outputId": "a11e5724-0e2a-428d-a65a-28064209e6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4081), dtype=float32, numpy=\n",
       "array([[ 3.5679474e-02, -8.5116491e-02, -2.7868379e-02, ...,\n",
       "         3.8362685e-01,  6.8785306e-03,  5.8818167e-01],\n",
       "       [ 2.0887703e-02,  5.4998733e-02,  2.2995186e-01, ...,\n",
       "         1.0724969e-01,  3.8565136e-04,  3.8573799e-01],\n",
       "       [-6.5227631e-03,  2.0269346e-01,  4.0428180e-01, ...,\n",
       "        -4.8758507e-01, -8.2871839e-02,  3.4979448e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFCgGG6jgkLL",
    "outputId": "5e799c27-fff1-41fc-d71f-3fc7ea88538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13627878, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "var = tf.math.reduce_variance(drawn_betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxnzX5lJQtjX"
   },
   "outputs": [],
   "source": [
    "np.mean(drawn_betas1 @ tf.transpose(drawn_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "TfAej5fqsfHh",
    "outputId": "fd888e0d-c076-4577-be0d-4de29be77dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ood_val_features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_data_to_features\u001b[49m(ood_val_data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#ood_test_features = convert_data_to_features(ood_test_data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ood_val_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m ood_val_data])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "#ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U07oDyUdsl1u"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ood_val_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m external_X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mood_val_features\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m external_Y \u001b[38;5;241m=\u001b[39m ood_val_labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ood_val_features' is not defined"
     ]
    }
   ],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "external_Y = ood_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "external_X = (external_X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "1a114f68-6f8b-4ede-b395-df1c2dfa2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      " -0.03134901]\n",
      "tf.Tensor(\n",
      "[-0.13125554  0.30866534  0.63319725 -0.5780234  -0.21648076 -0.3846287\n",
      "  0.99320394 -0.1799166   0.678208    0.62888056], shape=(10,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "5ec19308-4552-4326-ec47-b86470a2f7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 1024)\n",
      "(937, 2040)\n",
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "ae104348-bf03-48d8-a6c3-d3112b1dcb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " ...\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W32O3S_gKnEp",
    "outputId": "8797bdae-deb8-43ed-ef0c-cbf43fb4aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 50) (937, 50)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 50, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  # wghts = np.ones_like(projs > thresh[:, None])\n",
    "  wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return *aggregate_preds(preds), preds\n",
    "\n",
    "\n",
    "# drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(randfeats_X, X, random_dirs[:512], betas[:512]) # 0.622\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs, betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, dir1, drawn_betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas1) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand, _ = get_preds_and_aggregate(external_randfeats_X, betas) # 0.634\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, drawn_betas) # 0.85\n",
    "# ext_preds, mp_rand, sp_rand, pred = get_preds_and_aggregate(external_randfeats_X, betas) # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4600,2) and (10,2) not aligned: 2 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4600,2) and (10,2) not aligned: 2 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.dot(np.zeros((4600, 2)), np.zeros((10, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOyhZXJBrhaj",
    "outputId": "98cc9eb0-2e98-4b38-e823-337411f61e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0], tf.transpose(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "a9702828-7bea-436e-de46-89ca03c30b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.767455  ,  0.1001012 , -0.54182774, ..., -6.8707843 ,\n",
       "         7.257238  , -0.5050444 ], dtype=float32),\n",
       " array([-0.0186294 ,  0.06830448, -0.27256313, ..., -0.72351612,\n",
       "         0.81710885,  0.05805234]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "2fac824c-8637-4a35-8dfb-7b60045b10b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mext_preds\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ext_preds' is not defined"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "06eb05c6-f5f0-4d58-a8a2-ad76ee40b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [False False  True False  True False False False False False]\n",
      "Total Positive Preds:  463\n",
      "Total Preds:  937\n",
      "% Positive Preds:  0.49413020277481323\n",
      "\n",
      "First 10 Ground Truth:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Total Positive Ground Truth:  421.0\n",
      "Total Ground Truth:  937\n",
      "% Positive Ground Truth:  0.44930629669156885\n",
      "\n",
      "Accuracy:  0.6350053361792957\n"
     ]
    }
   ],
   "source": [
    "# testing_Y = Y\n",
    "ext_preds = ext_preds > 0.5\n",
    "testing_Y = external_Y\n",
    "\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", testing_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(testing_Y))\n",
    "print(\"Total Ground Truth: \", len(testing_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(testing_Y) / len(testing_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == testing_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "L7kl6J3wLped",
    "outputId": "a5d20ac1-f339-47e4-cf12-18073bb4b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f56b4751160>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.2, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8571428571428571, 0.8536585365853658, 0.7857142857142857, 0.81, 0.7898550724637681, 0.7909604519774012, 0.7813953488372093, 0.7649402390438247, 0.738831615120275, 0.7284345047923323, 0.7308781869688386, 0.7222222222222222, 0.7126696832579186, 0.7034764826175869, 0.6950998185117967, 0.6829268292682927, 0.6651917404129793, 0.6557591623036649, 0.6211312700106724]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7ba833a580>]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_preds)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
