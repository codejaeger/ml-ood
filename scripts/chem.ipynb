{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 01:21:03.816190: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 01:21:04.803237: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-05 01:21:04.803356: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-05-05 01:21:04.803370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "20637ffb-f819-4f0a-b926-d24784746a9e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkF1N4olXTtZ",
    "outputId": "de7467ba-69c5-42b4-a918-755fe87765d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "weKR7jCuMvQl"
   },
   "outputs": [],
   "source": [
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataX = np.float32(np.array([line.strip().split(',')[2:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataY = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "X = dataX\n",
    "Y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "2948628d-0085-48ba-8e9b-4bbe93f27e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5997, 1024)\n",
      "(5997,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, d, D=128):\n",
    "\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3S8unT73bEtM"
   },
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print((prd[:, 0]==Y).mean())\n",
    "  return wgts\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    beta = linear_coefs(get_rand_feats(X_sub@pca_projs, model), Y_sub)\n",
    "    # print(beta.shape)\n",
    "      \n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 01:21:13.007883: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 01:21:13.180080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:0a:00.0, compute capability: 7.5\n",
      "2024-05-05 01:21:13.378203: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x9376e20\n"
     ]
    }
   ],
   "source": [
    "s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(232.08453, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [0.05, 0.2, 0.3, 0.4, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5997, 1024), TensorShape([1024, 204]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, pca_projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "f3d09ad5-4e02-44a5-e001-fe83e3d99938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m random_dirs \u001b[38;5;241m=\u001b[39m random_dirs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(random_dirs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m betas \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_projs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mget_models\u001b[0;34m(X, Y, pca_projs, dirs, model, percentile)\u001b[0m\n\u001b[1;32m     10\u001b[0m Y_sub \u001b[38;5;241m=\u001b[39m Y[X_ids]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print((X_sub@pca_projs).shape)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_coefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_rand_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;129;43m@pca_projs\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(beta.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#X_subsets.append(X_sub)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#data_ids.append(X_ids)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Y_subsets.append(Y_sub)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m betas\u001b[38;5;241m.\u001b[39mappend(beta)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mlinear_coefs\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_coefs\u001b[39m(X, Y):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    X: N x d matrix of input features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Beta: d x 1 matrix of linear coefficients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m   clf \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mscore(X, Y))\n\u001b[1;32m     12\u001b[0m   wgts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((clf\u001b[38;5;241m.\u001b[39mintercept_[:,\u001b[38;5;28;01mNone\u001b[39;00m], clf\u001b[38;5;241m.\u001b[39mcoef_))\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1354\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1351\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1352\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1353\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)))\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([n_iter_])\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:964\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    960\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X,\n\u001b[1;32m    961\u001b[0m                                      dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    963\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m--> 964\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misspmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m    972\u001b[0m n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "\n",
    "N = 2**9    # ~ 8k\n",
    "# N = 2**2    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas = get_models(X, Y, pca_projs, random_dirs, model, percentile=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "# np.save('random_dirs-chem2.npy', random_dirs)\n",
    "# np.save('betas-chem2.npy', betas)\n",
    "# np.save('Ws-chem2.npy', model.Ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "CnxEkcWwWlwn"
   },
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-chem2.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./betas-chem2.npy')))\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "model.Ws = tf.constant(np.load('./Ws-chem2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "BCHvO4qeupNQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(random_dirs1, random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(random_dirs1 - random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "264b9e19-74f8-4d13-ee50-bb4796f0d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 4081)\n",
      "(512, 1024)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "15119544-cde9-4462-b565-7deb6dd61daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.01111806548776194, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "b44e65f3-b44b-40fd-e564-63e35d43997c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958333333333333"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs1[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas1[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0NZNorQW0vg",
    "outputId": "afcb4822-0cda-4c13-ea68-8454473bb529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6204268292682927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n",
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[sample], 70)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 32\n",
    "\n",
    "# Encoder\n",
    "beta_input = layers.Input(shape=(beta_dim,))\n",
    "beta_x = layers.Dense(512, activation=tf.nn.elu)(beta_input)\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([beta_x, dir_input])\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(32, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(32, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([beta_input, dir_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs, decoder_dir_input], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([beta_input, dir_input])[2], dir_input])\n",
    "vae = models.Model([beta_input, dir_input], outputs, name=\"vae\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, z_mean, z_log_var, reg=1.0):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs, dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs, axis=-1)[0], axis=-1))\n",
    "  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  # total_loss = recon_loss + 0.05 * kl_loss\n",
    "  total_loss = recon_loss + 0.005 * kl_loss\n",
    "  return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "def train_step(model, inputs, dir_inputs):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z_mean, z_log_var, z = model.encoder([inputs, dir_inputs])\n",
    "    outputs = model.decoder([z, dir_inputs])\n",
    "    total_loss, recon_loss, kl_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(betas, dirs, batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield tf.constant(betas[i:i+batch_size]), tf.constant(dirs[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "a6e1ccc1-2cd0-4549-e61a-6dae8aa1062a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 0.999082088470459, recon_loss = 0.996230959892273, kl_loss = 0.5702270865440369\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.4513305127620697, recon_loss = 0.41232872009277344, kl_loss = 7.800360679626465\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.42799705266952515, recon_loss = 0.4058411419391632, kl_loss = 4.431185245513916\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.4310673475265503, recon_loss = 0.40511810779571533, kl_loss = 5.189845561981201\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.43990251421928406, recon_loss = 0.4051375985145569, kl_loss = 6.952983856201172\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.42221516370773315, recon_loss = 0.4055999517440796, kl_loss = 3.323042392730713\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.42366519570350647, recon_loss = 0.4046902656555176, kl_loss = 3.7949881553649902\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.42843925952911377, recon_loss = 0.4035003185272217, kl_loss = 4.987785339355469\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.4136664569377899, recon_loss = 0.3992936313152313, kl_loss = 2.874565601348877\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.41961386799812317, recon_loss = 0.4032144546508789, kl_loss = 3.2798829078674316\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.414552241563797, recon_loss = 0.4018639922142029, kl_loss = 2.537651538848877\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.40879490971565247, recon_loss = 0.4011109471321106, kl_loss = 1.5367896556854248\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.40715208649635315, recon_loss = 0.40035009384155273, kl_loss = 1.3603988885879517\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.39990687370300293, recon_loss = 0.3950434923171997, kl_loss = 0.972678542137146\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.39599496126174927, recon_loss = 0.3878799080848694, kl_loss = 1.6230107545852661\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.39252540469169617, recon_loss = 0.3865070044994354, kl_loss = 1.20368230342865\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.3915906548500061, recon_loss = 0.3864731788635254, kl_loss = 1.0234955549240112\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.3912605047225952, recon_loss = 0.3839413821697235, kl_loss = 1.4638242721557617\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.38693875074386597, recon_loss = 0.38187986612319946, kl_loss = 1.0117791891098022\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.37919437885284424, recon_loss = 0.3735150098800659, kl_loss = 1.1358766555786133\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.38373616337776184, recon_loss = 0.3791700601577759, kl_loss = 0.9132206439971924\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.3760622441768646, recon_loss = 0.36944520473480225, kl_loss = 1.3234097957611084\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.3811662495136261, recon_loss = 0.3761060833930969, kl_loss = 1.0120353698730469\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.37342679500579834, recon_loss = 0.3689435124397278, kl_loss = 0.8966560959815979\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.3714786767959595, recon_loss = 0.3681373596191406, kl_loss = 0.6682627201080322\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.3607054054737091, recon_loss = 0.35865509510040283, kl_loss = 0.41006210446357727\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.35788121819496155, recon_loss = 0.35647064447402954, kl_loss = 0.28211382031440735\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.3471972644329071, recon_loss = 0.3463090658187866, kl_loss = 0.17764116823673248\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.3519304394721985, recon_loss = 0.3513602912425995, kl_loss = 0.11403167247772217\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.3490665853023529, recon_loss = 0.34859466552734375, kl_loss = 0.09438157081604004\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.33598488569259644, recon_loss = 0.33553141355514526, kl_loss = 0.09069162607192993\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.3447834551334381, recon_loss = 0.3443327844142914, kl_loss = 0.09013467282056808\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.3420194089412689, recon_loss = 0.3415895402431488, kl_loss = 0.08597168326377869\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.3330577313899994, recon_loss = 0.3324306607246399, kl_loss = 0.12541182339191437\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.32829204201698303, recon_loss = 0.3278992772102356, kl_loss = 0.07855334877967834\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.32541975378990173, recon_loss = 0.32501810789108276, kl_loss = 0.08032776415348053\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.33302438259124756, recon_loss = 0.3326122760772705, kl_loss = 0.0824185162782669\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.31781893968582153, recon_loss = 0.3175542950630188, kl_loss = 0.052928633987903595\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.31083792448043823, recon_loss = 0.310518741607666, kl_loss = 0.06383474171161652\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.3141765296459198, recon_loss = 0.3139500617980957, kl_loss = 0.04529547691345215\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.31077075004577637, recon_loss = 0.3105432391166687, kl_loss = 0.045501332730054855\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.3028118908405304, recon_loss = 0.3026340901851654, kl_loss = 0.03555801883339882\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.2993752658367157, recon_loss = 0.29920655488967896, kl_loss = 0.03374287486076355\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.3030144274234772, recon_loss = 0.3028740882873535, kl_loss = 0.028066281229257584\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.2992075979709625, recon_loss = 0.29908838868141174, kl_loss = 0.0238431878387928\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.30057063698768616, recon_loss = 0.30046746134757996, kl_loss = 0.020633317530155182\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.3000042736530304, recon_loss = 0.29990777373313904, kl_loss = 0.019301101565361023\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.29209211468696594, recon_loss = 0.29197973012924194, kl_loss = 0.022475097328424454\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.2981971502304077, recon_loss = 0.29811811447143555, kl_loss = 0.0158062856644392\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.29525792598724365, recon_loss = 0.2951871156692505, kl_loss = 0.014163097366690636\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.2907848358154297, recon_loss = 0.29068809747695923, kl_loss = 0.019345227628946304\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.29443851113319397, recon_loss = 0.29436707496643066, kl_loss = 0.014288771897554398\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.28683608770370483, recon_loss = 0.286766916513443, kl_loss = 0.013833433389663696\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.2901741564273834, recon_loss = 0.29010868072509766, kl_loss = 0.013093728572130203\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.2944633960723877, recon_loss = 0.29438430070877075, kl_loss = 0.01582077518105507\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.2824297547340393, recon_loss = 0.2823639214038849, kl_loss = 0.013169032521545887\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.2848532795906067, recon_loss = 0.28478917479515076, kl_loss = 0.012818820774555206\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.2823828160762787, recon_loss = 0.28232401609420776, kl_loss = 0.0117588359862566\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.2864777147769928, recon_loss = 0.28642624616622925, kl_loss = 0.01029618177562952\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.2786146104335785, recon_loss = 0.2785547375679016, kl_loss = 0.011974545195698738\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.2787236273288727, recon_loss = 0.27867478132247925, kl_loss = 0.009768079034984112\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.2773991823196411, recon_loss = 0.2773479223251343, kl_loss = 0.010250586085021496\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.27920830249786377, recon_loss = 0.27915647625923157, kl_loss = 0.010365413501858711\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.26864397525787354, recon_loss = 0.26859551668167114, kl_loss = 0.009691581130027771\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.2772921025753021, recon_loss = 0.27725112438201904, kl_loss = 0.008198288269340992\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.27553585171699524, recon_loss = 0.2754920721054077, kl_loss = 0.008754364214837551\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.27392488718032837, recon_loss = 0.27388158440589905, kl_loss = 0.008660851046442986\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.2665409743785858, recon_loss = 0.2664969265460968, kl_loss = 0.00881106499582529\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.2776987552642822, recon_loss = 0.27765965461730957, kl_loss = 0.007820800878107548\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.27603358030319214, recon_loss = 0.2759894132614136, kl_loss = 0.008833990432322025\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.2745347023010254, recon_loss = 0.27450239658355713, kl_loss = 0.006462140940129757\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.2719571888446808, recon_loss = 0.2719123959541321, kl_loss = 0.008958021178841591\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.274242639541626, recon_loss = 0.2742076516151428, kl_loss = 0.006996638141572475\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.27423855662345886, recon_loss = 0.2742082476615906, kl_loss = 0.006063675507903099\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.2678769826889038, recon_loss = 0.26784783601760864, kl_loss = 0.0058313943445682526\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.2767387628555298, recon_loss = 0.2767042815685272, kl_loss = 0.006894217804074287\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.2683853209018707, recon_loss = 0.268354594707489, kl_loss = 0.00614815391600132\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.27702656388282776, recon_loss = 0.27699941396713257, kl_loss = 0.005428021773695946\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.27011969685554504, recon_loss = 0.27009379863739014, kl_loss = 0.005182335153222084\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.27078649401664734, recon_loss = 0.27075380086898804, kl_loss = 0.006539054214954376\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.26836541295051575, recon_loss = 0.2683427035808563, kl_loss = 0.004540883935987949\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.26308971643447876, recon_loss = 0.26306143403053284, kl_loss = 0.005658866837620735\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.2746051549911499, recon_loss = 0.27457767724990845, kl_loss = 0.005498518235981464\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.26923105120658875, recon_loss = 0.269209623336792, kl_loss = 0.004288170486688614\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.2643255889415741, recon_loss = 0.2643060088157654, kl_loss = 0.003915486857295036\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.2608066499233246, recon_loss = 0.2607799768447876, kl_loss = 0.0053350673988461494\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.25948500633239746, recon_loss = 0.2594580054283142, kl_loss = 0.0054001351818442345\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.25944751501083374, recon_loss = 0.2594265043735504, kl_loss = 0.004201303236186504\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.26587069034576416, recon_loss = 0.26584744453430176, kl_loss = 0.004646964371204376\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.2611561119556427, recon_loss = 0.26113229990005493, kl_loss = 0.004759981296956539\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.2667333781719208, recon_loss = 0.26670771837234497, kl_loss = 0.005130397155880928\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.2567654252052307, recon_loss = 0.2567474842071533, kl_loss = 0.0035892585292458534\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.262603759765625, recon_loss = 0.2625858783721924, kl_loss = 0.00357627309858799\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.2538735568523407, recon_loss = 0.2538536787033081, kl_loss = 0.003977994434535503\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.2588789761066437, recon_loss = 0.2588562071323395, kl_loss = 0.004556217230856419\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.26045462489128113, recon_loss = 0.2604373097419739, kl_loss = 0.003463633358478546\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.2594352662563324, recon_loss = 0.2594224214553833, kl_loss = 0.002566124312579632\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.2608747184276581, recon_loss = 0.26085954904556274, kl_loss = 0.0030367188155651093\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.26447272300720215, recon_loss = 0.2644578814506531, kl_loss = 0.0029708845540881157\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.2532116174697876, recon_loss = 0.25319933891296387, kl_loss = 0.0024578887969255447\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.2510915994644165, recon_loss = 0.25107747316360474, kl_loss = 0.002824016846716404\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.26406270265579224, recon_loss = 0.26404869556427, kl_loss = 0.0028029996901750565\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.26412898302078247, recon_loss = 0.2641170024871826, kl_loss = 0.0023983800783753395\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.2534193694591522, recon_loss = 0.253409743309021, kl_loss = 0.0019244467839598656\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.2540627717971802, recon_loss = 0.2540522515773773, kl_loss = 0.0021037189289927483\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.2595898509025574, recon_loss = 0.25957968831062317, kl_loss = 0.00203016959130764\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.2571869194507599, recon_loss = 0.2571759819984436, kl_loss = 0.002186358906328678\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.2507292330265045, recon_loss = 0.25071966648101807, kl_loss = 0.0019109807908535004\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.25645872950553894, recon_loss = 0.25644993782043457, kl_loss = 0.0017564613372087479\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.26085811853408813, recon_loss = 0.26084834337234497, kl_loss = 0.0019575199112296104\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.25138387084007263, recon_loss = 0.2513715624809265, kl_loss = 0.002464206889271736\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.25265225768089294, recon_loss = 0.2526407837867737, kl_loss = 0.002294890582561493\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.2587074935436249, recon_loss = 0.2586978077888489, kl_loss = 0.0019382666796445847\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.2548657953739166, recon_loss = 0.25485724210739136, kl_loss = 0.0017114188522100449\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.24840232729911804, recon_loss = 0.248392254114151, kl_loss = 0.0020145233720541\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.2565345764160156, recon_loss = 0.25652289390563965, kl_loss = 0.0023349830880761147\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.2518079876899719, recon_loss = 0.2517961859703064, kl_loss = 0.0023621171712875366\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.2446373701095581, recon_loss = 0.24462571740150452, kl_loss = 0.002329745329916477\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.24985697865486145, recon_loss = 0.24984894692897797, kl_loss = 0.001606544479727745\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.2514844238758087, recon_loss = 0.2514742612838745, kl_loss = 0.0020304452627897263\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.25210416316986084, recon_loss = 0.25208979845046997, kl_loss = 0.0028751567006111145\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.25798285007476807, recon_loss = 0.2579715847969055, kl_loss = 0.0022523561492562294\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.24648897349834442, recon_loss = 0.24648092687129974, kl_loss = 0.0016088616102933884\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.2537177503108978, recon_loss = 0.2537091374397278, kl_loss = 0.0017220452427864075\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.2527909576892853, recon_loss = 0.2527783513069153, kl_loss = 0.002518819645047188\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.2522600591182709, recon_loss = 0.2522495985031128, kl_loss = 0.0020932117477059364\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.2511017322540283, recon_loss = 0.25109127163887024, kl_loss = 0.002094663679599762\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.24599464237689972, recon_loss = 0.24598747491836548, kl_loss = 0.0014345133677124977\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.2519817650318146, recon_loss = 0.25197291374206543, kl_loss = 0.0017701666802167892\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.2497563660144806, recon_loss = 0.24974602460861206, kl_loss = 0.0020674308761954308\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.24968838691711426, recon_loss = 0.24967685341835022, kl_loss = 0.0023064138367772102\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.24569986760616302, recon_loss = 0.24568890035152435, kl_loss = 0.0021943822503089905\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.25302448868751526, recon_loss = 0.25300952792167664, kl_loss = 0.0029907245188951492\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.25349634885787964, recon_loss = 0.2534850835800171, kl_loss = 0.002254345454275608\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.24590487778186798, recon_loss = 0.24589502811431885, kl_loss = 0.0019692247733473778\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.25272130966186523, recon_loss = 0.2527116537094116, kl_loss = 0.0019310740754008293\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.2478925883769989, recon_loss = 0.24788224697113037, kl_loss = 0.002068311907351017\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.24341627955436707, recon_loss = 0.24339953064918518, kl_loss = 0.003348815254867077\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.25068336725234985, recon_loss = 0.2506728172302246, kl_loss = 0.0021073371171951294\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.23916776478290558, recon_loss = 0.23915904760360718, kl_loss = 0.0017423750832676888\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.25140899419784546, recon_loss = 0.25139978528022766, kl_loss = 0.0018436238169670105\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.24982793629169464, recon_loss = 0.24981699883937836, kl_loss = 0.0021881302818655968\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.23886512219905853, recon_loss = 0.23885652422904968, kl_loss = 0.0017181206494569778\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.24498715996742249, recon_loss = 0.24497731029987335, kl_loss = 0.001971152611076832\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.23730550706386566, recon_loss = 0.23729729652404785, kl_loss = 0.0016432274132966995\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.23958666622638702, recon_loss = 0.2395799607038498, kl_loss = 0.0013418737798929214\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.24665537476539612, recon_loss = 0.2466442734003067, kl_loss = 0.002221275120973587\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.2463841289281845, recon_loss = 0.24637800455093384, kl_loss = 0.0012255311012268066\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.24256247282028198, recon_loss = 0.24255646765232086, kl_loss = 0.0012005968019366264\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.24914255738258362, recon_loss = 0.24913595616817474, kl_loss = 0.0013189837336540222\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.24349163472652435, recon_loss = 0.24348607659339905, kl_loss = 0.0011125467717647552\n",
      "\n",
      "Epoch 151\n",
      "Step 0: loss = 0.24343982338905334, recon_loss = 0.24343523383140564, kl_loss = 0.0009189685806632042\n",
      "\n",
      "Epoch 152\n",
      "Step 0: loss = 0.24569982290267944, recon_loss = 0.245696023106575, kl_loss = 0.000759015791118145\n",
      "\n",
      "Epoch 153\n",
      "Step 0: loss = 0.2454746514558792, recon_loss = 0.24547195434570312, kl_loss = 0.0005385205149650574\n",
      "\n",
      "Epoch 154\n",
      "Step 0: loss = 0.24348048865795135, recon_loss = 0.2434772253036499, kl_loss = 0.0006528021767735481\n",
      "\n",
      "Epoch 155\n",
      "Step 0: loss = 0.2365700751543045, recon_loss = 0.23656758666038513, kl_loss = 0.0004980461671948433\n",
      "\n",
      "Epoch 156\n",
      "Step 0: loss = 0.24243851006031036, recon_loss = 0.24243609607219696, kl_loss = 0.00048253126442432404\n",
      "\n",
      "Epoch 157\n",
      "Step 0: loss = 0.2402919977903366, recon_loss = 0.24028964340686798, kl_loss = 0.00047001149505376816\n",
      "\n",
      "Epoch 158\n",
      "Step 0: loss = 0.24050787091255188, recon_loss = 0.24050626158714294, kl_loss = 0.000322035513818264\n",
      "\n",
      "Epoch 159\n",
      "Step 0: loss = 0.24533513188362122, recon_loss = 0.2453330010175705, kl_loss = 0.00042659323662519455\n",
      "\n",
      "Epoch 160\n",
      "Step 0: loss = 0.2381381094455719, recon_loss = 0.23813629150390625, kl_loss = 0.00036439113318920135\n",
      "\n",
      "Epoch 161\n",
      "Step 0: loss = 0.24042613804340363, recon_loss = 0.24042442440986633, kl_loss = 0.0003423066809773445\n",
      "\n",
      "Epoch 162\n",
      "Step 0: loss = 0.24018248915672302, recon_loss = 0.2401811182498932, kl_loss = 0.00027396995574235916\n",
      "\n",
      "Epoch 163\n",
      "Step 0: loss = 0.2405523806810379, recon_loss = 0.2405499517917633, kl_loss = 0.0004860423505306244\n",
      "\n",
      "Epoch 164\n",
      "Step 0: loss = 0.232421413064003, recon_loss = 0.23241975903511047, kl_loss = 0.0003320099785923958\n",
      "\n",
      "Epoch 165\n",
      "Step 0: loss = 0.2374580055475235, recon_loss = 0.237456277012825, kl_loss = 0.000346415676176548\n",
      "\n",
      "Epoch 166\n",
      "Step 0: loss = 0.23321977257728577, recon_loss = 0.23321856558322906, kl_loss = 0.0002404041588306427\n",
      "\n",
      "Epoch 167\n",
      "Step 0: loss = 0.23286552727222443, recon_loss = 0.23286405205726624, kl_loss = 0.0002961978316307068\n",
      "\n",
      "Epoch 168\n",
      "Step 0: loss = 0.23860345780849457, recon_loss = 0.23860210180282593, kl_loss = 0.0002721855416893959\n",
      "\n",
      "Epoch 169\n",
      "Step 0: loss = 0.2294473946094513, recon_loss = 0.22944611310958862, kl_loss = 0.00025665201246738434\n",
      "\n",
      "Epoch 170\n",
      "Step 0: loss = 0.24021919071674347, recon_loss = 0.24021777510643005, kl_loss = 0.00028398726135492325\n",
      "\n",
      "Epoch 171\n",
      "Step 0: loss = 0.23168347775936127, recon_loss = 0.23168227076530457, kl_loss = 0.00024140160530805588\n",
      "\n",
      "Epoch 172\n",
      "Step 0: loss = 0.236997589468956, recon_loss = 0.23699578642845154, kl_loss = 0.00036055687814950943\n",
      "\n",
      "Epoch 173\n",
      "Step 0: loss = 0.23045705258846283, recon_loss = 0.23045599460601807, kl_loss = 0.00021088216453790665\n",
      "\n",
      "Epoch 174\n",
      "Step 0: loss = 0.23431585729122162, recon_loss = 0.2343144416809082, kl_loss = 0.0002834843471646309\n",
      "\n",
      "Epoch 175\n",
      "Step 0: loss = 0.22962871193885803, recon_loss = 0.22962772846221924, kl_loss = 0.00019552651792764664\n",
      "\n",
      "Epoch 176\n",
      "Step 0: loss = 0.23982170224189758, recon_loss = 0.239820659160614, kl_loss = 0.0002097487449645996\n",
      "\n",
      "Epoch 177\n",
      "Step 0: loss = 0.23214995861053467, recon_loss = 0.23214896023273468, kl_loss = 0.00019834283739328384\n",
      "\n",
      "Epoch 178\n",
      "Step 0: loss = 0.23312513530254364, recon_loss = 0.23312397301197052, kl_loss = 0.00023230630904436111\n",
      "\n",
      "Epoch 179\n",
      "Step 0: loss = 0.22907377779483795, recon_loss = 0.22907274961471558, kl_loss = 0.0002052420750260353\n",
      "\n",
      "Epoch 180\n",
      "Step 0: loss = 0.23240284621715546, recon_loss = 0.2324017882347107, kl_loss = 0.00021055620163679123\n",
      "\n",
      "Epoch 181\n",
      "Step 0: loss = 0.22752393782138824, recon_loss = 0.22752299904823303, kl_loss = 0.00018913671374320984\n",
      "\n",
      "Epoch 182\n",
      "Step 0: loss = 0.22455501556396484, recon_loss = 0.22455374896526337, kl_loss = 0.00025215931236743927\n",
      "\n",
      "Epoch 183\n",
      "Step 0: loss = 0.2284461259841919, recon_loss = 0.2284451127052307, kl_loss = 0.00020224321633577347\n",
      "\n",
      "Epoch 184\n",
      "Step 0: loss = 0.22330902516841888, recon_loss = 0.2233075499534607, kl_loss = 0.00029494427144527435\n",
      "\n",
      "Epoch 185\n",
      "Step 0: loss = 0.22384044528007507, recon_loss = 0.2238384187221527, kl_loss = 0.00040531065315008163\n",
      "\n",
      "Epoch 186\n",
      "Step 0: loss = 0.2193484604358673, recon_loss = 0.21934595704078674, kl_loss = 0.0004991916939616203\n",
      "\n",
      "Epoch 187\n",
      "Step 0: loss = 0.217269167304039, recon_loss = 0.21726801991462708, kl_loss = 0.00022987741976976395\n",
      "\n",
      "Epoch 188\n",
      "Step 0: loss = 0.2183767855167389, recon_loss = 0.21837478876113892, kl_loss = 0.00039853528141975403\n",
      "\n",
      "Epoch 189\n",
      "Step 0: loss = 0.2204645276069641, recon_loss = 0.2204630970954895, kl_loss = 0.00028594210743904114\n",
      "\n",
      "Epoch 190\n",
      "Step 0: loss = 0.21591117978096008, recon_loss = 0.21590903401374817, kl_loss = 0.00042889732867479324\n",
      "\n",
      "Epoch 191\n",
      "Step 0: loss = 0.21491138637065887, recon_loss = 0.21490919589996338, kl_loss = 0.00043866876512765884\n",
      "\n",
      "Epoch 192\n",
      "Step 0: loss = 0.21279525756835938, recon_loss = 0.21279241144657135, kl_loss = 0.0005698353052139282\n",
      "\n",
      "Epoch 193\n",
      "Step 0: loss = 0.21166422963142395, recon_loss = 0.2116612046957016, kl_loss = 0.0006048670038580894\n",
      "\n",
      "Epoch 194\n",
      "Step 0: loss = 0.2130092829465866, recon_loss = 0.21300600469112396, kl_loss = 0.0006543528288602829\n",
      "\n",
      "Epoch 195\n",
      "Step 0: loss = 0.2096695899963379, recon_loss = 0.20966683328151703, kl_loss = 0.0005521932616829872\n",
      "\n",
      "Epoch 196\n",
      "Step 0: loss = 0.2146434336900711, recon_loss = 0.21463847160339355, kl_loss = 0.0009925421327352524\n",
      "\n",
      "Epoch 197\n",
      "Step 0: loss = 0.21174779534339905, recon_loss = 0.21174368262290955, kl_loss = 0.0008232360705733299\n",
      "\n",
      "Epoch 198\n",
      "Step 0: loss = 0.20711392164230347, recon_loss = 0.20710870623588562, kl_loss = 0.001043093390762806\n",
      "\n",
      "Epoch 199\n",
      "Step 0: loss = 0.21028833091259003, recon_loss = 0.2102852165699005, kl_loss = 0.0006214966997504234\n",
      "\n",
      "Epoch 200\n",
      "Step 0: loss = 0.20484712719917297, recon_loss = 0.20484478771686554, kl_loss = 0.0004686256870627403\n",
      "\n",
      "Epoch 201\n",
      "Step 0: loss = 0.20657233893871307, recon_loss = 0.20656917989253998, kl_loss = 0.0006325682625174522\n",
      "\n",
      "Epoch 202\n",
      "Step 0: loss = 0.20129233598709106, recon_loss = 0.20128974318504333, kl_loss = 0.0005172900855541229\n",
      "\n",
      "Epoch 203\n",
      "Step 0: loss = 0.20798836648464203, recon_loss = 0.20798641443252563, kl_loss = 0.000389990396797657\n",
      "\n",
      "Epoch 204\n",
      "Step 0: loss = 0.20742817223072052, recon_loss = 0.20742636919021606, kl_loss = 0.0003600623458623886\n",
      "\n",
      "Epoch 205\n",
      "Step 0: loss = 0.20531049370765686, recon_loss = 0.20530715584754944, kl_loss = 0.0006673084571957588\n",
      "\n",
      "Epoch 206\n",
      "Step 0: loss = 0.20272797346115112, recon_loss = 0.2027256041765213, kl_loss = 0.00047528743743896484\n",
      "\n",
      "Epoch 207\n",
      "Step 0: loss = 0.1937675029039383, recon_loss = 0.19376400113105774, kl_loss = 0.0007001860067248344\n",
      "\n",
      "Epoch 208\n",
      "Step 0: loss = 0.1900625377893448, recon_loss = 0.1900576651096344, kl_loss = 0.0009743673726916313\n",
      "\n",
      "Epoch 209\n",
      "Step 0: loss = 0.19980311393737793, recon_loss = 0.19979780912399292, kl_loss = 0.0010617384687066078\n",
      "\n",
      "Epoch 210\n",
      "Step 0: loss = 0.1883765608072281, recon_loss = 0.18837228417396545, kl_loss = 0.0008556079119443893\n",
      "\n",
      "Epoch 211\n",
      "Step 0: loss = 0.18958298861980438, recon_loss = 0.18957826495170593, kl_loss = 0.0009452216327190399\n",
      "\n",
      "Epoch 212\n",
      "Step 0: loss = 0.18244577944278717, recon_loss = 0.1824391782283783, kl_loss = 0.001319671981036663\n",
      "\n",
      "Epoch 213\n",
      "Step 0: loss = 0.18325252830982208, recon_loss = 0.18324865400791168, kl_loss = 0.0007745297625660896\n",
      "\n",
      "Epoch 214\n",
      "Step 0: loss = 0.1900683492422104, recon_loss = 0.19006475806236267, kl_loss = 0.0007192203775048256\n",
      "\n",
      "Epoch 215\n",
      "Step 0: loss = 0.1797676682472229, recon_loss = 0.17976346611976624, kl_loss = 0.0008411966264247894\n",
      "\n",
      "Epoch 216\n",
      "Step 0: loss = 0.17891156673431396, recon_loss = 0.17890843749046326, kl_loss = 0.0006265779957175255\n",
      "\n",
      "Epoch 217\n",
      "Step 0: loss = 0.17346420884132385, recon_loss = 0.17346015572547913, kl_loss = 0.0008111260831356049\n",
      "\n",
      "Epoch 218\n",
      "Step 0: loss = 0.17703081667423248, recon_loss = 0.17702528834342957, kl_loss = 0.0011060452088713646\n",
      "\n",
      "Epoch 219\n",
      "Step 0: loss = 0.17509137094020844, recon_loss = 0.17508813738822937, kl_loss = 0.0006475560367107391\n",
      "\n",
      "Epoch 220\n",
      "Step 0: loss = 0.1823103278875351, recon_loss = 0.1823032796382904, kl_loss = 0.001408844254910946\n",
      "\n",
      "Epoch 221\n",
      "Step 0: loss = 0.1703401505947113, recon_loss = 0.17032960057258606, kl_loss = 0.002110063098371029\n",
      "\n",
      "Epoch 222\n",
      "Step 0: loss = 0.17025701701641083, recon_loss = 0.17024564743041992, kl_loss = 0.0022745253518223763\n",
      "\n",
      "Epoch 223\n",
      "Step 0: loss = 0.1712910234928131, recon_loss = 0.17127832770347595, kl_loss = 0.002540445886552334\n",
      "\n",
      "Epoch 224\n",
      "Step 0: loss = 0.16249032318592072, recon_loss = 0.1624850630760193, kl_loss = 0.0010529803112149239\n",
      "\n",
      "Epoch 225\n",
      "Step 0: loss = 0.17126904428005219, recon_loss = 0.17126479744911194, kl_loss = 0.0008499836549162865\n",
      "\n",
      "Epoch 226\n",
      "Step 0: loss = 0.15999563038349152, recon_loss = 0.15999096632003784, kl_loss = 0.0009339777752757072\n",
      "\n",
      "Epoch 227\n",
      "Step 0: loss = 0.16306781768798828, recon_loss = 0.1630650907754898, kl_loss = 0.0005449568852782249\n",
      "\n",
      "Epoch 228\n",
      "Step 0: loss = 0.15891939401626587, recon_loss = 0.15891581773757935, kl_loss = 0.0007164822891354561\n",
      "\n",
      "Epoch 229\n",
      "Step 0: loss = 0.1682220697402954, recon_loss = 0.1682189702987671, kl_loss = 0.0006187595427036285\n",
      "\n",
      "Epoch 230\n",
      "Step 0: loss = 0.1511455476284027, recon_loss = 0.1511407196521759, kl_loss = 0.0009655142202973366\n",
      "\n",
      "Epoch 231\n",
      "Step 0: loss = 0.1613403558731079, recon_loss = 0.16133755445480347, kl_loss = 0.0005595330148935318\n",
      "\n",
      "Epoch 232\n",
      "Step 0: loss = 0.1509469747543335, recon_loss = 0.15094348788261414, kl_loss = 0.0006965510547161102\n",
      "\n",
      "Epoch 233\n",
      "Step 0: loss = 0.15192514657974243, recon_loss = 0.15192140638828278, kl_loss = 0.0007492806762456894\n",
      "\n",
      "Epoch 234\n",
      "Step 0: loss = 0.1513386070728302, recon_loss = 0.15133601427078247, kl_loss = 0.0005193091928958893\n",
      "\n",
      "Epoch 235\n",
      "Step 0: loss = 0.15160001814365387, recon_loss = 0.15159645676612854, kl_loss = 0.0007110387086868286\n",
      "\n",
      "Epoch 236\n",
      "Step 0: loss = 0.14882127940654755, recon_loss = 0.148818701505661, kl_loss = 0.000514104962348938\n",
      "\n",
      "Epoch 237\n",
      "Step 0: loss = 0.14724735915660858, recon_loss = 0.1472432017326355, kl_loss = 0.0008322326466441154\n",
      "\n",
      "Epoch 238\n",
      "Step 0: loss = 0.14798888564109802, recon_loss = 0.14798396825790405, kl_loss = 0.0009827157482504845\n",
      "\n",
      "Epoch 239\n",
      "Step 0: loss = 0.13893260061740875, recon_loss = 0.1389269232749939, kl_loss = 0.0011345110833644867\n",
      "\n",
      "Epoch 240\n",
      "Step 0: loss = 0.14463354647159576, recon_loss = 0.14462420344352722, kl_loss = 0.0018672049045562744\n",
      "\n",
      "Epoch 241\n",
      "Step 0: loss = 0.14062167704105377, recon_loss = 0.14061178267002106, kl_loss = 0.0019790809601545334\n",
      "\n",
      "Epoch 242\n",
      "Step 0: loss = 0.1400604099035263, recon_loss = 0.14005349576473236, kl_loss = 0.001384219154715538\n",
      "\n",
      "Epoch 243\n",
      "Step 0: loss = 0.1405400186777115, recon_loss = 0.14052928984165192, kl_loss = 0.0021469946950674057\n",
      "\n",
      "Epoch 244\n",
      "Step 0: loss = 0.13550901412963867, recon_loss = 0.135503888130188, kl_loss = 0.0010239733383059502\n",
      "\n",
      "Epoch 245\n",
      "Step 0: loss = 0.1368911862373352, recon_loss = 0.13688817620277405, kl_loss = 0.0006033452227711678\n",
      "\n",
      "Epoch 246\n",
      "Step 0: loss = 0.13415955007076263, recon_loss = 0.1341564655303955, kl_loss = 0.0006167935207486153\n",
      "\n",
      "Epoch 247\n",
      "Step 0: loss = 0.1349928081035614, recon_loss = 0.1349899172782898, kl_loss = 0.0005791820585727692\n",
      "\n",
      "Epoch 248\n",
      "Step 0: loss = 0.12833255529403687, recon_loss = 0.12832915782928467, kl_loss = 0.0006808629259467125\n",
      "\n",
      "Epoch 249\n",
      "Step 0: loss = 0.12687937915325165, recon_loss = 0.12686952948570251, kl_loss = 0.0019687749445438385\n",
      "\n",
      "Epoch 250\n",
      "Step 0: loss = 0.12759920954704285, recon_loss = 0.1275942325592041, kl_loss = 0.0009960802271962166\n",
      "\n",
      "Epoch 251\n",
      "Step 0: loss = 0.1349353790283203, recon_loss = 0.1349291056394577, kl_loss = 0.0012535350397229195\n",
      "\n",
      "Epoch 252\n",
      "Step 0: loss = 0.12484689801931381, recon_loss = 0.12483853101730347, kl_loss = 0.0016731247305870056\n",
      "\n",
      "Epoch 253\n",
      "Step 0: loss = 0.11770360916852951, recon_loss = 0.11769775301218033, kl_loss = 0.0011713048443198204\n",
      "\n",
      "Epoch 254\n",
      "Step 0: loss = 0.11985909938812256, recon_loss = 0.11985353380441666, kl_loss = 0.0011128149926662445\n",
      "\n",
      "Epoch 255\n",
      "Step 0: loss = 0.12128161638975143, recon_loss = 0.12127685546875, kl_loss = 0.000951678492128849\n",
      "\n",
      "Epoch 256\n",
      "Step 0: loss = 0.11740629374980927, recon_loss = 0.11740322411060333, kl_loss = 0.0006133848801255226\n",
      "\n",
      "Epoch 257\n",
      "Step 0: loss = 0.1258978396654129, recon_loss = 0.1258942186832428, kl_loss = 0.0007253400981426239\n",
      "\n",
      "Epoch 258\n",
      "Step 0: loss = 0.11804726719856262, recon_loss = 0.11804423481225967, kl_loss = 0.000606117770075798\n",
      "\n",
      "Epoch 259\n",
      "Step 0: loss = 0.11709705740213394, recon_loss = 0.11709484457969666, kl_loss = 0.00044235028326511383\n",
      "\n",
      "Epoch 260\n",
      "Step 0: loss = 0.11298585683107376, recon_loss = 0.1129821240901947, kl_loss = 0.0007465993985533714\n",
      "\n",
      "Epoch 261\n",
      "Step 0: loss = 0.11370544135570526, recon_loss = 0.11370040476322174, kl_loss = 0.0010071378201246262\n",
      "\n",
      "Epoch 262\n",
      "Step 0: loss = 0.11713552474975586, recon_loss = 0.11713191866874695, kl_loss = 0.0007206369191408157\n",
      "\n",
      "Epoch 263\n",
      "Step 0: loss = 0.11695728451013565, recon_loss = 0.1169537827372551, kl_loss = 0.0007006414234638214\n",
      "\n",
      "Epoch 264\n",
      "Step 0: loss = 0.12036623060703278, recon_loss = 0.12036153674125671, kl_loss = 0.0009385589510202408\n",
      "\n",
      "Epoch 265\n",
      "Step 0: loss = 0.1097041666507721, recon_loss = 0.10969690978527069, kl_loss = 0.0014518620446324348\n",
      "\n",
      "Epoch 266\n",
      "Step 0: loss = 0.11081532388925552, recon_loss = 0.11080336570739746, kl_loss = 0.0023910533636808395\n",
      "\n",
      "Epoch 267\n",
      "Step 0: loss = 0.10710500180721283, recon_loss = 0.10709553956985474, kl_loss = 0.0018924279138445854\n",
      "\n",
      "Epoch 268\n",
      "Step 0: loss = 0.10240792483091354, recon_loss = 0.1024007573723793, kl_loss = 0.0014341920614242554\n",
      "\n",
      "Epoch 269\n",
      "Step 0: loss = 0.10897935926914215, recon_loss = 0.10897456109523773, kl_loss = 0.0009600706398487091\n",
      "\n",
      "Epoch 270\n",
      "Step 0: loss = 0.1111578568816185, recon_loss = 0.11115027964115143, kl_loss = 0.0015159239992499352\n",
      "\n",
      "Epoch 271\n",
      "Step 0: loss = 0.10527943074703217, recon_loss = 0.10527573525905609, kl_loss = 0.0007396666333079338\n",
      "\n",
      "Epoch 272\n",
      "Step 0: loss = 0.09973400831222534, recon_loss = 0.09973101317882538, kl_loss = 0.0005986234173178673\n",
      "\n",
      "Epoch 273\n",
      "Step 0: loss = 0.09643208980560303, recon_loss = 0.0964265763759613, kl_loss = 0.0011028433218598366\n",
      "\n",
      "Epoch 274\n",
      "Step 0: loss = 0.09970197826623917, recon_loss = 0.09969569742679596, kl_loss = 0.001256389543414116\n",
      "\n",
      "Epoch 275\n",
      "Step 0: loss = 0.09719417244195938, recon_loss = 0.09718826413154602, kl_loss = 0.0011815335601568222\n",
      "\n",
      "Epoch 276\n",
      "Step 0: loss = 0.10189533233642578, recon_loss = 0.10188980400562286, kl_loss = 0.0011062882840633392\n",
      "\n",
      "Epoch 277\n",
      "Step 0: loss = 0.09975797683000565, recon_loss = 0.09975163638591766, kl_loss = 0.001268540509045124\n",
      "\n",
      "Epoch 278\n",
      "Step 0: loss = 0.09482043236494064, recon_loss = 0.0948159396648407, kl_loss = 0.0008977996185421944\n",
      "\n",
      "Epoch 279\n",
      "Step 0: loss = 0.10140333324670792, recon_loss = 0.1014004498720169, kl_loss = 0.0005765818059444427\n",
      "\n",
      "Epoch 280\n",
      "Step 0: loss = 0.09514864534139633, recon_loss = 0.09514587372541428, kl_loss = 0.0005547236651182175\n",
      "\n",
      "Epoch 281\n",
      "Step 0: loss = 0.09478539228439331, recon_loss = 0.09478282183408737, kl_loss = 0.0005138777196407318\n",
      "\n",
      "Epoch 282\n",
      "Step 0: loss = 0.0926700308918953, recon_loss = 0.09266771376132965, kl_loss = 0.00046374648809432983\n",
      "\n",
      "Epoch 283\n",
      "Step 0: loss = 0.09224828332662582, recon_loss = 0.09224364161491394, kl_loss = 0.000928463414311409\n",
      "\n",
      "Epoch 284\n",
      "Step 0: loss = 0.092923603951931, recon_loss = 0.09292107820510864, kl_loss = 0.0005045570433139801\n",
      "\n",
      "Epoch 285\n",
      "Step 0: loss = 0.0907098576426506, recon_loss = 0.0907067209482193, kl_loss = 0.000628042034804821\n",
      "\n",
      "Epoch 286\n",
      "Step 0: loss = 0.09352746605873108, recon_loss = 0.09352312982082367, kl_loss = 0.0008679032325744629\n",
      "\n",
      "Epoch 287\n",
      "Step 0: loss = 0.08737870305776596, recon_loss = 0.08737486600875854, kl_loss = 0.0007676500827074051\n",
      "\n",
      "Epoch 288\n",
      "Step 0: loss = 0.08257336169481277, recon_loss = 0.08256760984659195, kl_loss = 0.0011505885049700737\n",
      "\n",
      "Epoch 289\n",
      "Step 0: loss = 0.08504685014486313, recon_loss = 0.08504410088062286, kl_loss = 0.0005492549389600754\n",
      "\n",
      "Epoch 290\n",
      "Step 0: loss = 0.08986534923315048, recon_loss = 0.08986261487007141, kl_loss = 0.000547436997294426\n",
      "\n",
      "Epoch 291\n",
      "Step 0: loss = 0.08417081087827682, recon_loss = 0.08416713774204254, kl_loss = 0.0007344717159867287\n",
      "\n",
      "Epoch 292\n",
      "Step 0: loss = 0.08369951695203781, recon_loss = 0.08368847519159317, kl_loss = 0.002208021469414234\n",
      "\n",
      "Epoch 293\n",
      "Step 0: loss = 0.08177555352449417, recon_loss = 0.08176448196172714, kl_loss = 0.002213922329246998\n",
      "\n",
      "Epoch 294\n",
      "Step 0: loss = 0.0821293294429779, recon_loss = 0.08212213963270187, kl_loss = 0.0014383383095264435\n",
      "\n",
      "Epoch 295\n",
      "Step 0: loss = 0.08095120638608932, recon_loss = 0.08094903826713562, kl_loss = 0.0004330286756157875\n",
      "\n",
      "Epoch 296\n",
      "Step 0: loss = 0.08280850946903229, recon_loss = 0.08280406147241592, kl_loss = 0.000889820046722889\n",
      "\n",
      "Epoch 297\n",
      "Step 0: loss = 0.07933490723371506, recon_loss = 0.07932720333337784, kl_loss = 0.0015411656349897385\n",
      "\n",
      "Epoch 298\n",
      "Step 0: loss = 0.07706958055496216, recon_loss = 0.07706301659345627, kl_loss = 0.0013127624988555908\n",
      "\n",
      "Epoch 299\n",
      "Step 0: loss = 0.07958357036113739, recon_loss = 0.07957924902439117, kl_loss = 0.0008642766624689102\n",
      "\n",
      "Epoch 300\n",
      "Step 0: loss = 0.0766829401254654, recon_loss = 0.07667863368988037, kl_loss = 0.0008605942130088806\n",
      "\n",
      "Epoch 301\n",
      "Step 0: loss = 0.08355557918548584, recon_loss = 0.08355017751455307, kl_loss = 0.0010809022933244705\n",
      "\n",
      "Epoch 302\n",
      "Step 0: loss = 0.08261557668447495, recon_loss = 0.08261154592037201, kl_loss = 0.0008057057857513428\n",
      "\n",
      "Epoch 303\n",
      "Step 0: loss = 0.07983570545911789, recon_loss = 0.07983227074146271, kl_loss = 0.0006875507533550262\n",
      "\n",
      "Epoch 304\n",
      "Step 0: loss = 0.07485835254192352, recon_loss = 0.0748559832572937, kl_loss = 0.00047425832599401474\n",
      "\n",
      "Epoch 305\n",
      "Step 0: loss = 0.07944170385599136, recon_loss = 0.07943850010633469, kl_loss = 0.000640111044049263\n",
      "\n",
      "Epoch 306\n",
      "Step 0: loss = 0.0758538618683815, recon_loss = 0.07584705203771591, kl_loss = 0.0013620154932141304\n",
      "\n",
      "Epoch 307\n",
      "Step 0: loss = 0.07468592375516891, recon_loss = 0.07468174397945404, kl_loss = 0.0008356878533959389\n",
      "\n",
      "Epoch 308\n",
      "Step 0: loss = 0.07167244702577591, recon_loss = 0.0716686025261879, kl_loss = 0.0007683020085096359\n",
      "\n",
      "Epoch 309\n",
      "Step 0: loss = 0.07284042239189148, recon_loss = 0.07283724099397659, kl_loss = 0.0006366567686200142\n",
      "\n",
      "Epoch 310\n",
      "Step 0: loss = 0.0742199644446373, recon_loss = 0.0742168053984642, kl_loss = 0.0006320634856820107\n",
      "\n",
      "Epoch 311\n",
      "Step 0: loss = 0.0702553391456604, recon_loss = 0.07024850696325302, kl_loss = 0.0013669580221176147\n",
      "\n",
      "Epoch 312\n",
      "Step 0: loss = 0.07494144886732101, recon_loss = 0.07493609935045242, kl_loss = 0.001070445403456688\n",
      "\n",
      "Epoch 313\n",
      "Step 0: loss = 0.0680384486913681, recon_loss = 0.06803455203771591, kl_loss = 0.0007789898663759232\n",
      "\n",
      "Epoch 314\n",
      "Step 0: loss = 0.06670890003442764, recon_loss = 0.06670315563678741, kl_loss = 0.0011494392529129982\n",
      "\n",
      "Epoch 315\n",
      "Step 0: loss = 0.06946088373661041, recon_loss = 0.06945781409740448, kl_loss = 0.0006145592778921127\n",
      "\n",
      "Epoch 316\n",
      "Step 0: loss = 0.07237937301397324, recon_loss = 0.07237455993890762, kl_loss = 0.0009627584367990494\n",
      "\n",
      "Epoch 317\n",
      "Step 0: loss = 0.07017101347446442, recon_loss = 0.07016561925411224, kl_loss = 0.001078338362276554\n",
      "\n",
      "Epoch 318\n",
      "Step 0: loss = 0.06291884928941727, recon_loss = 0.06291579455137253, kl_loss = 0.0006106588989496231\n",
      "\n",
      "Epoch 319\n",
      "Step 0: loss = 0.07319249957799911, recon_loss = 0.07317608594894409, kl_loss = 0.00328235886991024\n",
      "\n",
      "Epoch 320\n",
      "Step 0: loss = 0.06619633734226227, recon_loss = 0.06619296967983246, kl_loss = 0.0006729243323206902\n",
      "\n",
      "Epoch 321\n",
      "Step 0: loss = 0.06833680719137192, recon_loss = 0.06831100583076477, kl_loss = 0.005159533582627773\n",
      "\n",
      "Epoch 322\n",
      "Step 0: loss = 0.06254822015762329, recon_loss = 0.06254570186138153, kl_loss = 0.0005036210641264915\n",
      "\n",
      "Epoch 323\n",
      "Step 0: loss = 0.059786491096019745, recon_loss = 0.05978080630302429, kl_loss = 0.0011370619758963585\n",
      "\n",
      "Epoch 324\n",
      "Step 0: loss = 0.06519858539104462, recon_loss = 0.06519479304552078, kl_loss = 0.0007582986727356911\n",
      "\n",
      "Epoch 325\n",
      "Step 0: loss = 0.06501229107379913, recon_loss = 0.065008245408535, kl_loss = 0.0008086254820227623\n",
      "\n",
      "Epoch 326\n",
      "Step 0: loss = 0.058950185775756836, recon_loss = 0.05894593149423599, kl_loss = 0.0008505275472998619\n",
      "\n",
      "Epoch 327\n",
      "Step 0: loss = 0.06498666852712631, recon_loss = 0.0649806335568428, kl_loss = 0.0012068096548318863\n",
      "\n",
      "Epoch 328\n",
      "Step 0: loss = 0.06322977691888809, recon_loss = 0.06322632730007172, kl_loss = 0.0006895903497934341\n",
      "\n",
      "Epoch 329\n",
      "Step 0: loss = 0.05893659591674805, recon_loss = 0.05893237888813019, kl_loss = 0.0008436078205704689\n",
      "\n",
      "Epoch 330\n",
      "Step 0: loss = 0.05784875899553299, recon_loss = 0.05784345418214798, kl_loss = 0.001061062328517437\n",
      "\n",
      "Epoch 331\n",
      "Step 0: loss = 0.06120194494724274, recon_loss = 0.06119745969772339, kl_loss = 0.0008969223126769066\n",
      "\n",
      "Epoch 332\n",
      "Step 0: loss = 0.059438467025756836, recon_loss = 0.05943581834435463, kl_loss = 0.0005301004275679588\n",
      "\n",
      "Epoch 333\n",
      "Step 0: loss = 0.05767396464943886, recon_loss = 0.057671815156936646, kl_loss = 0.0004299413412809372\n",
      "\n",
      "Epoch 334\n",
      "Step 0: loss = 0.05859774723649025, recon_loss = 0.05859562009572983, kl_loss = 0.00042546726763248444\n",
      "\n",
      "Epoch 335\n",
      "Step 0: loss = 0.05613204091787338, recon_loss = 0.05613026022911072, kl_loss = 0.00035626906901597977\n",
      "\n",
      "Epoch 336\n",
      "Step 0: loss = 0.06469370424747467, recon_loss = 0.06468544155359268, kl_loss = 0.0016531040892004967\n",
      "\n",
      "Epoch 337\n",
      "Step 0: loss = 0.054879482835531235, recon_loss = 0.054877109825611115, kl_loss = 0.0004748590290546417\n",
      "\n",
      "Epoch 338\n",
      "Step 0: loss = 0.06049064174294472, recon_loss = 0.060487449169158936, kl_loss = 0.0006381664425134659\n",
      "\n",
      "Epoch 339\n",
      "Step 0: loss = 0.056751836091279984, recon_loss = 0.056748319417238235, kl_loss = 0.0007036970928311348\n",
      "\n",
      "Epoch 340\n",
      "Step 0: loss = 0.05568750202655792, recon_loss = 0.055677466094493866, kl_loss = 0.002006862312555313\n",
      "\n",
      "Epoch 341\n",
      "Step 0: loss = 0.056938640773296356, recon_loss = 0.0569276437163353, kl_loss = 0.002199186012148857\n",
      "\n",
      "Epoch 342\n",
      "Step 0: loss = 0.0526089109480381, recon_loss = 0.05260222405195236, kl_loss = 0.0013371454551815987\n",
      "\n",
      "Epoch 343\n",
      "Step 0: loss = 0.05380933731794357, recon_loss = 0.05379744991660118, kl_loss = 0.002377639524638653\n",
      "\n",
      "Epoch 344\n",
      "Step 0: loss = 0.05388450250029564, recon_loss = 0.05386523902416229, kl_loss = 0.0038529830053448677\n",
      "\n",
      "Epoch 345\n",
      "Step 0: loss = 0.05424530431628227, recon_loss = 0.05424000322818756, kl_loss = 0.0010601570829749107\n",
      "\n",
      "Epoch 346\n",
      "Step 0: loss = 0.05107884109020233, recon_loss = 0.05107423663139343, kl_loss = 0.0009211953729391098\n",
      "\n",
      "Epoch 347\n",
      "Step 0: loss = 0.050047870725393295, recon_loss = 0.05004355311393738, kl_loss = 0.0008638883009552956\n",
      "\n",
      "Epoch 348\n",
      "Step 0: loss = 0.052435990422964096, recon_loss = 0.052426572889089584, kl_loss = 0.0018837768584489822\n",
      "\n",
      "Epoch 349\n",
      "Step 0: loss = 0.04972950369119644, recon_loss = 0.04972660541534424, kl_loss = 0.0005800239741802216\n",
      "\n",
      "Epoch 350\n",
      "Step 0: loss = 0.05053149536252022, recon_loss = 0.050527699291706085, kl_loss = 0.0007590204477310181\n",
      "\n",
      "Epoch 351\n",
      "Step 0: loss = 0.05096123740077019, recon_loss = 0.05095706135034561, kl_loss = 0.000835116021335125\n",
      "\n",
      "Epoch 352\n",
      "Step 0: loss = 0.05016360059380531, recon_loss = 0.05016094073653221, kl_loss = 0.0005320683121681213\n",
      "\n",
      "Epoch 353\n",
      "Step 0: loss = 0.04900306090712547, recon_loss = 0.04900059849023819, kl_loss = 0.0004925848916172981\n",
      "\n",
      "Epoch 354\n",
      "Step 0: loss = 0.05158922076225281, recon_loss = 0.05158586800098419, kl_loss = 0.0006702560931444168\n",
      "\n",
      "Epoch 355\n",
      "Step 0: loss = 0.047517139464616776, recon_loss = 0.047514863312244415, kl_loss = 0.00045503396540880203\n",
      "\n",
      "Epoch 356\n",
      "Step 0: loss = 0.047022439539432526, recon_loss = 0.04701879248023033, kl_loss = 0.000729546882212162\n",
      "\n",
      "Epoch 357\n",
      "Step 0: loss = 0.045255064964294434, recon_loss = 0.04525270313024521, kl_loss = 0.00047230347990989685\n",
      "\n",
      "Epoch 358\n",
      "Step 0: loss = 0.0498306043446064, recon_loss = 0.04982638359069824, kl_loss = 0.000844467431306839\n",
      "\n",
      "Epoch 359\n",
      "Step 0: loss = 0.043625544756650925, recon_loss = 0.043619781732559204, kl_loss = 0.0011525694280862808\n",
      "\n",
      "Epoch 360\n",
      "Step 0: loss = 0.04179653897881508, recon_loss = 0.041786156594753265, kl_loss = 0.002076738514006138\n",
      "\n",
      "Epoch 361\n",
      "Step 0: loss = 0.043650347739458084, recon_loss = 0.04364057630300522, kl_loss = 0.001954449340701103\n",
      "\n",
      "Epoch 362\n",
      "Step 0: loss = 0.047485288232564926, recon_loss = 0.047479815781116486, kl_loss = 0.0010948032140731812\n",
      "\n",
      "Epoch 363\n",
      "Step 0: loss = 0.04577752575278282, recon_loss = 0.04577108100056648, kl_loss = 0.0012886328622698784\n",
      "\n",
      "Epoch 364\n",
      "Step 0: loss = 0.045998912304639816, recon_loss = 0.04599545896053314, kl_loss = 0.0006908224895596504\n",
      "\n",
      "Epoch 365\n",
      "Step 0: loss = 0.043351393193006516, recon_loss = 0.043341681361198425, kl_loss = 0.001942603848874569\n",
      "\n",
      "Epoch 366\n",
      "Step 0: loss = 0.041449517011642456, recon_loss = 0.04144115746021271, kl_loss = 0.0016718804836273193\n",
      "\n",
      "Epoch 367\n",
      "Step 0: loss = 0.045397255569696426, recon_loss = 0.0453927144408226, kl_loss = 0.0009082825854420662\n",
      "\n",
      "Epoch 368\n",
      "Step 0: loss = 0.0424429252743721, recon_loss = 0.042435385286808014, kl_loss = 0.0015077115967869759\n",
      "\n",
      "Epoch 369\n",
      "Step 0: loss = 0.045426297932863235, recon_loss = 0.04542010650038719, kl_loss = 0.0012384578585624695\n",
      "\n",
      "Epoch 370\n",
      "Step 0: loss = 0.04367817938327789, recon_loss = 0.043674029409885406, kl_loss = 0.0008297329768538475\n",
      "\n",
      "Epoch 371\n",
      "Step 0: loss = 0.038794174790382385, recon_loss = 0.0387892872095108, kl_loss = 0.0009777965024113655\n",
      "\n",
      "Epoch 372\n",
      "Step 0: loss = 0.04037576913833618, recon_loss = 0.04036877676844597, kl_loss = 0.0013981899246573448\n",
      "\n",
      "Epoch 373\n",
      "Step 0: loss = 0.036630529910326004, recon_loss = 0.03662729263305664, kl_loss = 0.0006478093564510345\n",
      "\n",
      "Epoch 374\n",
      "Step 0: loss = 0.043973762542009354, recon_loss = 0.043969493359327316, kl_loss = 0.000853504054248333\n",
      "\n",
      "Epoch 375\n",
      "Step 0: loss = 0.04226161167025566, recon_loss = 0.04225656017661095, kl_loss = 0.0010100146755576134\n",
      "\n",
      "Epoch 376\n",
      "Step 0: loss = 0.041063155978918076, recon_loss = 0.04105798155069351, kl_loss = 0.0010346733033657074\n",
      "\n",
      "Epoch 377\n",
      "Step 0: loss = 0.04114050790667534, recon_loss = 0.041130609810352325, kl_loss = 0.001979408785700798\n",
      "\n",
      "Epoch 378\n",
      "Step 0: loss = 0.04015250876545906, recon_loss = 0.040135759860277176, kl_loss = 0.0033496730029582977\n",
      "\n",
      "Epoch 379\n",
      "Step 0: loss = 0.03785726800560951, recon_loss = 0.03785216435790062, kl_loss = 0.001020745374262333\n",
      "\n",
      "Epoch 380\n",
      "Step 0: loss = 0.03772655501961708, recon_loss = 0.03772181272506714, kl_loss = 0.0009483508765697479\n",
      "\n",
      "Epoch 381\n",
      "Step 0: loss = 0.039040178060531616, recon_loss = 0.03903423249721527, kl_loss = 0.0011891154572367668\n",
      "\n",
      "Epoch 382\n",
      "Step 0: loss = 0.03597364202141762, recon_loss = 0.03596842288970947, kl_loss = 0.0010435543954372406\n",
      "\n",
      "Epoch 383\n",
      "Step 0: loss = 0.0367460735142231, recon_loss = 0.036742378026247025, kl_loss = 0.0007392261177301407\n",
      "\n",
      "Epoch 384\n",
      "Step 0: loss = 0.03912199288606644, recon_loss = 0.03911849111318588, kl_loss = 0.0007005389779806137\n",
      "\n",
      "Epoch 385\n",
      "Step 0: loss = 0.03579885885119438, recon_loss = 0.035796329379081726, kl_loss = 0.000506172887980938\n",
      "\n",
      "Epoch 386\n",
      "Step 0: loss = 0.036177728325128555, recon_loss = 0.03617450222373009, kl_loss = 0.0006450442597270012\n",
      "\n",
      "Epoch 387\n",
      "Step 0: loss = 0.03768939524888992, recon_loss = 0.037684082984924316, kl_loss = 0.0010626539587974548\n",
      "\n",
      "Epoch 388\n",
      "Step 0: loss = 0.04012960195541382, recon_loss = 0.04012539982795715, kl_loss = 0.0008403677493333817\n",
      "\n",
      "Epoch 389\n",
      "Step 0: loss = 0.0383756160736084, recon_loss = 0.03837183862924576, kl_loss = 0.0007554832845926285\n",
      "\n",
      "Epoch 390\n",
      "Step 0: loss = 0.03624619543552399, recon_loss = 0.03624391928315163, kl_loss = 0.0004549184814095497\n",
      "\n",
      "Epoch 391\n",
      "Step 0: loss = 0.03664068505167961, recon_loss = 0.036638207733631134, kl_loss = 0.0004954831674695015\n",
      "\n",
      "Epoch 392\n",
      "Step 0: loss = 0.035707518458366394, recon_loss = 0.03570488840341568, kl_loss = 0.0005260249599814415\n",
      "\n",
      "Epoch 393\n",
      "Step 0: loss = 0.0335298515856266, recon_loss = 0.03352560102939606, kl_loss = 0.0008501587435603142\n",
      "\n",
      "Epoch 394\n",
      "Step 0: loss = 0.03542470559477806, recon_loss = 0.03541341423988342, kl_loss = 0.0022579673677682877\n",
      "\n",
      "Epoch 395\n",
      "Step 0: loss = 0.03437632694840431, recon_loss = 0.03437207639217377, kl_loss = 0.0008500907570123672\n",
      "\n",
      "Epoch 396\n",
      "Step 0: loss = 0.03587624803185463, recon_loss = 0.03586722910404205, kl_loss = 0.001803847961127758\n",
      "\n",
      "Epoch 397\n",
      "Step 0: loss = 0.03636781871318817, recon_loss = 0.03636210039258003, kl_loss = 0.001143486239016056\n",
      "\n",
      "Epoch 398\n",
      "Step 0: loss = 0.037420403212308884, recon_loss = 0.0374162495136261, kl_loss = 0.0008306819945573807\n",
      "\n",
      "Epoch 399\n",
      "Step 0: loss = 0.03305632993578911, recon_loss = 0.03305163234472275, kl_loss = 0.0009393719956278801\n",
      "\n",
      "Epoch 400\n",
      "Step 0: loss = 0.03236810863018036, recon_loss = 0.032362572848796844, kl_loss = 0.0011073099449276924\n",
      "\n",
      "Epoch 401\n",
      "Step 0: loss = 0.034088391810655594, recon_loss = 0.034078821539878845, kl_loss = 0.0019140997901558876\n",
      "\n",
      "Epoch 402\n",
      "Step 0: loss = 0.030826590955257416, recon_loss = 0.030822910368442535, kl_loss = 0.0007362989708781242\n",
      "\n",
      "Epoch 403\n",
      "Step 0: loss = 0.03185450658202171, recon_loss = 0.031849220395088196, kl_loss = 0.001057405024766922\n",
      "\n",
      "Epoch 404\n",
      "Step 0: loss = 0.03261213004589081, recon_loss = 0.032608695328235626, kl_loss = 0.0006868802011013031\n",
      "\n",
      "Epoch 405\n",
      "Step 0: loss = 0.028722451999783516, recon_loss = 0.028714608401060104, kl_loss = 0.0015688026323914528\n",
      "\n",
      "Epoch 406\n",
      "Step 0: loss = 0.03365554288029671, recon_loss = 0.033649053424596786, kl_loss = 0.0012976918369531631\n",
      "\n",
      "Epoch 407\n",
      "Step 0: loss = 0.02975960448384285, recon_loss = 0.029752736911177635, kl_loss = 0.0013733496889472008\n",
      "\n",
      "Epoch 408\n",
      "Step 0: loss = 0.03056502714753151, recon_loss = 0.030560055747628212, kl_loss = 0.0009944476187229156\n",
      "\n",
      "Epoch 409\n",
      "Step 0: loss = 0.03193441033363342, recon_loss = 0.03192979842424393, kl_loss = 0.000922229140996933\n",
      "\n",
      "Epoch 410\n",
      "Step 0: loss = 0.028697419911623, recon_loss = 0.02869068831205368, kl_loss = 0.0013463245704770088\n",
      "\n",
      "Epoch 411\n",
      "Step 0: loss = 0.02882459945976734, recon_loss = 0.028821811079978943, kl_loss = 0.0005578519776463509\n",
      "\n",
      "Epoch 412\n",
      "Step 0: loss = 0.027088666334748268, recon_loss = 0.02707911655306816, kl_loss = 0.0019098687916994095\n",
      "\n",
      "Epoch 413\n",
      "Step 0: loss = 0.031503964215517044, recon_loss = 0.0314982570707798, kl_loss = 0.0011413907632231712\n",
      "\n",
      "Epoch 414\n",
      "Step 0: loss = 0.028633106499910355, recon_loss = 0.028619583696126938, kl_loss = 0.002704532817006111\n",
      "\n",
      "Epoch 415\n",
      "Step 0: loss = 0.0281654205173254, recon_loss = 0.0281501617282629, kl_loss = 0.003051663748919964\n",
      "\n",
      "Epoch 416\n",
      "Step 0: loss = 0.028655696660280228, recon_loss = 0.028648143634200096, kl_loss = 0.0015104953199625015\n",
      "\n",
      "Epoch 417\n",
      "Step 0: loss = 0.028073584660887718, recon_loss = 0.028066877275705338, kl_loss = 0.0013416251167654991\n",
      "\n",
      "Epoch 418\n",
      "Step 0: loss = 0.025668740272521973, recon_loss = 0.025658445432782173, kl_loss = 0.0020590713247656822\n",
      "\n",
      "Epoch 419\n",
      "Step 0: loss = 0.02900911495089531, recon_loss = 0.029005249962210655, kl_loss = 0.0007730871438980103\n",
      "\n",
      "Epoch 420\n",
      "Step 0: loss = 0.02701721340417862, recon_loss = 0.027005571871995926, kl_loss = 0.0023282701149582863\n",
      "\n",
      "Epoch 421\n",
      "Step 0: loss = 0.029114868491888046, recon_loss = 0.02911091037094593, kl_loss = 0.0007916633039712906\n",
      "\n",
      "Epoch 422\n",
      "Step 0: loss = 0.02751775085926056, recon_loss = 0.02751278504729271, kl_loss = 0.0009931400418281555\n",
      "\n",
      "Epoch 423\n",
      "Step 0: loss = 0.02653617411851883, recon_loss = 0.026528215035796165, kl_loss = 0.0015918491408228874\n",
      "\n",
      "Epoch 424\n",
      "Step 0: loss = 0.024021856486797333, recon_loss = 0.024017229676246643, kl_loss = 0.000925237312912941\n",
      "\n",
      "Epoch 425\n",
      "Step 0: loss = 0.026847338303923607, recon_loss = 0.026844356209039688, kl_loss = 0.0005964580923318863\n",
      "\n",
      "Epoch 426\n",
      "Step 0: loss = 0.026353538036346436, recon_loss = 0.026343483477830887, kl_loss = 0.002010740339756012\n",
      "\n",
      "Epoch 427\n",
      "Step 0: loss = 0.026766514405608177, recon_loss = 0.026761336252093315, kl_loss = 0.0010356903076171875\n",
      "\n",
      "Epoch 428\n",
      "Step 0: loss = 0.023679809644818306, recon_loss = 0.023675117641687393, kl_loss = 0.0009384760633111\n",
      "\n",
      "Epoch 429\n",
      "Step 0: loss = 0.0253671295940876, recon_loss = 0.025363562628626823, kl_loss = 0.0007132356986403465\n",
      "\n",
      "Epoch 430\n",
      "Step 0: loss = 0.023862266913056374, recon_loss = 0.023860041052103043, kl_loss = 0.0004451824352145195\n",
      "\n",
      "Epoch 431\n",
      "Step 0: loss = 0.023744462057948112, recon_loss = 0.023741470649838448, kl_loss = 0.0005981475114822388\n",
      "\n",
      "Epoch 432\n",
      "Step 0: loss = 0.02504611387848854, recon_loss = 0.025037992745637894, kl_loss = 0.0016243644058704376\n",
      "\n",
      "Epoch 433\n",
      "Step 0: loss = 0.02496465854346752, recon_loss = 0.02495788224041462, kl_loss = 0.0013552280142903328\n",
      "\n",
      "Epoch 434\n",
      "Step 0: loss = 0.026188500225543976, recon_loss = 0.02618459425866604, kl_loss = 0.0007813731208443642\n",
      "\n",
      "Epoch 435\n",
      "Step 0: loss = 0.02519727125763893, recon_loss = 0.025191280990839005, kl_loss = 0.0011981818825006485\n",
      "\n",
      "Epoch 436\n",
      "Step 0: loss = 0.022355567663908005, recon_loss = 0.022345194593071938, kl_loss = 0.002074676565825939\n",
      "\n",
      "Epoch 437\n",
      "Step 0: loss = 0.02338596247136593, recon_loss = 0.023379620164632797, kl_loss = 0.0012685703113675117\n",
      "\n",
      "Epoch 438\n",
      "Step 0: loss = 0.02178523689508438, recon_loss = 0.021782437339425087, kl_loss = 0.0005599707365036011\n",
      "\n",
      "Epoch 439\n",
      "Step 0: loss = 0.023956969380378723, recon_loss = 0.023951377719640732, kl_loss = 0.001118311658501625\n",
      "\n",
      "Epoch 440\n",
      "Step 0: loss = 0.02245405688881874, recon_loss = 0.022446660324931145, kl_loss = 0.0014792513102293015\n",
      "\n",
      "Epoch 441\n",
      "Step 0: loss = 0.02518432028591633, recon_loss = 0.02517986111342907, kl_loss = 0.0008920133113861084\n",
      "\n",
      "Epoch 442\n",
      "Step 0: loss = 0.022254223003983498, recon_loss = 0.022249538451433182, kl_loss = 0.0009370399639010429\n",
      "\n",
      "Epoch 443\n",
      "Step 0: loss = 0.023907890543341637, recon_loss = 0.02390340343117714, kl_loss = 0.000897502526640892\n",
      "\n",
      "Epoch 444\n",
      "Step 0: loss = 0.020793989300727844, recon_loss = 0.020791493356227875, kl_loss = 0.0004991311579942703\n",
      "\n",
      "Epoch 445\n",
      "Step 0: loss = 0.02280900627374649, recon_loss = 0.022805362939834595, kl_loss = 0.0007287804037332535\n",
      "\n",
      "Epoch 446\n",
      "Step 0: loss = 0.02280629239976406, recon_loss = 0.022799072787165642, kl_loss = 0.0014440026134252548\n",
      "\n",
      "Epoch 447\n",
      "Step 0: loss = 0.024470876902341843, recon_loss = 0.024467431008815765, kl_loss = 0.0006890557706356049\n",
      "\n",
      "Epoch 448\n",
      "Step 0: loss = 0.02282940410077572, recon_loss = 0.022822745144367218, kl_loss = 0.0013317251577973366\n",
      "\n",
      "Epoch 449\n",
      "Step 0: loss = 0.019602598622441292, recon_loss = 0.01959901489317417, kl_loss = 0.0007168296724557877\n",
      "\n",
      "Epoch 450\n",
      "Step 0: loss = 0.020629456266760826, recon_loss = 0.020625343546271324, kl_loss = 0.0008224537596106529\n",
      "\n",
      "Epoch 451\n",
      "Step 0: loss = 0.022753078490495682, recon_loss = 0.022748898714780807, kl_loss = 0.0008357921615242958\n",
      "\n",
      "Epoch 452\n",
      "Step 0: loss = 0.021665705367922783, recon_loss = 0.02166183851659298, kl_loss = 0.0007731970399618149\n",
      "\n",
      "Epoch 453\n",
      "Step 0: loss = 0.021377533674240112, recon_loss = 0.021373003721237183, kl_loss = 0.0009059468284249306\n",
      "\n",
      "Epoch 454\n",
      "Step 0: loss = 0.018506942316889763, recon_loss = 0.018499495461583138, kl_loss = 0.0014892565086483955\n",
      "\n",
      "Epoch 455\n",
      "Step 0: loss = 0.020347364246845245, recon_loss = 0.020342154428362846, kl_loss = 0.0010419068858027458\n",
      "\n",
      "Epoch 456\n",
      "Step 0: loss = 0.02176433801651001, recon_loss = 0.02175970748066902, kl_loss = 0.000926104374229908\n",
      "\n",
      "Epoch 457\n",
      "Step 0: loss = 0.021494029089808464, recon_loss = 0.021484456956386566, kl_loss = 0.0019143112003803253\n",
      "\n",
      "Epoch 458\n",
      "Step 0: loss = 0.019193921238183975, recon_loss = 0.01918811909854412, kl_loss = 0.0011603347957134247\n",
      "\n",
      "Epoch 459\n",
      "Step 0: loss = 0.01756916753947735, recon_loss = 0.017565598711371422, kl_loss = 0.0007137442007660866\n",
      "\n",
      "Epoch 460\n",
      "Step 0: loss = 0.01802959479391575, recon_loss = 0.018025893718004227, kl_loss = 0.00074032973498106\n",
      "\n",
      "Epoch 461\n",
      "Step 0: loss = 0.020312508568167686, recon_loss = 0.020308267325162888, kl_loss = 0.0008481843397021294\n",
      "\n",
      "Epoch 462\n",
      "Step 0: loss = 0.019254429265856743, recon_loss = 0.01924474909901619, kl_loss = 0.0019359225407242775\n",
      "\n",
      "Epoch 463\n",
      "Step 0: loss = 0.017923792824149132, recon_loss = 0.017911512404680252, kl_loss = 0.0024562161415815353\n",
      "\n",
      "Epoch 464\n",
      "Step 0: loss = 0.017346270382404327, recon_loss = 0.01734226942062378, kl_loss = 0.0008002622053027153\n",
      "\n",
      "Epoch 465\n",
      "Step 0: loss = 0.01910627819597721, recon_loss = 0.019100377336144447, kl_loss = 0.0011800257489085197\n",
      "\n",
      "Epoch 466\n",
      "Step 0: loss = 0.020308073610067368, recon_loss = 0.020304657518863678, kl_loss = 0.0006831232458353043\n",
      "\n",
      "Epoch 467\n",
      "Step 0: loss = 0.019993843510746956, recon_loss = 0.019986355677247047, kl_loss = 0.0014976318925619125\n",
      "\n",
      "Epoch 468\n",
      "Step 0: loss = 0.01737518236041069, recon_loss = 0.01736987568438053, kl_loss = 0.0010613314807415009\n",
      "\n",
      "Epoch 469\n",
      "Step 0: loss = 0.020760688930749893, recon_loss = 0.020755942910909653, kl_loss = 0.0009493231773376465\n",
      "\n",
      "Epoch 470\n",
      "Step 0: loss = 0.01729731261730194, recon_loss = 0.017291687428951263, kl_loss = 0.001125073991715908\n",
      "\n",
      "Epoch 471\n",
      "Step 0: loss = 0.019306743517518044, recon_loss = 0.01930440217256546, kl_loss = 0.00046820659190416336\n",
      "\n",
      "Epoch 472\n",
      "Step 0: loss = 0.018585972487926483, recon_loss = 0.018581174314022064, kl_loss = 0.0009595230221748352\n",
      "\n",
      "Epoch 473\n",
      "Step 0: loss = 0.016748495399951935, recon_loss = 0.016743920743465424, kl_loss = 0.0009150085970759392\n",
      "\n",
      "Epoch 474\n",
      "Step 0: loss = 0.017868725582957268, recon_loss = 0.017863452434539795, kl_loss = 0.0010547284036874771\n",
      "\n",
      "Epoch 475\n",
      "Step 0: loss = 0.01635880209505558, recon_loss = 0.01635444536805153, kl_loss = 0.0008714450523257256\n",
      "\n",
      "Epoch 476\n",
      "Step 0: loss = 0.017098477110266685, recon_loss = 0.01709093153476715, kl_loss = 0.0015089306980371475\n",
      "\n",
      "Epoch 477\n",
      "Step 0: loss = 0.016474753618240356, recon_loss = 0.01646900735795498, kl_loss = 0.0011492790654301643\n",
      "\n",
      "Epoch 478\n",
      "Step 0: loss = 0.018168533220887184, recon_loss = 0.018165167421102524, kl_loss = 0.0006732037290930748\n",
      "\n",
      "Epoch 479\n",
      "Step 0: loss = 0.015133704990148544, recon_loss = 0.015128932893276215, kl_loss = 0.0009544119238853455\n",
      "\n",
      "Epoch 480\n",
      "Step 0: loss = 0.016095951199531555, recon_loss = 0.016082527115941048, kl_loss = 0.002684704028069973\n",
      "\n",
      "Epoch 481\n",
      "Step 0: loss = 0.016910409554839134, recon_loss = 0.016905810683965683, kl_loss = 0.0009197909384965897\n",
      "\n",
      "Epoch 482\n",
      "Step 0: loss = 0.016619540750980377, recon_loss = 0.016616065055131912, kl_loss = 0.0006949827075004578\n",
      "\n",
      "Epoch 483\n",
      "Step 0: loss = 0.015500485897064209, recon_loss = 0.015497185289859772, kl_loss = 0.0006601391360163689\n",
      "\n",
      "Epoch 484\n",
      "Step 0: loss = 0.015634141862392426, recon_loss = 0.015628281980752945, kl_loss = 0.0011720042675733566\n",
      "\n",
      "Epoch 485\n",
      "Step 0: loss = 0.015612655319273472, recon_loss = 0.015606541186571121, kl_loss = 0.0012229066342115402\n",
      "\n",
      "Epoch 486\n",
      "Step 0: loss = 0.015269548632204533, recon_loss = 0.015265807509422302, kl_loss = 0.0007483046501874924\n",
      "\n",
      "Epoch 487\n",
      "Step 0: loss = 0.017160454764962196, recon_loss = 0.017154688015580177, kl_loss = 0.0011532753705978394\n",
      "\n",
      "Epoch 488\n",
      "Step 0: loss = 0.015526533126831055, recon_loss = 0.01552136056125164, kl_loss = 0.0010344348847866058\n",
      "\n",
      "Epoch 489\n",
      "Step 0: loss = 0.014623356983065605, recon_loss = 0.01461622305214405, kl_loss = 0.0014267750084400177\n",
      "\n",
      "Epoch 490\n",
      "Step 0: loss = 0.013771342113614082, recon_loss = 0.013764195144176483, kl_loss = 0.0014294078573584557\n",
      "\n",
      "Epoch 491\n",
      "Step 0: loss = 0.01624976471066475, recon_loss = 0.01624300330877304, kl_loss = 0.0013522608205676079\n",
      "\n",
      "Epoch 492\n",
      "Step 0: loss = 0.015553484670817852, recon_loss = 0.01554950699210167, kl_loss = 0.0007954565808176994\n",
      "\n",
      "Epoch 493\n",
      "Step 0: loss = 0.013599617406725883, recon_loss = 0.013595491647720337, kl_loss = 0.000825139693915844\n",
      "\n",
      "Epoch 494\n",
      "Step 0: loss = 0.015135275200009346, recon_loss = 0.015130272135138512, kl_loss = 0.0010006623342633247\n",
      "\n",
      "Epoch 495\n",
      "Step 0: loss = 0.015346555970609188, recon_loss = 0.015341823920607567, kl_loss = 0.0009463876485824585\n",
      "\n",
      "Epoch 496\n",
      "Step 0: loss = 0.013907603919506073, recon_loss = 0.013901440426707268, kl_loss = 0.0012326166033744812\n",
      "\n",
      "Epoch 497\n",
      "Step 0: loss = 0.012945332564413548, recon_loss = 0.012942105531692505, kl_loss = 0.0006453199312090874\n",
      "\n",
      "Epoch 498\n",
      "Step 0: loss = 0.013226003386080265, recon_loss = 0.013219326734542847, kl_loss = 0.0013353070244193077\n",
      "\n",
      "Epoch 499\n",
      "Step 0: loss = 0.013092481531202793, recon_loss = 0.013085497543215752, kl_loss = 0.0013968348503112793\n",
      "\n",
      "Epoch 500\n",
      "Step 0: loss = 0.012755286879837513, recon_loss = 0.012750726193189621, kl_loss = 0.0009121838957071304\n",
      "\n",
      "Epoch 501\n",
      "Step 0: loss = 0.013299095444381237, recon_loss = 0.01329461857676506, kl_loss = 0.0008953623473644257\n",
      "\n",
      "Epoch 502\n",
      "Step 0: loss = 0.013020890764892101, recon_loss = 0.013010505586862564, kl_loss = 0.0020770803093910217\n",
      "\n",
      "Epoch 503\n",
      "Step 0: loss = 0.014308583922684193, recon_loss = 0.014302654191851616, kl_loss = 0.001185898669064045\n",
      "\n",
      "Epoch 504\n",
      "Step 0: loss = 0.012895816937088966, recon_loss = 0.012891437858343124, kl_loss = 0.0008759023621678352\n",
      "\n",
      "Epoch 505\n",
      "Step 0: loss = 0.013574836775660515, recon_loss = 0.013570809736847878, kl_loss = 0.000805336982011795\n",
      "\n",
      "Epoch 506\n",
      "Step 0: loss = 0.013136597350239754, recon_loss = 0.013126282021403313, kl_loss = 0.0020630815997719765\n",
      "\n",
      "Epoch 507\n",
      "Step 0: loss = 0.01224451418966055, recon_loss = 0.012237455695867538, kl_loss = 0.0014117276296019554\n",
      "\n",
      "Epoch 508\n",
      "Step 0: loss = 0.012279274873435497, recon_loss = 0.012274270877242088, kl_loss = 0.0010007461532950401\n",
      "\n",
      "Epoch 509\n",
      "Step 0: loss = 0.013422338292002678, recon_loss = 0.013417446985840797, kl_loss = 0.0009781895205378532\n",
      "\n",
      "Epoch 510\n",
      "Step 0: loss = 0.011468048207461834, recon_loss = 0.011461328715085983, kl_loss = 0.001343882642686367\n",
      "\n",
      "Epoch 511\n",
      "Step 0: loss = 0.012345056980848312, recon_loss = 0.012334480881690979, kl_loss = 0.0021151667460799217\n",
      "\n",
      "Epoch 512\n",
      "Step 0: loss = 0.012943042442202568, recon_loss = 0.012937324121594429, kl_loss = 0.0011435868218541145\n",
      "\n",
      "Epoch 513\n",
      "Step 0: loss = 0.012134267948567867, recon_loss = 0.012125395238399506, kl_loss = 0.001774592325091362\n",
      "\n",
      "Epoch 514\n",
      "Step 0: loss = 0.012316028587520123, recon_loss = 0.012311747297644615, kl_loss = 0.0008562682196497917\n",
      "\n",
      "Epoch 515\n",
      "Step 0: loss = 0.012600812129676342, recon_loss = 0.01259547844529152, kl_loss = 0.0010667406022548676\n",
      "\n",
      "Epoch 516\n",
      "Step 0: loss = 0.012579361908137798, recon_loss = 0.012568864971399307, kl_loss = 0.00209941528737545\n",
      "\n",
      "Epoch 517\n",
      "Step 0: loss = 0.012184254825115204, recon_loss = 0.012177497148513794, kl_loss = 0.001351463608443737\n",
      "\n",
      "Epoch 518\n",
      "Step 0: loss = 0.012780346907675266, recon_loss = 0.012771708890795708, kl_loss = 0.0017275651916861534\n",
      "\n",
      "Epoch 519\n",
      "Step 0: loss = 0.011580455116927624, recon_loss = 0.011574432253837585, kl_loss = 0.0012046396732330322\n",
      "\n",
      "Epoch 520\n",
      "Step 0: loss = 0.012372177094221115, recon_loss = 0.012357303872704506, kl_loss = 0.00297465268522501\n",
      "\n",
      "Epoch 521\n",
      "Step 0: loss = 0.012020201422274113, recon_loss = 0.012005593627691269, kl_loss = 0.002921534702181816\n",
      "\n",
      "Epoch 522\n",
      "Step 0: loss = 0.011712044477462769, recon_loss = 0.011704064905643463, kl_loss = 0.0015958640724420547\n",
      "\n",
      "Epoch 523\n",
      "Step 0: loss = 0.012917160987854004, recon_loss = 0.01291167363524437, kl_loss = 0.0010974342003464699\n",
      "\n",
      "Epoch 524\n",
      "Step 0: loss = 0.012232786044478416, recon_loss = 0.012227954342961311, kl_loss = 0.0009662508964538574\n",
      "\n",
      "Epoch 525\n",
      "Step 0: loss = 0.011515660211443901, recon_loss = 0.011501634493470192, kl_loss = 0.002805126830935478\n",
      "\n",
      "Epoch 526\n",
      "Step 0: loss = 0.010385424830019474, recon_loss = 0.01037791557610035, kl_loss = 0.0015018759295344353\n",
      "\n",
      "Epoch 527\n",
      "Step 0: loss = 0.010702624917030334, recon_loss = 0.010690851137042046, kl_loss = 0.002354671247303486\n",
      "\n",
      "Epoch 528\n",
      "Step 0: loss = 0.01208542287349701, recon_loss = 0.012077780440449715, kl_loss = 0.0015284446999430656\n",
      "\n",
      "Epoch 529\n",
      "Step 0: loss = 0.011552203446626663, recon_loss = 0.011543406173586845, kl_loss = 0.001759381964802742\n",
      "\n",
      "Epoch 530\n",
      "Step 0: loss = 0.011401657946407795, recon_loss = 0.011397594586014748, kl_loss = 0.0008127288892865181\n",
      "\n",
      "Epoch 531\n",
      "Step 0: loss = 0.010801554657518864, recon_loss = 0.010798400267958641, kl_loss = 0.0006308546289801598\n",
      "\n",
      "Epoch 532\n",
      "Step 0: loss = 0.010213642381131649, recon_loss = 0.01020801067352295, kl_loss = 0.0011262716725468636\n",
      "\n",
      "Epoch 533\n",
      "Step 0: loss = 0.009654263965785503, recon_loss = 0.00964909978210926, kl_loss = 0.0010327557101845741\n",
      "\n",
      "Epoch 534\n",
      "Step 0: loss = 0.009646482765674591, recon_loss = 0.009643642231822014, kl_loss = 0.0005680331960320473\n",
      "\n",
      "Epoch 535\n",
      "Step 0: loss = 0.009586465544998646, recon_loss = 0.00958334095776081, kl_loss = 0.0006248503923416138\n",
      "\n",
      "Epoch 536\n",
      "Step 0: loss = 0.010320832021534443, recon_loss = 0.010318325832486153, kl_loss = 0.0005011940374970436\n",
      "\n",
      "Epoch 537\n",
      "Step 0: loss = 0.009843815118074417, recon_loss = 0.009840130805969238, kl_loss = 0.000736885704100132\n",
      "\n",
      "Epoch 538\n",
      "Step 0: loss = 0.009986802004277706, recon_loss = 0.009983504191040993, kl_loss = 0.000659596174955368\n",
      "\n",
      "Epoch 539\n",
      "Step 0: loss = 0.010673124343156815, recon_loss = 0.01067068986594677, kl_loss = 0.0004869699478149414\n",
      "\n",
      "Epoch 540\n",
      "Step 0: loss = 0.010253986343741417, recon_loss = 0.010251441970467567, kl_loss = 0.0005087843164801598\n",
      "\n",
      "Epoch 541\n",
      "Step 0: loss = 0.010041238740086555, recon_loss = 0.0100389514118433, kl_loss = 0.0004574693739414215\n",
      "\n",
      "Epoch 542\n",
      "Step 0: loss = 0.010305807925760746, recon_loss = 0.010303692892193794, kl_loss = 0.0004230709746479988\n",
      "\n",
      "Epoch 543\n",
      "Step 0: loss = 0.00908024050295353, recon_loss = 0.009076839312911034, kl_loss = 0.0006801830604672432\n",
      "\n",
      "Epoch 544\n",
      "Step 0: loss = 0.010182205587625504, recon_loss = 0.010179033502936363, kl_loss = 0.0006344718858599663\n",
      "\n",
      "Epoch 545\n",
      "Step 0: loss = 0.009484461508691311, recon_loss = 0.009480338543653488, kl_loss = 0.0008245036005973816\n",
      "\n",
      "Epoch 546\n",
      "Step 0: loss = 0.009426010772585869, recon_loss = 0.009419852867722511, kl_loss = 0.0012315697968006134\n",
      "\n",
      "Epoch 547\n",
      "Step 0: loss = 0.010311052203178406, recon_loss = 0.010306470096111298, kl_loss = 0.0009163487702608109\n",
      "\n",
      "Epoch 548\n",
      "Step 0: loss = 0.009243268519639969, recon_loss = 0.009237166494131088, kl_loss = 0.0012203417718410492\n",
      "\n",
      "Epoch 549\n",
      "Step 0: loss = 0.009761709719896317, recon_loss = 0.009758220985531807, kl_loss = 0.0006976621225476265\n",
      "\n",
      "Epoch 550\n",
      "Step 0: loss = 0.008959307335317135, recon_loss = 0.008949443697929382, kl_loss = 0.001972682774066925\n",
      "\n",
      "Epoch 551\n",
      "Step 0: loss = 0.009526409208774567, recon_loss = 0.009524418041110039, kl_loss = 0.0003983229398727417\n",
      "\n",
      "Epoch 552\n",
      "Step 0: loss = 0.008656464517116547, recon_loss = 0.008654393255710602, kl_loss = 0.00041429512202739716\n",
      "\n",
      "Epoch 553\n",
      "Step 0: loss = 0.009692388586699963, recon_loss = 0.00969000905752182, kl_loss = 0.00047585461288690567\n",
      "\n",
      "Epoch 554\n",
      "Step 0: loss = 0.008117768913507462, recon_loss = 0.008113622665405273, kl_loss = 0.0008292794227600098\n",
      "\n",
      "Epoch 555\n",
      "Step 0: loss = 0.00804574228823185, recon_loss = 0.00804021954536438, kl_loss = 0.0011045755818486214\n",
      "\n",
      "Epoch 556\n",
      "Step 0: loss = 0.007758916821330786, recon_loss = 0.007754480466246605, kl_loss = 0.0008872728794813156\n",
      "\n",
      "Epoch 557\n",
      "Step 0: loss = 0.008885215036571026, recon_loss = 0.008878635242581367, kl_loss = 0.0013158805668354034\n",
      "\n",
      "Epoch 558\n",
      "Step 0: loss = 0.008564677089452744, recon_loss = 0.008557889610528946, kl_loss = 0.00135747529566288\n",
      "\n",
      "Epoch 559\n",
      "Step 0: loss = 0.008727669715881348, recon_loss = 0.008721429854631424, kl_loss = 0.0012479359284043312\n",
      "\n",
      "Epoch 560\n",
      "Step 0: loss = 0.007966515608131886, recon_loss = 0.007961636409163475, kl_loss = 0.0009758612141013145\n",
      "\n",
      "Epoch 561\n",
      "Step 0: loss = 0.008083243854343891, recon_loss = 0.008079128339886665, kl_loss = 0.0008230879902839661\n",
      "\n",
      "Epoch 562\n",
      "Step 0: loss = 0.007565330248326063, recon_loss = 0.007557615637779236, kl_loss = 0.001542927697300911\n",
      "\n",
      "Epoch 563\n",
      "Step 0: loss = 0.007446442265063524, recon_loss = 0.007443485781550407, kl_loss = 0.0005912529304623604\n",
      "\n",
      "Epoch 564\n",
      "Step 0: loss = 0.008101683109998703, recon_loss = 0.008098829537630081, kl_loss = 0.0005707694217562675\n",
      "\n",
      "Epoch 565\n",
      "Step 0: loss = 0.007395484019070864, recon_loss = 0.007391057908535004, kl_loss = 0.00088521558791399\n",
      "\n",
      "Epoch 566\n",
      "Step 0: loss = 0.007232685573399067, recon_loss = 0.007228611037135124, kl_loss = 0.0008148951455950737\n",
      "\n",
      "Epoch 567\n",
      "Step 0: loss = 0.007981759496033192, recon_loss = 0.007979314774274826, kl_loss = 0.0004888791590929031\n",
      "\n",
      "Epoch 568\n",
      "Step 0: loss = 0.008033139631152153, recon_loss = 0.008030641824007034, kl_loss = 0.0004995185881853104\n",
      "\n",
      "Epoch 569\n",
      "Step 0: loss = 0.007852165959775448, recon_loss = 0.007848156616091728, kl_loss = 0.0008018296211957932\n",
      "\n",
      "Epoch 570\n",
      "Step 0: loss = 0.007036246825009584, recon_loss = 0.007031377404928207, kl_loss = 0.0009738504886627197\n",
      "\n",
      "Epoch 571\n",
      "Step 0: loss = 0.007544045336544514, recon_loss = 0.007534945383667946, kl_loss = 0.0018200119957327843\n",
      "\n",
      "Epoch 572\n",
      "Step 0: loss = 0.008032153360545635, recon_loss = 0.008022967725992203, kl_loss = 0.001837054267525673\n",
      "\n",
      "Epoch 573\n",
      "Step 0: loss = 0.007954193279147148, recon_loss = 0.007950371131300926, kl_loss = 0.0007644519209861755\n",
      "\n",
      "Epoch 574\n",
      "Step 0: loss = 0.007798515725880861, recon_loss = 0.0077897533774375916, kl_loss = 0.0017524752765893936\n",
      "\n",
      "Epoch 575\n",
      "Step 0: loss = 0.007525627966970205, recon_loss = 0.007516797631978989, kl_loss = 0.0017660707235336304\n",
      "\n",
      "Epoch 576\n",
      "Step 0: loss = 0.0075998567044734955, recon_loss = 0.007595539093017578, kl_loss = 0.0008635539561510086\n",
      "\n",
      "Epoch 577\n",
      "Step 0: loss = 0.007689615711569786, recon_loss = 0.007684675976634026, kl_loss = 0.0009879041463136673\n",
      "\n",
      "Epoch 578\n",
      "Step 0: loss = 0.006910845637321472, recon_loss = 0.0069051627069711685, kl_loss = 0.0011366186663508415\n",
      "\n",
      "Epoch 579\n",
      "Step 0: loss = 0.0073715876787900925, recon_loss = 0.007365327328443527, kl_loss = 0.0012520793825387955\n",
      "\n",
      "Epoch 580\n",
      "Step 0: loss = 0.007061228156089783, recon_loss = 0.007056806236505508, kl_loss = 0.0008844286203384399\n",
      "\n",
      "Epoch 581\n",
      "Step 0: loss = 0.006535633932799101, recon_loss = 0.006528349593281746, kl_loss = 0.0014568772166967392\n",
      "\n",
      "Epoch 582\n",
      "Step 0: loss = 0.006315059494227171, recon_loss = 0.006304671987891197, kl_loss = 0.0020775236189365387\n",
      "\n",
      "Epoch 583\n",
      "Step 0: loss = 0.00635929498821497, recon_loss = 0.006354808807373047, kl_loss = 0.0008972538635134697\n",
      "\n",
      "Epoch 584\n",
      "Step 0: loss = 0.006726919207721949, recon_loss = 0.006704339757561684, kl_loss = 0.0045158639550209045\n",
      "\n",
      "Epoch 585\n",
      "Step 0: loss = 0.006226900964975357, recon_loss = 0.006218574941158295, kl_loss = 0.0016652382910251617\n",
      "\n",
      "Epoch 586\n",
      "Step 0: loss = 0.006494337692856789, recon_loss = 0.006488636136054993, kl_loss = 0.001140323467552662\n",
      "\n",
      "Epoch 587\n",
      "Step 0: loss = 0.007297882344573736, recon_loss = 0.0072725750505924225, kl_loss = 0.005061455070972443\n",
      "\n",
      "Epoch 588\n",
      "Step 0: loss = 0.006222222000360489, recon_loss = 0.006213558837771416, kl_loss = 0.0017326222732663155\n",
      "\n",
      "Epoch 589\n",
      "Step 0: loss = 0.0066407350823283195, recon_loss = 0.006634734570980072, kl_loss = 0.001200093887746334\n",
      "\n",
      "Epoch 590\n",
      "Step 0: loss = 0.005798884201794863, recon_loss = 0.0057925209403038025, kl_loss = 0.0012726150453090668\n",
      "\n",
      "Epoch 591\n",
      "Step 0: loss = 0.005516775883734226, recon_loss = 0.005513213574886322, kl_loss = 0.0007124301046133041\n",
      "\n",
      "Epoch 592\n",
      "Step 0: loss = 0.006177641451358795, recon_loss = 0.0061704982072114944, kl_loss = 0.0014286022633314133\n",
      "\n",
      "Epoch 593\n",
      "Step 0: loss = 0.005755250342190266, recon_loss = 0.005745498463511467, kl_loss = 0.0019503487274050713\n",
      "\n",
      "Epoch 594\n",
      "Step 0: loss = 0.00603850744664669, recon_loss = 0.006035227328538895, kl_loss = 0.0006560515612363815\n",
      "\n",
      "Epoch 595\n",
      "Step 0: loss = 0.005651379004120827, recon_loss = 0.005649209022521973, kl_loss = 0.000433972105383873\n",
      "\n",
      "Epoch 596\n",
      "Step 0: loss = 0.005865405313670635, recon_loss = 0.005861252546310425, kl_loss = 0.0008305897936224937\n",
      "\n",
      "Epoch 597\n",
      "Step 0: loss = 0.006094000302255154, recon_loss = 0.006089385598897934, kl_loss = 0.0009229285642504692\n",
      "\n",
      "Epoch 598\n",
      "Step 0: loss = 0.006423423532396555, recon_loss = 0.006419561803340912, kl_loss = 0.0007723895832896233\n",
      "\n",
      "Epoch 599\n",
      "Step 0: loss = 0.006127757951617241, recon_loss = 0.006124308332800865, kl_loss = 0.0006899535655975342\n",
      "\n",
      "Epoch 600\n",
      "Step 0: loss = 0.006294106598943472, recon_loss = 0.006291108205914497, kl_loss = 0.0005997046828269958\n",
      "\n",
      "Epoch 601\n",
      "Step 0: loss = 0.006806173361837864, recon_loss = 0.006802409887313843, kl_loss = 0.000752672553062439\n",
      "\n",
      "Epoch 602\n",
      "Step 0: loss = 0.0058257621712982655, recon_loss = 0.005821527913212776, kl_loss = 0.0008468953892588615\n",
      "\n",
      "Epoch 603\n",
      "Step 0: loss = 0.005421255249530077, recon_loss = 0.00541737861931324, kl_loss = 0.000775323249399662\n",
      "\n",
      "Epoch 604\n",
      "Step 0: loss = 0.005790265742689371, recon_loss = 0.005786517634987831, kl_loss = 0.0007495936006307602\n",
      "\n",
      "Epoch 605\n",
      "Step 0: loss = 0.005970964208245277, recon_loss = 0.0059663280844688416, kl_loss = 0.0009272284805774689\n",
      "\n",
      "Epoch 606\n",
      "Step 0: loss = 0.006401685066521168, recon_loss = 0.006396016106009483, kl_loss = 0.001133800484240055\n",
      "\n",
      "Epoch 607\n",
      "Step 0: loss = 0.0063801915384829044, recon_loss = 0.006376901641488075, kl_loss = 0.0006579924374818802\n",
      "\n",
      "Epoch 608\n",
      "Step 0: loss = 0.005867457948625088, recon_loss = 0.005863575264811516, kl_loss = 0.0007765293121337891\n",
      "\n",
      "Epoch 609\n",
      "Step 0: loss = 0.0053617088124156, recon_loss = 0.005356587469577789, kl_loss = 0.0010242592543363571\n",
      "\n",
      "Epoch 610\n",
      "Step 0: loss = 0.005909719970077276, recon_loss = 0.005905283614993095, kl_loss = 0.0008873064070940018\n",
      "\n",
      "Epoch 611\n",
      "Step 0: loss = 0.005632051732391119, recon_loss = 0.005626888945698738, kl_loss = 0.0010325666517019272\n",
      "\n",
      "Epoch 612\n",
      "Step 0: loss = 0.00541418232023716, recon_loss = 0.0054086558520793915, kl_loss = 0.0011052722111344337\n",
      "\n",
      "Epoch 613\n",
      "Step 0: loss = 0.00587793905287981, recon_loss = 0.0058739520609378815, kl_loss = 0.0007973611354827881\n",
      "\n",
      "Epoch 614\n",
      "Step 0: loss = 0.005562251899391413, recon_loss = 0.005557011812925339, kl_loss = 0.0010479940101504326\n",
      "\n",
      "Epoch 615\n",
      "Step 0: loss = 0.00554755749180913, recon_loss = 0.005540663376450539, kl_loss = 0.0013788165524601936\n",
      "\n",
      "Epoch 616\n",
      "Step 0: loss = 0.005912096705287695, recon_loss = 0.00590812973678112, kl_loss = 0.0007933909073472023\n",
      "\n",
      "Epoch 617\n",
      "Step 0: loss = 0.005516605451703072, recon_loss = 0.005512699484825134, kl_loss = 0.0007811523973941803\n",
      "\n",
      "Epoch 618\n",
      "Step 0: loss = 0.005027743522077799, recon_loss = 0.00502362847328186, kl_loss = 0.0008230367675423622\n",
      "\n",
      "Epoch 619\n",
      "Step 0: loss = 0.005468738730996847, recon_loss = 0.00546480156481266, kl_loss = 0.0007874229922890663\n",
      "\n",
      "Epoch 620\n",
      "Step 0: loss = 0.005050299223512411, recon_loss = 0.005047624930739403, kl_loss = 0.0005348613485693932\n",
      "\n",
      "Epoch 621\n",
      "Step 0: loss = 0.00489901565015316, recon_loss = 0.004895893856883049, kl_loss = 0.0006243716925382614\n",
      "\n",
      "Epoch 622\n",
      "Step 0: loss = 0.005077740643173456, recon_loss = 0.005074754357337952, kl_loss = 0.0005972757935523987\n",
      "\n",
      "Epoch 623\n",
      "Step 0: loss = 0.004686789121478796, recon_loss = 0.004683207720518112, kl_loss = 0.0007162550464272499\n",
      "\n",
      "Epoch 624\n",
      "Step 0: loss = 0.005194542929530144, recon_loss = 0.005191106349229813, kl_loss = 0.0006873011589050293\n",
      "\n",
      "Epoch 625\n",
      "Step 0: loss = 0.004724937491118908, recon_loss = 0.00472177192568779, kl_loss = 0.0006330972537398338\n",
      "\n",
      "Epoch 626\n",
      "Step 0: loss = 0.004941937047988176, recon_loss = 0.004931161180138588, kl_loss = 0.0021551456302404404\n",
      "\n",
      "Epoch 627\n",
      "Step 0: loss = 0.004922046326100826, recon_loss = 0.004914451390504837, kl_loss = 0.0015189871191978455\n",
      "\n",
      "Epoch 628\n",
      "Step 0: loss = 0.004467685241252184, recon_loss = 0.004461865872144699, kl_loss = 0.001163853332400322\n",
      "\n",
      "Epoch 629\n",
      "Step 0: loss = 0.004834349267184734, recon_loss = 0.0048285964876413345, kl_loss = 0.0011505698785185814\n",
      "\n",
      "Epoch 630\n",
      "Step 0: loss = 0.005336441099643707, recon_loss = 0.005329413339495659, kl_loss = 0.0014055604115128517\n",
      "\n",
      "Epoch 631\n",
      "Step 0: loss = 0.004676772281527519, recon_loss = 0.00467371940612793, kl_loss = 0.0006105909124016762\n",
      "\n",
      "Epoch 632\n",
      "Step 0: loss = 0.004598264582455158, recon_loss = 0.004594435915350914, kl_loss = 0.0007657576352357864\n",
      "\n",
      "Epoch 633\n",
      "Step 0: loss = 0.004221921321004629, recon_loss = 0.004218984395265579, kl_loss = 0.0005874112248420715\n",
      "\n",
      "Epoch 634\n",
      "Step 0: loss = 0.0046284981071949005, recon_loss = 0.004624737426638603, kl_loss = 0.0007521277293562889\n",
      "\n",
      "Epoch 635\n",
      "Step 0: loss = 0.004813688807189465, recon_loss = 0.004811622202396393, kl_loss = 0.0004133004695177078\n",
      "\n",
      "Epoch 636\n",
      "Step 0: loss = 0.004340550862252712, recon_loss = 0.004337236285209656, kl_loss = 0.0006628837436437607\n",
      "\n",
      "Epoch 637\n",
      "Step 0: loss = 0.004243436735123396, recon_loss = 0.004237353801727295, kl_loss = 0.0012165820226073265\n",
      "\n",
      "Epoch 638\n",
      "Step 0: loss = 0.00459487596526742, recon_loss = 0.0045891813933849335, kl_loss = 0.0011389357969164848\n",
      "\n",
      "Epoch 639\n",
      "Step 0: loss = 0.004536135122179985, recon_loss = 0.004530256614089012, kl_loss = 0.0011756904423236847\n",
      "\n",
      "Epoch 640\n",
      "Step 0: loss = 0.004301621112972498, recon_loss = 0.004297561943531036, kl_loss = 0.000811869278550148\n",
      "\n",
      "Epoch 641\n",
      "Step 0: loss = 0.004410486668348312, recon_loss = 0.0044059306383132935, kl_loss = 0.0009112395346164703\n",
      "\n",
      "Epoch 642\n",
      "Step 0: loss = 0.0048072668723762035, recon_loss = 0.004799133166670799, kl_loss = 0.001626705750823021\n",
      "\n",
      "Epoch 643\n",
      "Step 0: loss = 0.004899649415165186, recon_loss = 0.004893042147159576, kl_loss = 0.001321457326412201\n",
      "\n",
      "Epoch 644\n",
      "Step 0: loss = 0.004448383580893278, recon_loss = 0.0044434089213609695, kl_loss = 0.000994962640106678\n",
      "\n",
      "Epoch 645\n",
      "Step 0: loss = 0.004108728375285864, recon_loss = 0.004100073128938675, kl_loss = 0.0017310138791799545\n",
      "\n",
      "Epoch 646\n",
      "Step 0: loss = 0.0042353845201432705, recon_loss = 0.004224399104714394, kl_loss = 0.0021970709785819054\n",
      "\n",
      "Epoch 647\n",
      "Step 0: loss = 0.00424499437212944, recon_loss = 0.004238946363329887, kl_loss = 0.0012096306309103966\n",
      "\n",
      "Epoch 648\n",
      "Step 0: loss = 0.004256531596183777, recon_loss = 0.004250451922416687, kl_loss = 0.001215956173837185\n",
      "\n",
      "Epoch 649\n",
      "Step 0: loss = 0.003865164238959551, recon_loss = 0.003862157464027405, kl_loss = 0.0006013410165905952\n",
      "\n",
      "Epoch 650\n",
      "Step 0: loss = 0.003889047773554921, recon_loss = 0.0038843434303998947, kl_loss = 0.0009408602491021156\n",
      "\n",
      "Epoch 651\n",
      "Step 0: loss = 0.004078041762113571, recon_loss = 0.004068966954946518, kl_loss = 0.0018149567767977715\n",
      "\n",
      "Epoch 652\n",
      "Step 0: loss = 0.003853288246318698, recon_loss = 0.0038476623594760895, kl_loss = 0.0011251624673604965\n",
      "\n",
      "Epoch 653\n",
      "Step 0: loss = 0.003984683193266392, recon_loss = 0.003979360684752464, kl_loss = 0.0010644672438502312\n",
      "\n",
      "Epoch 654\n",
      "Step 0: loss = 0.003992634825408459, recon_loss = 0.003987215459346771, kl_loss = 0.0010838564485311508\n",
      "\n",
      "Epoch 655\n",
      "Step 0: loss = 0.0038734127301722765, recon_loss = 0.00387011282145977, kl_loss = 0.0006599687039852142\n",
      "\n",
      "Epoch 656\n",
      "Step 0: loss = 0.003741686698049307, recon_loss = 0.003736065700650215, kl_loss = 0.0011241957545280457\n",
      "\n",
      "Epoch 657\n",
      "Step 0: loss = 0.003667839802801609, recon_loss = 0.003664229065179825, kl_loss = 0.0007221270352602005\n",
      "\n",
      "Epoch 658\n",
      "Step 0: loss = 0.003751034615561366, recon_loss = 0.003744233399629593, kl_loss = 0.0013602543622255325\n",
      "\n",
      "Epoch 659\n",
      "Step 0: loss = 0.004140352830290794, recon_loss = 0.004135407507419586, kl_loss = 0.000989103689789772\n",
      "\n",
      "Epoch 660\n",
      "Step 0: loss = 0.0036731669679284096, recon_loss = 0.0036696866154670715, kl_loss = 0.0006960658356547356\n",
      "\n",
      "Epoch 661\n",
      "Step 0: loss = 0.003773635718971491, recon_loss = 0.0037711597979068756, kl_loss = 0.0004952028393745422\n",
      "\n",
      "Epoch 662\n",
      "Step 0: loss = 0.003663009498268366, recon_loss = 0.003657981753349304, kl_loss = 0.0010055508464574814\n",
      "\n",
      "Epoch 663\n",
      "Step 0: loss = 0.003954299725592136, recon_loss = 0.003950923681259155, kl_loss = 0.0006752507761120796\n",
      "\n",
      "Epoch 664\n",
      "Step 0: loss = 0.003988469950854778, recon_loss = 0.003981182351708412, kl_loss = 0.0014574872329831123\n",
      "\n",
      "Epoch 665\n",
      "Step 0: loss = 0.004051370080560446, recon_loss = 0.004047676920890808, kl_loss = 0.0007386514917016029\n",
      "\n",
      "Epoch 666\n",
      "Step 0: loss = 0.0038014259189367294, recon_loss = 0.0037975087761878967, kl_loss = 0.0007834183052182198\n",
      "\n",
      "Epoch 667\n",
      "Step 0: loss = 0.0035099880769848824, recon_loss = 0.0035079773515462875, kl_loss = 0.0004021674394607544\n",
      "\n",
      "Epoch 668\n",
      "Step 0: loss = 0.0036146906204521656, recon_loss = 0.003609197214245796, kl_loss = 0.0010986803099513054\n",
      "\n",
      "Epoch 669\n",
      "Step 0: loss = 0.003761312924325466, recon_loss = 0.003756195306777954, kl_loss = 0.0010235011577606201\n",
      "\n",
      "Epoch 670\n",
      "Step 0: loss = 0.0036127641797065735, recon_loss = 0.0036093778908252716, kl_loss = 0.0006772410124540329\n",
      "\n",
      "Epoch 671\n",
      "Step 0: loss = 0.0037857850547879934, recon_loss = 0.0037828702479600906, kl_loss = 0.0005829678848385811\n",
      "\n",
      "Epoch 672\n",
      "Step 0: loss = 0.003614127403125167, recon_loss = 0.0036092381924390793, kl_loss = 0.000977856107056141\n",
      "\n",
      "Epoch 673\n",
      "Step 0: loss = 0.0033432592172175646, recon_loss = 0.0033408254384994507, kl_loss = 0.00048674363642930984\n",
      "\n",
      "Epoch 674\n",
      "Step 0: loss = 0.0033208420500159264, recon_loss = 0.0033188462257385254, kl_loss = 0.0003991471603512764\n",
      "\n",
      "Epoch 675\n",
      "Step 0: loss = 0.0034629569854587317, recon_loss = 0.0034585092216730118, kl_loss = 0.0008895685896277428\n",
      "\n",
      "Epoch 676\n",
      "Step 0: loss = 0.0034837983548641205, recon_loss = 0.0034806150943040848, kl_loss = 0.0006366604939103127\n",
      "\n",
      "Epoch 677\n",
      "Step 0: loss = 0.0035187334287911654, recon_loss = 0.0035152696073055267, kl_loss = 0.0006927596405148506\n",
      "\n",
      "Epoch 678\n",
      "Step 0: loss = 0.0028869006782770157, recon_loss = 0.0028841719031333923, kl_loss = 0.0005457652732729912\n",
      "\n",
      "Epoch 679\n",
      "Step 0: loss = 0.0033802613615989685, recon_loss = 0.003377869725227356, kl_loss = 0.000478317029774189\n",
      "\n",
      "Epoch 680\n",
      "Step 0: loss = 0.0033261815551668406, recon_loss = 0.003320489078760147, kl_loss = 0.0011385148391127586\n",
      "\n",
      "Epoch 681\n",
      "Step 0: loss = 0.00332871382124722, recon_loss = 0.0033248700201511383, kl_loss = 0.000768737867474556\n",
      "\n",
      "Epoch 682\n",
      "Step 0: loss = 0.0031413526739925146, recon_loss = 0.003138171508908272, kl_loss = 0.0006362460553646088\n",
      "\n",
      "Epoch 683\n",
      "Step 0: loss = 0.0030142778996378183, recon_loss = 0.0030116401612758636, kl_loss = 0.000527537427842617\n",
      "\n",
      "Epoch 684\n",
      "Step 0: loss = 0.003153271274641156, recon_loss = 0.0031509380787611008, kl_loss = 0.00046661775559186935\n",
      "\n",
      "Epoch 685\n",
      "Step 0: loss = 0.002972641261294484, recon_loss = 0.0029699206352233887, kl_loss = 0.0005441242828965187\n",
      "\n",
      "Epoch 686\n",
      "Step 0: loss = 0.0031187168788164854, recon_loss = 0.003113146871328354, kl_loss = 0.0011140024289488792\n",
      "\n",
      "Epoch 687\n",
      "Step 0: loss = 0.0031434837728738785, recon_loss = 0.003139650449156761, kl_loss = 0.0007666591554880142\n",
      "\n",
      "Epoch 688\n",
      "Step 0: loss = 0.0035025402903556824, recon_loss = 0.0034990422427654266, kl_loss = 0.0006996029987931252\n",
      "\n",
      "Epoch 689\n",
      "Step 0: loss = 0.00343483523465693, recon_loss = 0.0034316834062337875, kl_loss = 0.0006303749978542328\n",
      "\n",
      "Epoch 690\n",
      "Step 0: loss = 0.0035556324291974306, recon_loss = 0.0035520363599061966, kl_loss = 0.0007192334160208702\n",
      "\n",
      "Epoch 691\n",
      "Step 0: loss = 0.0032142302952706814, recon_loss = 0.0032098498195409775, kl_loss = 0.0008761072531342506\n",
      "\n",
      "Epoch 692\n",
      "Step 0: loss = 0.0036590411327779293, recon_loss = 0.0036530792713165283, kl_loss = 0.001192362979054451\n",
      "\n",
      "Epoch 693\n",
      "Step 0: loss = 0.003433328354731202, recon_loss = 0.003424696624279022, kl_loss = 0.001726331189274788\n",
      "\n",
      "Epoch 694\n",
      "Step 0: loss = 0.0034088764805346727, recon_loss = 0.0034047067165374756, kl_loss = 0.0008339565247297287\n",
      "\n",
      "Epoch 695\n",
      "Step 0: loss = 0.003415773855522275, recon_loss = 0.0034116990864276886, kl_loss = 0.0008149612694978714\n",
      "\n",
      "Epoch 696\n",
      "Step 0: loss = 0.0033606274519115686, recon_loss = 0.00335763581097126, kl_loss = 0.0005983291193842888\n",
      "\n",
      "Epoch 697\n",
      "Step 0: loss = 0.003582631004974246, recon_loss = 0.0035788044333457947, kl_loss = 0.0007653338834643364\n",
      "\n",
      "Epoch 698\n",
      "Step 0: loss = 0.003489534603431821, recon_loss = 0.0034852977842092514, kl_loss = 0.00084737129509449\n",
      "\n",
      "Epoch 699\n",
      "Step 0: loss = 0.003347051329910755, recon_loss = 0.003344159573316574, kl_loss = 0.0005783289670944214\n",
      "\n",
      "Epoch 700\n",
      "Step 0: loss = 0.003355044638738036, recon_loss = 0.0033503323793411255, kl_loss = 0.0009424565359950066\n",
      "\n",
      "Epoch 701\n",
      "Step 0: loss = 0.0032073843758553267, recon_loss = 0.0032037943601608276, kl_loss = 0.0007179882377386093\n",
      "\n",
      "Epoch 702\n",
      "Step 0: loss = 0.003370867343619466, recon_loss = 0.003368021920323372, kl_loss = 0.0005691051483154297\n",
      "\n",
      "Epoch 703\n",
      "Step 0: loss = 0.00317965866997838, recon_loss = 0.0031763985753059387, kl_loss = 0.0006520049646496773\n",
      "\n",
      "Epoch 704\n",
      "Step 0: loss = 0.0032508892472833395, recon_loss = 0.0032464265823364258, kl_loss = 0.0008925255388021469\n",
      "\n",
      "Epoch 705\n",
      "Step 0: loss = 0.003184075467288494, recon_loss = 0.003180084750056267, kl_loss = 0.0007981341332197189\n",
      "\n",
      "Epoch 706\n",
      "Step 0: loss = 0.003016538918018341, recon_loss = 0.0030123721808195114, kl_loss = 0.0008333418518304825\n",
      "\n",
      "Epoch 707\n",
      "Step 0: loss = 0.002819326240569353, recon_loss = 0.0028161071240901947, kl_loss = 0.0006438437849283218\n",
      "\n",
      "Epoch 708\n",
      "Step 0: loss = 0.002925821812823415, recon_loss = 0.0029220599681138992, kl_loss = 0.0007523773238062859\n",
      "\n",
      "Epoch 709\n",
      "Step 0: loss = 0.0025674537755548954, recon_loss = 0.0025653354823589325, kl_loss = 0.00042367633432149887\n",
      "\n",
      "Epoch 710\n",
      "Step 0: loss = 0.0026827550027519464, recon_loss = 0.0026790518313646317, kl_loss = 0.0007406184449791908\n",
      "\n",
      "Epoch 711\n",
      "Step 0: loss = 0.0025781719014048576, recon_loss = 0.0025734808295965195, kl_loss = 0.0009382292628288269\n",
      "\n",
      "Epoch 712\n",
      "Step 0: loss = 0.002518388908356428, recon_loss = 0.002513924613595009, kl_loss = 0.0008928757160902023\n",
      "\n",
      "Epoch 713\n",
      "Step 0: loss = 0.0026434101164340973, recon_loss = 0.0026400741189718246, kl_loss = 0.0006672143936157227\n",
      "\n",
      "Epoch 714\n",
      "Step 0: loss = 0.002843555761501193, recon_loss = 0.0028395485132932663, kl_loss = 0.0008014580234885216\n",
      "\n",
      "Epoch 715\n",
      "Step 0: loss = 0.002745317295193672, recon_loss = 0.0027412325143814087, kl_loss = 0.0008169440552592278\n",
      "\n",
      "Epoch 716\n",
      "Step 0: loss = 0.0027325968258082867, recon_loss = 0.002729445695877075, kl_loss = 0.0006302082911133766\n",
      "\n",
      "Epoch 717\n",
      "Step 0: loss = 0.0026792865246534348, recon_loss = 0.0026751793920993805, kl_loss = 0.0008214330300688744\n",
      "\n",
      "Epoch 718\n",
      "Step 0: loss = 0.0027533406391739845, recon_loss = 0.0027511008083820343, kl_loss = 0.00044794753193855286\n",
      "\n",
      "Epoch 719\n",
      "Step 0: loss = 0.0026526181027293205, recon_loss = 0.0026492848992347717, kl_loss = 0.0006666602566838264\n",
      "\n",
      "Epoch 720\n",
      "Step 0: loss = 0.0028186992276459932, recon_loss = 0.0028165243566036224, kl_loss = 0.0004349593073129654\n",
      "\n",
      "Epoch 721\n",
      "Step 0: loss = 0.002652747556567192, recon_loss = 0.002647658810019493, kl_loss = 0.0010177725926041603\n",
      "\n",
      "Epoch 722\n",
      "Step 0: loss = 0.0026207170449197292, recon_loss = 0.0026178117841482162, kl_loss = 0.0005810307338833809\n",
      "\n",
      "Epoch 723\n",
      "Step 0: loss = 0.002450527623295784, recon_loss = 0.0024454742670059204, kl_loss = 0.0010106880217790604\n",
      "\n",
      "Epoch 724\n",
      "Step 0: loss = 0.0025337666738778353, recon_loss = 0.0025289542973041534, kl_loss = 0.0009624594822525978\n",
      "\n",
      "Epoch 725\n",
      "Step 0: loss = 0.0027740823570638895, recon_loss = 0.002772180363535881, kl_loss = 0.0003803875297307968\n",
      "\n",
      "Epoch 726\n",
      "Step 0: loss = 0.00282590021379292, recon_loss = 0.0028213895857334137, kl_loss = 0.0009021367877721786\n",
      "\n",
      "Epoch 727\n",
      "Step 0: loss = 0.0028633365873247385, recon_loss = 0.0028605442494153976, kl_loss = 0.0005584610626101494\n",
      "\n",
      "Epoch 728\n",
      "Step 0: loss = 0.0026660130824893713, recon_loss = 0.0026633702218532562, kl_loss = 0.0005285833030939102\n",
      "\n",
      "Epoch 729\n",
      "Step 0: loss = 0.0028118747286498547, recon_loss = 0.0028083622455596924, kl_loss = 0.0007024947553873062\n",
      "\n",
      "Epoch 730\n",
      "Step 0: loss = 0.0029093013145029545, recon_loss = 0.002905936911702156, kl_loss = 0.0006728582084178925\n",
      "\n",
      "Epoch 731\n",
      "Step 0: loss = 0.0024872873909771442, recon_loss = 0.002482891082763672, kl_loss = 0.0008792504668235779\n",
      "\n",
      "Epoch 732\n",
      "Step 0: loss = 0.0028072840068489313, recon_loss = 0.0028047319501638412, kl_loss = 0.0005103955045342445\n",
      "\n",
      "Epoch 733\n",
      "Step 0: loss = 0.0028553269803524017, recon_loss = 0.0028524883091449738, kl_loss = 0.0005677472800016403\n",
      "\n",
      "Epoch 734\n",
      "Step 0: loss = 0.0027205320075154305, recon_loss = 0.0027157794684171677, kl_loss = 0.0009505264461040497\n",
      "\n",
      "Epoch 735\n",
      "Step 0: loss = 0.0027170854154974222, recon_loss = 0.0027140043675899506, kl_loss = 0.0006162170320749283\n",
      "\n",
      "Epoch 736\n",
      "Step 0: loss = 0.002444547601044178, recon_loss = 0.0024384595453739166, kl_loss = 0.001217634417116642\n",
      "\n",
      "Epoch 737\n",
      "Step 0: loss = 0.0026010768488049507, recon_loss = 0.002597671002149582, kl_loss = 0.0006811833009123802\n",
      "\n",
      "Epoch 738\n",
      "Step 0: loss = 0.0026669420767575502, recon_loss = 0.0026648230850696564, kl_loss = 0.00042378343641757965\n",
      "\n",
      "Epoch 739\n",
      "Step 0: loss = 0.0028867856599390507, recon_loss = 0.0028817206621170044, kl_loss = 0.0010130004957318306\n",
      "\n",
      "Epoch 740\n",
      "Step 0: loss = 0.002753224689513445, recon_loss = 0.0027448199689388275, kl_loss = 0.0016809357330203056\n",
      "\n",
      "Epoch 741\n",
      "Step 0: loss = 0.0026428771670907736, recon_loss = 0.002637946978211403, kl_loss = 0.0009860238060355186\n",
      "\n",
      "Epoch 742\n",
      "Step 0: loss = 0.002664973959326744, recon_loss = 0.002657884731888771, kl_loss = 0.0014178520068526268\n",
      "\n",
      "Epoch 743\n",
      "Step 0: loss = 0.0026832183357328176, recon_loss = 0.0026786550879478455, kl_loss = 0.0009126653894782066\n",
      "\n",
      "Epoch 744\n",
      "Step 0: loss = 0.002695373259484768, recon_loss = 0.0026919692754745483, kl_loss = 0.0006807809695601463\n",
      "\n",
      "Epoch 745\n",
      "Step 0: loss = 0.0024943361058831215, recon_loss = 0.002490248531103134, kl_loss = 0.0008174963295459747\n",
      "\n",
      "Epoch 746\n",
      "Step 0: loss = 0.002567467512562871, recon_loss = 0.0025638602674007416, kl_loss = 0.000721462070941925\n",
      "\n",
      "Epoch 747\n",
      "Step 0: loss = 0.0025191621389240026, recon_loss = 0.0025169048458337784, kl_loss = 0.0004514651373028755\n",
      "\n",
      "Epoch 748\n",
      "Step 0: loss = 0.002489665523171425, recon_loss = 0.0024871770292520523, kl_loss = 0.0004977146163582802\n",
      "\n",
      "Epoch 749\n",
      "Step 0: loss = 0.002756562316790223, recon_loss = 0.002753043547272682, kl_loss = 0.0007037706673145294\n",
      "\n",
      "Epoch 750\n",
      "Step 0: loss = 0.0026293150149285793, recon_loss = 0.0026270411908626556, kl_loss = 0.00045474153012037277\n",
      "\n",
      "Epoch 751\n",
      "Step 0: loss = 0.002709132619202137, recon_loss = 0.0027056019753217697, kl_loss = 0.000706142745912075\n",
      "\n",
      "Epoch 752\n",
      "Step 0: loss = 0.002580116270110011, recon_loss = 0.002576330676674843, kl_loss = 0.0007571354508399963\n",
      "\n",
      "Epoch 753\n",
      "Step 0: loss = 0.0027657479513436556, recon_loss = 0.0027630142867565155, kl_loss = 0.0005467301234602928\n",
      "\n",
      "Epoch 754\n",
      "Step 0: loss = 0.002616665791720152, recon_loss = 0.002613356336951256, kl_loss = 0.0006618713960051537\n",
      "\n",
      "Epoch 755\n",
      "Step 0: loss = 0.002434676280245185, recon_loss = 0.0024318527430295944, kl_loss = 0.0005647158250212669\n",
      "\n",
      "Epoch 756\n",
      "Step 0: loss = 0.0027086574118584394, recon_loss = 0.0027053412050008774, kl_loss = 0.0006632339209318161\n",
      "\n",
      "Epoch 757\n",
      "Step 0: loss = 0.0026542197447270155, recon_loss = 0.0026516560465097427, kl_loss = 0.0005127256736159325\n",
      "\n",
      "Epoch 758\n",
      "Step 0: loss = 0.0026134918443858624, recon_loss = 0.002611076459288597, kl_loss = 0.00048309750854969025\n",
      "\n",
      "Epoch 759\n",
      "Step 0: loss = 0.0026319180615246296, recon_loss = 0.002628408372402191, kl_loss = 0.0007019471377134323\n",
      "\n",
      "Epoch 760\n",
      "Step 0: loss = 0.002735990798100829, recon_loss = 0.002733146771788597, kl_loss = 0.000568801537156105\n",
      "\n",
      "Epoch 761\n",
      "Step 0: loss = 0.0026576421223580837, recon_loss = 0.00265374593436718, kl_loss = 0.0007792217656970024\n",
      "\n",
      "Epoch 762\n",
      "Step 0: loss = 0.0028177620843052864, recon_loss = 0.0028141438961029053, kl_loss = 0.0007236301898956299\n",
      "\n",
      "Epoch 763\n",
      "Step 0: loss = 0.0025564406532794237, recon_loss = 0.0025527793914079666, kl_loss = 0.0007322719320654869\n",
      "\n",
      "Epoch 764\n",
      "Step 0: loss = 0.0026633127126842737, recon_loss = 0.0026600677520036697, kl_loss = 0.0006490116938948631\n",
      "\n",
      "Epoch 765\n",
      "Step 0: loss = 0.002865168731659651, recon_loss = 0.002860710024833679, kl_loss = 0.0008917190134525299\n",
      "\n",
      "Epoch 766\n",
      "Step 0: loss = 0.002642135601490736, recon_loss = 0.0026387199759483337, kl_loss = 0.0006831344217061996\n",
      "\n",
      "Epoch 767\n",
      "Step 0: loss = 0.002932301489636302, recon_loss = 0.0029294732958078384, kl_loss = 0.0005656201392412186\n",
      "\n",
      "Epoch 768\n",
      "Step 0: loss = 0.0027726804837584496, recon_loss = 0.002769308164715767, kl_loss = 0.0006744414567947388\n",
      "\n",
      "Epoch 769\n",
      "Step 0: loss = 0.002639365615323186, recon_loss = 0.002635326236486435, kl_loss = 0.0008078590035438538\n",
      "\n",
      "Epoch 770\n",
      "Step 0: loss = 0.0028332264628261328, recon_loss = 0.002829158678650856, kl_loss = 0.0008135586977005005\n",
      "\n",
      "Epoch 771\n",
      "Step 0: loss = 0.002559246029704809, recon_loss = 0.002555783838033676, kl_loss = 0.0006924169138073921\n",
      "\n",
      "Epoch 772\n",
      "Step 0: loss = 0.002654086099937558, recon_loss = 0.002650158479809761, kl_loss = 0.0007855063304305077\n",
      "\n",
      "Epoch 773\n",
      "Step 0: loss = 0.002473454922437668, recon_loss = 0.002470036968588829, kl_loss = 0.0006836140528321266\n",
      "\n",
      "Epoch 774\n",
      "Step 0: loss = 0.002688528271391988, recon_loss = 0.002685554325580597, kl_loss = 0.0005947984755039215\n",
      "\n",
      "Epoch 775\n",
      "Step 0: loss = 0.0026748140808194876, recon_loss = 0.0026717912405729294, kl_loss = 0.0006045857444405556\n",
      "\n",
      "Epoch 776\n",
      "Step 0: loss = 0.002404302591457963, recon_loss = 0.0024016834795475006, kl_loss = 0.0005238009616732597\n",
      "\n",
      "Epoch 777\n",
      "Step 0: loss = 0.002555340062826872, recon_loss = 0.0025521032512187958, kl_loss = 0.0006473483517765999\n",
      "\n",
      "Epoch 778\n",
      "Step 0: loss = 0.0026306617073714733, recon_loss = 0.0026275180280208588, kl_loss = 0.0006287246942520142\n",
      "\n",
      "Epoch 779\n",
      "Step 0: loss = 0.002406453713774681, recon_loss = 0.0024038683623075485, kl_loss = 0.0005170572549104691\n",
      "\n",
      "Epoch 780\n",
      "Step 0: loss = 0.002297235419973731, recon_loss = 0.002294706180691719, kl_loss = 0.0005058469250798225\n",
      "\n",
      "Epoch 781\n",
      "Step 0: loss = 0.0025186107959598303, recon_loss = 0.002513708546757698, kl_loss = 0.0009804414585232735\n",
      "\n",
      "Epoch 782\n",
      "Step 0: loss = 0.0022897415328770876, recon_loss = 0.0022865869104862213, kl_loss = 0.0006309105083346367\n",
      "\n",
      "Epoch 783\n",
      "Step 0: loss = 0.0023738143499940634, recon_loss = 0.002370554953813553, kl_loss = 0.0006518838927149773\n",
      "\n",
      "Epoch 784\n",
      "Step 0: loss = 0.0023516954388469458, recon_loss = 0.002348879352211952, kl_loss = 0.0005632098764181137\n",
      "\n",
      "Epoch 785\n",
      "Step 0: loss = 0.002521326532587409, recon_loss = 0.00251578725874424, kl_loss = 0.0011078482493758202\n",
      "\n",
      "Epoch 786\n",
      "Step 0: loss = 0.002469059079885483, recon_loss = 0.0024655982851982117, kl_loss = 0.0006921794265508652\n",
      "\n",
      "Epoch 787\n",
      "Step 0: loss = 0.0023154770024120808, recon_loss = 0.002312924712896347, kl_loss = 0.0005104467272758484\n",
      "\n",
      "Epoch 788\n",
      "Step 0: loss = 0.0023995484225451946, recon_loss = 0.0023950692266225815, kl_loss = 0.0008958326652646065\n",
      "\n",
      "Epoch 789\n",
      "Step 0: loss = 0.0024037715047597885, recon_loss = 0.0024010781198740005, kl_loss = 0.0005386956036090851\n",
      "\n",
      "Epoch 790\n",
      "Step 0: loss = 0.002610137453302741, recon_loss = 0.0026046186685562134, kl_loss = 0.0011037755757570267\n",
      "\n",
      "Epoch 791\n",
      "Step 0: loss = 0.0024466421455144882, recon_loss = 0.002441542223095894, kl_loss = 0.0010199937969446182\n",
      "\n",
      "Epoch 792\n",
      "Step 0: loss = 0.0026187847834080458, recon_loss = 0.002614792436361313, kl_loss = 0.000798448920249939\n",
      "\n",
      "Epoch 793\n",
      "Step 0: loss = 0.0026643420569598675, recon_loss = 0.0026619359850883484, kl_loss = 0.0004812292754650116\n",
      "\n",
      "Epoch 794\n",
      "Step 0: loss = 0.002520904177799821, recon_loss = 0.0025182999670505524, kl_loss = 0.000520835630595684\n",
      "\n",
      "Epoch 795\n",
      "Step 0: loss = 0.0026574209332466125, recon_loss = 0.0026549994945526123, kl_loss = 0.00048429984599351883\n",
      "\n",
      "Epoch 796\n",
      "Step 0: loss = 0.0026159321423619986, recon_loss = 0.002611294388771057, kl_loss = 0.0009275283664464951\n",
      "\n",
      "Epoch 797\n",
      "Step 0: loss = 0.0023358084727078676, recon_loss = 0.0023320112377405167, kl_loss = 0.0007594414055347443\n",
      "\n",
      "Epoch 798\n",
      "Step 0: loss = 0.0026055332273244858, recon_loss = 0.0026010796427726746, kl_loss = 0.0008907048031687737\n",
      "\n",
      "Epoch 799\n",
      "Step 0: loss = 0.0022186776623129845, recon_loss = 0.002216145396232605, kl_loss = 0.0005064308643341064\n",
      "\n",
      "Epoch 800\n",
      "Step 0: loss = 0.002433278365060687, recon_loss = 0.0024271998554468155, kl_loss = 0.001215718686580658\n",
      "\n",
      "Epoch 801\n",
      "Step 0: loss = 0.0023178691044449806, recon_loss = 0.0023127440363168716, kl_loss = 0.0010250117629766464\n",
      "\n",
      "Epoch 802\n",
      "Step 0: loss = 0.0025932223070412874, recon_loss = 0.002590758726000786, kl_loss = 0.0004927366971969604\n",
      "\n",
      "Epoch 803\n",
      "Step 0: loss = 0.002417028183117509, recon_loss = 0.002413244917988777, kl_loss = 0.0007566418498754501\n",
      "\n",
      "Epoch 804\n",
      "Step 0: loss = 0.00236658682115376, recon_loss = 0.0023632124066352844, kl_loss = 0.0006748875603079796\n",
      "\n",
      "Epoch 805\n",
      "Step 0: loss = 0.0022666435688734055, recon_loss = 0.0022647585719823837, kl_loss = 0.00037698354572057724\n",
      "\n",
      "Epoch 806\n",
      "Step 0: loss = 0.0024103461764752865, recon_loss = 0.002407677471637726, kl_loss = 0.0005337540060281754\n",
      "\n",
      "Epoch 807\n",
      "Step 0: loss = 0.002405625768005848, recon_loss = 0.002402348443865776, kl_loss = 0.000655478797852993\n",
      "\n",
      "Epoch 808\n",
      "Step 0: loss = 0.0027040312997996807, recon_loss = 0.0026993528008461, kl_loss = 0.0009356848895549774\n",
      "\n",
      "Epoch 809\n",
      "Step 0: loss = 0.002560609020292759, recon_loss = 0.0025577209889888763, kl_loss = 0.0005775857716798782\n",
      "\n",
      "Epoch 810\n",
      "Step 0: loss = 0.002413565758615732, recon_loss = 0.0024114996194839478, kl_loss = 0.0004132073372602463\n",
      "\n",
      "Epoch 811\n",
      "Step 0: loss = 0.002173056360334158, recon_loss = 0.0021689720451831818, kl_loss = 0.000816849060356617\n",
      "\n",
      "Epoch 812\n",
      "Step 0: loss = 0.0024441280402243137, recon_loss = 0.00244196318089962, kl_loss = 0.00043298397213220596\n",
      "\n",
      "Epoch 813\n",
      "Step 0: loss = 0.001895708730444312, recon_loss = 0.0018932502716779709, kl_loss = 0.0004916908219456673\n",
      "\n",
      "Epoch 814\n",
      "Step 0: loss = 0.0021571703255176544, recon_loss = 0.0021537113934755325, kl_loss = 0.0006917696446180344\n",
      "\n",
      "Epoch 815\n",
      "Step 0: loss = 0.002031866693869233, recon_loss = 0.0020289085805416107, kl_loss = 0.000591612420976162\n",
      "\n",
      "Epoch 816\n",
      "Step 0: loss = 0.001967273885384202, recon_loss = 0.001965384930372238, kl_loss = 0.0003777686506509781\n",
      "\n",
      "Epoch 817\n",
      "Step 0: loss = 0.0018595895962789655, recon_loss = 0.0018580053001642227, kl_loss = 0.0003168666735291481\n",
      "\n",
      "Epoch 818\n",
      "Step 0: loss = 0.001984126167371869, recon_loss = 0.001982327550649643, kl_loss = 0.00035973265767097473\n",
      "\n",
      "Epoch 819\n",
      "Step 0: loss = 0.0020485310815274715, recon_loss = 0.00204593688249588, kl_loss = 0.0005188370123505592\n",
      "\n",
      "Epoch 820\n",
      "Step 0: loss = 0.0019898333121091127, recon_loss = 0.001988135278224945, kl_loss = 0.000339629128575325\n",
      "\n",
      "Epoch 821\n",
      "Step 0: loss = 0.0020904308184981346, recon_loss = 0.0020872317254543304, kl_loss = 0.0006398344412446022\n",
      "\n",
      "Epoch 822\n",
      "Step 0: loss = 0.002189814578741789, recon_loss = 0.002187442034482956, kl_loss = 0.00047449860721826553\n",
      "\n",
      "Epoch 823\n",
      "Step 0: loss = 0.002157250652089715, recon_loss = 0.002154456451535225, kl_loss = 0.0005588289350271225\n",
      "\n",
      "Epoch 824\n",
      "Step 0: loss = 0.0021293044555932283, recon_loss = 0.002126559615135193, kl_loss = 0.0005489839240908623\n",
      "\n",
      "Epoch 825\n",
      "Step 0: loss = 0.002275054808706045, recon_loss = 0.002272237092256546, kl_loss = 0.0005635302513837814\n",
      "\n",
      "Epoch 826\n",
      "Step 0: loss = 0.002458137460052967, recon_loss = 0.002455061301589012, kl_loss = 0.0006152130663394928\n",
      "\n",
      "Epoch 827\n",
      "Step 0: loss = 0.0024059033021330833, recon_loss = 0.0024025477468967438, kl_loss = 0.0006710998713970184\n",
      "\n",
      "Epoch 828\n",
      "Step 0: loss = 0.0023265969939529896, recon_loss = 0.0023238621652126312, kl_loss = 0.0005469527095556259\n",
      "\n",
      "Epoch 829\n",
      "Step 0: loss = 0.002324592089280486, recon_loss = 0.0023211315274238586, kl_loss = 0.0006921142339706421\n",
      "\n",
      "Epoch 830\n",
      "Step 0: loss = 0.002426587510854006, recon_loss = 0.002423420548439026, kl_loss = 0.00063337292522192\n",
      "\n",
      "Epoch 831\n",
      "Step 0: loss = 0.0025247051380574703, recon_loss = 0.0025223512202501297, kl_loss = 0.00047076959162950516\n",
      "\n",
      "Epoch 832\n",
      "Step 0: loss = 0.0022284488659352064, recon_loss = 0.002225613221526146, kl_loss = 0.0005671074613928795\n",
      "\n",
      "Epoch 833\n",
      "Step 0: loss = 0.0024532703682780266, recon_loss = 0.0024511869996786118, kl_loss = 0.0004166914150118828\n",
      "\n",
      "Epoch 834\n",
      "Step 0: loss = 0.002340608276426792, recon_loss = 0.0023392215371131897, kl_loss = 0.00027736276388168335\n",
      "\n",
      "Epoch 835\n",
      "Step 0: loss = 0.001966045005246997, recon_loss = 0.0019631460309028625, kl_loss = 0.0005797818303108215\n",
      "\n",
      "Epoch 836\n",
      "Step 0: loss = 0.0019423171179369092, recon_loss = 0.0019377879798412323, kl_loss = 0.000905829481780529\n",
      "\n",
      "Epoch 837\n",
      "Step 0: loss = 0.002333536744117737, recon_loss = 0.0023312941193580627, kl_loss = 0.0004485463723540306\n",
      "\n",
      "Epoch 838\n",
      "Step 0: loss = 0.002269894117489457, recon_loss = 0.002267010509967804, kl_loss = 0.000576704740524292\n",
      "\n",
      "Epoch 839\n",
      "Step 0: loss = 0.0021787097211927176, recon_loss = 0.002175167202949524, kl_loss = 0.0007085129618644714\n",
      "\n",
      "Epoch 840\n",
      "Step 0: loss = 0.002111628884449601, recon_loss = 0.0021096859127283096, kl_loss = 0.0003886064514517784\n",
      "\n",
      "Epoch 841\n",
      "Step 0: loss = 0.002092763315886259, recon_loss = 0.0020892880856990814, kl_loss = 0.0006950283423066139\n",
      "\n",
      "Epoch 842\n",
      "Step 0: loss = 0.0019603620748966932, recon_loss = 0.0019570868462324142, kl_loss = 0.0006550662219524384\n",
      "\n",
      "Epoch 843\n",
      "Step 0: loss = 0.002034159377217293, recon_loss = 0.002030918374657631, kl_loss = 0.0006481995806097984\n",
      "\n",
      "Epoch 844\n",
      "Step 0: loss = 0.0020141121931374073, recon_loss = 0.002009900286793709, kl_loss = 0.0008423971012234688\n",
      "\n",
      "Epoch 845\n",
      "Step 0: loss = 0.002112128771841526, recon_loss = 0.0021094493567943573, kl_loss = 0.0005358606576919556\n",
      "\n",
      "Epoch 846\n",
      "Step 0: loss = 0.002090201945975423, recon_loss = 0.002085357904434204, kl_loss = 0.0009688083082437515\n",
      "\n",
      "Epoch 847\n",
      "Step 0: loss = 0.002039575483649969, recon_loss = 0.0020372197031974792, kl_loss = 0.0004711691290140152\n",
      "\n",
      "Epoch 848\n",
      "Step 0: loss = 0.00214859819971025, recon_loss = 0.002146005630493164, kl_loss = 0.0005185185000300407\n",
      "\n",
      "Epoch 849\n",
      "Step 0: loss = 0.001998880412429571, recon_loss = 0.0019960515201091766, kl_loss = 0.0005657859146595001\n",
      "\n",
      "Epoch 850\n",
      "Step 0: loss = 0.0021877186372876167, recon_loss = 0.002185765653848648, kl_loss = 0.0003905901685357094\n",
      "\n",
      "Epoch 851\n",
      "Step 0: loss = 0.00222264532931149, recon_loss = 0.002220219001173973, kl_loss = 0.0004852712154388428\n",
      "\n",
      "Epoch 852\n",
      "Step 0: loss = 0.002078843303024769, recon_loss = 0.002076242119073868, kl_loss = 0.0005202349275350571\n",
      "\n",
      "Epoch 853\n",
      "Step 0: loss = 0.002055009361356497, recon_loss = 0.00205317884683609, kl_loss = 0.00036609265953302383\n",
      "\n",
      "Epoch 854\n",
      "Step 0: loss = 0.0019337014527991414, recon_loss = 0.0019315332174301147, kl_loss = 0.0004336368292570114\n",
      "\n",
      "Epoch 855\n",
      "Step 0: loss = 0.0021245672833174467, recon_loss = 0.0021222084760665894, kl_loss = 0.0004717772826552391\n",
      "\n",
      "Epoch 856\n",
      "Step 0: loss = 0.0020673328544944525, recon_loss = 0.00206608884036541, kl_loss = 0.0002488112077116966\n",
      "\n",
      "Epoch 857\n",
      "Step 0: loss = 0.0020947386510670185, recon_loss = 0.0020926371216773987, kl_loss = 0.00042032357305288315\n",
      "\n",
      "Epoch 858\n",
      "Step 0: loss = 0.0018181579653173685, recon_loss = 0.0018141940236091614, kl_loss = 0.0007927790284156799\n",
      "\n",
      "Epoch 859\n",
      "Step 0: loss = 0.0021428307518363, recon_loss = 0.002138391137123108, kl_loss = 0.0008879099041223526\n",
      "\n",
      "Epoch 860\n",
      "Step 0: loss = 0.002146727405488491, recon_loss = 0.002143135294318199, kl_loss = 0.0007184222340583801\n",
      "\n",
      "Epoch 861\n",
      "Step 0: loss = 0.00205608201213181, recon_loss = 0.002053758129477501, kl_loss = 0.0004647625610232353\n",
      "\n",
      "Epoch 862\n",
      "Step 0: loss = 0.002090724650770426, recon_loss = 0.002088431268930435, kl_loss = 0.00045865681022405624\n",
      "\n",
      "Epoch 863\n",
      "Step 0: loss = 0.002056497149169445, recon_loss = 0.0020543336868286133, kl_loss = 0.00043269433081150055\n",
      "\n",
      "Epoch 864\n",
      "Step 0: loss = 0.0019184726988896728, recon_loss = 0.0019164998084306717, kl_loss = 0.0003945697098970413\n",
      "\n",
      "Epoch 865\n",
      "Step 0: loss = 0.0017873967299237847, recon_loss = 0.0017849765717983246, kl_loss = 0.0004840260371565819\n",
      "\n",
      "Epoch 866\n",
      "Step 0: loss = 0.0018790100002661347, recon_loss = 0.0018754452466964722, kl_loss = 0.000712960958480835\n",
      "\n",
      "Epoch 867\n",
      "Step 0: loss = 0.0018953931285068393, recon_loss = 0.0018917061388492584, kl_loss = 0.0007373988628387451\n",
      "\n",
      "Epoch 868\n",
      "Step 0: loss = 0.0019266146700829268, recon_loss = 0.0019241832196712494, kl_loss = 0.00048629939556121826\n",
      "\n",
      "Epoch 869\n",
      "Step 0: loss = 0.002090883906930685, recon_loss = 0.0020880941301584244, kl_loss = 0.0005579637363553047\n",
      "\n",
      "Epoch 870\n",
      "Step 0: loss = 0.0019960629288107157, recon_loss = 0.0019931327551603317, kl_loss = 0.0005860356613993645\n",
      "\n",
      "Epoch 871\n",
      "Step 0: loss = 0.0021213481668382883, recon_loss = 0.002119552344083786, kl_loss = 0.00035915616899728775\n",
      "\n",
      "Epoch 872\n",
      "Step 0: loss = 0.001988534815609455, recon_loss = 0.001986190676689148, kl_loss = 0.00046885013580322266\n",
      "\n",
      "Epoch 873\n",
      "Step 0: loss = 0.0021378768142312765, recon_loss = 0.0021358393132686615, kl_loss = 0.0004074983298778534\n",
      "\n",
      "Epoch 874\n",
      "Step 0: loss = 0.002177242189645767, recon_loss = 0.002173883840441704, kl_loss = 0.0006716679781675339\n",
      "\n",
      "Epoch 875\n",
      "Step 0: loss = 0.0021783635020256042, recon_loss = 0.002175150439143181, kl_loss = 0.0006425976753234863\n",
      "\n",
      "Epoch 876\n",
      "Step 0: loss = 0.002189867664128542, recon_loss = 0.0021873395889997482, kl_loss = 0.0005056168884038925\n",
      "\n",
      "Epoch 877\n",
      "Step 0: loss = 0.00230579380877316, recon_loss = 0.0023019686341285706, kl_loss = 0.0007650554180145264\n",
      "\n",
      "Epoch 878\n",
      "Step 0: loss = 0.0020752165000885725, recon_loss = 0.0020728297531604767, kl_loss = 0.00047735031694173813\n",
      "\n",
      "Epoch 879\n",
      "Step 0: loss = 0.002110817702487111, recon_loss = 0.0021091829985380173, kl_loss = 0.00032694730907678604\n",
      "\n",
      "Epoch 880\n",
      "Step 0: loss = 0.0020477711223065853, recon_loss = 0.0020450931042432785, kl_loss = 0.000535598024725914\n",
      "\n",
      "Epoch 881\n",
      "Step 0: loss = 0.002217375673353672, recon_loss = 0.0022148732095956802, kl_loss = 0.000500483438372612\n",
      "\n",
      "Epoch 882\n",
      "Step 0: loss = 0.0021675468888133764, recon_loss = 0.0021653324365615845, kl_loss = 0.00044288020581007004\n",
      "\n",
      "Epoch 883\n",
      "Step 0: loss = 0.0023946266155689955, recon_loss = 0.002391509711742401, kl_loss = 0.0006233779713511467\n",
      "\n",
      "Epoch 884\n",
      "Step 0: loss = 0.002141085220500827, recon_loss = 0.0021380074322223663, kl_loss = 0.0006155567243695259\n",
      "\n",
      "Epoch 885\n",
      "Step 0: loss = 0.0020192700903862715, recon_loss = 0.002016805112361908, kl_loss = 0.0004929741844534874\n",
      "\n",
      "Epoch 886\n",
      "Step 0: loss = 0.002040897961705923, recon_loss = 0.002036105841398239, kl_loss = 0.000958416610956192\n",
      "\n",
      "Epoch 887\n",
      "Step 0: loss = 0.002178442431613803, recon_loss = 0.0021750759333372116, kl_loss = 0.0006733015179634094\n",
      "\n",
      "Epoch 888\n",
      "Step 0: loss = 0.002020173240453005, recon_loss = 0.0020140763372182846, kl_loss = 0.0012193573638796806\n",
      "\n",
      "Epoch 889\n",
      "Step 0: loss = 0.0022687369491904974, recon_loss = 0.0022641681134700775, kl_loss = 0.0009137466549873352\n",
      "\n",
      "Epoch 890\n",
      "Step 0: loss = 0.0022571426816284657, recon_loss = 0.002254616469144821, kl_loss = 0.0005052220076322556\n",
      "\n",
      "Epoch 891\n",
      "Step 0: loss = 0.0020126434974372387, recon_loss = 0.0020103398710489273, kl_loss = 0.00046073365956544876\n",
      "\n",
      "Epoch 892\n",
      "Step 0: loss = 0.001959331566467881, recon_loss = 0.0019540172070264816, kl_loss = 0.001062893308699131\n",
      "\n",
      "Epoch 893\n",
      "Step 0: loss = 0.002048640977591276, recon_loss = 0.002045782282948494, kl_loss = 0.0005717258900403976\n",
      "\n",
      "Epoch 894\n",
      "Step 0: loss = 0.0020343821961432695, recon_loss = 0.002029377967119217, kl_loss = 0.0010008634999394417\n",
      "\n",
      "Epoch 895\n",
      "Step 0: loss = 0.001999832922592759, recon_loss = 0.001997295767068863, kl_loss = 0.0005074478685855865\n",
      "\n",
      "Epoch 896\n",
      "Step 0: loss = 0.0020098716486245394, recon_loss = 0.0020056497305631638, kl_loss = 0.0008443733677268028\n",
      "\n",
      "Epoch 897\n",
      "Step 0: loss = 0.002048516646027565, recon_loss = 0.0020464900881052017, kl_loss = 0.000405309721827507\n",
      "\n",
      "Epoch 898\n",
      "Step 0: loss = 0.001967237563803792, recon_loss = 0.0019649770110845566, kl_loss = 0.00045212823897600174\n",
      "\n",
      "Epoch 899\n",
      "Step 0: loss = 0.002125529106706381, recon_loss = 0.0021236836910247803, kl_loss = 0.0003690626472234726\n",
      "\n",
      "Epoch 900\n",
      "Step 0: loss = 0.0020484502892941236, recon_loss = 0.002044590190052986, kl_loss = 0.0007720310240983963\n",
      "\n",
      "Epoch 901\n",
      "Step 0: loss = 0.0022994736209511757, recon_loss = 0.002297075465321541, kl_loss = 0.0004796544089913368\n",
      "\n",
      "Epoch 902\n",
      "Step 0: loss = 0.0018408136675134301, recon_loss = 0.0018390621989965439, kl_loss = 0.000350290909409523\n",
      "\n",
      "Epoch 903\n",
      "Step 0: loss = 0.0017592995427548885, recon_loss = 0.0017563384026288986, kl_loss = 0.0005922336131334305\n",
      "\n",
      "Epoch 904\n",
      "Step 0: loss = 0.0018539540469646454, recon_loss = 0.001852467656135559, kl_loss = 0.0002972874790430069\n",
      "\n",
      "Epoch 905\n",
      "Step 0: loss = 0.0018298757495358586, recon_loss = 0.0018273405730724335, kl_loss = 0.0005070352926850319\n",
      "\n",
      "Epoch 906\n",
      "Step 0: loss = 0.0017194976098835468, recon_loss = 0.0017162766307592392, kl_loss = 0.0006441865116357803\n",
      "\n",
      "Epoch 907\n",
      "Step 0: loss = 0.00200278265401721, recon_loss = 0.0020012687891721725, kl_loss = 0.0003027571365237236\n",
      "\n",
      "Epoch 908\n",
      "Step 0: loss = 0.002012863289564848, recon_loss = 0.0020110420882701874, kl_loss = 0.00036425888538360596\n",
      "\n",
      "Epoch 909\n",
      "Step 0: loss = 0.0019312521908432245, recon_loss = 0.0019282400608062744, kl_loss = 0.0006024278700351715\n",
      "\n",
      "Epoch 910\n",
      "Step 0: loss = 0.001919891219586134, recon_loss = 0.0019167084246873856, kl_loss = 0.000636558048427105\n",
      "\n",
      "Epoch 911\n",
      "Step 0: loss = 0.0019210097379982471, recon_loss = 0.0019180309027433395, kl_loss = 0.0005957717075943947\n",
      "\n",
      "Epoch 912\n",
      "Step 0: loss = 0.002092415699735284, recon_loss = 0.0020873192697763443, kl_loss = 0.0010192804038524628\n",
      "\n",
      "Epoch 913\n",
      "Step 0: loss = 0.002069044392555952, recon_loss = 0.0020648259669542313, kl_loss = 0.0008436674252152443\n",
      "\n",
      "Epoch 914\n",
      "Step 0: loss = 0.002133964793756604, recon_loss = 0.0021293945610523224, kl_loss = 0.0009140344336628914\n",
      "\n",
      "Epoch 915\n",
      "Step 0: loss = 0.0024471597280353308, recon_loss = 0.002440791577100754, kl_loss = 0.0012736478820443153\n",
      "\n",
      "Epoch 916\n",
      "Step 0: loss = 0.0021278238855302334, recon_loss = 0.002124333754181862, kl_loss = 0.0006980197504162788\n",
      "\n",
      "Epoch 917\n",
      "Step 0: loss = 0.00198701536282897, recon_loss = 0.0019839443266391754, kl_loss = 0.0006141848862171173\n",
      "\n",
      "Epoch 918\n",
      "Step 0: loss = 0.0019909983966499567, recon_loss = 0.001987617462873459, kl_loss = 0.000676189549267292\n",
      "\n",
      "Epoch 919\n",
      "Step 0: loss = 0.0022304460871964693, recon_loss = 0.002228204160928726, kl_loss = 0.00044837407767772675\n",
      "\n",
      "Epoch 920\n",
      "Step 0: loss = 0.0021118775475770235, recon_loss = 0.0021101050078868866, kl_loss = 0.0003544976934790611\n",
      "\n",
      "Epoch 921\n",
      "Step 0: loss = 0.0022253962233662605, recon_loss = 0.002222605049610138, kl_loss = 0.0005582328885793686\n",
      "\n",
      "Epoch 922\n",
      "Step 0: loss = 0.002074763411656022, recon_loss = 0.0020723864436149597, kl_loss = 0.00047540292143821716\n",
      "\n",
      "Epoch 923\n",
      "Step 0: loss = 0.0019851040560752153, recon_loss = 0.00198274664580822, kl_loss = 0.00047148484736680984\n",
      "\n",
      "Epoch 924\n",
      "Step 0: loss = 0.001917865825816989, recon_loss = 0.0019155684858560562, kl_loss = 0.00045947637408971786\n",
      "\n",
      "Epoch 925\n",
      "Step 0: loss = 0.0020703880582004786, recon_loss = 0.00206746906042099, kl_loss = 0.0005838172510266304\n",
      "\n",
      "Epoch 926\n",
      "Step 0: loss = 0.002073225798085332, recon_loss = 0.002070702612400055, kl_loss = 0.000504656694829464\n",
      "\n",
      "Epoch 927\n",
      "Step 0: loss = 0.0018749324372038245, recon_loss = 0.0018735956400632858, kl_loss = 0.00026735756546258926\n",
      "\n",
      "Epoch 928\n",
      "Step 0: loss = 0.0017538982210680842, recon_loss = 0.0017519686371088028, kl_loss = 0.000385921448469162\n",
      "\n",
      "Epoch 929\n",
      "Step 0: loss = 0.0016896345186978579, recon_loss = 0.001687983050942421, kl_loss = 0.00033029913902282715\n",
      "\n",
      "Epoch 930\n",
      "Step 0: loss = 0.0016957605257630348, recon_loss = 0.001693645492196083, kl_loss = 0.0004230048507452011\n",
      "\n",
      "Epoch 931\n",
      "Step 0: loss = 0.0016323387390002608, recon_loss = 0.001629447564482689, kl_loss = 0.0005782265216112137\n",
      "\n",
      "Epoch 932\n",
      "Step 0: loss = 0.0016794714611023664, recon_loss = 0.0016773641109466553, kl_loss = 0.00042148027569055557\n",
      "\n",
      "Epoch 933\n",
      "Step 0: loss = 0.001633558771573007, recon_loss = 0.0016315504908561707, kl_loss = 0.0004016486927866936\n",
      "\n",
      "Epoch 934\n",
      "Step 0: loss = 0.0015086105559021235, recon_loss = 0.0015074200928211212, kl_loss = 0.00023809541016817093\n",
      "\n",
      "Epoch 935\n",
      "Step 0: loss = 0.001548506785184145, recon_loss = 0.001547299325466156, kl_loss = 0.0002414938062429428\n",
      "\n",
      "Epoch 936\n",
      "Step 0: loss = 0.0014971275813877583, recon_loss = 0.0014951955527067184, kl_loss = 0.0003863973543047905\n",
      "\n",
      "Epoch 937\n",
      "Step 0: loss = 0.0015521901659667492, recon_loss = 0.0015494301915168762, kl_loss = 0.0005519865080714226\n",
      "\n",
      "Epoch 938\n",
      "Step 0: loss = 0.001459094462916255, recon_loss = 0.0014553889632225037, kl_loss = 0.0007410980761051178\n",
      "\n",
      "Epoch 939\n",
      "Step 0: loss = 0.001487191068008542, recon_loss = 0.0014838352799415588, kl_loss = 0.0006711594760417938\n",
      "\n",
      "Epoch 940\n",
      "Step 0: loss = 0.0016145083354786038, recon_loss = 0.0016129538416862488, kl_loss = 0.00031090620905160904\n",
      "\n",
      "Epoch 941\n",
      "Step 0: loss = 0.0018116363789886236, recon_loss = 0.0018095746636390686, kl_loss = 0.00041233934462070465\n",
      "\n",
      "Epoch 942\n",
      "Step 0: loss = 0.001632889499887824, recon_loss = 0.00163179449737072, kl_loss = 0.00021901074796915054\n",
      "\n",
      "Epoch 943\n",
      "Step 0: loss = 0.0018751909956336021, recon_loss = 0.0018734224140644073, kl_loss = 0.0003537200391292572\n",
      "\n",
      "Epoch 944\n",
      "Step 0: loss = 0.0016970786964520812, recon_loss = 0.0016948990523815155, kl_loss = 0.0004359176382422447\n",
      "\n",
      "Epoch 945\n",
      "Step 0: loss = 0.0017058361554518342, recon_loss = 0.0017044320702552795, kl_loss = 0.0002808067947626114\n",
      "\n",
      "Epoch 946\n",
      "Step 0: loss = 0.0018044039607048035, recon_loss = 0.0018032826483249664, kl_loss = 0.00022425875067710876\n",
      "\n",
      "Epoch 947\n",
      "Step 0: loss = 0.001707745366729796, recon_loss = 0.001706056296825409, kl_loss = 0.00033781304955482483\n",
      "\n",
      "Epoch 948\n",
      "Step 0: loss = 0.0017663146136328578, recon_loss = 0.0017647519707679749, kl_loss = 0.0003125173971056938\n",
      "\n",
      "Epoch 949\n",
      "Step 0: loss = 0.001883589313365519, recon_loss = 0.0018815472722053528, kl_loss = 0.0004084128886461258\n",
      "\n",
      "Epoch 950\n",
      "Step 0: loss = 0.001966660376638174, recon_loss = 0.0019643716514110565, kl_loss = 0.0004577590152621269\n",
      "\n",
      "Epoch 951\n",
      "Step 0: loss = 0.0019747952464967966, recon_loss = 0.0019718166440725327, kl_loss = 0.0005957288667559624\n",
      "\n",
      "Epoch 952\n",
      "Step 0: loss = 0.001681318273767829, recon_loss = 0.0016790106892585754, kl_loss = 0.0004615131765604019\n",
      "\n",
      "Epoch 953\n",
      "Step 0: loss = 0.002015362260863185, recon_loss = 0.0020131822675466537, kl_loss = 0.0004359893500804901\n",
      "\n",
      "Epoch 954\n",
      "Step 0: loss = 0.0019278749823570251, recon_loss = 0.0019250158220529556, kl_loss = 0.0005718432366847992\n",
      "\n",
      "Epoch 955\n",
      "Step 0: loss = 0.001953920815140009, recon_loss = 0.001951012760400772, kl_loss = 0.000581599771976471\n",
      "\n",
      "Epoch 956\n",
      "Step 0: loss = 0.0022323098964989185, recon_loss = 0.0022285450249910355, kl_loss = 0.000752951018512249\n",
      "\n",
      "Epoch 957\n",
      "Step 0: loss = 0.002200160175561905, recon_loss = 0.002196134999394417, kl_loss = 0.0008050510659813881\n",
      "\n",
      "Epoch 958\n",
      "Step 0: loss = 0.0021213190630078316, recon_loss = 0.0021181143820285797, kl_loss = 0.0006409576162695885\n",
      "\n",
      "Epoch 959\n",
      "Step 0: loss = 0.0019983064848929644, recon_loss = 0.001996004953980446, kl_loss = 0.0004603154957294464\n",
      "\n",
      "Epoch 960\n",
      "Step 0: loss = 0.002334021031856537, recon_loss = 0.0023304391652345657, kl_loss = 0.0007163826376199722\n",
      "\n",
      "Epoch 961\n",
      "Step 0: loss = 0.002172209322452545, recon_loss = 0.002169288694858551, kl_loss = 0.0005841050297021866\n",
      "\n",
      "Epoch 962\n",
      "Step 0: loss = 0.0020256920251995325, recon_loss = 0.0020220521837472916, kl_loss = 0.0007279478013515472\n",
      "\n",
      "Epoch 963\n",
      "Step 0: loss = 0.0023953327909111977, recon_loss = 0.002393055707216263, kl_loss = 0.00045541394501924515\n",
      "\n",
      "Epoch 964\n",
      "Step 0: loss = 0.002215632703155279, recon_loss = 0.002212114632129669, kl_loss = 0.0007036123424768448\n",
      "\n",
      "Epoch 965\n",
      "Step 0: loss = 0.0021993746049702168, recon_loss = 0.0021961238235235214, kl_loss = 0.0006501385942101479\n",
      "\n",
      "Epoch 966\n",
      "Step 0: loss = 0.0021886953618377447, recon_loss = 0.0021863002330064774, kl_loss = 0.00047900527715682983\n",
      "\n",
      "Epoch 967\n",
      "Step 0: loss = 0.0021114032715559006, recon_loss = 0.002108165994286537, kl_loss = 0.0006474647670984268\n",
      "\n",
      "Epoch 968\n",
      "Step 0: loss = 0.0019389358349144459, recon_loss = 0.0019363295286893845, kl_loss = 0.0005212603136897087\n",
      "\n",
      "Epoch 969\n",
      "Step 0: loss = 0.0020059740636497736, recon_loss = 0.002004167065024376, kl_loss = 0.00036139972507953644\n",
      "\n",
      "Epoch 970\n",
      "Step 0: loss = 0.001985108945518732, recon_loss = 0.0019829794764518738, kl_loss = 0.00042587704956531525\n",
      "\n",
      "Epoch 971\n",
      "Step 0: loss = 0.0018672521691769361, recon_loss = 0.0018650535494089127, kl_loss = 0.00043972115963697433\n",
      "\n",
      "Epoch 972\n",
      "Step 0: loss = 0.0017055278876796365, recon_loss = 0.001703346148133278, kl_loss = 0.0004363423213362694\n",
      "\n",
      "Epoch 973\n",
      "Step 0: loss = 0.0016358158318325877, recon_loss = 0.0016339905560016632, kl_loss = 0.0003650560975074768\n",
      "\n",
      "Epoch 974\n",
      "Step 0: loss = 0.0016975841717794538, recon_loss = 0.0016956273466348648, kl_loss = 0.00039135850965976715\n",
      "\n",
      "Epoch 975\n",
      "Step 0: loss = 0.0018619111506268382, recon_loss = 0.001860128715634346, kl_loss = 0.0003564832732081413\n",
      "\n",
      "Epoch 976\n",
      "Step 0: loss = 0.0016631302423775196, recon_loss = 0.001661159098148346, kl_loss = 0.0003942269831895828\n",
      "\n",
      "Epoch 977\n",
      "Step 0: loss = 0.0017412371234968305, recon_loss = 0.0017391927540302277, kl_loss = 0.00040888041257858276\n",
      "\n",
      "Epoch 978\n",
      "Step 0: loss = 0.0016098914202302694, recon_loss = 0.0016075987368822098, kl_loss = 0.00045853666961193085\n",
      "\n",
      "Epoch 979\n",
      "Step 0: loss = 0.0017738918540999293, recon_loss = 0.0017717182636260986, kl_loss = 0.00043470878154039383\n",
      "\n",
      "Epoch 980\n",
      "Step 0: loss = 0.0017307490343227983, recon_loss = 0.0017287135124206543, kl_loss = 0.0004070959985256195\n",
      "\n",
      "Epoch 981\n",
      "Step 0: loss = 0.001837476622313261, recon_loss = 0.001835683360695839, kl_loss = 0.00035866256803274155\n",
      "\n",
      "Epoch 982\n",
      "Step 0: loss = 0.0017141069984063506, recon_loss = 0.0017120558768510818, kl_loss = 0.00041021406650543213\n",
      "\n",
      "Epoch 983\n",
      "Step 0: loss = 0.0019216126529499888, recon_loss = 0.0019199736416339874, kl_loss = 0.0003278125077486038\n",
      "\n",
      "Epoch 984\n",
      "Step 0: loss = 0.0018800271209329367, recon_loss = 0.0018783509731292725, kl_loss = 0.0003352304920554161\n",
      "\n",
      "Epoch 985\n",
      "Step 0: loss = 0.001653564628213644, recon_loss = 0.0016518477350473404, kl_loss = 0.00034337956458330154\n",
      "\n",
      "Epoch 986\n",
      "Step 0: loss = 0.0018055399414151907, recon_loss = 0.0018030554056167603, kl_loss = 0.0004968969151377678\n",
      "\n",
      "Epoch 987\n",
      "Step 0: loss = 0.0017700945027172565, recon_loss = 0.0017679892480373383, kl_loss = 0.0004210546612739563\n",
      "\n",
      "Epoch 988\n",
      "Step 0: loss = 0.0015397639945149422, recon_loss = 0.001537589356303215, kl_loss = 0.0004349350929260254\n",
      "\n",
      "Epoch 989\n",
      "Step 0: loss = 0.001581365941092372, recon_loss = 0.001579735428094864, kl_loss = 0.0003261137753725052\n",
      "\n",
      "Epoch 990\n",
      "Step 0: loss = 0.0014534374931827188, recon_loss = 0.0014515984803438187, kl_loss = 0.0003677932545542717\n",
      "\n",
      "Epoch 991\n",
      "Step 0: loss = 0.0014986810274422169, recon_loss = 0.001497594639658928, kl_loss = 0.00021727196872234344\n",
      "\n",
      "Epoch 992\n",
      "Step 0: loss = 0.0015980039024725556, recon_loss = 0.0015966389328241348, kl_loss = 0.00027299486100673676\n",
      "\n",
      "Epoch 993\n",
      "Step 0: loss = 0.0017063517589122057, recon_loss = 0.0017044153064489365, kl_loss = 0.00038729142397642136\n",
      "\n",
      "Epoch 994\n",
      "Step 0: loss = 0.0016548697603866458, recon_loss = 0.001653062179684639, kl_loss = 0.00036152638494968414\n",
      "\n",
      "Epoch 995\n",
      "Step 0: loss = 0.0015812984202057123, recon_loss = 0.0015781503170728683, kl_loss = 0.000629618763923645\n",
      "\n",
      "Epoch 996\n",
      "Step 0: loss = 0.0016142395325005054, recon_loss = 0.001612624153494835, kl_loss = 0.00032307300716638565\n",
      "\n",
      "Epoch 997\n",
      "Step 0: loss = 0.0018091438105329871, recon_loss = 0.0018077492713928223, kl_loss = 0.00027890875935554504\n",
      "\n",
      "Epoch 998\n",
      "Step 0: loss = 0.0016638976521790028, recon_loss = 0.0016615446656942368, kl_loss = 0.00047060195356607437\n",
      "\n",
      "Epoch 999\n",
      "Step 0: loss = 0.001767091453075409, recon_loss = 0.0017647165805101395, kl_loss = 0.00047498568892478943\n",
      "\n",
      "Epoch 1000\n",
      "Step 0: loss = 0.0017627590568736196, recon_loss = 0.0017593801021575928, kl_loss = 0.0006757937371730804\n",
      "\n",
      "Epoch 1001\n",
      "Step 0: loss = 0.0016378764994442463, recon_loss = 0.0016336794942617416, kl_loss = 0.0008394112810492516\n",
      "\n",
      "Epoch 1002\n",
      "Step 0: loss = 0.0020830973517149687, recon_loss = 0.0020806025713682175, kl_loss = 0.0004989393055438995\n",
      "\n",
      "Epoch 1003\n",
      "Step 0: loss = 0.002114693634212017, recon_loss = 0.002110857516527176, kl_loss = 0.0007672086358070374\n",
      "\n",
      "Epoch 1004\n",
      "Step 0: loss = 0.0021797223016619682, recon_loss = 0.0021772384643554688, kl_loss = 0.0004967721179127693\n",
      "\n",
      "Epoch 1005\n",
      "Step 0: loss = 0.002214225474745035, recon_loss = 0.0022116675972938538, kl_loss = 0.0005115680396556854\n",
      "\n",
      "Epoch 1006\n",
      "Step 0: loss = 0.0020967593882232904, recon_loss = 0.002093454822897911, kl_loss = 0.000660906545817852\n",
      "\n",
      "Epoch 1007\n",
      "Step 0: loss = 0.00214025960303843, recon_loss = 0.0021368786692619324, kl_loss = 0.0006762035191059113\n",
      "\n",
      "Epoch 1008\n",
      "Step 0: loss = 0.00209995498880744, recon_loss = 0.0020966753363609314, kl_loss = 0.0006559444591403008\n",
      "\n",
      "Epoch 1009\n",
      "Step 0: loss = 0.0022160355001688004, recon_loss = 0.00221363827586174, kl_loss = 0.0004794243723154068\n",
      "\n",
      "Epoch 1010\n",
      "Step 0: loss = 0.0023163296282291412, recon_loss = 0.0023147743195295334, kl_loss = 0.0003110421821475029\n",
      "\n",
      "Epoch 1011\n",
      "Step 0: loss = 0.001921910559758544, recon_loss = 0.0019182153046131134, kl_loss = 0.0007390538230538368\n",
      "\n",
      "Epoch 1012\n",
      "Step 0: loss = 0.002022975357249379, recon_loss = 0.002020571380853653, kl_loss = 0.0004807906225323677\n",
      "\n",
      "Epoch 1013\n",
      "Step 0: loss = 0.001940020709298551, recon_loss = 0.001937318593263626, kl_loss = 0.0005404232069849968\n",
      "\n",
      "Epoch 1014\n",
      "Step 0: loss = 0.001891482388600707, recon_loss = 0.0018895510584115982, kl_loss = 0.00038625579327344894\n",
      "\n",
      "Epoch 1015\n",
      "Step 0: loss = 0.0017928383313119411, recon_loss = 0.0017910916358232498, kl_loss = 0.00034934934228658676\n",
      "\n",
      "Epoch 1016\n",
      "Step 0: loss = 0.0017411790322512388, recon_loss = 0.0017388314008712769, kl_loss = 0.00046953465789556503\n",
      "\n",
      "Epoch 1017\n",
      "Step 0: loss = 0.0017928287852555513, recon_loss = 0.001790560781955719, kl_loss = 0.0004536062479019165\n",
      "\n",
      "Epoch 1018\n",
      "Step 0: loss = 0.0017582750879228115, recon_loss = 0.0017559006810188293, kl_loss = 0.00047488603740930557\n",
      "\n",
      "Epoch 1019\n",
      "Step 0: loss = 0.0016905271913856268, recon_loss = 0.001689217984676361, kl_loss = 0.00026184506714344025\n",
      "\n",
      "Epoch 1020\n",
      "Step 0: loss = 0.0015639410121366382, recon_loss = 0.0015620309859514236, kl_loss = 0.00038201455026865005\n",
      "\n",
      "Epoch 1021\n",
      "Step 0: loss = 0.0017494874773547053, recon_loss = 0.001746688038110733, kl_loss = 0.000559881329536438\n",
      "\n",
      "Epoch 1022\n",
      "Step 0: loss = 0.001899875234812498, recon_loss = 0.0018978416919708252, kl_loss = 0.00040671229362487793\n",
      "\n",
      "Epoch 1023\n",
      "Step 0: loss = 0.0017823041416704655, recon_loss = 0.0017800051718950272, kl_loss = 0.0004598023369908333\n",
      "\n",
      "Epoch 1024\n",
      "Step 0: loss = 0.002006008056923747, recon_loss = 0.002003813162446022, kl_loss = 0.0004389658570289612\n",
      "\n",
      "Epoch 1025\n",
      "Step 0: loss = 0.0018368979217484593, recon_loss = 0.0018355250358581543, kl_loss = 0.00027458276599645615\n",
      "\n",
      "Epoch 1026\n",
      "Step 0: loss = 0.0019220345420762897, recon_loss = 0.001919889822602272, kl_loss = 0.0004289550706744194\n",
      "\n",
      "Epoch 1027\n",
      "Step 0: loss = 0.0018832784844562411, recon_loss = 0.0018818005919456482, kl_loss = 0.0002955785021185875\n",
      "\n",
      "Epoch 1028\n",
      "Step 0: loss = 0.001798395998775959, recon_loss = 0.0017971992492675781, kl_loss = 0.0002393573522567749\n",
      "\n",
      "Epoch 1029\n",
      "Step 0: loss = 0.0016278639668598771, recon_loss = 0.0016268249601125717, kl_loss = 0.00020779110491275787\n",
      "\n",
      "Epoch 1030\n",
      "Step 0: loss = 0.0015167369274422526, recon_loss = 0.001515289768576622, kl_loss = 0.0002894364297389984\n",
      "\n",
      "Epoch 1031\n",
      "Step 0: loss = 0.0015533018158748746, recon_loss = 0.0015518534928560257, kl_loss = 0.0002896590158343315\n",
      "\n",
      "Epoch 1032\n",
      "Step 0: loss = 0.0016192251350730658, recon_loss = 0.00161769799888134, kl_loss = 0.0003054337576031685\n",
      "\n",
      "Epoch 1033\n",
      "Step 0: loss = 0.0015596420271322131, recon_loss = 0.0015581771731376648, kl_loss = 0.0002929670736193657\n",
      "\n",
      "Epoch 1034\n",
      "Step 0: loss = 0.0015920869773253798, recon_loss = 0.001590687781572342, kl_loss = 0.0002798335626721382\n",
      "\n",
      "Epoch 1035\n",
      "Step 0: loss = 0.0017431487794965506, recon_loss = 0.0017405785620212555, kl_loss = 0.0005140453577041626\n",
      "\n",
      "Epoch 1036\n",
      "Step 0: loss = 0.001711845165118575, recon_loss = 0.001710057258605957, kl_loss = 0.0003575887531042099\n",
      "\n",
      "Epoch 1037\n",
      "Step 0: loss = 0.0016145631670951843, recon_loss = 0.0016128160059452057, kl_loss = 0.0003494415432214737\n",
      "\n",
      "Epoch 1038\n",
      "Step 0: loss = 0.0014321686467155814, recon_loss = 0.0014302749186754227, kl_loss = 0.00037874747067689896\n",
      "\n",
      "Epoch 1039\n",
      "Step 0: loss = 0.001627787365578115, recon_loss = 0.0016259904950857162, kl_loss = 0.00035938527435064316\n",
      "\n",
      "Epoch 1040\n",
      "Step 0: loss = 0.001513844938017428, recon_loss = 0.0015118308365345001, kl_loss = 0.00040280912071466446\n",
      "\n",
      "Epoch 1041\n",
      "Step 0: loss = 0.001392042962834239, recon_loss = 0.0013904254883527756, kl_loss = 0.0003234967589378357\n",
      "\n",
      "Epoch 1042\n",
      "Step 0: loss = 0.0014919989043846726, recon_loss = 0.001489708200097084, kl_loss = 0.0004581371322274208\n",
      "\n",
      "Epoch 1043\n",
      "Step 0: loss = 0.001685015275143087, recon_loss = 0.001682547852396965, kl_loss = 0.0004934947937726974\n",
      "\n",
      "Epoch 1044\n",
      "Step 0: loss = 0.001564111327752471, recon_loss = 0.001562146469950676, kl_loss = 0.00039298273622989655\n",
      "\n",
      "Epoch 1045\n",
      "Step 0: loss = 0.0016457756282761693, recon_loss = 0.0016431920230388641, kl_loss = 0.0005167238414287567\n",
      "\n",
      "Epoch 1046\n",
      "Step 0: loss = 0.0014886115677654743, recon_loss = 0.0014869142323732376, kl_loss = 0.00033946335315704346\n",
      "\n",
      "Epoch 1047\n",
      "Step 0: loss = 0.0016899328911677003, recon_loss = 0.001688338816165924, kl_loss = 0.00031880661845207214\n",
      "\n",
      "Epoch 1048\n",
      "Step 0: loss = 0.0015499969013035297, recon_loss = 0.001548711210489273, kl_loss = 0.00025712884962558746\n",
      "\n",
      "Epoch 1049\n",
      "Step 0: loss = 0.0017427897546440363, recon_loss = 0.0017418265342712402, kl_loss = 0.00019264966249465942\n",
      "\n",
      "Epoch 1050\n",
      "Step 0: loss = 0.0014371873112395406, recon_loss = 0.0014341790229082108, kl_loss = 0.0006016669794917107\n",
      "\n",
      "Epoch 1051\n",
      "Step 0: loss = 0.0016362424939870834, recon_loss = 0.0016343872994184494, kl_loss = 0.00037102866917848587\n",
      "\n",
      "Epoch 1052\n",
      "Step 0: loss = 0.0014811641303822398, recon_loss = 0.001479579135775566, kl_loss = 0.0003170100972056389\n",
      "\n",
      "Epoch 1053\n",
      "Step 0: loss = 0.0016368974465876818, recon_loss = 0.0016347076743841171, kl_loss = 0.000437944196164608\n",
      "\n",
      "Epoch 1054\n",
      "Step 0: loss = 0.001698934705927968, recon_loss = 0.0016973316669464111, kl_loss = 0.00032060686498880386\n",
      "\n",
      "Epoch 1055\n",
      "Step 0: loss = 0.001555555616505444, recon_loss = 0.0015543196350336075, kl_loss = 0.000247185118496418\n",
      "\n",
      "Epoch 1056\n",
      "Step 0: loss = 0.0017504586139693856, recon_loss = 0.001748969778418541, kl_loss = 0.0002977633848786354\n",
      "\n",
      "Epoch 1057\n",
      "Step 0: loss = 0.0018617063760757446, recon_loss = 0.001860080286860466, kl_loss = 0.00032521411776542664\n",
      "\n",
      "Epoch 1058\n",
      "Step 0: loss = 0.0018121462780982256, recon_loss = 0.001811005175113678, kl_loss = 0.00022821500897407532\n",
      "\n",
      "Epoch 1059\n",
      "Step 0: loss = 0.0017626126063987613, recon_loss = 0.0017604678869247437, kl_loss = 0.0004289476200938225\n",
      "\n",
      "Epoch 1060\n",
      "Step 0: loss = 0.0018080738373100758, recon_loss = 0.0018057283014059067, kl_loss = 0.0004691006615757942\n",
      "\n",
      "Epoch 1061\n",
      "Step 0: loss = 0.001842666999436915, recon_loss = 0.0018413923680782318, kl_loss = 0.00025491882115602493\n",
      "\n",
      "Epoch 1062\n",
      "Step 0: loss = 0.0017543609719723463, recon_loss = 0.0017514601349830627, kl_loss = 0.0005801599472761154\n",
      "\n",
      "Epoch 1063\n",
      "Step 0: loss = 0.0018391290213912725, recon_loss = 0.0018369462341070175, kl_loss = 0.0004365546628832817\n",
      "\n",
      "Epoch 1064\n",
      "Step 0: loss = 0.001728034927509725, recon_loss = 0.0017258208245038986, kl_loss = 0.00044282805174589157\n",
      "\n",
      "Epoch 1065\n",
      "Step 0: loss = 0.0018013878725469112, recon_loss = 0.0017995815724134445, kl_loss = 0.00036125723272562027\n",
      "\n",
      "Epoch 1066\n",
      "Step 0: loss = 0.0016817551804706454, recon_loss = 0.0016799811273813248, kl_loss = 0.0003548096865415573\n",
      "\n",
      "Epoch 1067\n",
      "Step 0: loss = 0.0018426356837153435, recon_loss = 0.0018408503383398056, kl_loss = 0.0003570783883333206\n",
      "\n",
      "Epoch 1068\n",
      "Step 0: loss = 0.0019006991060450673, recon_loss = 0.0018988680094480515, kl_loss = 0.0003662118688225746\n",
      "\n",
      "Epoch 1069\n",
      "Step 0: loss = 0.0017090757610276341, recon_loss = 0.001705629751086235, kl_loss = 0.0006891945376992226\n",
      "\n",
      "Epoch 1070\n",
      "Step 0: loss = 0.001836491166613996, recon_loss = 0.001834740862250328, kl_loss = 0.0003500673919916153\n",
      "\n",
      "Epoch 1071\n",
      "Step 0: loss = 0.0017420880030840635, recon_loss = 0.0017405208200216293, kl_loss = 0.00031344126909971237\n",
      "\n",
      "Epoch 1072\n",
      "Step 0: loss = 0.001821556594222784, recon_loss = 0.001819409430027008, kl_loss = 0.000429423525929451\n",
      "\n",
      "Epoch 1073\n",
      "Step 0: loss = 0.0018785373540595174, recon_loss = 0.0018754638731479645, kl_loss = 0.0006147008389234543\n",
      "\n",
      "Epoch 1074\n",
      "Step 0: loss = 0.0016748992493376136, recon_loss = 0.0016721077263355255, kl_loss = 0.0005583129823207855\n",
      "\n",
      "Epoch 1075\n",
      "Step 0: loss = 0.0018487006891518831, recon_loss = 0.0018470175564289093, kl_loss = 0.00033663492649793625\n",
      "\n",
      "Epoch 1076\n",
      "Step 0: loss = 0.0017358082113787532, recon_loss = 0.0017334651201963425, kl_loss = 0.00046862754970788956\n",
      "\n",
      "Epoch 1077\n",
      "Step 0: loss = 0.002030000789090991, recon_loss = 0.0020278040319681168, kl_loss = 0.0004393449053168297\n",
      "\n",
      "Epoch 1078\n",
      "Step 0: loss = 0.0020457138307392597, recon_loss = 0.0020424984395503998, kl_loss = 0.0006430884823203087\n",
      "\n",
      "Epoch 1079\n",
      "Step 0: loss = 0.0019109327113255858, recon_loss = 0.0019087903201580048, kl_loss = 0.00042847637087106705\n",
      "\n",
      "Epoch 1080\n",
      "Step 0: loss = 0.0016692116623744369, recon_loss = 0.001666860654950142, kl_loss = 0.00047019682824611664\n",
      "\n",
      "Epoch 1081\n",
      "Step 0: loss = 0.002009694930166006, recon_loss = 0.0020074080675840378, kl_loss = 0.0004573725163936615\n",
      "\n",
      "Epoch 1082\n",
      "Step 0: loss = 0.0018436163663864136, recon_loss = 0.0018418077379465103, kl_loss = 0.0003617219626903534\n",
      "\n",
      "Epoch 1083\n",
      "Step 0: loss = 0.0018442983273416758, recon_loss = 0.0018423255532979965, kl_loss = 0.0003945613279938698\n",
      "\n",
      "Epoch 1084\n",
      "Step 0: loss = 0.0017495844513177872, recon_loss = 0.00174778513610363, kl_loss = 0.0003598630428314209\n",
      "\n",
      "Epoch 1085\n",
      "Step 0: loss = 0.0017242428148165345, recon_loss = 0.0017219502478837967, kl_loss = 0.0004585087299346924\n",
      "\n",
      "Epoch 1086\n",
      "Step 0: loss = 0.0020058671943843365, recon_loss = 0.002003289759159088, kl_loss = 0.000515463761985302\n",
      "\n",
      "Epoch 1087\n",
      "Step 0: loss = 0.001777636120095849, recon_loss = 0.001775100827217102, kl_loss = 0.0005070660263299942\n",
      "\n",
      "Epoch 1088\n",
      "Step 0: loss = 0.0018117973813787103, recon_loss = 0.0018092207610607147, kl_loss = 0.0005153240635991096\n",
      "\n",
      "Epoch 1089\n",
      "Step 0: loss = 0.00181095814332366, recon_loss = 0.0018084030598402023, kl_loss = 0.0005110269412398338\n",
      "\n",
      "Epoch 1090\n",
      "Step 0: loss = 0.0015342440456151962, recon_loss = 0.0015316810458898544, kl_loss = 0.0005126046016812325\n",
      "\n",
      "Epoch 1091\n",
      "Step 0: loss = 0.0015949754742905498, recon_loss = 0.0015928689390420914, kl_loss = 0.00042131170630455017\n",
      "\n",
      "Epoch 1092\n",
      "Step 0: loss = 0.001635097898542881, recon_loss = 0.0016331858932971954, kl_loss = 0.00038241129368543625\n",
      "\n",
      "Epoch 1093\n",
      "Step 0: loss = 0.0015849313931539655, recon_loss = 0.0015830118209123611, kl_loss = 0.00038392283022403717\n",
      "\n",
      "Epoch 1094\n",
      "Step 0: loss = 0.0017512788763269782, recon_loss = 0.001749623566865921, kl_loss = 0.0003310590982437134\n",
      "\n",
      "Epoch 1095\n",
      "Step 0: loss = 0.0016622785478830338, recon_loss = 0.0016609597951173782, kl_loss = 0.0002637440338730812\n",
      "\n",
      "Epoch 1096\n",
      "Step 0: loss = 0.00162409374024719, recon_loss = 0.0016221646219491959, kl_loss = 0.0003858208656311035\n",
      "\n",
      "Epoch 1097\n",
      "Step 0: loss = 0.001463553518988192, recon_loss = 0.0014621671289205551, kl_loss = 0.00027727149426937103\n",
      "\n",
      "Epoch 1098\n",
      "Step 0: loss = 0.0015022457810118794, recon_loss = 0.0015004649758338928, kl_loss = 0.000356161966919899\n",
      "\n",
      "Epoch 1099\n",
      "Step 0: loss = 0.0014742589555680752, recon_loss = 0.0014726519584655762, kl_loss = 0.000321398489177227\n",
      "\n",
      "Epoch 1100\n",
      "Step 0: loss = 0.0014355391031131148, recon_loss = 0.00143330916762352, kl_loss = 0.0004459768533706665\n",
      "\n",
      "Epoch 1101\n",
      "Step 0: loss = 0.0015513628022745252, recon_loss = 0.0015499405562877655, kl_loss = 0.00028443988412618637\n",
      "\n",
      "Epoch 1102\n",
      "Step 0: loss = 0.001439391286112368, recon_loss = 0.0014382824301719666, kl_loss = 0.0002217823639512062\n",
      "\n",
      "Epoch 1103\n",
      "Step 0: loss = 0.0015730930026620626, recon_loss = 0.0015721376985311508, kl_loss = 0.00019105710089206696\n",
      "\n",
      "Epoch 1104\n",
      "Step 0: loss = 0.0014152996009215713, recon_loss = 0.0014143027365207672, kl_loss = 0.00019936729222536087\n",
      "\n",
      "Epoch 1105\n",
      "Step 0: loss = 0.0013627292355522513, recon_loss = 0.001361539587378502, kl_loss = 0.0002379259094595909\n",
      "\n",
      "Epoch 1106\n",
      "Step 0: loss = 0.0013373874826356769, recon_loss = 0.0013364069163799286, kl_loss = 0.00019610486924648285\n",
      "\n",
      "Epoch 1107\n",
      "Step 0: loss = 0.0013360701268538833, recon_loss = 0.0013351179659366608, kl_loss = 0.0001904284581542015\n",
      "\n",
      "Epoch 1108\n",
      "Step 0: loss = 0.00131449184846133, recon_loss = 0.001312965527176857, kl_loss = 0.00030526891350746155\n",
      "\n",
      "Epoch 1109\n",
      "Step 0: loss = 0.0015070544322952628, recon_loss = 0.001505386084318161, kl_loss = 0.0003336584195494652\n",
      "\n",
      "Epoch 1110\n",
      "Step 0: loss = 0.0014004572294652462, recon_loss = 0.001399373635649681, kl_loss = 0.00021671131253242493\n",
      "\n",
      "Epoch 1111\n",
      "Step 0: loss = 0.0014842816162854433, recon_loss = 0.0014828182756900787, kl_loss = 0.00029265880584716797\n",
      "\n",
      "Epoch 1112\n",
      "Step 0: loss = 0.0014661999884992838, recon_loss = 0.0014637764543294907, kl_loss = 0.0004847021773457527\n",
      "\n",
      "Epoch 1113\n",
      "Step 0: loss = 0.0016044501680880785, recon_loss = 0.0016020052134990692, kl_loss = 0.0004889983683824539\n",
      "\n",
      "Epoch 1114\n",
      "Step 0: loss = 0.0014745353255420923, recon_loss = 0.0014730002731084824, kl_loss = 0.00030701514333486557\n",
      "\n",
      "Epoch 1115\n",
      "Step 0: loss = 0.0014652656391263008, recon_loss = 0.0014637298882007599, kl_loss = 0.0003071427345275879\n",
      "\n",
      "Epoch 1116\n",
      "Step 0: loss = 0.001376558793708682, recon_loss = 0.0013755019754171371, kl_loss = 0.00021137017756700516\n",
      "\n",
      "Epoch 1117\n",
      "Step 0: loss = 0.0015030886279419065, recon_loss = 0.0015017297118902206, kl_loss = 0.0002717766910791397\n",
      "\n",
      "Epoch 1118\n",
      "Step 0: loss = 0.0015485696494579315, recon_loss = 0.0015472732484340668, kl_loss = 0.00025928299874067307\n",
      "\n",
      "Epoch 1119\n",
      "Step 0: loss = 0.00153587874956429, recon_loss = 0.0015328768640756607, kl_loss = 0.0006003743037581444\n",
      "\n",
      "Epoch 1120\n",
      "Step 0: loss = 0.0015757905784994364, recon_loss = 0.0015731658786535263, kl_loss = 0.0005249390378594398\n",
      "\n",
      "Epoch 1121\n",
      "Step 0: loss = 0.001739323022775352, recon_loss = 0.0017370004206895828, kl_loss = 0.00046451669186353683\n",
      "\n",
      "Epoch 1122\n",
      "Step 0: loss = 0.0016494839219376445, recon_loss = 0.0016479287296533585, kl_loss = 0.00031102821230888367\n",
      "\n",
      "Epoch 1123\n",
      "Step 0: loss = 0.001503926469013095, recon_loss = 0.0015017874538898468, kl_loss = 0.00042781420052051544\n",
      "\n",
      "Epoch 1124\n",
      "Step 0: loss = 0.0016149443108588457, recon_loss = 0.0016126260161399841, kl_loss = 0.0004636617377400398\n",
      "\n",
      "Epoch 1125\n",
      "Step 0: loss = 0.0016111752483993769, recon_loss = 0.0016099438071250916, kl_loss = 0.0002462947741150856\n",
      "\n",
      "Epoch 1126\n",
      "Step 0: loss = 0.001523708924651146, recon_loss = 0.0015208963304758072, kl_loss = 0.0005625123158097267\n",
      "\n",
      "Epoch 1127\n",
      "Step 0: loss = 0.001614307751879096, recon_loss = 0.0016113705933094025, kl_loss = 0.00058743916451931\n",
      "\n",
      "Epoch 1128\n",
      "Step 0: loss = 0.0014386086259037256, recon_loss = 0.0014361515641212463, kl_loss = 0.0004914160817861557\n",
      "\n",
      "Epoch 1129\n",
      "Step 0: loss = 0.0015418261755257845, recon_loss = 0.0015398487448692322, kl_loss = 0.0003954777494072914\n",
      "\n",
      "Epoch 1130\n",
      "Step 0: loss = 0.0015731335151940584, recon_loss = 0.0015712473541498184, kl_loss = 0.0003772331401705742\n",
      "\n",
      "Epoch 1131\n",
      "Step 0: loss = 0.001505690161138773, recon_loss = 0.0015045702457427979, kl_loss = 0.00022397376596927643\n",
      "\n",
      "Epoch 1132\n",
      "Step 0: loss = 0.001526987412944436, recon_loss = 0.0015245769172906876, kl_loss = 0.00048209819942712784\n",
      "\n",
      "Epoch 1133\n",
      "Step 0: loss = 0.001667188829742372, recon_loss = 0.0016657300293445587, kl_loss = 0.0002917693927884102\n",
      "\n",
      "Epoch 1134\n",
      "Step 0: loss = 0.0015823144931346178, recon_loss = 0.00157974474132061, kl_loss = 0.000513952225446701\n",
      "\n",
      "Epoch 1135\n",
      "Step 0: loss = 0.0014857673086225986, recon_loss = 0.0014836564660072327, kl_loss = 0.0004221722483634949\n",
      "\n",
      "Epoch 1136\n",
      "Step 0: loss = 0.0016206771833822131, recon_loss = 0.0016173794865608215, kl_loss = 0.0006595337763428688\n",
      "\n",
      "Epoch 1137\n",
      "Step 0: loss = 0.0015403159195557237, recon_loss = 0.0015383437275886536, kl_loss = 0.00039444491267204285\n",
      "\n",
      "Epoch 1138\n",
      "Step 0: loss = 0.0015587438829243183, recon_loss = 0.0015567559748888016, kl_loss = 0.00039759185165166855\n",
      "\n",
      "Epoch 1139\n",
      "Step 0: loss = 0.001510496949777007, recon_loss = 0.001509184017777443, kl_loss = 0.0002625882625579834\n",
      "\n",
      "Epoch 1140\n",
      "Step 0: loss = 0.0016362625174224377, recon_loss = 0.0016348548233509064, kl_loss = 0.00028153974562883377\n",
      "\n",
      "Epoch 1141\n",
      "Step 0: loss = 0.0017499449895694852, recon_loss = 0.0017479676753282547, kl_loss = 0.00039545632898807526\n",
      "\n",
      "Epoch 1142\n",
      "Step 0: loss = 0.0016463150968775153, recon_loss = 0.0016442332416772842, kl_loss = 0.0004163682460784912\n",
      "\n",
      "Epoch 1143\n",
      "Step 0: loss = 0.0016654054634273052, recon_loss = 0.001663990318775177, kl_loss = 0.0002830401062965393\n",
      "\n",
      "Epoch 1144\n",
      "Step 0: loss = 0.0016254772199317813, recon_loss = 0.0016245171427726746, kl_loss = 0.00019201263785362244\n",
      "\n",
      "Epoch 1145\n",
      "Step 0: loss = 0.0015777493827044964, recon_loss = 0.0015761349350214005, kl_loss = 0.0003228951245546341\n",
      "\n",
      "Epoch 1146\n",
      "Step 0: loss = 0.0019464967772364616, recon_loss = 0.0019445549696683884, kl_loss = 0.0003883708268404007\n",
      "\n",
      "Epoch 1147\n",
      "Step 0: loss = 0.0018420781707391143, recon_loss = 0.0018399562686681747, kl_loss = 0.00042438972741365433\n",
      "\n",
      "Epoch 1148\n",
      "Step 0: loss = 0.001803610473871231, recon_loss = 0.0018009897321462631, kl_loss = 0.0005241390317678452\n",
      "\n",
      "Epoch 1149\n",
      "Step 0: loss = 0.0016270008636638522, recon_loss = 0.0016251187771558762, kl_loss = 0.0003764098510146141\n",
      "\n",
      "Epoch 1150\n",
      "Step 0: loss = 0.0018391746561974287, recon_loss = 0.0018370170146226883, kl_loss = 0.0004315301775932312\n",
      "\n",
      "Epoch 1151\n",
      "Step 0: loss = 0.0015352724585682154, recon_loss = 0.001532798632979393, kl_loss = 0.0004947734996676445\n",
      "\n",
      "Epoch 1152\n",
      "Step 0: loss = 0.0018611379200592637, recon_loss = 0.001858329400420189, kl_loss = 0.0005616936832666397\n",
      "\n",
      "Epoch 1153\n",
      "Step 0: loss = 0.0016036099987104535, recon_loss = 0.0016019977629184723, kl_loss = 0.0003224508836865425\n",
      "\n",
      "Epoch 1154\n",
      "Step 0: loss = 0.0017404917161911726, recon_loss = 0.0017388220876455307, kl_loss = 0.000333922915160656\n",
      "\n",
      "Epoch 1155\n",
      "Step 0: loss = 0.0015668109990656376, recon_loss = 0.001564808189868927, kl_loss = 0.00040056463330984116\n",
      "\n",
      "Epoch 1156\n",
      "Step 0: loss = 0.0015934782568365335, recon_loss = 0.0015922263264656067, kl_loss = 0.0002503925934433937\n",
      "\n",
      "Epoch 1157\n",
      "Step 0: loss = 0.0015848932089284062, recon_loss = 0.0015829261392354965, kl_loss = 0.0003934083506464958\n",
      "\n",
      "Epoch 1158\n",
      "Step 0: loss = 0.0013913956936448812, recon_loss = 0.001390073448419571, kl_loss = 0.00026443973183631897\n",
      "\n",
      "Epoch 1159\n",
      "Step 0: loss = 0.0015965886414051056, recon_loss = 0.0015935394912958145, kl_loss = 0.0006098253652453423\n",
      "\n",
      "Epoch 1160\n",
      "Step 0: loss = 0.001456143450923264, recon_loss = 0.0014537833631038666, kl_loss = 0.00047201942652463913\n",
      "\n",
      "Epoch 1161\n",
      "Step 0: loss = 0.0015044688479974866, recon_loss = 0.0015029404312372208, kl_loss = 0.0003056749701499939\n",
      "\n",
      "Epoch 1162\n",
      "Step 0: loss = 0.0015003207372501493, recon_loss = 0.0014988891780376434, kl_loss = 0.0002863192930817604\n",
      "\n",
      "Epoch 1163\n",
      "Step 0: loss = 0.00154414726421237, recon_loss = 0.0015423428267240524, kl_loss = 0.00036087632179260254\n",
      "\n",
      "Epoch 1164\n",
      "Step 0: loss = 0.0015202871290966868, recon_loss = 0.001518692821264267, kl_loss = 0.00031886156648397446\n",
      "\n",
      "Epoch 1165\n",
      "Step 0: loss = 0.0015709594590589404, recon_loss = 0.0015697143971920013, kl_loss = 0.00024902261793613434\n",
      "\n",
      "Epoch 1166\n",
      "Step 0: loss = 0.0015636167954653502, recon_loss = 0.0015618819743394852, kl_loss = 0.00034696701914072037\n",
      "\n",
      "Epoch 1167\n",
      "Step 0: loss = 0.0015326830325648189, recon_loss = 0.0015311725437641144, kl_loss = 0.0003020875155925751\n",
      "\n",
      "Epoch 1168\n",
      "Step 0: loss = 0.0014399494975805283, recon_loss = 0.0014382656663656235, kl_loss = 0.00033676810562610626\n",
      "\n",
      "Epoch 1169\n",
      "Step 0: loss = 0.001395078026689589, recon_loss = 0.0013935882598161697, kl_loss = 0.00029795803129673004\n",
      "\n",
      "Epoch 1170\n",
      "Step 0: loss = 0.001572390552610159, recon_loss = 0.0015706419944763184, kl_loss = 0.0003497013822197914\n",
      "\n",
      "Epoch 1171\n",
      "Step 0: loss = 0.0014180723810568452, recon_loss = 0.0014167707413434982, kl_loss = 0.0002603214234113693\n",
      "\n",
      "Epoch 1172\n",
      "Step 0: loss = 0.0013718189438804984, recon_loss = 0.001369725912809372, kl_loss = 0.000418616458773613\n",
      "\n",
      "Epoch 1173\n",
      "Step 0: loss = 0.00153249385766685, recon_loss = 0.0015308409929275513, kl_loss = 0.00033056270331144333\n",
      "\n",
      "Epoch 1174\n",
      "Step 0: loss = 0.0016134149627760053, recon_loss = 0.0016119517385959625, kl_loss = 0.00029263831675052643\n",
      "\n",
      "Epoch 1175\n",
      "Step 0: loss = 0.001665378105826676, recon_loss = 0.0016629919409751892, kl_loss = 0.0004772394895553589\n",
      "\n",
      "Epoch 1176\n",
      "Step 0: loss = 0.0017323504434898496, recon_loss = 0.00173117034137249, kl_loss = 0.00023602787405252457\n",
      "\n",
      "Epoch 1177\n",
      "Step 0: loss = 0.001750307041220367, recon_loss = 0.0017479974776506424, kl_loss = 0.00046191923320293427\n",
      "\n",
      "Epoch 1178\n",
      "Step 0: loss = 0.0016744660679250956, recon_loss = 0.0016725920140743256, kl_loss = 0.00037481915205717087\n",
      "\n",
      "Epoch 1179\n",
      "Step 0: loss = 0.001693502883426845, recon_loss = 0.001691628247499466, kl_loss = 0.00037493743002414703\n",
      "\n",
      "Epoch 1180\n",
      "Step 0: loss = 0.0017025655834004283, recon_loss = 0.0017008669674396515, kl_loss = 0.00033972691744565964\n",
      "\n",
      "Epoch 1181\n",
      "Step 0: loss = 0.0016702008433640003, recon_loss = 0.001668361946940422, kl_loss = 0.0003677885979413986\n",
      "\n",
      "Epoch 1182\n",
      "Step 0: loss = 0.0016012504929676652, recon_loss = 0.0015997979789972305, kl_loss = 0.0002904944121837616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_betas, batch_dirs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch(betas, random_dirs, batch_size)):\n\u001b[0;32m----> 7\u001b[0m   loss_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_betas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dirs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# tmp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_vals[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, recon_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_vals[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, kl_loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_vals[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, inputs, dir_inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m   total_loss, recon_loss, kl_loss \u001b[38;5;241m=\u001b[39m vae_loss(inputs, outputs, z_mean, z_log_var)\n\u001b[1;32m      8\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(total_loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss, recon_loss, kl_loss\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1140\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_gradients_aggregation \u001b[38;5;129;01mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1139\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:634\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    633\u001b[0m grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 634\u001b[0m iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal_apply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1166\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_internal_apply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m-> 1166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_merge_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distributed_apply_gradients_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_distribution_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads_and_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m distribution_strategy_context\u001b[38;5;241m.\u001b[39mget_replica_context()\u001b[38;5;241m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1216\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m grad, var \u001b[38;5;129;01min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1216\u001b[0m     \u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_grad_to_update_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n\u001b[1;32m   1221\u001b[0m     _, var_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2637\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2634\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   2635\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   2636\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2639\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2640\u001b[0m       var, fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs, group\u001b[38;5;241m=\u001b[39mgroup)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3710\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   3707\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   3708\u001b[0m   \u001b[38;5;66;03m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   3709\u001b[0m   \u001b[38;5;66;03m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 3710\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_non_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3716\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_non_slot\u001b[39m(\u001b[38;5;28mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   3713\u001b[0m   \u001b[38;5;66;03m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   3714\u001b[0m   \u001b[38;5;66;03m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 3716\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_group:\n\u001b[1;32m   3718\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:1211\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_grad_to_update_var\u001b[39m(var, grad):\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit_compile:\n\u001b[0;32m-> 1211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_step_xla\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_var_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_step(grad, var)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    917\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 919\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[1;32m    921\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    922\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:133\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m--> 133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:336\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m captures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_captures_container\u001b[38;5;241m.\u001b[39mget_snapshot()\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# cache_key_deletion_observer is useless here. It's based on all captures.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# A new cache key will be built later when saving ConcreteFunction because\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# only active captures should be saved.\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m lookup_func_key, _ \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_cache_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mcaptures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mlookup(lookup_func_key, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m concrete_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/function_context.py:133\u001b[0m, in \u001b[0;36mmake_cache_key\u001b[0;34m(args, captures)\u001b[0m\n\u001b[1;32m    131\u001b[0m   captures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    132\u001b[0m signature_context \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mInternalTracingContext()\n\u001b[0;32m--> 133\u001b[0m args_signature \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m captures_dict_tracetype \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mfrom_value(\n\u001b[1;32m    136\u001b[0m     captures, signature_context)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# TODO(fmuham): Use the actual FunctionType\u001b[39;00m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:129\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_types\u001b[38;5;241m.\u001b[39mNamedTuple\u001b[38;5;241m.\u001b[39mfrom_type_and_attributes(\n\u001b[1;32m    127\u001b[0m         named_tuple_type, \u001b[38;5;28mtuple\u001b[39m(from_value(c, context) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m value))\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    132\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m default_types\u001b[38;5;241m.\u001b[39mDict({k: from_value(value[k], context) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m value})\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_types\u001b[38;5;241m.\u001b[39mNamedTuple\u001b[38;5;241m.\u001b[39mfrom_type_and_attributes(\n\u001b[1;32m    127\u001b[0m         named_tuple_type, \u001b[38;5;28mtuple\u001b[39m(from_value(c, context) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m value))\n\u001b[1;32m    128\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_types\u001b[38;5;241m.\u001b[39mTuple(\u001b[38;5;241m*\u001b[39m(\u001b[43mfrom_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m value))\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    132\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m default_types\u001b[38;5;241m.\u001b[39mDict({k: from_value(value[k], context) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m value})\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:114\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mis_legacy_signature \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, trace\u001b[38;5;241m.\u001b[39mTraceType):\n\u001b[1;32m    113\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSupportsTracingProtocol\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    115\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__wrapped__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:1013\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__instancecheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, instance):\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;66;03m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# assigned in __init__.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_protocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m             \u001b[43m_is_callable_members_only\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m             \u001b[38;5;28missubclass\u001b[39m(instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mcls\u001b[39m)):\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_protocol:\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:977\u001b[0m, in \u001b[0;36m_is_callable_members_only\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_callable_members_only\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# PEP 544 prohibits using issubclass() with protocols that have non-method members.\u001b[39;00m\n\u001b[0;32m--> 977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, attr, \u001b[38;5;28;01mNone\u001b[39;00m)) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_get_protocol_attrs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:968\u001b[0m, in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGeneric\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 968\u001b[0m annotations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m__annotations__\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(base\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(annotations\u001b[38;5;241m.\u001b[39mkeys()):\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_abc_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m attr \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m EXCLUDED_ATTRIBUTES:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon_loss = {loss_vals[1].numpy()}, kl_loss = {loss_vals[2].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHF0P4eISAH5"
   },
   "outputs": [],
   "source": [
    "vae.save_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JkrFRN7uOlf",
    "outputId": "90f21636-cdec-4d1e-eb4f-405a557a43b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7dfda7a63c70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS8sKCRfgUoH",
    "outputId": "b7f487ba-8083-4757-b9ea-250fd343a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  chem-checkpoint.zip\n",
      "   creating: my_checkpoint/\n",
      "  inflating: my_checkpoint/chekpont.index  \n",
      "  inflating: my_checkpoint/.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/checkpoint  \n",
      "  inflating: my_checkpoint/chekpont.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/.index    \n"
     ]
    }
   ],
   "source": [
    "# !zip -r my_checkpoint.zip my_checkpoint/\n",
    "!unzip chem-checkpoint.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbHImoN2QaKX"
   },
   "outputs": [],
   "source": [
    "random_dirs = np.random.randn(50_000, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "\n",
    "def generate_new_betas(model, rdirs, num_samples=1):\n",
    "  random_dirs1 = rdirs\n",
    "  random_dirs2 = random_dirs1 + np.random.randn(num_samples, d)*0.05\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  # latent_samples = tf.ones_like(tf.random.normal(shape=(num_samples, latent_dim)))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [],
   "source": [
    "# drawn_betas1, dir1, drawn_betas2 = generate_new_betas(vae, np.random.randn(50_0, d), 50_0) ## too much change between this and below\n",
    "drawn_betas1, dir1, drawn_betas2 = generate_new_betas(vae, random_dirs, len(random_dirs)) ## recall posterior sampling high correlation as well\n",
    "## even slight 0.5 deviations from training random directions totally changes the beta directions correlations hence not very stable \n",
    "## with resoect to training dircetions overfitting too much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IHicI0RhbDe",
    "outputId": "32426975-705a-4668-f061-08acd65bc48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.5788343>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas1, drawn_betas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOFI9xq2b7_3",
    "outputId": "a11e5724-0e2a-428d-a65a-28064209e6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 1793), dtype=float32, numpy=\n",
       "array([[  1.1108708 ,  -1.0370612 ,   1.0474906 , ...,  -8.708343  ,\n",
       "         -2.0336905 ,   0.90315366],\n",
       "       [  0.59400725,  -4.0057716 ,  -8.4961195 , ..., -12.938477  ,\n",
       "         20.990461  ,  -5.11088   ],\n",
       "       [  1.6494957 ,  -0.9708191 , -10.286863  , ...,  15.719532  ,\n",
       "         -2.167779  ,   1.6435473 ]], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFCgGG6jgkLL",
    "outputId": "5e799c27-fff1-41fc-d71f-3fc7ea88538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(53.601833, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(drawn_betas1, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxnzX5lJQtjX"
   },
   "outputs": [],
   "source": [
    "np.mean(drawn_betas1 @ tf.transpose(drawn_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "TfAej5fqsfHh",
    "outputId": "fd888e0d-c076-4577-be0d-4de29be77dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a3643f88f38a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mood_val_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_data_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mood_val_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#ood_test_features = convert_data_to_features(ood_test_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mood_val_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cls_label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mood_val_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "#ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U07oDyUdsl1u"
   },
   "outputs": [],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "external_Y = ood_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "external_X = (external_X-mu_x)/sigma_x\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "1a114f68-6f8b-4ede-b395-df1c2dfa2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      " -0.03134901]\n",
      "tf.Tensor(\n",
      "[-0.13125554  0.30866534  0.63319725 -0.5780234  -0.21648076 -0.3846287\n",
      "  0.99320394 -0.1799166   0.678208    0.62888056], shape=(10,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "5ec19308-4552-4326-ec47-b86470a2f7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 1024)\n",
      "(937, 2040)\n",
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "ae104348-bf03-48d8-a6c3-d3112b1dcb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " ...\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W32O3S_gKnEp",
    "outputId": "8797bdae-deb8-43ed-ef0c-cbf43fb4aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 512) (937, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 40, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  wghts = tf.ones_like(projs > thresh[:, None])\n",
    "  # wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return *aggregate_preds(preds), preds\n",
    "\n",
    "\n",
    "ext_probs, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs, betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas1) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, betas) # 0.634\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, drawn_betas) # 0.85\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, betas) # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOyhZXJBrhaj",
    "outputId": "98cc9eb0-2e98-4b38-e823-337411f61e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0], tf.transpose(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "a9702828-7bea-436e-de46-89ca03c30b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.767455  ,  0.1001012 , -0.54182774, ..., -6.8707843 ,\n",
       "         7.257238  , -0.5050444 ], dtype=float32),\n",
       " array([-0.0186294 ,  0.06830448, -0.27256313, ..., -0.72351612,\n",
       "         0.81710885,  0.05805234]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "2fac824c-8637-4a35-8dfb-7b60045b10b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 0.]\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "06eb05c6-f5f0-4d58-a8a2-ad76ee40b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [False  True False False  True False False False False False]\n",
      "Total Positive Preds:  436\n",
      "Total Preds:  937\n",
      "% Positive Preds:  0.46531483457844186\n",
      "\n",
      "First 10 Ground Truth:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Total Positive Ground Truth:  421.0\n",
      "Total Ground Truth:  937\n",
      "% Positive Ground Truth:  0.44930629669156885\n",
      "\n",
      "Accuracy:  0.6339381003201707\n"
     ]
    }
   ],
   "source": [
    "# testing_Y = Y\n",
    "ext_preds = ext_probs > 0.5\n",
    "testing_Y = external_Y\n",
    "\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", testing_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(testing_Y))\n",
    "print(\"Total Ground Truth: \", len(testing_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(testing_Y) / len(testing_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == testing_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "L7kl6J3wLped",
    "outputId": "a5d20ac1-f339-47e4-cf12-18073bb4b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f045c5f2310>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.2, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8, 0.6666666666666666, 0.6153846153846154, 0.5909090909090909, 0.4838709677419355, 0.4791666666666667, 0.4918032786885246, 0.5194805194805194, 0.5145631067961165, 0.48854961832061067, 0.46296296296296297, 0.4675925925925926, 0.45864661654135336, 0.4735202492211838, 0.47214854111405835, 0.4678111587982833, 0.4698581560283688, 0.4535031847133758, 0.44930629669156885]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f045c5034f0>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_probs)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
