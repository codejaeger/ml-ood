{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "20637ffb-f819-4f0a-b926-d24784746a9e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkF1N4olXTtZ",
    "outputId": "de7467ba-69c5-42b4-a918-755fe87765d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "weKR7jCuMvQl"
   },
   "outputs": [],
   "source": [
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataX = np.float32(np.array([line.strip().split(',')[2:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataY = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "X = dataX\n",
    "Y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "2948628d-0085-48ba-8e9b-4bbe93f27e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5997, 1024)\n",
      "(5997,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, d, D=128):\n",
    "\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot/2, sigma_rot/4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, X, d, D=128):\n",
    "\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot/2, sigma_rot/4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "    self.Ws = self.sample_features(X)\n",
    "    \n",
    "  def sample_features(self, X, ):\n",
    "    L = int(0.3 * len(X))\n",
    "    M = self.Ws.shape[0] * self.Ws.shape[2]\n",
    "    # N = np.random.choice(M, int(M/100), replace=False)\n",
    "    N = int(M/100)+10\n",
    "    phi_Xt = tf.transpose(self.get_features(X[np.random.choice(len(X), L)])) / np.sqrt(L)\n",
    "    phi_phi_T = phi_Xt @ tf.transpose(phi_Xt)\n",
    "    mu = np.power(10, 0)\n",
    "    diag = np.diag(phi_phi_T @ np.linalg.inv(phi_phi_T + mu))\n",
    "    diag = diag / np.sum(diag)\n",
    "    print(M, len(diag)//2, phi_Xt.shape, self.Ws.shape)\n",
    "    print(\"Diag\", M, diag, np.argsort(diag)[-N])\n",
    "    k, N, D = self.Ws.shape\n",
    "    diag = diag[:len(diag)//2] + diag[len(diag)//2:]\n",
    "    w_indices = np.unique(np.argsort(diag)[-N:])\n",
    "    _Ws = np.reshape(np.transpose(self.Ws, [1, 2, 0]), (N, -1))[:, w_indices]\n",
    "    return np.transpose(np.reshape(_Ws, (N, -1, 1)), [2, 0, 1])\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    # phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.transpose(phis, [1, 2, 0])[:, None, :, :]  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "3S8unT73bEtM"
   },
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, X_ids, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  # clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  clf = SVC(random_state=0, tol=1e-5, kernel='linear').fit(X, Y)\n",
    "  # clf = LinearSVC(random_state=0, tol=1e-5).fit(X, Y)\n",
    "  support = (clf.support_, clf.n_support_)\n",
    "\n",
    "  def get_supp(support):\n",
    "      supps_, n_supps_ = support\n",
    "      supps_0 = supps_[:n_supps_[0]]\n",
    "      supps_1 = supps_[n_supps_[0]:]\n",
    "      return X_ids[supps_0], X_ids[supps_1]\n",
    "\n",
    "  support = get_supp(support)\n",
    "    \n",
    "  # clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  print(wgts.shape)\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print(np.mean(prd[:, 0]==Y))\n",
    "  return wgts, None\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  supps = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    print(X_sub.shape, X_ids.shape)\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    beta, supp = linear_coefs(get_rand_feats(X_sub@pca_projs, model), np.argwhere(X_ids), Y_sub)\n",
    "    # beta = linear_coefs(X_sub, Y_sub)\n",
    "\n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    supps.append(supp)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas, supps\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997, 1024)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(232.08453, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims = [0.05, 0.25, 0.3, 0.4, 0.8]\n",
    "dims = [0.05, 0.2, 0.3, 0.4, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5997, 1024), TensorShape([1024, 204]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, pca_projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "f3d09ad5-4e02-44a5-e001-fe83e3d99938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "(1200, 1024) (5997,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[193], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m random_dirs \u001b[38;5;241m=\u001b[39m random_dirs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(random_dirs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m betas, supps \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_projs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[158], line 14\u001b[0m, in \u001b[0;36mget_models\u001b[0;34m(X, Y, pca_projs, dirs, model, percentile)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(X_sub\u001b[38;5;241m.\u001b[39mshape, X_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print((X_sub@pca_projs).shape)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m beta, supp \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_coefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_rand_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;129;43m@pca_projs\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# beta = linear_coefs(X_sub, Y_sub)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#X_subsets.append(X_sub)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#data_ids.append(X_ids)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#Y_subsets.append(Y_sub)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m betas\u001b[38;5;241m.\u001b[39mappend(beta)\n",
      "Cell \u001b[0;32mIn[156], line 11\u001b[0m, in \u001b[0;36mlinear_coefs\u001b[0;34m(X, X_ids, Y)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m  X: N x d matrix of input features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m  Beta: d x 1 matrix of linear coefficients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mSVC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# clf = LinearSVC(random_state=0, tol=1e-5).fit(X, Y)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m support \u001b[38;5;241m=\u001b[39m (clf\u001b[38;5;241m.\u001b[39msupport_, clf\u001b[38;5;241m.\u001b[39mn_support_)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:329\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    315\u001b[0m libsvm\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# we don't pass **self.get_params() to allow subclasses to\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# add other parameters to __init__\u001b[39;00m\n\u001b[1;32m    319\u001b[0m (\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_coef_,\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[0;32m--> 329\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO(1.4): Replace \"_class_weight\" with \"class_weight_\"\u001b[39;49;00m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_class_weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobability\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshrinking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "\n",
    "N = 1024    # ~ 8k\n",
    "# N = 16    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas, supps = get_models(X, Y, pca_projs, random_dirs, model, percentile=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional sampling\n",
    "# pick direction and find beta\n",
    "# find poor performing points' clusters means say at x+10, x+30 ... % data (x is what was trained on)\n",
    "# add all those cluster mean directions to the random_dir set\n",
    "# repeat \n",
    "# should improve accuracy since poor performing points get classified and also reduce number of betas required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dirs:  1\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  1\n",
      "New dirs:  10\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  11\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9916666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.997504159733777\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Total dirs:  61\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.9941763727121464\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Total dirs:  111\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9916666666666667\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Total dirs:  161\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9950041631973355\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Total dirs:  211\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9958368026644463\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  261\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9925062447960034\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.9966722129783694\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  311\n"
     ]
    }
   ],
   "source": [
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "nclusters = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "random_dirs = []\n",
    "betas, supps = [], []\n",
    "iters = 100\n",
    "max_rand_dirs = 300\n",
    "new_rand_dirs = np.random.randn(1, d)\n",
    "num_rand_dirs = 0\n",
    "for i in range(iters):\n",
    "    if num_rand_dirs > max_rand_dirs:\n",
    "        break\n",
    "    new_rand_dirs = new_rand_dirs / np.linalg.norm(new_rand_dirs, axis=1, keepdims=True)\n",
    "    new_rand_dirs = new_rand_dirs[np.random.choice(len(new_rand_dirs), size=min(len(new_rand_dirs), 50), replace=False, p=None)]\n",
    "    random_dirs.append(new_rand_dirs)\n",
    "    num_rand_dirs += len(new_rand_dirs)\n",
    "    #print(new_rand_dirs.shape)\n",
    "    print(\"New dirs: \", len(new_rand_dirs))\n",
    "    \n",
    "    _betas, _supps = get_models(X, Y, pca_projs, new_rand_dirs, model, percentile=20)\n",
    "    betas.append(_betas)\n",
    "    supps += _supps\n",
    "    _new_rand_dirs = []\n",
    "    for _dir, _beta in zip(new_rand_dirs, _betas):\n",
    "        for ix, prcnt in enumerate(range(25, 100, 20)):\n",
    "            X_sub, X_ids = project_and_filter(X, _dir, prcnt)\n",
    "            Y_sub = Y[X_ids]\n",
    "            _beta = tf.squeeze(_beta)\n",
    "            _dir = tf.constant(_dir)\n",
    "            prd = softmax(get_rand_feats(X_sub@pca_projs, model), _beta)\n",
    "\n",
    "            incrct = np.where(prd != Y_sub)\n",
    "            # print(len(prd), len(Y_sub), get_rand_feats(X_sub@pca_projs, model).shape, _beta.shape, prd)\n",
    "            incrct_X = get_rand_feats(X_sub[incrct]@pca_projs, model)\n",
    "            if len(incrct_X) < 20:\n",
    "                continue\n",
    "            kmeans = KMeans(n_clusters=nclusters[ix], random_state=0, n_init=\"auto\").fit(incrct_X)\n",
    "            mp = kmeans.cluster_centers_\n",
    "            neigh = NearestNeighbors(n_neighbors=5)\n",
    "            neigh.fit(incrct_X)\n",
    "\n",
    "            n_ind = neigh.kneighbors(mp, 3, return_distance=False)\n",
    "            nngr_id = [_n_ind[np.random.choice(3, 1)[0]] for _n_ind in n_ind]\n",
    "\n",
    "            _new_rand_dirs.append(X_sub[incrct][nngr_id] + np.random.randn(*X_sub[incrct][nngr_id].shape)*0.1)\n",
    "    new_rand_dirs = np.concatenate(_new_rand_dirs, axis=0)\n",
    "    print(\"Total dirs: \", num_rand_dirs)\n",
    "\n",
    "betas = np.concatenate(betas, axis=0)\n",
    "random_dirs = np.concatenate(random_dirs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5697, 1024)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-0.1706338411346348>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_simi(a, b):\n",
    "    return tf.losses.CosineSimilarity()(a, b)    \n",
    "\n",
    "check_simi(betas[2], betas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "# np.save('random_dirs-chem2.npy', random_dirs)\n",
    "# np.save('betas-chem2.npy', betas)\n",
    "# np.save('Ws-chem2.npy', model.Ws)\n",
    "\n",
    "np.save('chem-random_dirs-svm_large_3841f.npy', random_dirs)\n",
    "np.save('chem-betas-svm_large_3841f.npy', betas)\n",
    "np.save('chem-Ws-svm_large_3841f.npy', model.Ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"chem-supps_large_3841f.npy\", \"wb\") as fp:\n",
    "    pickle.dump(supps, fp)\n",
    "\n",
    "# with open(\"chem-supps_large.npy\", \"rb\") as fp:\n",
    "#     supps_b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-chem2.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./betas-chem2.npy')))\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "model.Ws = tf.constant(np.load('./Ws-chem2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "CnxEkcWwWlwn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "random_dirs = tf.constant(np.load('./chem-random_dirs-svm.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./chem-betas-svm.npy')))\n",
    "# model = define_rand_feats(X, 1.5)\n",
    "model = define_rand_feats(X, 2)\n",
    "model.Ws = tf.constant(np.load('./chem-Ws-svm.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BCHvO4qeupNQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mrandom_dirs1\u001b[49m, random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.allclose(random_dirs1, random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mrandom_dirs1\u001b[49m \u001b[38;5;241m-\u001b[39m random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(random_dirs1 - random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "264b9e19-74f8-4d13-ee50-bb4796f0d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 4081)\n",
      "(512, 1024)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "15119544-cde9-4462-b565-7deb6dd61daf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[0.67359007 0.07284031 0.06704679 0.11871486 0.08134009 0.07223204\n",
      " 0.06578048 0.09957357 0.06538183 0.11517227 0.06808361 0.07230289\n",
      " 0.08044564 0.06715028 0.07314073 0.10923933 0.0676486  0.07192759\n",
      " 0.07831634 0.10833581 0.07824756 0.11328711 0.08205179 0.07256502\n",
      " 0.06170149 0.1004982  0.08952043 0.06356012 0.10328347 0.12419592\n",
      " 0.08537051 0.0694497  0.07996752 0.06575992 0.0812195  0.06487855\n",
      " 0.07011374 0.0720129  0.07330054 0.06410784 0.06708717 0.06468315\n",
      " 0.07939502 0.06358906 0.0804094  0.0927862  0.06151002 0.1032934\n",
      " 0.06179255 0.07593843 0.11523812 0.05969495 0.09308085 0.06923283\n",
      " 0.06824253 0.11062312 0.1151929  0.06778733 0.06730518 0.07203664\n",
      " 0.07125041 0.06838243 0.06803359 0.07240119 0.06192699 0.06835125\n",
      " 0.07637007 0.06941051 0.07204794 0.07405202 0.07071403 0.0716552\n",
      " 0.06128182 0.06445627 0.06350282 0.09007524 0.06571353 0.06488772\n",
      " 0.09448381 0.10456254 0.07256466 0.0826147  0.11120242 0.06792213\n",
      " 0.07659697 0.10339679 0.10981638 0.07132365 0.06307783 0.06444445\n",
      " 0.06684251 0.07105874 0.07350659 0.07283988 0.06555721 0.06875365\n",
      " 0.07771249 0.08177914 0.08985236 0.07188106 0.07675703 0.10150176\n",
      " 0.07951672 0.07137738 0.0899919  0.12333257 0.08871342 0.07753638\n",
      " 0.06669545 0.06940231 0.10745739 0.08085401 0.0738521  0.07647884\n",
      " 0.07877275 0.11535464 0.08160566 0.07936491 0.06190091 0.11871917\n",
      " 0.06864454 0.07614553 0.10851702 0.06271755 0.07709637 0.10149163\n",
      " 0.07770493 0.11035173 0.06979246 0.10631867 0.07888436 0.06787953\n",
      " 0.08423547 0.10515728 0.07582477 0.0734661  0.10395725 0.06035749\n",
      " 0.0631407  0.0806462  0.08260861 0.09184647 0.068541   0.12568466\n",
      " 0.10442394 0.07659739 0.05975658 0.07882387 0.07214825 0.07896739\n",
      " 0.06872708 0.06486798 0.07122052 0.07464958 0.06453745 0.0634111\n",
      " 0.0662826  0.06654381 0.10670793 0.06650188 0.07053742 0.06895638\n",
      " 0.08531659 0.0623953  0.0719764  0.12199535 0.07293387 0.07820626\n",
      " 0.06657765 0.06642344 0.06700614 0.07489021 0.10503389 0.08345477\n",
      " 0.07488761 0.09799307 0.07678562 0.07001266 0.08343272 0.08036478\n",
      " 0.07365696 0.11470803 0.07338435 0.06677797 0.06311477 0.11898425\n",
      " 0.06795641 0.07696223 0.06905686 0.06900609 0.10380625 0.11290169\n",
      " 0.11155094 0.1112395  0.06454098 0.11166833 0.1094071  0.12826356\n",
      " 0.06993088 0.06590311 0.06874009 0.07766671 0.07133087 0.11792594\n",
      " 0.06525787 0.08324586 0.09629141 0.06194838 0.07124203 0.11783804\n",
      " 0.06815657 0.0607546  0.06693446 0.10850154 0.07398581 0.07738857\n",
      " 0.07726125 0.07287352 0.1134871  0.07371912 0.10460267 0.07542637\n",
      " 0.06546611 0.07119475 0.07628828 0.07204596 0.07614054 0.08029243\n",
      " 0.06610101 0.08319586 0.07345918 0.06535736 0.08412251 0.10606798\n",
      " 0.08054666 0.06559355 0.06910889 0.07583357 0.07010034 0.08182172\n",
      " 0.07706366 0.10390144 0.10820366 0.06846014 0.1101813  0.06299624\n",
      " 0.0593843  0.11353021 0.06856831 0.07802539 0.07272289 0.06296969\n",
      " 0.0726189  0.07849582 0.06299832 0.11008786 0.08136675 0.07709117\n",
      " 0.0652505  0.1325067  0.08348418 0.07170477 0.07714294 0.11151411\n",
      " 0.06134501 0.12567265 0.06549819 0.08394777 0.07784765 0.06696031\n",
      " 0.07159942 0.11327072 0.07226371 0.06664225 0.07598495 0.12594664\n",
      " 0.07987921 0.12448053 0.07726267 0.06369147 0.06952551 0.10776697\n",
      " 0.07965835 0.06599504 0.12093706 0.11748523 0.07339495 0.0670207\n",
      " 0.08325976 0.06529279 0.07796387 0.05881728 0.06088162 0.06621873\n",
      " 0.07956693 0.07103588 0.0721318  0.06403643 0.09237153 0.07068583\n",
      " 0.0730793  0.11418394 0.06575713 0.09560428 0.06499788 0.08190548\n",
      " 0.11165975 0.06522763 0.08388134 0.06802737 0.06461838 0.13378638\n",
      " 0.11121587 0.06667375 0.06494507 0.07570385 0.08829263 0.06605769\n",
      " 0.06974071 0.06723919 0.06552938 0.06718214 0.08325711 0.06901438\n",
      " 0.06773014 0.07984122 0.06940914 0.06924522 0.06451015 0.0679001\n",
      " 0.06648319 0.08186894 0.06209855 0.0611996  0.08036507 0.12240631\n",
      " 0.06568438 0.08648885 0.10786134 0.06835054 0.08056951 0.10800634\n",
      " 0.11687485 0.06881811 0.06355119 0.05349949 0.06612925 0.06838357\n",
      " 0.06119686 0.07697211 0.06508836 0.0705764  0.0841587  0.07956836\n",
      " 0.08247901 0.05739133 0.08251776 0.1256077  0.07631475 0.064527\n",
      " 0.08337335 0.1601182  0.07372621 0.08418385 0.07237256 0.0672489\n",
      " 0.101327   0.06642675 0.06340661 0.07676449 0.07973438 0.11528165\n",
      " 0.07960648 0.0759026  0.06853811 0.11570762 0.06938733 0.07765978\n",
      " 0.12561249 0.0650887  0.07456394 0.09193828 0.0852762  0.10506755\n",
      " 0.06640076 0.10230254 0.0771688  0.06197063 0.08613533 0.13525301\n",
      " 0.0834248  0.07319377 0.11439297 0.0665981  0.06586734 0.07459015\n",
      " 0.07707511 0.10632927 0.06062746 0.12512862 0.10379305 0.06430525\n",
      " 0.07034892 0.08056908 0.06561404 0.08310136 0.06709763 0.06463852\n",
      " 0.06691782 0.07592535 0.06327734 0.07283729 0.06605232 0.07031086\n",
      " 0.12875961 0.06591877 0.07229704 0.07054235 0.07773231 0.06906528\n",
      " 0.07265849 0.13278246 0.07460847 0.08035734 0.06098541 0.06707917\n",
      " 0.06598712 0.06078946 0.11787558 0.08832482 0.07531717 0.13841577\n",
      " 0.07556599 0.07146349 0.08449668 0.07844524 0.07132198 0.09771668\n",
      " 0.06839068 0.06377992 0.06898036 0.10430917 0.07472968 0.05980775\n",
      " 0.06359695 0.06405768 0.10160754 0.12855025 0.11988558 0.11682202\n",
      " 0.06542892 0.10991988 0.11954437 0.11098318 0.06431843 0.06077834\n",
      " 0.0654115  0.07250371 0.07422685 0.0952617  0.06418537 0.07490838\n",
      " 0.10698372 0.06941697 0.06732365 0.13738661 0.06083894 0.07078123\n",
      " 0.07261508 0.11990347 0.07567515 0.07530812 0.06997038 0.0759028\n",
      " 0.10168026 0.07081962 0.14240684 0.07333767 0.07139324 0.0664926\n",
      " 0.0957106  0.0634167  0.06718275 0.07696012 0.06479369 0.08271429\n",
      " 0.08189362 0.06581419 0.07375805 0.13133612 0.07252054 0.06524863\n",
      " 0.06175089 0.06471248 0.07419543 0.08081585 0.08248397 0.12651499\n",
      " 0.13342273 0.07025605 0.11654938 0.0629159  0.06546607 0.12357783\n",
      " 0.07535722 0.07790079 0.07652992 0.06643886 0.07553796 0.08494396\n",
      " 0.06636504 0.11303028 0.07421495], shape=(513,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "b44e65f3-b44b-40fd-e564-63e35d43997c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.932"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "def softmax_prob(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) ) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 25)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd1 = prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0NZNorQW0vg",
    "outputId": "afcb4822-0cda-4c13-ea68-8454473bb529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5446808510638298"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[10], 25)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "perds = np.zeros((ex_X.shape[0],))\n",
    "cnt = np.zeros((ex_X.shape[0],))\n",
    "print(perds.shape)\n",
    "for sm in range(512):\n",
    "    X_sub, X_ids = project_and_filter(ex_X, random_dirs[sm], 15)\n",
    "    Y_sub = ex_Y[X_ids]\n",
    "    prd = softmax_prob(get_rand_feats(X_sub@pca_projs, model), betas[sm])\n",
    "    # print(prd.shape, X_ids, ex_X.shape)\n",
    "    perds[X_ids] += prd\n",
    "    cnt[X_ids] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.501992552056132"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zer_ids = np.argwhere(cnt!=0)\n",
    "veX = perds[zer_ids]\n",
    "veX /= cnt[zer_ids]\n",
    "((veX>0.5)==Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print((np.mean(tf.stack(perds, axis=0), axis=0)==Y_sub).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((381, 1),\n",
       " array([[  19],\n",
       "        [  63],\n",
       "        [  68],\n",
       "        [ 115],\n",
       "        [ 116],\n",
       "        [ 120],\n",
       "        [ 123],\n",
       "        [ 126],\n",
       "        [ 149],\n",
       "        [ 152],\n",
       "        [ 155],\n",
       "        [ 162],\n",
       "        [ 166],\n",
       "        [ 167],\n",
       "        [ 180],\n",
       "        [ 181],\n",
       "        [ 202],\n",
       "        [ 221],\n",
       "        [ 222],\n",
       "        [ 224],\n",
       "        [ 226],\n",
       "        [ 230],\n",
       "        [ 257],\n",
       "        [ 263],\n",
       "        [ 275],\n",
       "        [ 278],\n",
       "        [ 287],\n",
       "        [ 320],\n",
       "        [ 337],\n",
       "        [ 357],\n",
       "        [ 374],\n",
       "        [ 382],\n",
       "        [ 491],\n",
       "        [ 508],\n",
       "        [ 540],\n",
       "        [ 576],\n",
       "        [ 579],\n",
       "        [ 650],\n",
       "        [ 652],\n",
       "        [ 653],\n",
       "        [ 676],\n",
       "        [ 696],\n",
       "        [ 704],\n",
       "        [ 733],\n",
       "        [ 734],\n",
       "        [ 751],\n",
       "        [ 754],\n",
       "        [ 768],\n",
       "        [ 778],\n",
       "        [ 780],\n",
       "        [ 930],\n",
       "        [ 931],\n",
       "        [ 936],\n",
       "        [ 942],\n",
       "        [ 943],\n",
       "        [ 953],\n",
       "        [ 969],\n",
       "        [ 996],\n",
       "        [1023],\n",
       "        [1041],\n",
       "        [1042],\n",
       "        [1072],\n",
       "        [1116],\n",
       "        [1122],\n",
       "        [1125],\n",
       "        [1127],\n",
       "        [1135],\n",
       "        [1137],\n",
       "        [1149],\n",
       "        [1153],\n",
       "        [1156],\n",
       "        [1159],\n",
       "        [1160],\n",
       "        [1164],\n",
       "        [1169],\n",
       "        [1170],\n",
       "        [1181],\n",
       "        [1195],\n",
       "        [1196],\n",
       "        [1220],\n",
       "        [1238],\n",
       "        [1245],\n",
       "        [1268],\n",
       "        [1269],\n",
       "        [1270],\n",
       "        [1293],\n",
       "        [1298],\n",
       "        [1328],\n",
       "        [1333],\n",
       "        [1338],\n",
       "        [1347],\n",
       "        [1348],\n",
       "        [1390],\n",
       "        [1442],\n",
       "        [1451],\n",
       "        [1456],\n",
       "        [1466],\n",
       "        [1470],\n",
       "        [1503],\n",
       "        [1523],\n",
       "        [1528],\n",
       "        [1569],\n",
       "        [1591],\n",
       "        [1605],\n",
       "        [1626],\n",
       "        [1646],\n",
       "        [1648],\n",
       "        [1649],\n",
       "        [1651],\n",
       "        [1656],\n",
       "        [1657],\n",
       "        [1674],\n",
       "        [1738],\n",
       "        [1761],\n",
       "        [1787],\n",
       "        [1805],\n",
       "        [1812],\n",
       "        [1821],\n",
       "        [1845],\n",
       "        [1846],\n",
       "        [1848],\n",
       "        [1853],\n",
       "        [1874],\n",
       "        [1879],\n",
       "        [1882],\n",
       "        [1908],\n",
       "        [1932],\n",
       "        [1958],\n",
       "        [1962],\n",
       "        [1982],\n",
       "        [1983],\n",
       "        [2012],\n",
       "        [2026],\n",
       "        [2031],\n",
       "        [2034],\n",
       "        [2071],\n",
       "        [2073],\n",
       "        [2075],\n",
       "        [2086],\n",
       "        [2100],\n",
       "        [2101],\n",
       "        [2154],\n",
       "        [2178],\n",
       "        [2183],\n",
       "        [2186],\n",
       "        [2224],\n",
       "        [2228],\n",
       "        [2236],\n",
       "        [2322],\n",
       "        [2329],\n",
       "        [2340],\n",
       "        [2343],\n",
       "        [2403],\n",
       "        [2406],\n",
       "        [2427],\n",
       "        [2448],\n",
       "        [2463],\n",
       "        [2485],\n",
       "        [2540],\n",
       "        [2541],\n",
       "        [2544],\n",
       "        [2548],\n",
       "        [2579],\n",
       "        [2609],\n",
       "        [2611],\n",
       "        [2613],\n",
       "        [2634],\n",
       "        [2636],\n",
       "        [2642],\n",
       "        [2645],\n",
       "        [2647],\n",
       "        [2649],\n",
       "        [2673],\n",
       "        [2680],\n",
       "        [2703],\n",
       "        [2756],\n",
       "        [2758],\n",
       "        [2775],\n",
       "        [2784],\n",
       "        [2785],\n",
       "        [2794],\n",
       "        [2803],\n",
       "        [2804],\n",
       "        [2814],\n",
       "        [2837],\n",
       "        [2847],\n",
       "        [2848],\n",
       "        [2849],\n",
       "        [2863],\n",
       "        [2866],\n",
       "        [2868],\n",
       "        [2881],\n",
       "        [2888],\n",
       "        [2889],\n",
       "        [2890],\n",
       "        [2904],\n",
       "        [2908],\n",
       "        [2913],\n",
       "        [2935],\n",
       "        [2936],\n",
       "        [2937],\n",
       "        [2938],\n",
       "        [2939],\n",
       "        [2944],\n",
       "        [2945],\n",
       "        [2995],\n",
       "        [2997],\n",
       "        [3008],\n",
       "        [3017],\n",
       "        [3028],\n",
       "        [3032],\n",
       "        [3038],\n",
       "        [3042],\n",
       "        [3043],\n",
       "        [3053],\n",
       "        [3057],\n",
       "        [3079],\n",
       "        [3108],\n",
       "        [3111],\n",
       "        [3115],\n",
       "        [3116],\n",
       "        [3117],\n",
       "        [3129],\n",
       "        [3131],\n",
       "        [3133],\n",
       "        [3147],\n",
       "        [3153],\n",
       "        [3154],\n",
       "        [3156],\n",
       "        [3157],\n",
       "        [3158],\n",
       "        [3165],\n",
       "        [3166],\n",
       "        [3170],\n",
       "        [3171],\n",
       "        [3172],\n",
       "        [3174],\n",
       "        [3175],\n",
       "        [3177],\n",
       "        [3184],\n",
       "        [3197],\n",
       "        [3203],\n",
       "        [3249],\n",
       "        [3266],\n",
       "        [3270],\n",
       "        [3311],\n",
       "        [3312],\n",
       "        [3313],\n",
       "        [3318],\n",
       "        [3336],\n",
       "        [3338],\n",
       "        [3361],\n",
       "        [3368],\n",
       "        [3411],\n",
       "        [3412],\n",
       "        [3416],\n",
       "        [3445],\n",
       "        [3449],\n",
       "        [3451],\n",
       "        [3462],\n",
       "        [3471],\n",
       "        [3488],\n",
       "        [3518],\n",
       "        [3554],\n",
       "        [3561],\n",
       "        [3572],\n",
       "        [3574],\n",
       "        [3576],\n",
       "        [3591],\n",
       "        [3611],\n",
       "        [3654],\n",
       "        [3658],\n",
       "        [3660],\n",
       "        [3692],\n",
       "        [3693],\n",
       "        [3700],\n",
       "        [3710],\n",
       "        [3719],\n",
       "        [3720],\n",
       "        [3721],\n",
       "        [3722],\n",
       "        [3731],\n",
       "        [3732],\n",
       "        [3757],\n",
       "        [3761],\n",
       "        [3762],\n",
       "        [3773],\n",
       "        [3796],\n",
       "        [3848],\n",
       "        [3854],\n",
       "        [3855],\n",
       "        [3858],\n",
       "        [3859],\n",
       "        [3863],\n",
       "        [3880],\n",
       "        [3883],\n",
       "        [3890],\n",
       "        [3958],\n",
       "        [3960],\n",
       "        [3987],\n",
       "        [4010],\n",
       "        [4013],\n",
       "        [4014],\n",
       "        [4028],\n",
       "        [4064],\n",
       "        [4071],\n",
       "        [4072],\n",
       "        [4073],\n",
       "        [4075],\n",
       "        [4077],\n",
       "        [4078],\n",
       "        [4079],\n",
       "        [4080],\n",
       "        [4082],\n",
       "        [4125],\n",
       "        [4142],\n",
       "        [4143],\n",
       "        [4152],\n",
       "        [4155],\n",
       "        [4197],\n",
       "        [4198],\n",
       "        [4199],\n",
       "        [4200],\n",
       "        [4255],\n",
       "        [4315],\n",
       "        [4317],\n",
       "        [4322],\n",
       "        [4324],\n",
       "        [4328],\n",
       "        [4336],\n",
       "        [4367],\n",
       "        [4369],\n",
       "        [4370],\n",
       "        [4371],\n",
       "        [4372],\n",
       "        [4384],\n",
       "        [4399],\n",
       "        [4438],\n",
       "        [4439],\n",
       "        [4455],\n",
       "        [4486],\n",
       "        [4489],\n",
       "        [4490],\n",
       "        [4493],\n",
       "        [4494],\n",
       "        [4495],\n",
       "        [4499],\n",
       "        [4522],\n",
       "        [4540],\n",
       "        [4574],\n",
       "        [4584],\n",
       "        [4588],\n",
       "        [4596],\n",
       "        [4597],\n",
       "        [4616],\n",
       "        [4635],\n",
       "        [4642],\n",
       "        [4647],\n",
       "        [4657],\n",
       "        [4659],\n",
       "        [4672],\n",
       "        [4676],\n",
       "        [4691],\n",
       "        [4705],\n",
       "        [4764],\n",
       "        [4769],\n",
       "        [4781],\n",
       "        [4800],\n",
       "        [4805],\n",
       "        [4838],\n",
       "        [4849],\n",
       "        [4855],\n",
       "        [4878],\n",
       "        [4891],\n",
       "        [4894],\n",
       "        [4896],\n",
       "        [4899],\n",
       "        [4915],\n",
       "        [4947],\n",
       "        [4951],\n",
       "        [4955],\n",
       "        [4976],\n",
       "        [5009],\n",
       "        [5010],\n",
       "        [5013],\n",
       "        [5014],\n",
       "        [5041],\n",
       "        [5055],\n",
       "        [5061],\n",
       "        [5064],\n",
       "        [5066],\n",
       "        [5067],\n",
       "        [5069],\n",
       "        [5071],\n",
       "        [5072],\n",
       "        [5085],\n",
       "        [5096],\n",
       "        [5103],\n",
       "        [5104],\n",
       "        [5111],\n",
       "        [5113],\n",
       "        [5114],\n",
       "        [5117],\n",
       "        [5121],\n",
       "        [5134],\n",
       "        [5142],\n",
       "        [5149],\n",
       "        [5161],\n",
       "        [5163],\n",
       "        [5174],\n",
       "        [5176],\n",
       "        [5191],\n",
       "        [5198],\n",
       "        [5200],\n",
       "        [5226],\n",
       "        [5238],\n",
       "        [5252],\n",
       "        [5310],\n",
       "        [5311],\n",
       "        [5314],\n",
       "        [5332],\n",
       "        [5333],\n",
       "        [5335],\n",
       "        [5345],\n",
       "        [5353],\n",
       "        [5360],\n",
       "        [5361],\n",
       "        [5363],\n",
       "        [5366],\n",
       "        [5368],\n",
       "        [5375],\n",
       "        [5378],\n",
       "        [5390],\n",
       "        [5408],\n",
       "        [5423],\n",
       "        [5436],\n",
       "        [5437],\n",
       "        [5438],\n",
       "        [5445],\n",
       "        [5446],\n",
       "        [5487],\n",
       "        [5488],\n",
       "        [5491],\n",
       "        [5505],\n",
       "        [5533],\n",
       "        [5537],\n",
       "        [5549],\n",
       "        [5553],\n",
       "        [5570],\n",
       "        [5575],\n",
       "        [5581],\n",
       "        [5583],\n",
       "        [5598],\n",
       "        [5638],\n",
       "        [5641],\n",
       "        [5646],\n",
       "        [5653],\n",
       "        [5657],\n",
       "        [5672],\n",
       "        [5753],\n",
       "        [5755],\n",
       "        [5766],\n",
       "        [5777],\n",
       "        [5778],\n",
       "        [5781],\n",
       "        [5783],\n",
       "        [5788],\n",
       "        [5791],\n",
       "        [5794],\n",
       "        [5808],\n",
       "        [5811],\n",
       "        [5836],\n",
       "        [5897],\n",
       "        [5909],\n",
       "        [5923]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supps_0, supps_1 = supps_b[sample]\n",
    "supps_0.shape, supps_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026,  0.7023512 , -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       ...,\n",
       "       [-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882,  0.91829246, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882,  0.91829246, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppvs_0 = X[supps_0.reshape((-1,))]\n",
    "suppvs_1 = X[supps_1.reshape((-1,))]\n",
    "y_supp_0 = Y[supps_0.reshape((-1,))]\n",
    "y_supp_1 = Y[supps_1.reshape((-1,))]\n",
    "suppvs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 1024)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppvs_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99999557 -0.99999864 -1.00000015 -1.00000274 -0.99999844 -1.00000071\n",
      " -1.00000048 -0.99999712 -0.99999474 -1.00000096 -0.99999895 -1.0000031\n",
      " -1.00000271 -0.99999792 -0.99999714 -0.99999876 -0.99999873 -0.99999996\n",
      " -1.00000344 -0.99999897 -0.9999988  -1.00000246 -0.99999731 -0.99999866\n",
      " -0.99999845 -0.99999905 -1.00000161 -0.9999993  -0.99999992 -1.00000107\n",
      " -1.0000001  -0.99999536 -0.99999485 -0.99999945 -1.00000243 -0.99999581\n",
      " -0.99999901 -1.00000531 -1.         -0.9999974  -0.99999841 -0.99999726\n",
      " -1.0000001  -1.00000406 -1.00000235 -1.00000085 -1.00000044 -1.00000429\n",
      " -1.00000434 -1.00000024 -1.00000118 -0.999996   -0.99999972 -0.9999999\n",
      " -1.00000058 -0.99999397 -0.99999636 -1.00000286 -1.00000797 -1.00000088\n",
      " -1.00000028 -1.00000142 -1.0000043  -0.99999972 -0.99999621 -1.00000086\n",
      " -1.00000302 -1.00000322 -1.00000138 -1.00000425 -0.99999989 -1.0000025\n",
      " -1.00000054 -0.99999991 -0.9999996  -0.99999743 -0.99999847 -1.00000061\n",
      " -1.00000114 -1.00000165 -1.00000441 -0.99999933 -0.99999731 -0.99999625\n",
      " -1.0000037  -1.00000287 -0.99999944 -0.99999899 -1.0000044  -1.0000004\n",
      " -0.99999581 -1.00000248 -1.0000064  -0.99999798 -0.99999999 -0.99999881\n",
      " -0.99999808 -1.00000072 -1.00000203 -0.99999953 -0.99999995 -0.99999593\n",
      " -1.00000357 -1.00000529 -0.99999768 -1.00000481 -1.00000473 -0.99999755\n",
      " -0.99999789  0.89253063 -0.99999876 -1.00000252 -0.99999586 -1.00000442\n",
      " -0.99999629 -1.00000115 -0.9999997  -0.99999353 -1.00000014 -0.99999954\n",
      " -1.00000217 -0.9999988  -0.99999832 -0.99999735 -1.00000248 -0.99999779\n",
      " -1.00000045 -0.99999884 -1.00000063 -0.99999683 -0.9999978  -0.99999795\n",
      " -0.99999801 -1.00000365 -0.99999959 -0.99999692 -0.99999894 -1.00000214\n",
      " -0.99999843 -0.99999089 -0.99999655 -0.9999992  -1.00000184 -0.99999908\n",
      " -1.00000461 -0.99999957 -1.00000653 -1.00000206 -1.00000463 -0.99999801\n",
      " -0.99999928 -1.00000223 -0.9999973  -0.99999701 -0.9999971  -0.99999629\n",
      " -1.00000057 -1.00000184 -1.00000487 -0.99999784 -1.00000173 -1.00000592\n",
      " -1.00000484 -0.99999972 -0.9999973  -0.99999867 -0.99999415 -0.99999746\n",
      " -0.99999313 -1.0000024  -0.99999341 -0.99999853 -0.99999876 -0.99999852\n",
      " -1.00000494 -0.99999825 -1.00000011 -0.99999596 -0.9999956  -1.00000199\n",
      " -0.99999946 -0.99999383 -1.0000001  -0.99999952 -0.9999991  -1.00000252\n",
      " -0.999998   -1.00000384 -1.00000118 -0.99999517 -0.99999922 -1.0000009\n",
      " -0.99999826 -0.99999685 -0.99999451 -1.00000002 -0.99999946 -0.99999747\n",
      " -0.99999298 -0.99999739 -1.00000293 -1.00000114 -1.00000154 -0.99999784\n",
      " -0.99999468 -0.99999999 -0.99999775 -1.00000061 -1.00000053 -1.00000053\n",
      " -0.99999498 -0.99999889 -1.00000104 -0.99999952 -0.99999912 -1.0000007\n",
      " -1.00000199 -0.99999876 -1.00000152 -1.0000017  -0.99999833 -1.0000039\n",
      " -0.99999812 -1.00000013 -1.00000347 -0.99999859 -0.99999663 -0.99999868\n",
      " -1.0000014  -1.00000149 -1.00000252 -0.99999527 -0.99999682 -1.00000403\n",
      " -1.00000005 -0.9999948  -0.99999384 -0.99999698 -1.00000233 -0.99999743\n",
      " -0.99999908 -1.00000696 -0.99999999 -0.99999722 -0.9999993  -1.00000078\n",
      " -1.00000247 -0.99999992 -0.99999872  0.36856059 -0.99999907 -0.9999998\n",
      " -0.99999839 -0.99999575 -0.99999872 -1.00000171 -0.99999992 -0.99999876\n",
      " -1.00000106 -0.99999893 -0.99999619 -1.00000254 -0.17124262 -0.99999992\n",
      " -0.99999897 -1.00000023 -1.00000021 -0.99999993 -1.00000322 -0.99999314\n",
      " -1.00000368 -1.00000043 -0.99999718 -1.00000063 -0.99999859 -0.99999798\n",
      " -0.99999768 -0.9999984  -1.00000031 -0.99999497 -1.00000271 -1.\n",
      " -0.9999989  -1.00000092 -1.00000034 -0.99999778 -0.99999774 -0.99999749\n",
      " -1.00000044 -0.99999659 -0.99999723 -1.00000046 -1.00000401 -1.00000262\n",
      " -0.99999467 -0.99999473 -1.00000239 -0.99999617 -0.99999812 -1.00000387\n",
      " -1.00000201 -1.00000031 -0.99999892 -0.99999793 -1.00000155 -0.99999784\n",
      " -1.00000221 -0.99999727 -1.00000276 -1.00000142 -0.9999993  -0.9707472\n",
      " -1.00000239 -1.00000272 -0.9999995  -1.00000308 -0.99999798 -1.00000052\n",
      " -1.00000039 -0.99999548 -0.9999959  -1.00000357 -1.00000262 -0.9999942\n",
      " -0.99999939 -0.99999865 -0.99999932 -1.00000143 -0.99999898 -0.99999929\n",
      " -1.00000118 -1.00000508 -0.99999844 -0.99999526 -0.9999998  -0.9999979\n",
      " -0.99999888 -1.00000198 -1.00000279 -1.00000012 -1.00000345 -1.00000023\n",
      " -0.9999979  -1.00000409 -1.00000257 -0.99999924 -0.99999843 -0.99999737\n",
      " -0.99999582 -0.99999584 -0.99999923 -0.99999941 -1.00000319 -0.99999846\n",
      " -1.00000199 -0.99999927 -0.99999965 -1.00000136 -1.00000332 -0.99999471\n",
      " -1.00000336 -0.99999931 -1.00000327 -1.00000506 -0.15631103 -1.00000319\n",
      " -1.00000222 -0.99999952 -0.99999703 -0.99999816 -0.99999448 -0.99999711\n",
      " -1.0000037  -0.99999945  0.47949874 -1.000002   -1.00000096 -1.00000417\n",
      " -1.00000333 -0.99999964 -1.0000007 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "tf.Tensor(0.013705432773427331, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def get_margin(X, wgts):\n",
    "  sd = (np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T) * 1.0\n",
    "  return sd[:]\n",
    "\n",
    "def loss_2(labels, margins):\n",
    "    l = 2*labels - 1\n",
    "    return tf.reduce_sum(tf.reduce_mean(tf.nn.relu(-margins * l), axis=0))\n",
    "\n",
    "get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model).shape, betas[sample].shape\n",
    "print(get_margin(get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model), betas[sample]), y_supp_0)\n",
    "# loss_2 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "print(loss_2(y_supp_0[:len(suppvs_0)], get_margin(get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model), betas[sample]*3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "# beta_input = layers.Input(shape=(beta_dim,))\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([dir_input])\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(latent_dim, activation=tf.nn.elu)(x)\n",
    "# z_mean = layers.Dense(latent_dim)(x)\n",
    "# z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "# z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "z = layers.Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "# decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs])\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([dir_input], z, name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([dir_input])])\n",
    "vae = models.Model([dir_input], outputs, name=\"autoenc\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin_batched(supp_vs, wgts):\n",
    "  sd = (tf.concat([tf.ones((*supp_vs.shape[:-1], 1)), supp_vs], axis=-1) @ wgts[:, :, None]) * 1.0\n",
    "  return sd[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = vae((betas[:1], random_dirs[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "#def vae_loss(inputs, outputs, z_mean, z_log_var, reg=0.002):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "#  intercp_loss = tf.reduce_mean(tf.abs(tf.cast(inputs[:, :1], dtype=tf.float32) - outputs[:, :1]))\n",
    "#  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs[:, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs[:, 1:], axis=-1)[0], axis=-1))\n",
    "#  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "#  total_loss = recon_loss + intercp_loss + reg * kl_loss\n",
    "#  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, supp_vs, supp_ys, reg=1.0, regr=1.0):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs, dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs, axis=-1)[0], axis=-1))\n",
    "  # print(supp_vs.shape, outputs.shape)\n",
    "  supp_margins = get_margin_batched(supp_vs, outputs)\n",
    "  supp_loss = loss_2(supp_ys, supp_margins)\n",
    "  # print(supp_ys[0][30:], supp_margins[0][30:])\n",
    "  # print(supp_margins.shape, supp_ys.shape)\n",
    "  # kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = reg * recon_loss + regr * supp_loss\n",
    "  return total_loss, reg*recon_loss, regr*supp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.1964941>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0429045>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09860455>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=27.492508>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_loss(betas[:1], ots[:1], tf.ones((1, 32)), tf.ones((1, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def train_step(model, inputs, dir_inputs, supp_vs, supp_ys, alpha, ralpha=1.0):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z = model.encoder([dir_inputs])\n",
    "    outputs = model.decoder([z])\n",
    "    total_loss, recon_loss, supp_loss = vae_loss(inputs, outputs, supp_vs, supp_ys, alpha, ralpha)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, supp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:41:59.480502: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1576b9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-29 19:41:59.480542: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-04-29 19:41:59.486897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-29 19:41:59.594610: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-29 19:41:59.674717: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fb840184ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fb840184ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0526553>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.999847>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.051767856>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.52023363>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(vae, betas[:32], random_dirs[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(381, 204), dtype=float32, numpy=\n",
       "array([[-9.73291   , -0.08359981,  4.8285213 , ..., -0.418212  ,\n",
       "         0.04979858, -0.38527712],\n",
       "       [-1.4835713 ,  1.0568206 ,  2.5018466 , ..., -0.06100272,\n",
       "         0.1693849 , -0.38147366],\n",
       "       [-7.0123835 , -9.252638  , -1.9807861 , ...,  0.136215  ,\n",
       "         0.12370304,  0.3105353 ],\n",
       "       ...,\n",
       "       [-4.699366  ,  2.6286366 , -1.7926993 , ..., -0.3052967 ,\n",
       "         0.18089612, -0.36616707],\n",
       "       [-1.7197244 ,  1.9622226 ,  2.9822083 , ..., -0.33606544,\n",
       "        -0.01879999,  0.30114657],\n",
       "       [-1.7413275 ,  1.9585967 ,  3.080233  , ..., -0.47907415,\n",
       "         0.07559836,  0.45271814]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[supps_0.reshape((-1,))] @ pca_projs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(X, betas, dirs, supps, batch_size, support_batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  supps_ = [supps[i] for i in indices]\n",
    "  supps_ = np.array([np.stack([i[np.random.choice(len(i), support_batch_size)],\n",
    "                     j[np.random.choice(len(j), support_batch_size)]], axis=0) for i, j in supps_])\n",
    "  def supp_get_vs(_supps):\n",
    "      supps_0, supps_1 = np.transpose(_supps, (1, 0, 2, 3))\n",
    "      suppvs_0 = X[supps_0.reshape((-1,))] @ pca_projs\n",
    "      suppvs_1 = X[supps_1.reshape((-1,))] @ pca_projs\n",
    "      suppvs_0 = tf.reshape(get_rand_feats(tf.cast(suppvs_0, dtype=tf.float32), model), (batch_size, support_batch_size, -1))\n",
    "      suppvs_1 = tf.reshape(get_rand_feats(tf.cast(suppvs_1, dtype=tf.float32), model), (batch_size, support_batch_size, -1))\n",
    "      y_supp_0 = Y[supps_0.reshape((-1,))].reshape((batch_size, support_batch_size, -1))\n",
    "      y_supp_1 = Y[supps_1.reshape((-1,))].reshape((batch_size, support_batch_size, -1))\n",
    "      return np.concatenate([suppvs_0, suppvs_1], axis=1), np.concatenate([y_supp_0, y_supp_1], axis=1)\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield betas[i:i+batch_size], dirs[i:i+batch_size], *supp_get_vs(supps_[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch at 0x7fb77c7be890>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch(betas, random_dirs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "a6e1ccc1-2cd0-4549-e61a-6dae8aa1062a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 1.0042555332183838, recon loss = 1.0042555332183838, supp loss = 0.0\n",
      "Step 0: loss = 0.6852933168411255, recon loss = 0.6852933168411255, supp loss = 0.0\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.5890307426452637, recon loss = 0.5890307426452637, supp loss = 0.0\n",
      "Step 0: loss = 0.5855118036270142, recon loss = 0.5855118036270142, supp loss = 0.0\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.5907216668128967, recon loss = 0.5907216668128967, supp loss = 0.0\n",
      "Step 0: loss = 0.5872777700424194, recon loss = 0.5872777700424194, supp loss = 0.0\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.5776413083076477, recon loss = 0.5776413083076477, supp loss = 0.0\n",
      "Step 0: loss = 0.5796323418617249, recon loss = 0.5796323418617249, supp loss = 0.0\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.5833013653755188, recon loss = 0.5833013653755188, supp loss = 0.0\n",
      "Step 0: loss = 0.5857218503952026, recon loss = 0.5857218503952026, supp loss = 0.0\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.578827440738678, recon loss = 0.578827440738678, supp loss = 0.0\n",
      "Step 0: loss = 0.5807657241821289, recon loss = 0.5807657241821289, supp loss = 0.0\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.5597935318946838, recon loss = 0.5597935318946838, supp loss = 0.0\n",
      "Step 0: loss = 0.5642254948616028, recon loss = 0.5642254948616028, supp loss = 0.0\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.5692940950393677, recon loss = 0.5692940950393677, supp loss = 0.0\n",
      "Step 0: loss = 0.5721639394760132, recon loss = 0.5721639394760132, supp loss = 0.0\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.5468140244483948, recon loss = 0.5468140244483948, supp loss = 0.0\n",
      "Step 0: loss = 0.5705466270446777, recon loss = 0.5705466270446777, supp loss = 0.0\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.5546731352806091, recon loss = 0.5546731352806091, supp loss = 0.0\n",
      "Step 0: loss = 0.5715173482894897, recon loss = 0.5715173482894897, supp loss = 0.0\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.5451689958572388, recon loss = 0.5451689958572388, supp loss = 0.0\n",
      "Step 0: loss = 0.5563899278640747, recon loss = 0.5563899278640747, supp loss = 0.0\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.5485641956329346, recon loss = 0.5485641956329346, supp loss = 0.0\n",
      "Step 0: loss = 0.5772020816802979, recon loss = 0.5772020816802979, supp loss = 0.0\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.5570698380470276, recon loss = 0.5570698380470276, supp loss = 0.0\n",
      "Step 0: loss = 0.5731750726699829, recon loss = 0.5731750726699829, supp loss = 0.0\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.5487866401672363, recon loss = 0.5487866401672363, supp loss = 0.0\n",
      "Step 0: loss = 0.5671803951263428, recon loss = 0.5671803951263428, supp loss = 0.0\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.5453540086746216, recon loss = 0.5453540086746216, supp loss = 0.0\n",
      "Step 0: loss = 0.5705669522285461, recon loss = 0.5705669522285461, supp loss = 0.0\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.5264188051223755, recon loss = 0.5264188051223755, supp loss = 0.0\n",
      "Step 0: loss = 0.557187557220459, recon loss = 0.557187557220459, supp loss = 0.0\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.5639516711235046, recon loss = 0.5639516711235046, supp loss = 0.0\n",
      "Step 0: loss = 0.5667285919189453, recon loss = 0.5667285919189453, supp loss = 0.0\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.5454390048980713, recon loss = 0.5454390048980713, supp loss = 0.0\n",
      "Step 0: loss = 0.5714215636253357, recon loss = 0.5714215636253357, supp loss = 0.0\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.5128263235092163, recon loss = 0.5128263235092163, supp loss = 0.0\n",
      "Step 0: loss = 0.5413665771484375, recon loss = 0.5413665771484375, supp loss = 0.0\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.517399787902832, recon loss = 0.517399787902832, supp loss = 0.0\n",
      "Step 0: loss = 0.5611250400543213, recon loss = 0.5611250400543213, supp loss = 0.0\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.521978497505188, recon loss = 0.521978497505188, supp loss = 0.0\n",
      "Step 0: loss = 0.5696579217910767, recon loss = 0.5696579217910767, supp loss = 0.0\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.5147208571434021, recon loss = 0.5147208571434021, supp loss = 0.0\n",
      "Step 0: loss = 0.5676119327545166, recon loss = 0.5676119327545166, supp loss = 0.0\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.5065177083015442, recon loss = 0.5065177083015442, supp loss = 0.0\n",
      "Step 0: loss = 0.5380655527114868, recon loss = 0.5380655527114868, supp loss = 0.0\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.4950907230377197, recon loss = 0.4950907230377197, supp loss = 0.0\n",
      "Step 0: loss = 0.5465813279151917, recon loss = 0.5465813279151917, supp loss = 0.0\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.5096533298492432, recon loss = 0.5096533298492432, supp loss = 0.0\n",
      "Step 0: loss = 0.5566259622573853, recon loss = 0.5566259622573853, supp loss = 0.0\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.5155072212219238, recon loss = 0.5155072212219238, supp loss = 0.0\n",
      "Step 0: loss = 0.5622957944869995, recon loss = 0.5622957944869995, supp loss = 0.0\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.48227232694625854, recon loss = 0.48227232694625854, supp loss = 0.0\n",
      "Step 0: loss = 0.5682346820831299, recon loss = 0.5682346820831299, supp loss = 0.0\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.48908868432044983, recon loss = 0.48908868432044983, supp loss = 0.0\n",
      "Step 0: loss = 0.5502667427062988, recon loss = 0.5502667427062988, supp loss = 0.0\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.5043078064918518, recon loss = 0.5043078064918518, supp loss = 0.0\n",
      "Step 0: loss = 0.5549970269203186, recon loss = 0.5549970269203186, supp loss = 0.0\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.521740198135376, recon loss = 0.521740198135376, supp loss = 0.0\n",
      "Step 0: loss = 0.5657455325126648, recon loss = 0.5657455325126648, supp loss = 0.0\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.4818391799926758, recon loss = 0.4818391799926758, supp loss = 0.0\n",
      "Step 0: loss = 0.5580413341522217, recon loss = 0.5580413341522217, supp loss = 0.0\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.48943209648132324, recon loss = 0.48943209648132324, supp loss = 0.0\n",
      "Step 0: loss = 0.5493830442428589, recon loss = 0.5493830442428589, supp loss = 0.0\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.4818977117538452, recon loss = 0.4818977117538452, supp loss = 0.0\n",
      "Step 0: loss = 0.5493513345718384, recon loss = 0.5493513345718384, supp loss = 0.0\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.49236005544662476, recon loss = 0.49236005544662476, supp loss = 0.0\n",
      "Step 0: loss = 0.5498393774032593, recon loss = 0.5498393774032593, supp loss = 0.0\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.4638493061065674, recon loss = 0.4638493061065674, supp loss = 0.0\n",
      "Step 0: loss = 0.567531943321228, recon loss = 0.567531943321228, supp loss = 0.0\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.48951321840286255, recon loss = 0.48951321840286255, supp loss = 0.0\n",
      "Step 0: loss = 0.5650511980056763, recon loss = 0.5650511980056763, supp loss = 0.0\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.46689867973327637, recon loss = 0.46689867973327637, supp loss = 0.0\n",
      "Step 0: loss = 0.5684894323348999, recon loss = 0.5684894323348999, supp loss = 0.0\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.47684648633003235, recon loss = 0.47684648633003235, supp loss = 0.0\n",
      "Step 0: loss = 0.5434831976890564, recon loss = 0.5434831976890564, supp loss = 0.0\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.45619937777519226, recon loss = 0.45619937777519226, supp loss = 0.0\n",
      "Step 0: loss = 0.5503052473068237, recon loss = 0.5503052473068237, supp loss = 0.0\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.47440552711486816, recon loss = 0.47440552711486816, supp loss = 0.0\n",
      "Step 0: loss = 0.5584657192230225, recon loss = 0.5584657192230225, supp loss = 0.0\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.4625561237335205, recon loss = 0.4625561237335205, supp loss = 0.0\n",
      "Step 0: loss = 0.5683376789093018, recon loss = 0.5683376789093018, supp loss = 0.0\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.4897136688232422, recon loss = 0.4897136688232422, supp loss = 0.0\n",
      "Step 0: loss = 0.5670008063316345, recon loss = 0.5670008063316345, supp loss = 0.0\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.47720199823379517, recon loss = 0.47720199823379517, supp loss = 0.0\n",
      "Step 0: loss = 0.5529204607009888, recon loss = 0.5529204607009888, supp loss = 0.0\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.4630039930343628, recon loss = 0.4630039930343628, supp loss = 0.0\n",
      "Step 0: loss = 0.5483227968215942, recon loss = 0.5483227968215942, supp loss = 0.0\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.47808122634887695, recon loss = 0.47808122634887695, supp loss = 0.0\n",
      "Step 0: loss = 0.5663935542106628, recon loss = 0.5663935542106628, supp loss = 0.0\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.45988166332244873, recon loss = 0.45988166332244873, supp loss = 0.0\n",
      "Step 0: loss = 0.566714882850647, recon loss = 0.566714882850647, supp loss = 0.0\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.4706369638442993, recon loss = 0.4706369638442993, supp loss = 0.0\n",
      "Step 0: loss = 0.5406227111816406, recon loss = 0.5406227111816406, supp loss = 0.0\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.45513948798179626, recon loss = 0.45513948798179626, supp loss = 0.0\n",
      "Step 0: loss = 0.5411928296089172, recon loss = 0.5411928296089172, supp loss = 0.0\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.46712544560432434, recon loss = 0.46712544560432434, supp loss = 0.0\n",
      "Step 0: loss = 0.5493159294128418, recon loss = 0.5493159294128418, supp loss = 0.0\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.46600422263145447, recon loss = 0.46600422263145447, supp loss = 0.0\n",
      "Step 0: loss = 0.5580888986587524, recon loss = 0.5580888986587524, supp loss = 0.0\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.4446013271808624, recon loss = 0.4446013271808624, supp loss = 0.0\n",
      "Step 0: loss = 0.5464833974838257, recon loss = 0.5464833974838257, supp loss = 0.0\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.44230860471725464, recon loss = 0.44230860471725464, supp loss = 0.0\n",
      "Step 0: loss = 0.5373497605323792, recon loss = 0.5373497605323792, supp loss = 0.0\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.4466401934623718, recon loss = 0.4466401934623718, supp loss = 0.0\n",
      "Step 0: loss = 0.5521135330200195, recon loss = 0.5521135330200195, supp loss = 0.0\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.47584784030914307, recon loss = 0.47584784030914307, supp loss = 0.0\n",
      "Step 0: loss = 0.5655933618545532, recon loss = 0.5655933618545532, supp loss = 0.0\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.46929383277893066, recon loss = 0.46929383277893066, supp loss = 0.0\n",
      "Step 0: loss = 0.5444082021713257, recon loss = 0.5444082021713257, supp loss = 0.0\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.4622483551502228, recon loss = 0.4622483551502228, supp loss = 0.0\n",
      "Step 0: loss = 0.5622776746749878, recon loss = 0.5622776746749878, supp loss = 0.0\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.45162200927734375, recon loss = 0.45162200927734375, supp loss = 0.0\n",
      "Step 0: loss = 0.5461131930351257, recon loss = 0.5461131930351257, supp loss = 0.0\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.46431678533554077, recon loss = 0.46431678533554077, supp loss = 0.0\n",
      "Step 0: loss = 0.5579123497009277, recon loss = 0.5579123497009277, supp loss = 0.0\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.4446917772293091, recon loss = 0.4446917772293091, supp loss = 0.0\n",
      "Step 0: loss = 0.5347268581390381, recon loss = 0.5347268581390381, supp loss = 0.0\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.4457235634326935, recon loss = 0.4457235634326935, supp loss = 0.0\n",
      "Step 0: loss = 0.5408508777618408, recon loss = 0.5408508777618408, supp loss = 0.0\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.46020305156707764, recon loss = 0.46020305156707764, supp loss = 0.0\n",
      "Step 0: loss = 0.548920750617981, recon loss = 0.548920750617981, supp loss = 0.0\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.4542720317840576, recon loss = 0.4542720317840576, supp loss = 0.0\n",
      "Step 0: loss = 0.560981810092926, recon loss = 0.560981810092926, supp loss = 0.0\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.4310118556022644, recon loss = 0.4310118556022644, supp loss = 0.0\n",
      "Step 0: loss = 0.5432827472686768, recon loss = 0.5432827472686768, supp loss = 0.0\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.43623900413513184, recon loss = 0.43623900413513184, supp loss = 0.0\n",
      "Step 0: loss = 0.5384852886199951, recon loss = 0.5384852886199951, supp loss = 0.0\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.4457632601261139, recon loss = 0.4457632601261139, supp loss = 0.0\n",
      "Step 0: loss = 0.5330519080162048, recon loss = 0.5330519080162048, supp loss = 0.0\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.4425084888935089, recon loss = 0.4425084888935089, supp loss = 0.0\n",
      "Step 0: loss = 0.5482263565063477, recon loss = 0.5482263565063477, supp loss = 0.0\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.4707346558570862, recon loss = 0.4707346558570862, supp loss = 0.0\n",
      "Step 0: loss = 0.5734330415725708, recon loss = 0.5734330415725708, supp loss = 0.0\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.47442054748535156, recon loss = 0.47442054748535156, supp loss = 0.0\n",
      "Step 0: loss = 0.558100700378418, recon loss = 0.558100700378418, supp loss = 0.0\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.4619362950325012, recon loss = 0.4619362950325012, supp loss = 0.0\n",
      "Step 0: loss = 0.5564182996749878, recon loss = 0.5564182996749878, supp loss = 0.0\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.44840800762176514, recon loss = 0.44840800762176514, supp loss = 0.0\n",
      "Step 0: loss = 0.5610336065292358, recon loss = 0.5610336065292358, supp loss = 0.0\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.42382922768592834, recon loss = 0.42382922768592834, supp loss = 0.0\n",
      "Step 0: loss = 0.5214217901229858, recon loss = 0.5214217901229858, supp loss = 0.0\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.4437015652656555, recon loss = 0.4437015652656555, supp loss = 0.0\n",
      "Step 0: loss = 0.5528862476348877, recon loss = 0.5528862476348877, supp loss = 0.0\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.4311370849609375, recon loss = 0.4311370849609375, supp loss = 0.0\n",
      "Step 0: loss = 0.5493203401565552, recon loss = 0.5493203401565552, supp loss = 0.0\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.44969213008880615, recon loss = 0.44969213008880615, supp loss = 0.0\n",
      "Step 0: loss = 0.5668150186538696, recon loss = 0.5668150186538696, supp loss = 0.0\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.4622461199760437, recon loss = 0.4622461199760437, supp loss = 0.0\n",
      "Step 0: loss = 0.5587430596351624, recon loss = 0.5587430596351624, supp loss = 0.0\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.4483047127723694, recon loss = 0.4483047127723694, supp loss = 0.0\n",
      "Step 0: loss = 0.5616792440414429, recon loss = 0.5616792440414429, supp loss = 0.0\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.42373940348625183, recon loss = 0.42373940348625183, supp loss = 0.0\n",
      "Step 0: loss = 0.559605598449707, recon loss = 0.559605598449707, supp loss = 0.0\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.437059223651886, recon loss = 0.437059223651886, supp loss = 0.0\n",
      "Step 0: loss = 0.5714961290359497, recon loss = 0.5714961290359497, supp loss = 0.0\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.43524158000946045, recon loss = 0.43524158000946045, supp loss = 0.0\n",
      "Step 0: loss = 0.5425041913986206, recon loss = 0.5425041913986206, supp loss = 0.0\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.44529491662979126, recon loss = 0.44529491662979126, supp loss = 0.0\n",
      "Step 0: loss = 0.5788650512695312, recon loss = 0.5788650512695312, supp loss = 0.0\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.445456862449646, recon loss = 0.445456862449646, supp loss = 0.0\n",
      "Step 0: loss = 0.5644669532775879, recon loss = 0.5644669532775879, supp loss = 0.0\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.4512329697608948, recon loss = 0.4512329697608948, supp loss = 0.0\n",
      "Step 0: loss = 0.5237018465995789, recon loss = 0.5237018465995789, supp loss = 0.0\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.3954017758369446, recon loss = 0.3954017758369446, supp loss = 0.0\n",
      "Step 0: loss = 0.5588990449905396, recon loss = 0.5588990449905396, supp loss = 0.0\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.42990320920944214, recon loss = 0.42990320920944214, supp loss = 0.0\n",
      "Step 0: loss = 0.5325595736503601, recon loss = 0.5325595736503601, supp loss = 0.0\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.4180629849433899, recon loss = 0.4180629849433899, supp loss = 0.0\n",
      "Step 0: loss = 0.5420719385147095, recon loss = 0.5420719385147095, supp loss = 0.0\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.4368266761302948, recon loss = 0.4368266761302948, supp loss = 0.0\n",
      "Step 0: loss = 0.5459205508232117, recon loss = 0.5459205508232117, supp loss = 0.0\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.42999595403671265, recon loss = 0.42999595403671265, supp loss = 0.0\n",
      "Step 0: loss = 0.5386070609092712, recon loss = 0.5386070609092712, supp loss = 0.0\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.4276208281517029, recon loss = 0.4276208281517029, supp loss = 0.0\n",
      "Step 0: loss = 0.5147741436958313, recon loss = 0.5147741436958313, supp loss = 0.0\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.4473734498023987, recon loss = 0.4473734498023987, supp loss = 0.0\n",
      "Step 0: loss = 0.5451198816299438, recon loss = 0.5451198816299438, supp loss = 0.0\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.4064348340034485, recon loss = 0.4064348340034485, supp loss = 0.0\n",
      "Step 0: loss = 0.5701874494552612, recon loss = 0.5701874494552612, supp loss = 0.0\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.4211263656616211, recon loss = 0.4211263656616211, supp loss = 0.0\n",
      "Step 0: loss = 0.5361911654472351, recon loss = 0.5361911654472351, supp loss = 0.0\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.43628165125846863, recon loss = 0.43628165125846863, supp loss = 0.0\n",
      "Step 0: loss = 0.5357764959335327, recon loss = 0.5357764959335327, supp loss = 0.0\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.40461087226867676, recon loss = 0.40461087226867676, supp loss = 0.0\n",
      "Step 0: loss = 0.5469390153884888, recon loss = 0.5469390153884888, supp loss = 0.0\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.42312514781951904, recon loss = 0.42312514781951904, supp loss = 0.0\n",
      "Step 0: loss = 0.5571380853652954, recon loss = 0.5571380853652954, supp loss = 0.0\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.39402514696121216, recon loss = 0.39402514696121216, supp loss = 0.0\n",
      "Step 0: loss = 0.5221155285835266, recon loss = 0.5221155285835266, supp loss = 0.0\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.42049360275268555, recon loss = 0.42049360275268555, supp loss = 0.0\n",
      "Step 0: loss = 0.5354539155960083, recon loss = 0.5354539155960083, supp loss = 0.0\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.42465558648109436, recon loss = 0.42465558648109436, supp loss = 0.0\n",
      "Step 0: loss = 0.5335597395896912, recon loss = 0.5335597395896912, supp loss = 0.0\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.41361671686172485, recon loss = 0.41361671686172485, supp loss = 0.0\n",
      "Step 0: loss = 0.5320206880569458, recon loss = 0.5320206880569458, supp loss = 0.0\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.39464491605758667, recon loss = 0.39464491605758667, supp loss = 0.0\n",
      "Step 0: loss = 0.5679604411125183, recon loss = 0.5679604411125183, supp loss = 0.0\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.3843154013156891, recon loss = 0.3843154013156891, supp loss = 0.0\n",
      "Step 0: loss = 0.54737389087677, recon loss = 0.54737389087677, supp loss = 0.0\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.4179891347885132, recon loss = 0.4179891347885132, supp loss = 0.0\n",
      "Step 0: loss = 0.5484305024147034, recon loss = 0.5484305024147034, supp loss = 0.0\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.408954918384552, recon loss = 0.408954918384552, supp loss = 0.0\n",
      "Step 0: loss = 0.5350269675254822, recon loss = 0.5350269675254822, supp loss = 0.0\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.3750723898410797, recon loss = 0.3750723898410797, supp loss = 0.0\n",
      "Step 0: loss = 0.5464074611663818, recon loss = 0.5464074611663818, supp loss = 0.0\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.4284130334854126, recon loss = 0.4284130334854126, supp loss = 0.0\n",
      "Step 0: loss = 0.5413212776184082, recon loss = 0.5413212776184082, supp loss = 0.0\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.41088318824768066, recon loss = 0.41088318824768066, supp loss = 0.0\n",
      "Step 0: loss = 0.5534234046936035, recon loss = 0.5534234046936035, supp loss = 0.0\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.39110225439071655, recon loss = 0.39110225439071655, supp loss = 0.0\n",
      "Step 0: loss = 0.5382544994354248, recon loss = 0.5382544994354248, supp loss = 0.0\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.4005512595176697, recon loss = 0.4005512595176697, supp loss = 0.0\n",
      "Step 0: loss = 0.5495636463165283, recon loss = 0.5495636463165283, supp loss = 0.0\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.37299755215644836, recon loss = 0.37299755215644836, supp loss = 0.0\n",
      "Step 0: loss = 0.5208603143692017, recon loss = 0.5208603143692017, supp loss = 0.0\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.3801218271255493, recon loss = 0.3801218271255493, supp loss = 0.0\n",
      "Step 0: loss = 0.5440890192985535, recon loss = 0.5440890192985535, supp loss = 0.0\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.38982900977134705, recon loss = 0.38982900977134705, supp loss = 0.0\n",
      "Step 0: loss = 0.5370023250579834, recon loss = 0.5370023250579834, supp loss = 0.0\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.3891181945800781, recon loss = 0.3891181945800781, supp loss = 0.0\n",
      "Step 0: loss = 0.5494783520698547, recon loss = 0.5494783520698547, supp loss = 0.0\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.39468374848365784, recon loss = 0.39468374848365784, supp loss = 0.0\n",
      "Step 0: loss = 0.547770619392395, recon loss = 0.547770619392395, supp loss = 0.0\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.3805350065231323, recon loss = 0.3805350065231323, supp loss = 0.0\n",
      "Step 0: loss = 0.5260554552078247, recon loss = 0.5260554552078247, supp loss = 0.0\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.3726755976676941, recon loss = 0.3726755976676941, supp loss = 0.0\n",
      "Step 0: loss = 0.5291498899459839, recon loss = 0.5291498899459839, supp loss = 0.0\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.3616214692592621, recon loss = 0.3616214692592621, supp loss = 0.0\n",
      "Step 0: loss = 0.5178523063659668, recon loss = 0.5178523063659668, supp loss = 0.0\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.3836846649646759, recon loss = 0.3836846649646759, supp loss = 0.0\n",
      "Step 0: loss = 0.5520641803741455, recon loss = 0.5520641803741455, supp loss = 0.0\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.40408098697662354, recon loss = 0.40408098697662354, supp loss = 0.0\n",
      "Step 0: loss = 0.5383238196372986, recon loss = 0.5383238196372986, supp loss = 0.0\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.38884401321411133, recon loss = 0.38884401321411133, supp loss = 0.0\n",
      "Step 0: loss = 0.5160590410232544, recon loss = 0.5160590410232544, supp loss = 0.0\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.3740857243537903, recon loss = 0.3740857243537903, supp loss = 0.0\n",
      "Step 0: loss = 0.5489023923873901, recon loss = 0.5489023923873901, supp loss = 0.0\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.37671852111816406, recon loss = 0.37671852111816406, supp loss = 0.0\n",
      "Step 0: loss = 0.5391459465026855, recon loss = 0.5391459465026855, supp loss = 0.0\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.40356937050819397, recon loss = 0.40356937050819397, supp loss = 0.0\n",
      "Step 0: loss = 0.5524640083312988, recon loss = 0.5524640083312988, supp loss = 0.0\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.4057019352912903, recon loss = 0.4057019352912903, supp loss = 0.0\n",
      "Step 0: loss = 0.5492218136787415, recon loss = 0.5492218136787415, supp loss = 0.0\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.3739245533943176, recon loss = 0.3739245533943176, supp loss = 0.0\n",
      "Step 0: loss = 0.5648807287216187, recon loss = 0.5648807287216187, supp loss = 0.0\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.3730887770652771, recon loss = 0.3730887770652771, supp loss = 0.0\n",
      "Step 0: loss = 0.52293860912323, recon loss = 0.52293860912323, supp loss = 0.0\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.3932236135005951, recon loss = 0.3932236135005951, supp loss = 0.0\n",
      "Step 0: loss = 0.5362192392349243, recon loss = 0.5362192392349243, supp loss = 0.0\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.3476845324039459, recon loss = 0.3476845324039459, supp loss = 0.0\n",
      "Step 0: loss = 0.5223602652549744, recon loss = 0.5223602652549744, supp loss = 0.0\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.36167609691619873, recon loss = 0.36167609691619873, supp loss = 0.0\n",
      "Step 0: loss = 0.5365090370178223, recon loss = 0.5365090370178223, supp loss = 0.0\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.37149667739868164, recon loss = 0.37149667739868164, supp loss = 0.0\n",
      "Step 0: loss = 0.5329421162605286, recon loss = 0.5329421162605286, supp loss = 0.0\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.3787074089050293, recon loss = 0.3787074089050293, supp loss = 0.0\n",
      "Step 0: loss = 0.5242352485656738, recon loss = 0.5242352485656738, supp loss = 0.0\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.36335036158561707, recon loss = 0.36335036158561707, supp loss = 0.0\n",
      "Step 0: loss = 0.5216166377067566, recon loss = 0.5216166377067566, supp loss = 0.0\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.3692753314971924, recon loss = 0.3692753314971924, supp loss = 0.0\n",
      "Step 0: loss = 0.5166130065917969, recon loss = 0.5166130065917969, supp loss = 0.0\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.37235385179519653, recon loss = 0.37235385179519653, supp loss = 0.0\n",
      "Step 0: loss = 0.5273212194442749, recon loss = 0.5273212194442749, supp loss = 0.0\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.3473671078681946, recon loss = 0.3473671078681946, supp loss = 0.0\n",
      "Step 0: loss = 0.5160890817642212, recon loss = 0.5160890817642212, supp loss = 0.0\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.37658512592315674, recon loss = 0.37658512592315674, supp loss = 0.0\n",
      "Step 0: loss = 0.5534236431121826, recon loss = 0.5534236431121826, supp loss = 0.0\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.3577668368816376, recon loss = 0.3577668368816376, supp loss = 0.0\n",
      "Step 0: loss = 0.5391830205917358, recon loss = 0.5391830205917358, supp loss = 0.0\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.3593124747276306, recon loss = 0.3593124747276306, supp loss = 0.0\n",
      "Step 0: loss = 0.5114463567733765, recon loss = 0.5114463567733765, supp loss = 0.0\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.3905816674232483, recon loss = 0.3905816674232483, supp loss = 0.0\n",
      "Step 0: loss = 0.5276695489883423, recon loss = 0.5276695489883423, supp loss = 0.0\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.34998565912246704, recon loss = 0.34998565912246704, supp loss = 0.0\n",
      "Step 0: loss = 0.536384105682373, recon loss = 0.536384105682373, supp loss = 0.0\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.35049012303352356, recon loss = 0.35049012303352356, supp loss = 0.0\n",
      "Step 0: loss = 0.5454021692276001, recon loss = 0.5454021692276001, supp loss = 0.0\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.35012874007225037, recon loss = 0.35012874007225037, supp loss = 0.0\n",
      "Step 0: loss = 0.5329641103744507, recon loss = 0.5329641103744507, supp loss = 0.0\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.3460300862789154, recon loss = 0.3460300862789154, supp loss = 0.0\n",
      "Step 0: loss = 0.5044606924057007, recon loss = 0.5044606924057007, supp loss = 0.0\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.3274235427379608, recon loss = 0.3274235427379608, supp loss = 0.0\n",
      "Step 0: loss = 0.5690267086029053, recon loss = 0.5690267086029053, supp loss = 0.0\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.34682145714759827, recon loss = 0.34682145714759827, supp loss = 0.0\n",
      "Step 0: loss = 0.5032373666763306, recon loss = 0.5032373666763306, supp loss = 0.0\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.36522388458251953, recon loss = 0.36522388458251953, supp loss = 0.0\n",
      "Step 0: loss = 0.5480072498321533, recon loss = 0.5480072498321533, supp loss = 0.0\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.3301016688346863, recon loss = 0.3301016688346863, supp loss = 0.0\n",
      "Step 0: loss = 0.5188256502151489, recon loss = 0.5188256502151489, supp loss = 0.0\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.32685789465904236, recon loss = 0.32685789465904236, supp loss = 0.0\n",
      "Step 0: loss = 0.5156031250953674, recon loss = 0.5156031250953674, supp loss = 0.0\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.3405306041240692, recon loss = 0.3405306041240692, supp loss = 0.0\n",
      "Step 0: loss = 0.5143288969993591, recon loss = 0.5143288969993591, supp loss = 0.0\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.3404829502105713, recon loss = 0.3404829502105713, supp loss = 0.0\n",
      "Step 0: loss = 0.5337139368057251, recon loss = 0.5337139368057251, supp loss = 0.0\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.3114236891269684, recon loss = 0.3114236891269684, supp loss = 0.0\n",
      "Step 0: loss = 0.507530689239502, recon loss = 0.507530689239502, supp loss = 0.0\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.3116111755371094, recon loss = 0.3116111755371094, supp loss = 0.0\n",
      "Step 0: loss = 0.527229905128479, recon loss = 0.527229905128479, supp loss = 0.0\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.30508682131767273, recon loss = 0.30508682131767273, supp loss = 0.0\n",
      "Step 0: loss = 0.512191891670227, recon loss = 0.512191891670227, supp loss = 0.0\n",
      "\n",
      "Epoch 151\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "fine_tune_epochs = 3\n",
    "batch_size = 32\n",
    "support_batch_size = 300\n",
    "alpha0 = 4\n",
    "nalpha = alpha0\n",
    "for i in range(epochs):\n",
    "  nalpha = min(nalpha * 1.02, 60)\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs, supp_vs, supp_ys) in enumerate(batch(X, betas, random_dirs, supps_b, batch_size, support_batch_size)):\n",
    "    # print(supp_vs.shape, supp_ys.shape)\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs, supp_vs, supp_ys, 1.0, 0.0)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon loss = {loss_vals[1].numpy()}, supp loss = {loss_vals[2].numpy()}\")\n",
    "    for j in range(fine_tune_epochs):\n",
    "        loss_vals = train_step(vae, batch_betas, tf.linalg.normalize(batch_dirs + np.random.randn(*batch_dirs.shape)*0.1, axis=-1)[0], supp_vs, supp_ys, 1.0, 0.0)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon loss = {loss_vals[1].numpy()}, supp loss = {loss_vals[2].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHF0P4eISAH5"
   },
   "outputs": [],
   "source": [
    "vae.save_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JkrFRN7uOlf",
    "outputId": "90f21636-cdec-4d1e-eb4f-405a557a43b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7dfda7a63c70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS8sKCRfgUoH",
    "outputId": "b7f487ba-8083-4757-b9ea-250fd343a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  chem-checkpoint.zip\n",
      "   creating: my_checkpoint/\n",
      "  inflating: my_checkpoint/chekpont.index  \n",
      "  inflating: my_checkpoint/.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/checkpoint  \n",
      "  inflating: my_checkpoint/chekpont.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/.index    \n"
     ]
    }
   ],
   "source": [
    "# !zip -r my_checkpoint.zip my_checkpoint/\n",
    "!unzip chem-checkpoint.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EbHImoN2QaKX"
   },
   "outputs": [],
   "source": [
    "drawn_random_dirs = np.random.randn(50_000, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "\n",
    "def posterior_sampling(model, betas, random_dirs, num_samples=1):\n",
    "  zm, zlv, z = model.encoder((betas, random_dirs))\n",
    "  if num_samples == 1:\n",
    "      return model.decoder((z, random_dirs))\n",
    "  else:\n",
    "      samples = [sampling((zm, zlv)) for _ in range(num_samples)]\n",
    "      return tf.concat([model.decoder((sm, random_dirs))[:, None, :] for sm in samples], axis=1)\n",
    "\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs1 = np.random.randn(num_samples, d)\n",
    "  random_dirs2 = np.random.randn(num_samples, d)\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))    \n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m drawn_betas, dir1, drawn_betas2 \u001b[38;5;241m=\u001b[39m generate_new_betas(\u001b[43mvae\u001b[49m, \u001b[38;5;241m50_000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# generate_new_betas\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "drawn_betas, dir1, drawn_betas2 = generate_new_betas(vae, 50_000)\n",
    "# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\n",
    "# generate_new_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 200\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd1 = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "# prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample][0])\n",
    "prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([-0.00631047,  0.12106761,  0.46212596, ...,  0.47387064,\n",
       "        -0.281489  , -0.05550657], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check agreement between vae and training samples\n",
    "from sklearn.metrics import jaccard_score\n",
    "def agreement(y_pred1, y_pred2, y_true):\n",
    "    tp1 = np.float32(y_pred1==1) * np.float32(y_true==1)\n",
    "    fp1 = np.float32(y_pred1==1) * np.float32(y_true==0)\n",
    "    tn1 = np.float32(y_pred1==0) * np.float32(y_true==0)\n",
    "    fn1 = np.float32(y_pred1==0) * np.float32(y_true==1)\n",
    "\n",
    "    tp2 = np.float32(y_pred2==1) * np.float32(y_true==1)\n",
    "    fp2 = np.float32(y_pred2==1) * np.float32(y_true==0)\n",
    "    tn2 = np.float32(y_pred2==0) * np.float32(y_true==0)\n",
    "    fn2 = np.float32(y_pred2==0) * np.float32(y_true==1)\n",
    "    print(np.sum(tp1)/len(tp1), np.sum(fp1)/len(tp1), np.sum(tn1)/len(tp1), np.sum(fn1)/len(tp1))\n",
    "    print(np.sum(tp2)/len(tp1), np.sum(fp2)/len(tp1), np.sum(tn2)/len(tp1), np.sum(fn2)/len(tp1))\n",
    "    print(np.sum(fp1==fp2)/len(tp1), np.sum(fn1==fn2)/len(tp1))\n",
    "    return jaccard_score(tp1, tp2), jaccard_score(fp1, fp2), jaccard_score(tn1, tn2), jaccard_score(fn1, fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5735723218007504 0.0029178824510212586 0.4210087536473531 0.0025010421008753647\n",
      "0.5731554814506045 0.0025010421008753647 0.42142559399749896 0.0029178824510212586\n",
      "0.9979157982492706 0.9979157982492706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9963715529753265,\n",
       " 0.4444444444444444,\n",
       " 0.9950641658440277,\n",
       " 0.4444444444444444)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement(prd1, prd2, Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 10, 4081])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0054231044>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps= betas\n",
    "oups = drawn_betas\n",
    "tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(tf.expand_dims(inps, axis=1)[:, :, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(oups[:, :, 1:], axis=-1)[0], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.012491584>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.abs(tf.cast(tf.expand_dims(inps, axis=1)[:, :, :1], dtype=tf.float32)-oups[:, :, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([ 0.01558006,  0.08848727,  0.02163708, ...,  0.32644653,\n",
       "        -0.05755342,  0.21104604], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IHicI0RhbDe",
    "outputId": "32426975-705a-4668-f061-08acd65bc48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.99283946>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas1, drawn_betas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOFI9xq2b7_3",
    "outputId": "a11e5724-0e2a-428d-a65a-28064209e6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4081), dtype=float32, numpy=\n",
       "array([[ 3.5679474e-02, -8.5116491e-02, -2.7868379e-02, ...,\n",
       "         3.8362685e-01,  6.8785306e-03,  5.8818167e-01],\n",
       "       [ 2.0887703e-02,  5.4998733e-02,  2.2995186e-01, ...,\n",
       "         1.0724969e-01,  3.8565136e-04,  3.8573799e-01],\n",
       "       [-6.5227631e-03,  2.0269346e-01,  4.0428180e-01, ...,\n",
       "        -4.8758507e-01, -8.2871839e-02,  3.4979448e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFCgGG6jgkLL",
    "outputId": "5e799c27-fff1-41fc-d71f-3fc7ea88538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13627878, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "var = tf.math.reduce_variance(drawn_betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxnzX5lJQtjX"
   },
   "outputs": [],
   "source": [
    "np.mean(drawn_betas1 @ tf.transpose(drawn_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "TfAej5fqsfHh",
    "outputId": "fd888e0d-c076-4577-be0d-4de29be77dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ood_val_features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_data_to_features\u001b[49m(ood_val_data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#ood_test_features = convert_data_to_features(ood_test_data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ood_val_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m ood_val_data])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "#ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U07oDyUdsl1u"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ood_val_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m external_X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mood_val_features\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m external_Y \u001b[38;5;241m=\u001b[39m ood_val_labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ood_val_features' is not defined"
     ]
    }
   ],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "external_Y = ood_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "external_X = (external_X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "1a114f68-6f8b-4ede-b395-df1c2dfa2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      " -0.03134901]\n",
      "tf.Tensor(\n",
      "[-0.13125554  0.30866534  0.63319725 -0.5780234  -0.21648076 -0.3846287\n",
      "  0.99320394 -0.1799166   0.678208    0.62888056], shape=(10,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "5ec19308-4552-4326-ec47-b86470a2f7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 1024)\n",
      "(937, 2040)\n",
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "ae104348-bf03-48d8-a6c3-d3112b1dcb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " ...\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W32O3S_gKnEp",
    "outputId": "8797bdae-deb8-43ed-ef0c-cbf43fb4aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 512) (937, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 20, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  # wghts = np.ones_like(projs > thresh[:, None])\n",
    "  wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return *aggregate_preds(preds), preds\n",
    "\n",
    "\n",
    "# drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(randfeats_X, X, random_dirs[:512], betas[:512]) # 0.622\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs, betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, dir1, drawn_betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas1) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, betas) # 0.634\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, drawn_betas) # 0.85\n",
    "# ext_preds, mp_rand, sp_rand, pred = get_preds_and_aggregate(external_randfeats_X, betas) # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOyhZXJBrhaj",
    "outputId": "98cc9eb0-2e98-4b38-e823-337411f61e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0], tf.transpose(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "a9702828-7bea-436e-de46-89ca03c30b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.767455  ,  0.1001012 , -0.54182774, ..., -6.8707843 ,\n",
       "         7.257238  , -0.5050444 ], dtype=float32),\n",
       " array([-0.0186294 ,  0.06830448, -0.27256313, ..., -0.72351612,\n",
       "         0.81710885,  0.05805234]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "2fac824c-8637-4a35-8dfb-7b60045b10b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mext_preds\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ext_preds' is not defined"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "06eb05c6-f5f0-4d58-a8a2-ad76ee40b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [False  True False False  True False False False False False]\n",
      "Total Positive Preds:  421\n",
      "Total Preds:  937\n",
      "% Positive Preds:  0.44930629669156885\n",
      "\n",
      "First 10 Ground Truth:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Total Positive Ground Truth:  421.0\n",
      "Total Ground Truth:  937\n",
      "% Positive Ground Truth:  0.44930629669156885\n",
      "\n",
      "Accuracy:  0.6520811099252934\n"
     ]
    }
   ],
   "source": [
    "# testing_Y = Y\n",
    "ext_preds = ext_preds > 0.5\n",
    "testing_Y = external_Y\n",
    "\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", testing_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(testing_Y))\n",
    "print(\"Total Ground Truth: \", len(testing_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(testing_Y) / len(testing_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == testing_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "L7kl6J3wLped",
    "outputId": "a5d20ac1-f339-47e4-cf12-18073bb4b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f56b4751160>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.2, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8571428571428571, 0.8536585365853658, 0.7857142857142857, 0.81, 0.7898550724637681, 0.7909604519774012, 0.7813953488372093, 0.7649402390438247, 0.738831615120275, 0.7284345047923323, 0.7308781869688386, 0.7222222222222222, 0.7126696832579186, 0.7034764826175869, 0.6950998185117967, 0.6829268292682927, 0.6651917404129793, 0.6557591623036649, 0.6211312700106724]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f309823c2e0>]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_preds)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
