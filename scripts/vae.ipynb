{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f93bc20-5d8e-45f0-b6a1-d2192f5cda9f",
   "metadata": {},
   "source": [
    "vae architecture (option1 not very effective likely since we are learning v -> beta and not a distribution over it)\n",
    "head1: structure ae (request code)\n",
    "motivation: if some class separation is achieved in the latent space; any subset of support points can also be separated using a linear classifier like svm\n",
    "\n",
    "head 2: vq-vae trained on betas\n",
    "    - training input (encoder): directions, and betas, self-expressive support points' (m) latent vectors for each class (found from the original labelled dataset directly during training)\n",
    "    - inference input (decoder): latent vector (z sampled from normal dist or vq vae method), direction\n",
    "\n",
    "losses\n",
    "- latent space classification loss - basically maximise pairwise projections between opposite support points like an svm optimisation loss in the latent space of beta z's (containing both direction and bias component in latent space) and support vector z's\n",
    "\n",
    "motivation: this ensures that the semantic nature of the beta (i.e. classification in latent space still separates support points) is preserved in the latent space and the decoder is a good \"inverse\" of the encoder function \n",
    "\n",
    "important note: during training the latent z(mu(x, v), sigma(x, v)) actually gets the direction information through the encoder so it should be conditioned enough to adjust based on latent space classification loss (just approximate using the same monte carlo minimisation technique). If not replace with a slightly complex posterior i.e. Gaussian mixture models.\n",
    "\n",
    "- recons loss\n",
    "- kl loss\n",
    "\n",
    "RFF reduce feature dimensions using rfsvm literature\n",
    "\n",
    "use the p(z|y) variant of conditional autoencoders y being direction; read the amortized shared latent paper to get any ideas to implement it\n",
    "\n",
    "test on mainly drug and medical datasets use binary for now; then move to multi-class\n",
    "\n",
    "for multi class use multiple vaes enforce using similar losses for each class' set of support points; train simultaneously all heads i.e. optimise using same loss same gradient step but no shared weights but same summed loss across all classes\n",
    "\n",
    "for image datasets use any pre-trained classification network's non linear features from second last dense layer (before softmax) then follow same process as above using an MLP based VAE\n",
    "\n",
    "how to handle imbalanced datasets??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f76248-23a4-4195-ab93-80f414e6e595",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb941652-3506-4b1f-b319-ceb96b57dfb3",
   "metadata": {},
   "source": [
    "Approach 2 - better likely to work\n",
    "\n",
    "learn beta|v deterministically i.e. maybe modify vae into an autoencoder and then learn the mapping function\n",
    "\n",
    "after that explore the wide literature on few shot task as a meta learning instance to learn betas for unknown direction\n",
    "\n",
    "use two loss : reconstruction loss (target consistency) l_r, support points' margin (or full data subset in mini-batches) classification loss (data consistency loss)\n",
    "\n",
    "weight the network gradients for the two losses using and minimise end to end\n",
    "\n",
    "test mostly on binary and tabular data and once works well expand to multi class and image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7faf6-f3c1-46a1-8510-326ff233f733",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
