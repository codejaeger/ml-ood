{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:40:55.146842: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 19:40:56.128639: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-29 19:40:56.128757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-29 19:40:56.128771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "20637ffb-f819-4f0a-b926-d24784746a9e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkF1N4olXTtZ",
    "outputId": "de7467ba-69c5-42b4-a918-755fe87765d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "weKR7jCuMvQl"
   },
   "outputs": [],
   "source": [
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataX = np.float32(np.array([line.strip().split(',')[2:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataY = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "X = dataX\n",
    "Y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "2948628d-0085-48ba-8e9b-4bbe93f27e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5997, 1024)\n",
      "(5997,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, d, D=128):\n",
    "\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3S8unT73bEtM"
   },
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print((prd[:, 0]==Y).mean())\n",
    "  return wgts\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    beta = linear_coefs(get_rand_feats(X_sub@pca_projs, model), Y_sub)\n",
    "    # print(beta.shape)\n",
    "      \n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997, 1024)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:41:02.248949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 19:41:02.423808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "2024-04-29 19:41:02.636893: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x8d87c80\n"
     ]
    }
   ],
   "source": [
    "s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(232.08453, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [0.05, 0.2, 0.3, 0.4, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5997, 1024), TensorShape([1024, 204]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, pca_projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "f3d09ad5-4e02-44a5-e001-fe83e3d99938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m random_dirs \u001b[38;5;241m=\u001b[39m random_dirs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(random_dirs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m betas \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_projs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m, in \u001b[0;36mget_models\u001b[0;34m(X, Y, pca_projs, dirs, model, percentile)\u001b[0m\n\u001b[1;32m     10\u001b[0m Y_sub \u001b[38;5;241m=\u001b[39m Y[X_ids]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print((X_sub@pca_projs).shape)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_coefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_rand_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;129;43m@pca_projs\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(beta.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#X_subsets.append(X_sub)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#data_ids.append(X_ids)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Y_subsets.append(Y_sub)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m betas\u001b[38;5;241m.\u001b[39mappend(beta)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mlinear_coefs\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_coefs\u001b[39m(X, Y):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    X: N x d matrix of input features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Beta: d x 1 matrix of linear coefficients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m   clf \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mscore(X, Y))\n\u001b[1;32m     12\u001b[0m   wgts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((clf\u001b[38;5;241m.\u001b[39mintercept_[:,\u001b[38;5;28;01mNone\u001b[39;00m], clf\u001b[38;5;241m.\u001b[39mcoef_))\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1354\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1351\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1352\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1353\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)))\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([n_iter_])\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:964\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    960\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X,\n\u001b[1;32m    961\u001b[0m                                      dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    963\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m--> 964\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misspmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m    972\u001b[0m n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "\n",
    "N = 2**9    # ~ 8k\n",
    "# N = 2**2    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas = get_models(X, Y, pca_projs, random_dirs, model, percentile=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "# np.save('random_dirs-chem2.npy', random_dirs)\n",
    "# np.save('betas-chem2.npy', betas)\n",
    "# np.save('Ws-chem2.npy', model.Ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "CnxEkcWwWlwn"
   },
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-chem2.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./betas-chem2.npy')))\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "model.Ws = tf.constant(np.load('./Ws-chem2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BCHvO4qeupNQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mrandom_dirs1\u001b[49m, random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.allclose(random_dirs1, random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mrandom_dirs1\u001b[49m \u001b[38;5;241m-\u001b[39m random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(random_dirs1 - random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "264b9e19-74f8-4d13-ee50-bb4796f0d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 4081)\n",
      "(512, 1024)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "15119544-cde9-4462-b565-7deb6dd61daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.01111806548776194, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "b44e65f3-b44b-40fd-e564-63e35d43997c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "def softmax_prob(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) ) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0NZNorQW0vg",
    "outputId": "afcb4822-0cda-4c13-ea68-8454473bb529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6204268292682927"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n",
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[sample], 70)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "perds = np.zeros((ex_X.shape[0],))\n",
    "cnt = np.zeros((ex_X.shape[0],))\n",
    "print(perds.shape)\n",
    "for sm in range(512):\n",
    "    X_sub, X_ids = project_and_filter(ex_X, random_dirs[sm], 10)\n",
    "    Y_sub = ex_Y[X_ids]\n",
    "    prd = softmax_prob(get_rand_feats(X_sub@pca_projs, model), betas[sm])\n",
    "    # print(prd.shape, X_ids, ex_X.shape)\n",
    "    perds[X_ids] += prd\n",
    "    cnt[X_ids] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5036899112150593"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zer_ids = np.argwhere(cnt!=0)\n",
    "veX = perds[zer_ids]\n",
    "veX /= cnt[zer_ids]\n",
    "((veX>0.5)==Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_512_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [843] != values[12].shape = [844] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m((np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m==\u001b[39mY_sub)\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_512_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [843] != values[12].shape = [844] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "print((np.mean(tf.stack(perds, axis=0), axis=0)==Y_sub).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 32\n",
    "\n",
    "# Encoder\n",
    "beta_input = layers.Input(shape=(beta_dim,))\n",
    "beta_x = layers.Dense(512, activation=tf.nn.elu)(beta_input)\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([beta_x, dir_input])\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(32, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(8, activation=tf.nn.elu)(x)\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# x = layers.Dense(8, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(32, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([beta_input, dir_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs, decoder_dir_input], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([beta_input, dir_input])[2], dir_input])\n",
    "vae = models.Model([beta_input, dir_input], outputs, name=\"vae\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = vae((betas[:1], random_dirs[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, z_mean, z_log_var, reg=0.002):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  intercp_loss = tf.reduce_mean(tf.abs(tf.cast(inputs[:, :1], dtype=tf.float32) - outputs[:, :1]))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs[:, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs[:, 1:], axis=-1)[0], axis=-1))\n",
    "  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = recon_loss + intercp_loss + reg * kl_loss\n",
    "  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0535421>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.98786676>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.01069037>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=27.492508>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_loss(betas[:1], ots[:1], tf.ones((1, 32)), tf.ones((1, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# def regul(epoch):\n",
    "#     if epoch%10<=5:\n",
    "#         return epoch*0.2\n",
    "\n",
    "def train_step(model, inputs, dir_inputs, epoch=None):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z_mean, z_log_var, z = model.encoder([inputs, dir_inputs])\n",
    "    # outputs = model.decoder([z, dir_inputs])\n",
    "    outputs = model.decoder([z, dir_inputs])\n",
    "    \n",
    "    total_loss, recon_loss, intercp_loss, kl_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0347023>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0040238>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.029460892>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.6088182>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(vae, betas[:32], random_dirs[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(betas, dirs, batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield tf.constant(betas[i:i+batch_size]), tf.constant(dirs[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch at 0x7fb77c7be7b0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch(betas, random_dirs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "a6e1ccc1-2cd0-4549-e61a-6dae8aa1062a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 1.0557571649551392, recon_loss = 0.9978055357933044, 0.03668538108468056, kl_loss = 10.633100509643555\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.4861118197441101, recon_loss = 0.4136958420276642, 0.047912001609802246, kl_loss = 12.251980781555176\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.44380447268486023, recon_loss = 0.4021216034889221, 0.02058245614171028, kl_loss = 10.550195693969727\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.43202605843544006, recon_loss = 0.3962726593017578, 0.020759059116244316, kl_loss = 7.497182846069336\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.40745413303375244, recon_loss = 0.382815420627594, 0.013343770056962967, kl_loss = 5.647459506988525\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.41078630089759827, recon_loss = 0.3814244866371155, 0.0161611158400774, kl_loss = 6.600348472595215\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.3992433249950409, recon_loss = 0.3741559386253357, 0.014022395014762878, kl_loss = 5.532481670379639\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.39961716532707214, recon_loss = 0.3716088533401489, 0.00787605345249176, kl_loss = 10.066134452819824\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.39343446493148804, recon_loss = 0.37208160758018494, 0.008744271472096443, kl_loss = 6.304300308227539\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.3875097632408142, recon_loss = 0.36938589811325073, 0.006549478508532047, kl_loss = 5.787192344665527\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.38116827607154846, recon_loss = 0.35852283239364624, 0.011204268783330917, kl_loss = 5.720586776733398\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.38279008865356445, recon_loss = 0.36265304684638977, 0.008849725127220154, kl_loss = 5.6436614990234375\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.3851858377456665, recon_loss = 0.3624613285064697, 0.011378598399460316, kl_loss = 5.672956466674805\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.37711504101753235, recon_loss = 0.35489270091056824, 0.01089465245604515, kl_loss = 5.663846015930176\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.3683202266693115, recon_loss = 0.3503150939941406, 0.007678731344640255, kl_loss = 5.163201332092285\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.37230074405670166, recon_loss = 0.3534016013145447, 0.00725664384663105, kl_loss = 5.821247100830078\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.3609278202056885, recon_loss = 0.34517553448677063, 0.005584574770182371, kl_loss = 5.0838623046875\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.355655312538147, recon_loss = 0.33734291791915894, 0.007047893479466438, kl_loss = 5.632246971130371\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.3637312650680542, recon_loss = 0.34784361720085144, 0.005828427150845528, kl_loss = 5.029604911804199\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.3470972776412964, recon_loss = 0.3310103416442871, 0.004983107093721628, kl_loss = 5.5519208908081055\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.34681612253189087, recon_loss = 0.32971492409706116, 0.005684477277100086, kl_loss = 5.70836067199707\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.33425602316856384, recon_loss = 0.31620919704437256, 0.006596915889531374, kl_loss = 5.724949359893799\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.3345266282558441, recon_loss = 0.3194196820259094, 0.004316422622650862, kl_loss = 5.395258903503418\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.32388341426849365, recon_loss = 0.3085510730743408, 0.005729062482714653, kl_loss = 4.801643371582031\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.32874995470046997, recon_loss = 0.31422409415245056, 0.0057542333379387856, kl_loss = 4.385821342468262\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.3085061013698578, recon_loss = 0.2949654459953308, 0.005350703373551369, kl_loss = 4.094979763031006\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.3092871308326721, recon_loss = 0.2974257469177246, 0.0038204099982976913, kl_loss = 4.02048397064209\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.3050065040588379, recon_loss = 0.29307663440704346, 0.0047248005867004395, kl_loss = 3.602536201477051\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.30816885828971863, recon_loss = 0.29644590616226196, 0.0059055425226688385, kl_loss = 2.908701181411743\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.3084529936313629, recon_loss = 0.298092246055603, 0.005466802045702934, kl_loss = 2.4469735622406006\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.3018844425678253, recon_loss = 0.29274845123291016, 0.004736107774078846, kl_loss = 2.199953079223633\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.29956138134002686, recon_loss = 0.29005807638168335, 0.005574958864599466, kl_loss = 1.9641730785369873\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.28907936811447144, recon_loss = 0.28221702575683594, 0.003680641995742917, kl_loss = 1.5908461809158325\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.2911606431007385, recon_loss = 0.2850210964679718, 0.003545524552464485, kl_loss = 1.297019124031067\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.28528809547424316, recon_loss = 0.2792874872684479, 0.0033528050407767296, kl_loss = 1.3239035606384277\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.2796535789966583, recon_loss = 0.272544801235199, 0.005111023783683777, kl_loss = 0.9988645911216736\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.28652656078338623, recon_loss = 0.2808927297592163, 0.00391354551538825, kl_loss = 0.8601396083831787\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.2813900113105774, recon_loss = 0.2763778567314148, 0.003472276497632265, kl_loss = 0.7699471712112427\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.2772420346736908, recon_loss = 0.2719118595123291, 0.003967168740928173, kl_loss = 0.6815024018287659\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.28257834911346436, recon_loss = 0.2783973813056946, 0.0028799623250961304, kl_loss = 0.6504949331283569\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.27271726727485657, recon_loss = 0.2681185305118561, 0.0033866404555737972, kl_loss = 0.6060508489608765\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.27390795946121216, recon_loss = 0.26937344670295715, 0.0035569798201322556, kl_loss = 0.4887741208076477\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.28027164936065674, recon_loss = 0.27597424387931824, 0.003336837748065591, kl_loss = 0.48027312755584717\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.2628431022167206, recon_loss = 0.25943315029144287, 0.002612645272165537, kl_loss = 0.39864903688430786\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.2699376344680786, recon_loss = 0.26578736305236816, 0.003426298964768648, kl_loss = 0.3619798421859741\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.27310603857040405, recon_loss = 0.2690291404724121, 0.003286504652351141, kl_loss = 0.3951955735683441\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.271523654460907, recon_loss = 0.2674708962440491, 0.0034001506865024567, kl_loss = 0.3263096511363983\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.26106104254722595, recon_loss = 0.2579156458377838, 0.0025743194855749607, kl_loss = 0.2855343818664551\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.2687114477157593, recon_loss = 0.2652779817581177, 0.002815030049532652, kl_loss = 0.3092087507247925\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.26078179478645325, recon_loss = 0.2577401399612427, 0.00256174523383379, kl_loss = 0.23995524644851685\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.269197016954422, recon_loss = 0.26611995697021484, 0.0024765105918049812, kl_loss = 0.30027347803115845\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.26669973134994507, recon_loss = 0.2638128101825714, 0.0023743663914501667, kl_loss = 0.25627401471138\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.26831191778182983, recon_loss = 0.2658141553401947, 0.00210392102599144, kl_loss = 0.19691994786262512\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.2676209807395935, recon_loss = 0.26490938663482666, 0.002335083670914173, kl_loss = 0.18825817108154297\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.2642049193382263, recon_loss = 0.26130396127700806, 0.0026135409716516733, kl_loss = 0.14369943737983704\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.2706616520881653, recon_loss = 0.2683350443840027, 0.0020697584841400385, kl_loss = 0.1284169852733612\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.25743532180786133, recon_loss = 0.25495001673698425, 0.0022141304798424244, kl_loss = 0.13558784127235413\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.26049351692199707, recon_loss = 0.2581333816051483, 0.0020836719777435064, kl_loss = 0.1382368505001068\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.26494157314300537, recon_loss = 0.263076514005661, 0.0016535520553588867, kl_loss = 0.10576096922159195\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.25816985964775085, recon_loss = 0.2562016248703003, 0.0017469613812863827, kl_loss = 0.11064350605010986\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.25474074482917786, recon_loss = 0.25223323702812195, 0.0023353861179202795, kl_loss = 0.08605547249317169\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.2553709149360657, recon_loss = 0.25310802459716797, 0.002112114802002907, kl_loss = 0.07538606226444244\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.2599107027053833, recon_loss = 0.2573477327823639, 0.0023815000895410776, kl_loss = 0.09072813391685486\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.26118913292884827, recon_loss = 0.25893712043762207, 0.002092952374368906, kl_loss = 0.07953457534313202\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.2526770532131195, recon_loss = 0.25044354796409607, 0.002099697943776846, kl_loss = 0.06691299378871918\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.2570544183254242, recon_loss = 0.255113422870636, 0.0018372511258348823, kl_loss = 0.05187433958053589\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.26188477873802185, recon_loss = 0.25945910811424255, 0.0023416851181536913, kl_loss = 0.0419924333691597\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.2535114586353302, recon_loss = 0.25113222002983093, 0.0022728994954377413, kl_loss = 0.05316556990146637\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.2567835748195648, recon_loss = 0.25458765029907227, 0.002124985447153449, kl_loss = 0.0354653000831604\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.25609874725341797, recon_loss = 0.2541229724884033, 0.001905375742353499, kl_loss = 0.03520231693983078\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.2605128884315491, recon_loss = 0.2584605813026428, 0.0019854484125971794, kl_loss = 0.033429402858018875\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.2568225562572479, recon_loss = 0.25500643253326416, 0.0016994958277791739, kl_loss = 0.05830356106162071\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.2500491738319397, recon_loss = 0.24806860089302063, 0.001912096980959177, kl_loss = 0.03424263745546341\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.25776034593582153, recon_loss = 0.25556015968322754, 0.0021231737919151783, kl_loss = 0.038507964462041855\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.25576838850975037, recon_loss = 0.25420570373535156, 0.0015070091467350721, kl_loss = 0.027840182185173035\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.2554703950881958, recon_loss = 0.2533389627933502, 0.0020887895952910185, kl_loss = 0.021329648792743683\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.2513831555843353, recon_loss = 0.24972188472747803, 0.0016192016191780567, kl_loss = 0.02104482427239418\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.2510289251804352, recon_loss = 0.24893635511398315, 0.002058187033981085, kl_loss = 0.01720213145017624\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.2519778907299042, recon_loss = 0.249771386384964, 0.0021443464793264866, kl_loss = 0.031090399250388145\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.2521684169769287, recon_loss = 0.25050240755081177, 0.0016169112641364336, kl_loss = 0.024535249918699265\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.2538345158100128, recon_loss = 0.2521415948867798, 0.0016549554420635104, kl_loss = 0.018977787345647812\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.2524869441986084, recon_loss = 0.250559002161026, 0.0019029707182198763, kl_loss = 0.012479811906814575\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.255901038646698, recon_loss = 0.2541657090187073, 0.0016956798499450088, kl_loss = 0.019811231642961502\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.2551528513431549, recon_loss = 0.253616601228714, 0.0015102545730769634, kl_loss = 0.013000044040381908\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.25435879826545715, recon_loss = 0.2524919807910919, 0.001849489868618548, kl_loss = 0.008656410500407219\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.25260496139526367, recon_loss = 0.25117990374565125, 0.001402067020535469, kl_loss = 0.011491379700601101\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.2568916976451874, recon_loss = 0.2551436126232147, 0.001725677982904017, kl_loss = 0.011206639930605888\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.2527000308036804, recon_loss = 0.25120997428894043, 0.0014745935332030058, kl_loss = 0.0077303145080804825\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.2502429783344269, recon_loss = 0.24882608652114868, 0.0013984263641759753, kl_loss = 0.009240029379725456\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.2492447942495346, recon_loss = 0.24744856357574463, 0.001779037294909358, kl_loss = 0.008596187457442284\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.25144869089126587, recon_loss = 0.25013917684555054, 0.001295032911002636, kl_loss = 0.007244741544127464\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.2519048750400543, recon_loss = 0.25024962425231934, 0.0016042443457990885, kl_loss = 0.02549797296524048\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.25305426120758057, recon_loss = 0.25125277042388916, 0.0017718158196657896, kl_loss = 0.014841496013104916\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.2527158260345459, recon_loss = 0.2513946294784546, 0.0013066072715446353, kl_loss = 0.007306922227144241\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.255160391330719, recon_loss = 0.253561794757843, 0.0015853721415624022, kl_loss = 0.006618029437959194\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.2508867681026459, recon_loss = 0.2493349015712738, 0.0015372948255389929, kl_loss = 0.007291002199053764\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.25022101402282715, recon_loss = 0.2486191689968109, 0.0015891013899818063, kl_loss = 0.00637776218354702\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.25668978691101074, recon_loss = 0.25497108697891235, 0.0017043441766873002, kl_loss = 0.007182719185948372\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.24983830749988556, recon_loss = 0.24830032885074615, 0.0015205764211714268, kl_loss = 0.008701031096279621\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.2552509605884552, recon_loss = 0.25448131561279297, 0.0007545248372480273, kl_loss = 0.007560589350759983\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.24633565545082092, recon_loss = 0.24445492029190063, 0.0018649597186595201, kl_loss = 0.007891232147812843\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.2517196238040924, recon_loss = 0.2505779266357422, 0.0011282290797680616, kl_loss = 0.006737212650477886\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.24995970726013184, recon_loss = 0.24846917390823364, 0.0014671520330011845, kl_loss = 0.011686359532177448\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.2501154839992523, recon_loss = 0.2484963983297348, 0.0015990224201232195, kl_loss = 0.01002736296504736\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.25289055705070496, recon_loss = 0.251340389251709, 0.001530088484287262, kl_loss = 0.010050135664641857\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.25133052468299866, recon_loss = 0.24971452355384827, 0.0016012329142540693, kl_loss = 0.007387310266494751\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.24546320736408234, recon_loss = 0.24367758631706238, 0.001773615600541234, kl_loss = 0.006006995216012001\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.253172904253006, recon_loss = 0.25189515948295593, 0.0012596469605341554, kl_loss = 0.009050407446920872\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.24907344579696655, recon_loss = 0.2482311725616455, 0.0008295084116980433, kl_loss = 0.006387039087712765\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.25068598985671997, recon_loss = 0.24857720732688904, 0.0020975987426936626, kl_loss = 0.00558609701693058\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.2536584436893463, recon_loss = 0.25162094831466675, 0.002015689853578806, kl_loss = 0.010903211310505867\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.25684821605682373, recon_loss = 0.25498148798942566, 0.001850777305662632, kl_loss = 0.007971878163516521\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.25088751316070557, recon_loss = 0.24898763000965118, 0.0018115033162757754, kl_loss = 0.0441976897418499\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.24651499092578888, recon_loss = 0.24518752098083496, 0.001303979428485036, kl_loss = 0.011738897301256657\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.2479780614376068, recon_loss = 0.24647581577301025, 0.0014836846385151148, kl_loss = 0.009282166138291359\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.25574347376823425, recon_loss = 0.25433528423309326, 0.0013937200419604778, kl_loss = 0.007246708497405052\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.2497558742761612, recon_loss = 0.24817797541618347, 0.0015649074921384454, kl_loss = 0.006496800109744072\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.25009557604789734, recon_loss = 0.24861860275268555, 0.0014605647884309292, kl_loss = 0.008216694928705692\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.24904702603816986, recon_loss = 0.2478257715702057, 0.001212563831359148, kl_loss = 0.004340094514191151\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.24576900899410248, recon_loss = 0.24425148963928223, 0.001497215940617025, kl_loss = 0.010157091543078423\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.25195610523223877, recon_loss = 0.25067177414894104, 0.001267948537133634, kl_loss = 0.008188427425920963\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.24996301531791687, recon_loss = 0.24877646565437317, 0.0011753366561606526, kl_loss = 0.0056043509393930435\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.2503717243671417, recon_loss = 0.24901971220970154, 0.0013410515384748578, kl_loss = 0.005490829236805439\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.24929064512252808, recon_loss = 0.2484154999256134, 0.0008536537643522024, kl_loss = 0.010745223611593246\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.25183555483818054, recon_loss = 0.2509121596813202, 0.0009142487542703748, kl_loss = 0.004571358673274517\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.25245732069015503, recon_loss = 0.2510349750518799, 0.001416053157299757, kl_loss = 0.0031428225338459015\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.2548747956752777, recon_loss = 0.25377920269966125, 0.0010861833579838276, kl_loss = 0.004706366918981075\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.25354868173599243, recon_loss = 0.25246286392211914, 0.0010757441632449627, kl_loss = 0.0050335777923464775\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.25112125277519226, recon_loss = 0.24936707317829132, 0.0017443326069042087, kl_loss = 0.00491217989474535\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.2464163601398468, recon_loss = 0.24483847618103027, 0.001567743718624115, kl_loss = 0.0050673214718699455\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.24831867218017578, recon_loss = 0.24728164076805115, 0.0010303179733455181, kl_loss = 0.003360660746693611\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.24661582708358765, recon_loss = 0.24522018432617188, 0.0013891158159822226, kl_loss = 0.003260219469666481\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.25149667263031006, recon_loss = 0.2504183053970337, 0.0010654856450855732, kl_loss = 0.006441643461585045\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.24065420031547546, recon_loss = 0.2396017611026764, 0.0010292887454852462, kl_loss = 0.01158139854669571\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.2523013651371002, recon_loss = 0.25128817558288574, 0.0009988055098801851, kl_loss = 0.007202150300145149\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.2525847256183624, recon_loss = 0.2515117824077606, 0.0010616052895784378, kl_loss = 0.005658046342432499\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.2433348298072815, recon_loss = 0.2421286404132843, 0.0011856945930048823, kl_loss = 0.010244590230286121\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.2485428750514984, recon_loss = 0.2472882866859436, 0.001246921019628644, kl_loss = 0.0038360487669706345\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.2476014792919159, recon_loss = 0.24588292837142944, 0.0016987868584692478, kl_loss = 0.009882883168756962\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.24826113879680634, recon_loss = 0.2473127543926239, 0.0009399006958119571, kl_loss = 0.004240760579705238\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.25052350759506226, recon_loss = 0.24918636679649353, 0.0013301478466019034, kl_loss = 0.0034971553832292557\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.23849613964557648, recon_loss = 0.23712986707687378, 0.0013596066273748875, kl_loss = 0.003332654945552349\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.24625258147716522, recon_loss = 0.24491402506828308, 0.0013318874407559633, kl_loss = 0.003337162546813488\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.25438809394836426, recon_loss = 0.25280648469924927, 0.0015720955561846495, kl_loss = 0.004758263938128948\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.2485181987285614, recon_loss = 0.24692460894584656, 0.001586153986863792, kl_loss = 0.0037198280915617943\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.24910329282283783, recon_loss = 0.2477821558713913, 0.0013114074245095253, kl_loss = 0.004868084564805031\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.25355812907218933, recon_loss = 0.2522128224372864, 0.001334492932073772, kl_loss = 0.005415461957454681\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.25310590863227844, recon_loss = 0.2517595589160919, 0.00133877107873559, kl_loss = 0.0037912456318736076\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.24743801355361938, recon_loss = 0.24613970518112183, 0.001290306681767106, kl_loss = 0.004003740847110748\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.2484993189573288, recon_loss = 0.2472645342350006, 0.0012204479426145554, kl_loss = 0.0071678077802062035\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.2499912828207016, recon_loss = 0.24891813099384308, 0.0010657982202246785, kl_loss = 0.0036748945713043213\n",
      "\n",
      "Epoch 151\n",
      "Step 0: loss = 0.2520706355571747, recon_loss = 0.25087448954582214, 0.0011829882860183716, kl_loss = 0.0065742917358875275\n",
      "\n",
      "Epoch 152\n",
      "Step 0: loss = 0.2559744417667389, recon_loss = 0.2546905279159546, 0.0012656447943300009, kl_loss = 0.009128822013735771\n",
      "\n",
      "Epoch 153\n",
      "Step 0: loss = 0.25071972608566284, recon_loss = 0.24954214692115784, 0.001167738693766296, kl_loss = 0.004917340353131294\n",
      "\n",
      "Epoch 154\n",
      "Step 0: loss = 0.24674952030181885, recon_loss = 0.24548709392547607, 0.0012515768175944686, kl_loss = 0.005426757037639618\n",
      "\n",
      "Epoch 155\n",
      "Step 0: loss = 0.2524583339691162, recon_loss = 0.2509443759918213, 0.001506582833826542, kl_loss = 0.003684709779918194\n",
      "\n",
      "Epoch 156\n",
      "Step 0: loss = 0.25031620264053345, recon_loss = 0.24935008585453033, 0.0009566133376210928, kl_loss = 0.004757438786327839\n",
      "\n",
      "Epoch 157\n",
      "Step 0: loss = 0.2483903467655182, recon_loss = 0.2470502108335495, 0.0013284131418913603, kl_loss = 0.005864289589226246\n",
      "\n",
      "Epoch 158\n",
      "Step 0: loss = 0.24699734151363373, recon_loss = 0.2458883821964264, 0.0010978044010698795, kl_loss = 0.005579531192779541\n",
      "\n",
      "Epoch 159\n",
      "Step 0: loss = 0.2551690936088562, recon_loss = 0.2540948987007141, 0.001069508958607912, kl_loss = 0.00234265998005867\n",
      "\n",
      "Epoch 160\n",
      "Step 0: loss = 0.24371309578418732, recon_loss = 0.24211415648460388, 0.001588641433045268, kl_loss = 0.005149746313691139\n",
      "\n",
      "Epoch 161\n",
      "Step 0: loss = 0.24868595600128174, recon_loss = 0.247333824634552, 0.001339672482572496, kl_loss = 0.0062250541523098946\n",
      "\n",
      "Epoch 162\n",
      "Step 0: loss = 0.2491878867149353, recon_loss = 0.2480577528476715, 0.0011164380703121424, kl_loss = 0.006846782751381397\n",
      "\n",
      "Epoch 163\n",
      "Step 0: loss = 0.25206974148750305, recon_loss = 0.25100040435791016, 0.0010631037876009941, kl_loss = 0.0031104302033782005\n",
      "\n",
      "Epoch 164\n",
      "Step 0: loss = 0.24952098727226257, recon_loss = 0.24853883683681488, 0.0009710604790598154, kl_loss = 0.0055397432297468185\n",
      "\n",
      "Epoch 165\n",
      "Step 0: loss = 0.25533899664878845, recon_loss = 0.25384828448295593, 0.001478409394621849, kl_loss = 0.006156226620078087\n",
      "\n",
      "Epoch 166\n",
      "Step 0: loss = 0.25312113761901855, recon_loss = 0.25172337889671326, 0.0013763866154477, kl_loss = 0.010683035477995872\n",
      "\n",
      "Epoch 167\n",
      "Step 0: loss = 0.2514587342739105, recon_loss = 0.2500174045562744, 0.001417157705873251, kl_loss = 0.012091929093003273\n",
      "\n",
      "Epoch 168\n",
      "Step 0: loss = 0.24795503914356232, recon_loss = 0.24632865190505981, 0.0016104239039123058, kl_loss = 0.007979261688888073\n",
      "\n",
      "Epoch 169\n",
      "Step 0: loss = 0.24853141605854034, recon_loss = 0.24759331345558167, 0.0009246526169590652, kl_loss = 0.00672938022762537\n",
      "\n",
      "Epoch 170\n",
      "Step 0: loss = 0.25226372480392456, recon_loss = 0.25094449520111084, 0.0013059962075203657, kl_loss = 0.0066114068031311035\n",
      "\n",
      "Epoch 171\n",
      "Step 0: loss = 0.24784483015537262, recon_loss = 0.24662896990776062, 0.0012040168512612581, kl_loss = 0.005919621326029301\n",
      "\n",
      "Epoch 172\n",
      "Step 0: loss = 0.2507026195526123, recon_loss = 0.24936838448047638, 0.0013271060306578875, kl_loss = 0.0035687824711203575\n",
      "\n",
      "Epoch 173\n",
      "Step 0: loss = 0.24906803667545319, recon_loss = 0.2478395402431488, 0.0012169461697340012, kl_loss = 0.005776279605925083\n",
      "\n",
      "Epoch 174\n",
      "Step 0: loss = 0.24969889223575592, recon_loss = 0.2488185316324234, 0.0008736733580008149, kl_loss = 0.0033469684422016144\n",
      "\n",
      "Epoch 175\n",
      "Step 0: loss = 0.25618976354599, recon_loss = 0.2553749680519104, 0.0008027005824260414, kl_loss = 0.006048006936907768\n",
      "\n",
      "Epoch 176\n",
      "Step 0: loss = 0.25366494059562683, recon_loss = 0.25272804498672485, 0.0009250394068658352, kl_loss = 0.0059269024059176445\n",
      "\n",
      "Epoch 177\n",
      "Step 0: loss = 0.25191730260849, recon_loss = 0.25020575523376465, 0.001705361995846033, kl_loss = 0.003101442940533161\n",
      "\n",
      "Epoch 178\n",
      "Step 0: loss = 0.24839235842227936, recon_loss = 0.24690215289592743, 0.0014817918417975307, kl_loss = 0.004206311888992786\n",
      "\n",
      "Epoch 179\n",
      "Step 0: loss = 0.24945127964019775, recon_loss = 0.24770423769950867, 0.0017292667180299759, kl_loss = 0.008890224620699883\n",
      "\n",
      "Epoch 180\n",
      "Step 0: loss = 0.2544706463813782, recon_loss = 0.25288689136505127, 0.0015701593365520239, kl_loss = 0.00679776631295681\n",
      "\n",
      "Epoch 181\n",
      "Step 0: loss = 0.25280871987342834, recon_loss = 0.25105005502700806, 0.0017481014365330338, kl_loss = 0.00527242012321949\n",
      "\n",
      "Epoch 182\n",
      "Step 0: loss = 0.24897539615631104, recon_loss = 0.24719490110874176, 0.0017497693188488483, kl_loss = 0.015365014784038067\n",
      "\n",
      "Epoch 183\n",
      "Step 0: loss = 0.2552763819694519, recon_loss = 0.25400257110595703, 0.0012664948590099812, kl_loss = 0.003652065061032772\n",
      "\n",
      "Epoch 184\n",
      "Step 0: loss = 0.2570822536945343, recon_loss = 0.2555171549320221, 0.0015565836802124977, kl_loss = 0.004259416833519936\n",
      "\n",
      "Epoch 185\n",
      "Step 0: loss = 0.24672837555408478, recon_loss = 0.24556225538253784, 0.0011598033597692847, kl_loss = 0.0031582796946167946\n",
      "\n",
      "Epoch 186\n",
      "Step 0: loss = 0.24436739087104797, recon_loss = 0.24325135350227356, 0.0011060822289437056, kl_loss = 0.004978809505701065\n",
      "\n",
      "Epoch 187\n",
      "Step 0: loss = 0.2507789731025696, recon_loss = 0.24951443076133728, 0.0012592271668836474, kl_loss = 0.0026474669575691223\n",
      "\n",
      "Epoch 188\n",
      "Step 0: loss = 0.25004059076309204, recon_loss = 0.24889108538627625, 0.001139726722612977, kl_loss = 0.0048818448558449745\n",
      "\n",
      "Epoch 189\n",
      "Step 0: loss = 0.2481697052717209, recon_loss = 0.24695296585559845, 0.0012001234572380781, kl_loss = 0.008310439065098763\n",
      "\n",
      "Epoch 190\n",
      "Step 0: loss = 0.24597056210041046, recon_loss = 0.24474883079528809, 0.001214294694364071, kl_loss = 0.003714645281434059\n",
      "\n",
      "Epoch 191\n",
      "Step 0: loss = 0.24997669458389282, recon_loss = 0.24836979806423187, 0.0015988892409950495, kl_loss = 0.004003239795565605\n",
      "\n",
      "Epoch 192\n",
      "Step 0: loss = 0.251147598028183, recon_loss = 0.25004351139068604, 0.0010908108670264482, kl_loss = 0.006634199060499668\n",
      "\n",
      "Epoch 193\n",
      "Step 0: loss = 0.24452361464500427, recon_loss = 0.2430034875869751, 0.0015055156545713544, kl_loss = 0.007309696637094021\n",
      "\n",
      "Epoch 194\n",
      "Step 0: loss = 0.2513940632343292, recon_loss = 0.2502949833869934, 0.001088468125090003, kl_loss = 0.005300345830619335\n",
      "\n",
      "Epoch 195\n",
      "Step 0: loss = 0.25060683488845825, recon_loss = 0.24963310360908508, 0.0009441656293347478, kl_loss = 0.014783415012061596\n",
      "\n",
      "Epoch 196\n",
      "Step 0: loss = 0.24433287978172302, recon_loss = 0.24312826991081238, 0.0011958737159147859, kl_loss = 0.0043652961030602455\n",
      "\n",
      "Epoch 197\n",
      "Step 0: loss = 0.25390756130218506, recon_loss = 0.2529219686985016, 0.0009785662405192852, kl_loss = 0.0035237781703472137\n",
      "\n",
      "Epoch 198\n",
      "Step 0: loss = 0.2508186995983124, recon_loss = 0.2498406171798706, 0.000971247733104974, kl_loss = 0.003415863960981369\n",
      "\n",
      "Epoch 199\n",
      "Step 0: loss = 0.2498292475938797, recon_loss = 0.24864605069160461, 0.0011768157128244638, kl_loss = 0.003189239650964737\n",
      "\n",
      "Epoch 200\n",
      "Step 0: loss = 0.25499120354652405, recon_loss = 0.25375863909721375, 0.0012268107384443283, kl_loss = 0.002882526256144047\n",
      "\n",
      "Epoch 201\n",
      "Step 0: loss = 0.2482544332742691, recon_loss = 0.2469273954629898, 0.00130917364731431, kl_loss = 0.008933731354773045\n",
      "\n",
      "Epoch 202\n",
      "Step 0: loss = 0.24909542500972748, recon_loss = 0.2473645657300949, 0.0016951046418398619, kl_loss = 0.017876707017421722\n",
      "\n",
      "Epoch 203\n",
      "Step 0: loss = 0.24231816828250885, recon_loss = 0.2410210222005844, 0.001261626253835857, kl_loss = 0.017760153859853745\n",
      "\n",
      "Epoch 204\n",
      "Step 0: loss = 0.25235626101493835, recon_loss = 0.25120583176612854, 0.0011285371147096157, kl_loss = 0.010950309224426746\n",
      "\n",
      "Epoch 205\n",
      "Step 0: loss = 0.2493649572134018, recon_loss = 0.24820348620414734, 0.0011191833764314651, kl_loss = 0.02114271931350231\n",
      "\n",
      "Epoch 206\n",
      "Step 0: loss = 0.25192129611968994, recon_loss = 0.25090426206588745, 0.0009910778608173132, kl_loss = 0.012976487167179585\n",
      "\n",
      "Epoch 207\n",
      "Step 0: loss = 0.2506157159805298, recon_loss = 0.2489747405052185, 0.0016296145040541887, kl_loss = 0.005679824389517307\n",
      "\n",
      "Epoch 208\n",
      "Step 0: loss = 0.25404006242752075, recon_loss = 0.2526351511478424, 0.001398405060172081, kl_loss = 0.003244376741349697\n",
      "\n",
      "Epoch 209\n",
      "Step 0: loss = 0.24508652091026306, recon_loss = 0.24391821026802063, 0.001139806117862463, kl_loss = 0.014253349974751472\n",
      "\n",
      "Epoch 210\n",
      "Step 0: loss = 0.2510424852371216, recon_loss = 0.24979032576084137, 0.0012341671390458941, kl_loss = 0.009001807309687138\n",
      "\n",
      "Epoch 211\n",
      "Step 0: loss = 0.25078609585762024, recon_loss = 0.24927762150764465, 0.0014934891369193792, kl_loss = 0.007499244064092636\n",
      "\n",
      "Epoch 212\n",
      "Step 0: loss = 0.25928395986557007, recon_loss = 0.2578762173652649, 0.0013984870165586472, kl_loss = 0.004632274620234966\n",
      "\n",
      "Epoch 213\n",
      "Step 0: loss = 0.24763479828834534, recon_loss = 0.24631819128990173, 0.0013094471069052815, kl_loss = 0.0035780565813183784\n",
      "\n",
      "Epoch 214\n",
      "Step 0: loss = 0.2473042905330658, recon_loss = 0.24583739042282104, 0.0014559219125658274, kl_loss = 0.00549029465764761\n",
      "\n",
      "Epoch 215\n",
      "Step 0: loss = 0.2513050436973572, recon_loss = 0.25023752450942993, 0.0010627447627484798, kl_loss = 0.0023804139345884323\n",
      "\n",
      "Epoch 216\n",
      "Step 0: loss = 0.25079604983329773, recon_loss = 0.24907280504703522, 0.0017170757055282593, kl_loss = 0.0030802227556705475\n",
      "\n",
      "Epoch 217\n",
      "Step 0: loss = 0.2530379295349121, recon_loss = 0.25153157114982605, 0.0014978658873587847, kl_loss = 0.004239899106323719\n",
      "\n",
      "Epoch 218\n",
      "Step 0: loss = 0.2508476972579956, recon_loss = 0.24941307306289673, 0.001423398731276393, kl_loss = 0.005622967146337032\n",
      "\n",
      "Epoch 219\n",
      "Step 0: loss = 0.2461274117231369, recon_loss = 0.24457435309886932, 0.0015418134862557054, kl_loss = 0.005627228878438473\n",
      "\n",
      "Epoch 220\n",
      "Step 0: loss = 0.2511036992073059, recon_loss = 0.24953143298625946, 0.0015438683331012726, kl_loss = 0.014208005741238594\n",
      "\n",
      "Epoch 221\n",
      "Step 0: loss = 0.25151821970939636, recon_loss = 0.2503681778907776, 0.0011086114682257175, kl_loss = 0.020715467631816864\n",
      "\n",
      "Epoch 222\n",
      "Step 0: loss = 0.2581135928630829, recon_loss = 0.2565275728702545, 0.0015763196861371398, kl_loss = 0.004838433116674423\n",
      "\n",
      "Epoch 223\n",
      "Step 0: loss = 0.25278469920158386, recon_loss = 0.2510518729686737, 0.0017220604931935668, kl_loss = 0.005386072210967541\n",
      "\n",
      "Epoch 224\n",
      "Step 0: loss = 0.25397613644599915, recon_loss = 0.25279882550239563, 0.0011668188963085413, kl_loss = 0.005249452777206898\n",
      "\n",
      "Epoch 225\n",
      "Step 0: loss = 0.24816200137138367, recon_loss = 0.24679803848266602, 0.0013555665500462055, kl_loss = 0.004197396337985992\n",
      "\n",
      "Epoch 226\n",
      "Step 0: loss = 0.2408991903066635, recon_loss = 0.23929011821746826, 0.0015987211372703314, kl_loss = 0.0051769232377409935\n",
      "\n",
      "Epoch 227\n",
      "Step 0: loss = 0.2509464621543884, recon_loss = 0.24962300062179565, 0.0013100020587444305, kl_loss = 0.006734814494848251\n",
      "\n",
      "Epoch 228\n",
      "Step 0: loss = 0.2510971426963806, recon_loss = 0.2490176111459732, 0.0020549166947603226, kl_loss = 0.01230388693511486\n",
      "\n",
      "Epoch 229\n",
      "Step 0: loss = 0.25063759088516235, recon_loss = 0.24937038123607635, 0.001250113476999104, kl_loss = 0.008548111654818058\n",
      "\n",
      "Epoch 230\n",
      "Step 0: loss = 0.25021839141845703, recon_loss = 0.24900145828723907, 0.0012065853225067258, kl_loss = 0.005174132063984871\n",
      "\n",
      "Epoch 231\n",
      "Step 0: loss = 0.24603670835494995, recon_loss = 0.2448793351650238, 0.0011476606596261263, kl_loss = 0.004861043766140938\n",
      "\n",
      "Epoch 232\n",
      "Step 0: loss = 0.25212037563323975, recon_loss = 0.2507912218570709, 0.0013124921824783087, kl_loss = 0.008325801230967045\n",
      "\n",
      "Epoch 233\n",
      "Step 0: loss = 0.24743758141994476, recon_loss = 0.24615129828453064, 0.0012785412836819887, kl_loss = 0.0038708066567778587\n",
      "\n",
      "Epoch 234\n",
      "Step 0: loss = 0.24791103601455688, recon_loss = 0.24631160497665405, 0.0015901216538622975, kl_loss = 0.004656528122723103\n",
      "\n",
      "Epoch 235\n",
      "Step 0: loss = 0.2535819113254547, recon_loss = 0.25208795070648193, 0.0014837170019745827, kl_loss = 0.005130873993039131\n",
      "\n",
      "Epoch 236\n",
      "Step 0: loss = 0.24460648000240326, recon_loss = 0.24310891330242157, 0.0014750165864825249, kl_loss = 0.011269052512943745\n",
      "\n",
      "Epoch 237\n",
      "Step 0: loss = 0.2517932057380676, recon_loss = 0.25033706426620483, 0.0014348931144922972, kl_loss = 0.010623385198414326\n",
      "\n",
      "Epoch 238\n",
      "Step 0: loss = 0.2429097443819046, recon_loss = 0.2416583001613617, 0.0012374112848192453, kl_loss = 0.00701752956956625\n",
      "\n",
      "Epoch 239\n",
      "Step 0: loss = 0.2447309046983719, recon_loss = 0.24312663078308105, 0.0015893271192908287, kl_loss = 0.007476300932466984\n",
      "\n",
      "Epoch 240\n",
      "Step 0: loss = 0.2509879171848297, recon_loss = 0.24954070150852203, 0.0014342240756377578, kl_loss = 0.006503459066152573\n",
      "\n",
      "Epoch 241\n",
      "Step 0: loss = 0.2411469966173172, recon_loss = 0.24004864692687988, 0.001083581941202283, kl_loss = 0.00738170649856329\n",
      "\n",
      "Epoch 242\n",
      "Step 0: loss = 0.24921345710754395, recon_loss = 0.24783514440059662, 0.0013706748140975833, kl_loss = 0.003819730132818222\n",
      "\n",
      "Epoch 243\n",
      "Step 0: loss = 0.24941274523735046, recon_loss = 0.24800561368465424, 0.0013906587846577168, kl_loss = 0.008233586326241493\n",
      "\n",
      "Epoch 244\n",
      "Step 0: loss = 0.24961049854755402, recon_loss = 0.24803563952445984, 0.0015581632032990456, kl_loss = 0.00834115594625473\n",
      "\n",
      "Epoch 245\n",
      "Step 0: loss = 0.24832545220851898, recon_loss = 0.24705299735069275, 0.0012591071426868439, kl_loss = 0.00667219515889883\n",
      "\n",
      "Epoch 246\n",
      "Step 0: loss = 0.24290800094604492, recon_loss = 0.24186454713344574, 0.0010082721710205078, kl_loss = 0.01759300008416176\n",
      "\n",
      "Epoch 247\n",
      "Step 0: loss = 0.2505547106266022, recon_loss = 0.24930250644683838, 0.0012335344217717648, kl_loss = 0.009326881729066372\n",
      "\n",
      "Epoch 248\n",
      "Step 0: loss = 0.24369490146636963, recon_loss = 0.24267065525054932, 0.0010119338985532522, kl_loss = 0.0061543285846710205\n",
      "\n",
      "Epoch 249\n",
      "Step 0: loss = 0.2504373788833618, recon_loss = 0.24939313530921936, 0.001023088931106031, kl_loss = 0.010585300624370575\n",
      "\n",
      "Epoch 250\n",
      "Step 0: loss = 0.24700017273426056, recon_loss = 0.24581363797187805, 0.001143838046118617, kl_loss = 0.021345900371670723\n",
      "\n",
      "Epoch 251\n",
      "Step 0: loss = 0.2463502436876297, recon_loss = 0.2449040561914444, 0.0014059789245948195, kl_loss = 0.020104438066482544\n",
      "\n",
      "Epoch 252\n",
      "Step 0: loss = 0.24579071998596191, recon_loss = 0.24479424953460693, 0.000981899444013834, kl_loss = 0.007287045009434223\n",
      "\n",
      "Epoch 253\n",
      "Step 0: loss = 0.2522529065608978, recon_loss = 0.25105559825897217, 0.0011808003764599562, kl_loss = 0.00826230924576521\n",
      "\n",
      "Epoch 254\n",
      "Step 0: loss = 0.2388743758201599, recon_loss = 0.23763003945350647, 0.0012173476861789823, kl_loss = 0.013494481332600117\n",
      "\n",
      "Epoch 255\n",
      "Step 0: loss = 0.25272253155708313, recon_loss = 0.2516334652900696, 0.0010734450770542026, kl_loss = 0.007809369824826717\n",
      "\n",
      "Epoch 256\n",
      "Step 0: loss = 0.24173666536808014, recon_loss = 0.24049802124500275, 0.0012263875687494874, kl_loss = 0.006132703274488449\n",
      "\n",
      "Epoch 257\n",
      "Step 0: loss = 0.24137838184833527, recon_loss = 0.24011996388435364, 0.0012448972556740046, kl_loss = 0.006760295480489731\n",
      "\n",
      "Epoch 258\n",
      "Step 0: loss = 0.25200924277305603, recon_loss = 0.25113463401794434, 0.0008674588752910495, kl_loss = 0.0035790037363767624\n",
      "\n",
      "Epoch 259\n",
      "Step 0: loss = 0.24374917149543762, recon_loss = 0.24263328313827515, 0.0011097884271293879, kl_loss = 0.0030477652326226234\n",
      "\n",
      "Epoch 260\n",
      "Step 0: loss = 0.24658609926700592, recon_loss = 0.24517664313316345, 0.0013995787594467402, kl_loss = 0.004937213845551014\n",
      "\n",
      "Epoch 261\n",
      "Step 0: loss = 0.2513023912906647, recon_loss = 0.2502111792564392, 0.0010636323131620884, kl_loss = 0.013786776922643185\n",
      "\n",
      "Epoch 262\n",
      "Step 0: loss = 0.24650178849697113, recon_loss = 0.24535875022411346, 0.0011266956571489573, kl_loss = 0.008172251284122467\n",
      "\n",
      "Epoch 263\n",
      "Step 0: loss = 0.24473360180854797, recon_loss = 0.24367842078208923, 0.0010359599255025387, kl_loss = 0.009613671340048313\n",
      "\n",
      "Epoch 264\n",
      "Step 0: loss = 0.2428007274866104, recon_loss = 0.24183478951454163, 0.0009544255444779992, kl_loss = 0.005762198008596897\n",
      "\n",
      "Epoch 265\n",
      "Step 0: loss = 0.24678590893745422, recon_loss = 0.24538221955299377, 0.0013895223382860422, kl_loss = 0.007087165489792824\n",
      "\n",
      "Epoch 266\n",
      "Step 0: loss = 0.2485390454530716, recon_loss = 0.24713774025440216, 0.0013963954988867044, kl_loss = 0.0024521658197045326\n",
      "\n",
      "Epoch 267\n",
      "Step 0: loss = 0.24771404266357422, recon_loss = 0.2462022602558136, 0.001499783480539918, kl_loss = 0.005997669883072376\n",
      "\n",
      "Epoch 268\n",
      "Step 0: loss = 0.25356265902519226, recon_loss = 0.25235968828201294, 0.0011938198003917933, kl_loss = 0.004580585286021233\n",
      "\n",
      "Epoch 269\n",
      "Step 0: loss = 0.24968503415584564, recon_loss = 0.247902974486351, 0.0017460230737924576, kl_loss = 0.018012147396802902\n",
      "\n",
      "Epoch 270\n",
      "Step 0: loss = 0.25141698122024536, recon_loss = 0.2500801980495453, 0.001315520261414349, kl_loss = 0.01063100341707468\n",
      "\n",
      "Epoch 271\n",
      "Step 0: loss = 0.24339736998081207, recon_loss = 0.2416219413280487, 0.001763234264217317, kl_loss = 0.006091323681175709\n",
      "\n",
      "Epoch 272\n",
      "Step 0: loss = 0.24254204332828522, recon_loss = 0.24090123176574707, 0.001604046206921339, kl_loss = 0.018381750211119652\n",
      "\n",
      "Epoch 273\n",
      "Step 0: loss = 0.24852047860622406, recon_loss = 0.24715805053710938, 0.001331110019236803, kl_loss = 0.01566348597407341\n",
      "\n",
      "Epoch 274\n",
      "Step 0: loss = 0.24714776873588562, recon_loss = 0.2457599937915802, 0.0013702285941690207, kl_loss = 0.008778124116361141\n",
      "\n",
      "Epoch 275\n",
      "Step 0: loss = 0.24656526744365692, recon_loss = 0.24482032656669617, 0.001726415241137147, kl_loss = 0.009263324551284313\n",
      "\n",
      "Epoch 276\n",
      "Step 0: loss = 0.2406625598669052, recon_loss = 0.2392605096101761, 0.0013831511605530977, kl_loss = 0.009445181116461754\n",
      "\n",
      "Epoch 277\n",
      "Step 0: loss = 0.2514152526855469, recon_loss = 0.24983766674995422, 0.0015642867656424642, kl_loss = 0.006648973561823368\n",
      "\n",
      "Epoch 278\n",
      "Step 0: loss = 0.24744699895381927, recon_loss = 0.24629798531532288, 0.001139448955655098, kl_loss = 0.00478256493806839\n",
      "\n",
      "Epoch 279\n",
      "Step 0: loss = 0.24001215398311615, recon_loss = 0.23907053470611572, 0.0009342576377093792, kl_loss = 0.0036786459386348724\n",
      "\n",
      "Epoch 280\n",
      "Step 0: loss = 0.2438632845878601, recon_loss = 0.24248245358467102, 0.0013567598070949316, kl_loss = 0.012033562175929546\n",
      "\n",
      "Epoch 281\n",
      "Step 0: loss = 0.2475295513868332, recon_loss = 0.2462259829044342, 0.0012868030462414026, kl_loss = 0.00838512647897005\n",
      "\n",
      "Epoch 282\n",
      "Step 0: loss = 0.24241922795772552, recon_loss = 0.24116216599941254, 0.0012420963030308485, kl_loss = 0.007479359395802021\n",
      "\n",
      "Epoch 283\n",
      "Step 0: loss = 0.2514563798904419, recon_loss = 0.250537633895874, 0.0009096721769310534, kl_loss = 0.00453261099755764\n",
      "\n",
      "Epoch 284\n",
      "Step 0: loss = 0.24354873597621918, recon_loss = 0.2421131730079651, 0.0014012937899678946, kl_loss = 0.017135921865701675\n",
      "\n",
      "Epoch 285\n",
      "Step 0: loss = 0.24796593189239502, recon_loss = 0.2464505136013031, 0.0014928908785805106, kl_loss = 0.011263369582593441\n",
      "\n",
      "Epoch 286\n",
      "Step 0: loss = 0.24684444069862366, recon_loss = 0.24548888206481934, 0.0013288294430822134, kl_loss = 0.013369358144700527\n",
      "\n",
      "Epoch 287\n",
      "Step 0: loss = 0.24411845207214355, recon_loss = 0.24262762069702148, 0.001474302727729082, kl_loss = 0.008259778842329979\n",
      "\n",
      "Epoch 288\n",
      "Step 0: loss = 0.2439945787191391, recon_loss = 0.2427341490983963, 0.0012468922650441527, kl_loss = 0.006763424724340439\n",
      "\n",
      "Epoch 289\n",
      "Step 0: loss = 0.24081866443157196, recon_loss = 0.23975838720798492, 0.0010482403449714184, kl_loss = 0.006017493084073067\n",
      "\n",
      "Epoch 290\n",
      "Step 0: loss = 0.2407315969467163, recon_loss = 0.23922619223594666, 0.0014922881964594126, kl_loss = 0.006555723026394844\n",
      "\n",
      "Epoch 291\n",
      "Step 0: loss = 0.2460198998451233, recon_loss = 0.2446058839559555, 0.0013897102326154709, kl_loss = 0.012155548669397831\n",
      "\n",
      "Epoch 292\n",
      "Step 0: loss = 0.24707920849323273, recon_loss = 0.2457532286643982, 0.001311641652137041, kl_loss = 0.0071702990680933\n",
      "\n",
      "Epoch 293\n",
      "Step 0: loss = 0.24490909278392792, recon_loss = 0.24296633899211884, 0.0019281132845208049, kl_loss = 0.007325679063796997\n",
      "\n",
      "Epoch 294\n",
      "Step 0: loss = 0.23699058592319489, recon_loss = 0.2359095811843872, 0.0010602427646517754, kl_loss = 0.010382129810750484\n",
      "\n",
      "Epoch 295\n",
      "Step 0: loss = 0.24148039519786835, recon_loss = 0.2401648759841919, 0.0013072604779154062, kl_loss = 0.004124837927520275\n",
      "\n",
      "Epoch 296\n",
      "Step 0: loss = 0.24393635988235474, recon_loss = 0.2419254183769226, 0.001970937941223383, kl_loss = 0.02000260539352894\n",
      "\n",
      "Epoch 297\n",
      "Step 0: loss = 0.24445395171642303, recon_loss = 0.24318179488182068, 0.0012541367905214429, kl_loss = 0.009007046930491924\n",
      "\n",
      "Epoch 298\n",
      "Step 0: loss = 0.2355637103319168, recon_loss = 0.23406828939914703, 0.0014837294584140182, kl_loss = 0.005849328823387623\n",
      "\n",
      "Epoch 299\n",
      "Step 0: loss = 0.2395903319120407, recon_loss = 0.23824501037597656, 0.0013351013185456395, kl_loss = 0.0051139406859874725\n",
      "\n",
      "Epoch 300\n",
      "Step 0: loss = 0.2391575574874878, recon_loss = 0.2381402552127838, 0.0009935548296198249, kl_loss = 0.011879735626280308\n",
      "\n",
      "Epoch 301\n",
      "Step 0: loss = 0.24037538468837738, recon_loss = 0.23925870656967163, 0.0010886257514357567, kl_loss = 0.014032046310603619\n",
      "\n",
      "Epoch 302\n",
      "Step 0: loss = 0.2392859309911728, recon_loss = 0.23785211145877838, 0.0014027627184987068, kl_loss = 0.015523889102041721\n",
      "\n",
      "Epoch 303\n",
      "Step 0: loss = 0.23316137492656708, recon_loss = 0.23170307278633118, 0.0014498427044600248, kl_loss = 0.004233712330460548\n",
      "\n",
      "Epoch 304\n",
      "Step 0: loss = 0.2417016625404358, recon_loss = 0.24036076664924622, 0.0013288628542795777, kl_loss = 0.006019686348736286\n",
      "\n",
      "Epoch 305\n",
      "Step 0: loss = 0.23644423484802246, recon_loss = 0.2347995936870575, 0.001625179429538548, kl_loss = 0.009732217527925968\n",
      "\n",
      "Epoch 306\n",
      "Step 0: loss = 0.23643307387828827, recon_loss = 0.234915092587471, 0.0015046710614115, kl_loss = 0.0066513726487755775\n",
      "\n",
      "Epoch 307\n",
      "Step 0: loss = 0.24396418035030365, recon_loss = 0.24292989075183868, 0.001017772126942873, kl_loss = 0.008257140405476093\n",
      "\n",
      "Epoch 308\n",
      "Step 0: loss = 0.23933546245098114, recon_loss = 0.23816174268722534, 0.001164180925115943, kl_loss = 0.004765859805047512\n",
      "\n",
      "Epoch 309\n",
      "Step 0: loss = 0.23528806865215302, recon_loss = 0.23362058401107788, 0.001646476797759533, kl_loss = 0.010508071631193161\n",
      "\n",
      "Epoch 310\n",
      "Step 0: loss = 0.2380172610282898, recon_loss = 0.23642860352993011, 0.0015700971707701683, kl_loss = 0.00928239431232214\n",
      "\n",
      "Epoch 311\n",
      "Step 0: loss = 0.2382863461971283, recon_loss = 0.23636655509471893, 0.0018688322743400931, kl_loss = 0.02548336051404476\n",
      "\n",
      "Epoch 312\n",
      "Step 0: loss = 0.23542335629463196, recon_loss = 0.23378321528434753, 0.001600874587893486, kl_loss = 0.019633060321211815\n",
      "\n",
      "Epoch 313\n",
      "Step 0: loss = 0.2358073592185974, recon_loss = 0.2341446876525879, 0.0016170316375792027, kl_loss = 0.022821322083473206\n",
      "\n",
      "Epoch 314\n",
      "Step 0: loss = 0.23627378046512604, recon_loss = 0.23442988097667694, 0.0018304866971448064, kl_loss = 0.006702611222863197\n",
      "\n",
      "Epoch 315\n",
      "Step 0: loss = 0.2409542202949524, recon_loss = 0.2393176257610321, 0.0016226606676355004, kl_loss = 0.006965984590351582\n",
      "\n",
      "Epoch 316\n",
      "Step 0: loss = 0.23810598254203796, recon_loss = 0.23690369725227356, 0.0011874460615217686, kl_loss = 0.007421898655593395\n",
      "\n",
      "Epoch 317\n",
      "Step 0: loss = 0.23508678376674652, recon_loss = 0.23349878191947937, 0.0015833757352083921, kl_loss = 0.002307288348674774\n",
      "\n",
      "Epoch 318\n",
      "Step 0: loss = 0.2297092080116272, recon_loss = 0.2281934916973114, 0.0014957617968320847, kl_loss = 0.009977167472243309\n",
      "\n",
      "Epoch 319\n",
      "Step 0: loss = 0.236229807138443, recon_loss = 0.23426879942417145, 0.001947842538356781, kl_loss = 0.006587766110897064\n",
      "\n",
      "Epoch 320\n",
      "Step 0: loss = 0.23059897124767303, recon_loss = 0.2288949191570282, 0.0016862312331795692, kl_loss = 0.008913826197385788\n",
      "\n",
      "Epoch 321\n",
      "Step 0: loss = 0.23334328830242157, recon_loss = 0.2315577268600464, 0.001764314016327262, kl_loss = 0.010624428279697895\n",
      "\n",
      "Epoch 322\n",
      "Step 0: loss = 0.23467139899730682, recon_loss = 0.23318716883659363, 0.0014715467114001513, kl_loss = 0.006341172382235527\n",
      "\n",
      "Epoch 323\n",
      "Step 0: loss = 0.2339087426662445, recon_loss = 0.23256352543830872, 0.0013290572678670287, kl_loss = 0.008074845187366009\n",
      "\n",
      "Epoch 324\n",
      "Step 0: loss = 0.22974033653736115, recon_loss = 0.2282155305147171, 0.001507916022092104, kl_loss = 0.008444201201200485\n",
      "\n",
      "Epoch 325\n",
      "Step 0: loss = 0.23012825846672058, recon_loss = 0.22852501273155212, 0.0015899018617346883, kl_loss = 0.006666881963610649\n",
      "\n",
      "Epoch 326\n",
      "Step 0: loss = 0.22424960136413574, recon_loss = 0.222770094871521, 0.001463834778405726, kl_loss = 0.007836278527975082\n",
      "\n",
      "Epoch 327\n",
      "Step 0: loss = 0.23058055341243744, recon_loss = 0.2288013994693756, 0.0017634768737480044, kl_loss = 0.007837552577257156\n",
      "\n",
      "Epoch 328\n",
      "Step 0: loss = 0.2332727313041687, recon_loss = 0.23157504200935364, 0.0016771110240370035, kl_loss = 0.010286670178174973\n",
      "\n",
      "Epoch 329\n",
      "Step 0: loss = 0.22921067476272583, recon_loss = 0.22777125239372253, 0.0014130804920569062, kl_loss = 0.013172643259167671\n",
      "\n",
      "Epoch 330\n",
      "Step 0: loss = 0.22252048552036285, recon_loss = 0.22094790637493134, 0.001553225563839078, kl_loss = 0.009679492563009262\n",
      "\n",
      "Epoch 331\n",
      "Step 0: loss = 0.22652356326580048, recon_loss = 0.22479666769504547, 0.0017133529763668776, kl_loss = 0.006775371730327606\n",
      "\n",
      "Epoch 332\n",
      "Step 0: loss = 0.22740162909030914, recon_loss = 0.22568796575069427, 0.0016882696654647589, kl_loss = 0.012695545330643654\n",
      "\n",
      "Epoch 333\n",
      "Step 0: loss = 0.22727638483047485, recon_loss = 0.22584164142608643, 0.0014173041563481092, kl_loss = 0.008720498532056808\n",
      "\n",
      "Epoch 334\n",
      "Step 0: loss = 0.22164951264858246, recon_loss = 0.2201308012008667, 0.0014977487735450268, kl_loss = 0.010481736622750759\n",
      "\n",
      "Epoch 335\n",
      "Step 0: loss = 0.22215457260608673, recon_loss = 0.22078752517700195, 0.0013501776847988367, kl_loss = 0.008433640003204346\n",
      "\n",
      "Epoch 336\n",
      "Step 0: loss = 0.22569717466831207, recon_loss = 0.22402045130729675, 0.0016570601146668196, kl_loss = 0.009838074445724487\n",
      "\n",
      "Epoch 337\n",
      "Step 0: loss = 0.22534699738025665, recon_loss = 0.22365643084049225, 0.00167202134616673, kl_loss = 0.009275069460272789\n",
      "\n",
      "Epoch 338\n",
      "Step 0: loss = 0.22542314231395721, recon_loss = 0.22413848340511322, 0.0012698282953351736, kl_loss = 0.007413008250296116\n",
      "\n",
      "Epoch 339\n",
      "Step 0: loss = 0.21768039464950562, recon_loss = 0.2159980684518814, 0.0016713845543563366, kl_loss = 0.005470729433000088\n",
      "\n",
      "Epoch 340\n",
      "Step 0: loss = 0.22348536550998688, recon_loss = 0.22179964184761047, 0.0016740639694035053, kl_loss = 0.0058273449540138245\n",
      "\n",
      "Epoch 341\n",
      "Step 0: loss = 0.22143664956092834, recon_loss = 0.21934527158737183, 0.002080203965306282, kl_loss = 0.005585716105997562\n",
      "\n",
      "Epoch 342\n",
      "Step 0: loss = 0.2216597944498062, recon_loss = 0.21987420320510864, 0.0017743068747222424, kl_loss = 0.005642666481435299\n",
      "\n",
      "Epoch 343\n",
      "Step 0: loss = 0.2183903008699417, recon_loss = 0.21656319499015808, 0.0018116296268999577, kl_loss = 0.007743453606963158\n",
      "\n",
      "Epoch 344\n",
      "Step 0: loss = 0.22275850176811218, recon_loss = 0.22090445458889008, 0.001844018348492682, kl_loss = 0.005014922469854355\n",
      "\n",
      "Epoch 345\n",
      "Step 0: loss = 0.2190900444984436, recon_loss = 0.21763339638710022, 0.0014378338819369674, kl_loss = 0.009410138241946697\n",
      "\n",
      "Epoch 346\n",
      "Step 0: loss = 0.22189363837242126, recon_loss = 0.2199159860610962, 0.0019561334047466516, kl_loss = 0.010760422796010971\n",
      "\n",
      "Epoch 347\n",
      "Step 0: loss = 0.21663033962249756, recon_loss = 0.21470677852630615, 0.0019140904769301414, kl_loss = 0.004734949208796024\n",
      "\n",
      "Epoch 348\n",
      "Step 0: loss = 0.2164587527513504, recon_loss = 0.21483707427978516, 0.0016087926924228668, kl_loss = 0.006445037201046944\n",
      "\n",
      "Epoch 349\n",
      "Step 0: loss = 0.20561222732067108, recon_loss = 0.20385950803756714, 0.0017396174371242523, kl_loss = 0.006551406346261501\n",
      "\n",
      "Epoch 350\n",
      "Step 0: loss = 0.21808581054210663, recon_loss = 0.21657991409301758, 0.001497455406934023, kl_loss = 0.004214543849229813\n",
      "\n",
      "Epoch 351\n",
      "Step 0: loss = 0.21146944165229797, recon_loss = 0.20956915616989136, 0.0018907873891294003, kl_loss = 0.004748843610286713\n",
      "\n",
      "Epoch 352\n",
      "Step 0: loss = 0.21546851098537445, recon_loss = 0.21376824378967285, 0.001693589729256928, kl_loss = 0.0033360207453370094\n",
      "\n",
      "Epoch 353\n",
      "Step 0: loss = 0.2108919620513916, recon_loss = 0.20893308520317078, 0.0019369864603504539, kl_loss = 0.010942908003926277\n",
      "\n",
      "Epoch 354\n",
      "Step 0: loss = 0.21152076125144958, recon_loss = 0.2089376151561737, 0.0025717169046401978, kl_loss = 0.005714020691812038\n",
      "\n",
      "Epoch 355\n",
      "Step 0: loss = 0.21591535210609436, recon_loss = 0.21390649676322937, 0.0020004375837743282, kl_loss = 0.004206914454698563\n",
      "\n",
      "Epoch 356\n",
      "Step 0: loss = 0.20550906658172607, recon_loss = 0.20255404710769653, 0.0029391476418823004, kl_loss = 0.00793399102985859\n",
      "\n",
      "Epoch 357\n",
      "Step 0: loss = 0.22207993268966675, recon_loss = 0.2202133983373642, 0.0018509773071855307, kl_loss = 0.007780222222208977\n",
      "\n",
      "Epoch 358\n",
      "Step 0: loss = 0.20822599530220032, recon_loss = 0.20575472712516785, 0.002457797061651945, kl_loss = 0.006732635200023651\n",
      "\n",
      "Epoch 359\n",
      "Step 0: loss = 0.21480533480644226, recon_loss = 0.21263816952705383, 0.002152819186449051, kl_loss = 0.007178339175879955\n",
      "\n",
      "Epoch 360\n",
      "Step 0: loss = 0.21499423682689667, recon_loss = 0.21247906982898712, 0.002501692622900009, kl_loss = 0.0067374007776379585\n",
      "\n",
      "Epoch 361\n",
      "Step 0: loss = 0.21309499442577362, recon_loss = 0.2110745906829834, 0.0020077840890735388, kl_loss = 0.006310808472335339\n",
      "\n",
      "Epoch 362\n",
      "Step 0: loss = 0.20819109678268433, recon_loss = 0.20595285296440125, 0.0022274714428931475, kl_loss = 0.005386166274547577\n",
      "\n",
      "Epoch 363\n",
      "Step 0: loss = 0.19993215799331665, recon_loss = 0.19788773357868195, 0.0020260699093341827, kl_loss = 0.009180771186947823\n",
      "\n",
      "Epoch 364\n",
      "Step 0: loss = 0.21722771227359772, recon_loss = 0.2150801122188568, 0.002131921239197254, kl_loss = 0.007839307188987732\n",
      "\n",
      "Epoch 365\n",
      "Step 0: loss = 0.21389228105545044, recon_loss = 0.21172171831130981, 0.002155538648366928, kl_loss = 0.007509756833314896\n",
      "\n",
      "Epoch 366\n",
      "Step 0: loss = 0.21276341378688812, recon_loss = 0.21002522110939026, 0.0027280356734991074, kl_loss = 0.005079823546111584\n",
      "\n",
      "Epoch 367\n",
      "Step 0: loss = 0.20886579155921936, recon_loss = 0.20660026371479034, 0.002256792038679123, kl_loss = 0.004367689602077007\n",
      "\n",
      "Epoch 368\n",
      "Step 0: loss = 0.20671233534812927, recon_loss = 0.20459936559200287, 0.0021035042591392994, kl_loss = 0.004732717759907246\n",
      "\n",
      "Epoch 369\n",
      "Step 0: loss = 0.19767047464847565, recon_loss = 0.19559431076049805, 0.002066708169877529, kl_loss = 0.004729359410703182\n",
      "\n",
      "Epoch 370\n",
      "Step 0: loss = 0.20329418778419495, recon_loss = 0.2003343105316162, 0.0029492941685020924, kl_loss = 0.005289275199174881\n",
      "\n",
      "Epoch 371\n",
      "Step 0: loss = 0.20071570575237274, recon_loss = 0.19787321984767914, 0.0028302099090069532, kl_loss = 0.006139284931123257\n",
      "\n",
      "Epoch 372\n",
      "Step 0: loss = 0.21501338481903076, recon_loss = 0.2129523754119873, 0.002045120345428586, kl_loss = 0.007940591312944889\n",
      "\n",
      "Epoch 373\n",
      "Step 0: loss = 0.20650385320186615, recon_loss = 0.20472095906734467, 0.00177503633312881, kl_loss = 0.003926292061805725\n",
      "\n",
      "Epoch 374\n",
      "Step 0: loss = 0.20830537378787994, recon_loss = 0.205967977643013, 0.0023246295750141144, kl_loss = 0.0063880132511258125\n",
      "\n",
      "Epoch 375\n",
      "Step 0: loss = 0.2051256000995636, recon_loss = 0.20277360081672668, 0.0023345905356109142, kl_loss = 0.008705093525350094\n",
      "\n",
      "Epoch 376\n",
      "Step 0: loss = 0.20111598074436188, recon_loss = 0.198727548122406, 0.002379610203206539, kl_loss = 0.004410088993608952\n",
      "\n",
      "Epoch 377\n",
      "Step 0: loss = 0.19745460152626038, recon_loss = 0.1957838088274002, 0.0016617699293419719, kl_loss = 0.004514111205935478\n",
      "\n",
      "Epoch 378\n",
      "Step 0: loss = 0.20160794258117676, recon_loss = 0.1998923122882843, 0.001705090282484889, kl_loss = 0.005266345106065273\n",
      "\n",
      "Epoch 379\n",
      "Step 0: loss = 0.19091573357582092, recon_loss = 0.1880558431148529, 0.0028480333276093006, kl_loss = 0.0059320200234651566\n",
      "\n",
      "Epoch 380\n",
      "Step 0: loss = 0.19800236821174622, recon_loss = 0.1956680417060852, 0.002325043547898531, kl_loss = 0.0046448176726698875\n",
      "\n",
      "Epoch 381\n",
      "Step 0: loss = 0.19135597348213196, recon_loss = 0.18928754329681396, 0.0020555267110466957, kl_loss = 0.006454700604081154\n",
      "\n",
      "Epoch 382\n",
      "Step 0: loss = 0.1963224709033966, recon_loss = 0.1944568157196045, 0.0018589472165331244, kl_loss = 0.0033554090186953545\n",
      "\n",
      "Epoch 383\n",
      "Step 0: loss = 0.19465821981430054, recon_loss = 0.19310994446277618, 0.001539811259135604, kl_loss = 0.004234429448843002\n",
      "\n",
      "Epoch 384\n",
      "Step 0: loss = 0.19920219480991364, recon_loss = 0.19724607467651367, 0.0019387122010812163, kl_loss = 0.008703839033842087\n",
      "\n",
      "Epoch 385\n",
      "Step 0: loss = 0.207986518740654, recon_loss = 0.2055700719356537, 0.002407804597169161, kl_loss = 0.004324566572904587\n",
      "\n",
      "Epoch 386\n",
      "Step 0: loss = 0.19891177117824554, recon_loss = 0.196161150932312, 0.002744104713201523, kl_loss = 0.003257063217461109\n",
      "\n",
      "Epoch 387\n",
      "Step 0: loss = 0.1975172907114029, recon_loss = 0.19484373927116394, 0.0026633618399500847, kl_loss = 0.005093136802315712\n",
      "\n",
      "Epoch 388\n",
      "Step 0: loss = 0.19443035125732422, recon_loss = 0.19175347685813904, 0.002654405776411295, kl_loss = 0.011237205006182194\n",
      "\n",
      "Epoch 389\n",
      "Step 0: loss = 0.1979140192270279, recon_loss = 0.19433456659317017, 0.0035645365715026855, kl_loss = 0.007456174120306969\n",
      "\n",
      "Epoch 390\n",
      "Step 0: loss = 0.1932152658700943, recon_loss = 0.18948492407798767, 0.003721346613019705, kl_loss = 0.004501861520111561\n",
      "\n",
      "Epoch 391\n",
      "Step 0: loss = 0.2020418792963028, recon_loss = 0.19946254789829254, 0.0025691676419228315, kl_loss = 0.005084087140858173\n",
      "\n",
      "Epoch 392\n",
      "Step 0: loss = 0.18953168392181396, recon_loss = 0.18635588884353638, 0.003168052528053522, kl_loss = 0.0038708392530679703\n",
      "\n",
      "Epoch 393\n",
      "Step 0: loss = 0.19390758872032166, recon_loss = 0.19128118455410004, 0.002619596431031823, kl_loss = 0.0034012850373983383\n",
      "\n",
      "Epoch 394\n",
      "Step 0: loss = 0.1914597898721695, recon_loss = 0.1886238157749176, 0.0028218217194080353, kl_loss = 0.007077792659401894\n",
      "\n",
      "Epoch 395\n",
      "Step 0: loss = 0.19450898468494415, recon_loss = 0.19192706048488617, 0.002566636772826314, kl_loss = 0.00764643307775259\n",
      "\n",
      "Epoch 396\n",
      "Step 0: loss = 0.19071219861507416, recon_loss = 0.18843479454517365, 0.002265641698613763, kl_loss = 0.005877147428691387\n",
      "\n",
      "Epoch 397\n",
      "Step 0: loss = 0.1872428059577942, recon_loss = 0.18390201032161713, 0.003327571786940098, kl_loss = 0.00660855695605278\n",
      "\n",
      "Epoch 398\n",
      "Step 0: loss = 0.19066408276557922, recon_loss = 0.18788224458694458, 0.0027653484139591455, kl_loss = 0.008244575932621956\n",
      "\n",
      "Epoch 399\n",
      "Step 0: loss = 0.19060540199279785, recon_loss = 0.18776343762874603, 0.0028204061090946198, kl_loss = 0.01077848207205534\n",
      "\n",
      "Epoch 400\n",
      "Step 0: loss = 0.1849955916404724, recon_loss = 0.18253344297409058, 0.0024496756959706545, kl_loss = 0.006233600899577141\n",
      "\n",
      "Epoch 401\n",
      "Step 0: loss = 0.1903374195098877, recon_loss = 0.18810346722602844, 0.002224936615675688, kl_loss = 0.004504046402871609\n",
      "\n",
      "Epoch 402\n",
      "Step 0: loss = 0.1787915825843811, recon_loss = 0.17565396428108215, 0.003128363750874996, kl_loss = 0.004626716487109661\n",
      "\n",
      "Epoch 403\n",
      "Step 0: loss = 0.1815357804298401, recon_loss = 0.17871621251106262, 0.0028116190806031227, kl_loss = 0.003973742946982384\n",
      "\n",
      "Epoch 404\n",
      "Step 0: loss = 0.1922614723443985, recon_loss = 0.18925786018371582, 0.00299651175737381, kl_loss = 0.003552328795194626\n",
      "\n",
      "Epoch 405\n",
      "Step 0: loss = 0.18740501999855042, recon_loss = 0.18509265780448914, 0.0023046103306114674, kl_loss = 0.003872537985444069\n",
      "\n",
      "Epoch 406\n",
      "Step 0: loss = 0.18445071578025818, recon_loss = 0.18164056539535522, 0.00280332425609231, kl_loss = 0.003409387543797493\n",
      "\n",
      "Epoch 407\n",
      "Step 0: loss = 0.18797874450683594, recon_loss = 0.18447166681289673, 0.0034980308264493942, kl_loss = 0.00452073197811842\n",
      "\n",
      "Epoch 408\n",
      "Step 0: loss = 0.1910095363855362, recon_loss = 0.18731242418289185, 0.0036878075916320086, kl_loss = 0.004645854234695435\n",
      "\n",
      "Epoch 409\n",
      "Step 0: loss = 0.1800970584154129, recon_loss = 0.1772574484348297, 0.002822276670485735, kl_loss = 0.008668464608490467\n",
      "\n",
      "Epoch 410\n",
      "Step 0: loss = 0.18617643415927887, recon_loss = 0.18289166688919067, 0.0032752500846982002, kl_loss = 0.00475938618183136\n",
      "\n",
      "Epoch 411\n",
      "Step 0: loss = 0.18144993484020233, recon_loss = 0.17918488383293152, 0.002257920103147626, kl_loss = 0.0035715242847800255\n",
      "\n",
      "Epoch 412\n",
      "Step 0: loss = 0.18347012996673584, recon_loss = 0.18083347380161285, 0.002626038622111082, kl_loss = 0.005312532186508179\n",
      "\n",
      "Epoch 413\n",
      "Step 0: loss = 0.17926311492919922, recon_loss = 0.17599189281463623, 0.003259433899074793, kl_loss = 0.00589646864682436\n",
      "\n",
      "Epoch 414\n",
      "Step 0: loss = 0.1837373822927475, recon_loss = 0.17990127205848694, 0.0038261408917605877, kl_loss = 0.004985378123819828\n",
      "\n",
      "Epoch 415\n",
      "Step 0: loss = 0.1767757683992386, recon_loss = 0.17260518670082092, 0.004160972312092781, kl_loss = 0.004806550219655037\n",
      "\n",
      "Epoch 416\n",
      "Step 0: loss = 0.18497705459594727, recon_loss = 0.180435448884964, 0.004531262442469597, kl_loss = 0.005168943665921688\n",
      "\n",
      "Epoch 417\n",
      "Step 0: loss = 0.17083755135536194, recon_loss = 0.1686432659626007, 0.0021867856848984957, kl_loss = 0.0037441961467266083\n",
      "\n",
      "Epoch 418\n",
      "Step 0: loss = 0.18185792863368988, recon_loss = 0.17874261736869812, 0.0030813843477517366, kl_loss = 0.016962222754955292\n",
      "\n",
      "Epoch 419\n",
      "Step 0: loss = 0.17934323847293854, recon_loss = 0.17586517333984375, 0.003459460334852338, kl_loss = 0.009307769127190113\n",
      "\n",
      "Epoch 420\n",
      "Step 0: loss = 0.17223714292049408, recon_loss = 0.16918903589248657, 0.0030340971425175667, kl_loss = 0.007002468220889568\n",
      "\n",
      "Epoch 421\n",
      "Step 0: loss = 0.17983578145503998, recon_loss = 0.17706267535686493, 0.0027628829702734947, kl_loss = 0.005110805854201317\n",
      "\n",
      "Epoch 422\n",
      "Step 0: loss = 0.16704869270324707, recon_loss = 0.16438645124435425, 0.0026453821919858456, kl_loss = 0.008422957733273506\n",
      "\n",
      "Epoch 423\n",
      "Step 0: loss = 0.17309890687465668, recon_loss = 0.17035582661628723, 0.002705078339204192, kl_loss = 0.01900237426161766\n",
      "\n",
      "Epoch 424\n",
      "Step 0: loss = 0.16917848587036133, recon_loss = 0.16568981111049652, 0.0034737433306872845, kl_loss = 0.007461767643690109\n",
      "\n",
      "Epoch 425\n",
      "Step 0: loss = 0.1698436737060547, recon_loss = 0.16693855822086334, 0.0028896252624690533, kl_loss = 0.007752187550067902\n",
      "\n",
      "Epoch 426\n",
      "Step 0: loss = 0.17279480397701263, recon_loss = 0.16961146891117096, 0.003167396178469062, kl_loss = 0.007969759404659271\n",
      "\n",
      "Epoch 427\n",
      "Step 0: loss = 0.16771525144577026, recon_loss = 0.16331782937049866, 0.004386206157505512, kl_loss = 0.005611562170088291\n",
      "\n",
      "Epoch 428\n",
      "Step 0: loss = 0.17412890493869781, recon_loss = 0.17084035277366638, 0.0032674476969987154, kl_loss = 0.010552970692515373\n",
      "\n",
      "Epoch 429\n",
      "Step 0: loss = 0.17134590446949005, recon_loss = 0.16743838787078857, 0.0038907285779714584, kl_loss = 0.008396435528993607\n",
      "\n",
      "Epoch 430\n",
      "Step 0: loss = 0.17186173796653748, recon_loss = 0.16810762882232666, 0.0037366370670497417, kl_loss = 0.008738587610423565\n",
      "\n",
      "Epoch 431\n",
      "Step 0: loss = 0.17446637153625488, recon_loss = 0.1712447553873062, 0.0032051154412329197, kl_loss = 0.00824789796024561\n",
      "\n",
      "Epoch 432\n",
      "Step 0: loss = 0.168141707777977, recon_loss = 0.16507670283317566, 0.0030461829155683517, kl_loss = 0.00940735638141632\n",
      "\n",
      "Epoch 433\n",
      "Step 0: loss = 0.17106740176677704, recon_loss = 0.1674051284790039, 0.003650340484455228, kl_loss = 0.005965246818959713\n",
      "\n",
      "Epoch 434\n",
      "Step 0: loss = 0.16633307933807373, recon_loss = 0.1629585325717926, 0.0033591026440262794, kl_loss = 0.007715829648077488\n",
      "\n",
      "Epoch 435\n",
      "Step 0: loss = 0.17171970009803772, recon_loss = 0.168008953332901, 0.0036935992538928986, kl_loss = 0.008578505367040634\n",
      "\n",
      "Epoch 436\n",
      "Step 0: loss = 0.16631458699703217, recon_loss = 0.1634724885225296, 0.0028312397189438343, kl_loss = 0.005428103730082512\n",
      "\n",
      "Epoch 437\n",
      "Step 0: loss = 0.16848231852054596, recon_loss = 0.1640625298023224, 0.004410159774124622, kl_loss = 0.004815081134438515\n",
      "\n",
      "Epoch 438\n",
      "Step 0: loss = 0.16540049016475677, recon_loss = 0.16167590022087097, 0.003715506521984935, kl_loss = 0.004548544995486736\n",
      "\n",
      "Epoch 439\n",
      "Step 0: loss = 0.1655334234237671, recon_loss = 0.16204530000686646, 0.0034802532754838467, kl_loss = 0.003930241800844669\n",
      "\n",
      "Epoch 440\n",
      "Step 0: loss = 0.16775259375572205, recon_loss = 0.16427792608737946, 0.0034675709903240204, kl_loss = 0.003546648658812046\n",
      "\n",
      "Epoch 441\n",
      "Step 0: loss = 0.16860125958919525, recon_loss = 0.16502249240875244, 0.003569195978343487, kl_loss = 0.004782547242939472\n",
      "\n",
      "Epoch 442\n",
      "Step 0: loss = 0.16099649667739868, recon_loss = 0.15753649175167084, 0.003450540592893958, kl_loss = 0.004731037653982639\n",
      "\n",
      "Epoch 443\n",
      "Step 0: loss = 0.1672670692205429, recon_loss = 0.1634940803050995, 0.003765740664675832, kl_loss = 0.0036208387464284897\n",
      "\n",
      "Epoch 444\n",
      "Step 0: loss = 0.16918626427650452, recon_loss = 0.16484299302101135, 0.004335369914770126, kl_loss = 0.0039511388167738914\n",
      "\n",
      "Epoch 445\n",
      "Step 0: loss = 0.16434833407402039, recon_loss = 0.16031183302402496, 0.004022563807666302, kl_loss = 0.006968808360397816\n",
      "\n",
      "Epoch 446\n",
      "Step 0: loss = 0.16579878330230713, recon_loss = 0.16112269461154938, 0.004663271829485893, kl_loss = 0.006408797577023506\n",
      "\n",
      "Epoch 447\n",
      "Step 0: loss = 0.16571246087551117, recon_loss = 0.16218729317188263, 0.0035045002587139606, kl_loss = 0.010335483588278294\n",
      "\n",
      "Epoch 448\n",
      "Step 0: loss = 0.16301168501377106, recon_loss = 0.15893352031707764, 0.004068255424499512, kl_loss = 0.0049525052309036255\n",
      "\n",
      "Epoch 449\n",
      "Step 0: loss = 0.16504041850566864, recon_loss = 0.1614830493927002, 0.003548054024577141, kl_loss = 0.004658134654164314\n",
      "\n",
      "Epoch 450\n",
      "Step 0: loss = 0.160162091255188, recon_loss = 0.15721963346004486, 0.0029318779706954956, kl_loss = 0.005291510373353958\n",
      "\n",
      "Epoch 451\n",
      "Step 0: loss = 0.15624889731407166, recon_loss = 0.15303277969360352, 0.0032095382921397686, kl_loss = 0.003293141722679138\n",
      "\n",
      "Epoch 452\n",
      "Step 0: loss = 0.1557377427816391, recon_loss = 0.15259253978729248, 0.0031353398226201534, kl_loss = 0.004930486902594566\n",
      "\n",
      "Epoch 453\n",
      "Step 0: loss = 0.16245819628238678, recon_loss = 0.1588520109653473, 0.0035901330411434174, kl_loss = 0.008021388202905655\n",
      "\n",
      "Epoch 454\n",
      "Step 0: loss = 0.16289544105529785, recon_loss = 0.15967348217964172, 0.0032008809503167868, kl_loss = 0.010542494244873524\n",
      "\n",
      "Epoch 455\n",
      "Step 0: loss = 0.16178439557552338, recon_loss = 0.1581716537475586, 0.003600178984925151, kl_loss = 0.006282146088778973\n",
      "\n",
      "Epoch 456\n",
      "Step 0: loss = 0.1578134000301361, recon_loss = 0.1545795202255249, 0.0032283044420182705, kl_loss = 0.002785966731607914\n",
      "\n",
      "Epoch 457\n",
      "Step 0: loss = 0.15267224609851837, recon_loss = 0.14964145421981812, 0.0030248011462390423, kl_loss = 0.0029916539788246155\n",
      "\n",
      "Epoch 458\n",
      "Step 0: loss = 0.1525546759366989, recon_loss = 0.1479678750038147, 0.004581248387694359, kl_loss = 0.0027794940397143364\n",
      "\n",
      "Epoch 459\n",
      "Step 0: loss = 0.15888085961341858, recon_loss = 0.15486270189285278, 0.004009061958640814, kl_loss = 0.004544938914477825\n",
      "\n",
      "Epoch 460\n",
      "Step 0: loss = 0.16287033259868622, recon_loss = 0.158182293176651, 0.00468065682798624, kl_loss = 0.0036901049315929413\n",
      "\n",
      "Epoch 461\n",
      "Step 0: loss = 0.1643368899822235, recon_loss = 0.16153627634048462, 0.002793805208057165, kl_loss = 0.0034079784527420998\n",
      "\n",
      "Epoch 462\n",
      "Step 0: loss = 0.16176965832710266, recon_loss = 0.1557115763425827, 0.006045255344361067, kl_loss = 0.006414548493921757\n",
      "\n",
      "Epoch 463\n",
      "Step 0: loss = 0.15599049627780914, recon_loss = 0.1532026082277298, 0.002773728221654892, kl_loss = 0.007080134004354477\n",
      "\n",
      "Epoch 464\n",
      "Step 0: loss = 0.15464641153812408, recon_loss = 0.1512434482574463, 0.0033929352648556232, kl_loss = 0.005010850727558136\n",
      "\n",
      "Epoch 465\n",
      "Step 0: loss = 0.15336734056472778, recon_loss = 0.14926400780677795, 0.004091962240636349, kl_loss = 0.005687178112566471\n",
      "\n",
      "Epoch 466\n",
      "Step 0: loss = 0.15914787352085114, recon_loss = 0.1552479863166809, 0.0038897970225661993, kl_loss = 0.0050460221245884895\n",
      "\n",
      "Epoch 467\n",
      "Step 0: loss = 0.15290452539920807, recon_loss = 0.1487427055835724, 0.00415521115064621, kl_loss = 0.0032982593402266502\n",
      "\n",
      "Epoch 468\n",
      "Step 0: loss = 0.15383286774158478, recon_loss = 0.14977657794952393, 0.00404713861644268, kl_loss = 0.004576707258820534\n",
      "\n",
      "Epoch 469\n",
      "Step 0: loss = 0.15815582871437073, recon_loss = 0.1545887291431427, 0.0035584380384534597, kl_loss = 0.0043251728639006615\n",
      "\n",
      "Epoch 470\n",
      "Step 0: loss = 0.15676343441009521, recon_loss = 0.15248306095600128, 0.004267535172402859, kl_loss = 0.006426007486879826\n",
      "\n",
      "Epoch 471\n",
      "Step 0: loss = 0.15239699184894562, recon_loss = 0.14879243075847626, 0.003593148197978735, kl_loss = 0.005707052536308765\n",
      "\n",
      "Epoch 472\n",
      "Step 0: loss = 0.15247763693332672, recon_loss = 0.1495097130537033, 0.002960020210593939, kl_loss = 0.003949027508497238\n",
      "\n",
      "Epoch 473\n",
      "Step 0: loss = 0.1543722152709961, recon_loss = 0.1499219536781311, 0.004441454540938139, kl_loss = 0.0044036610051989555\n",
      "\n",
      "Epoch 474\n",
      "Step 0: loss = 0.15614138543605804, recon_loss = 0.15252432227134705, 0.003598792478442192, kl_loss = 0.00913457851856947\n",
      "\n",
      "Epoch 475\n",
      "Step 0: loss = 0.14840777218341827, recon_loss = 0.14396727085113525, 0.004429996013641357, kl_loss = 0.005255004391074181\n",
      "\n",
      "Epoch 476\n",
      "Step 0: loss = 0.1532275378704071, recon_loss = 0.15001922845840454, 0.003191351657733321, kl_loss = 0.008478114381432533\n",
      "\n",
      "Epoch 477\n",
      "Step 0: loss = 0.15052813291549683, recon_loss = 0.14681653678417206, 0.003700526896864176, kl_loss = 0.0055348826572299\n",
      "\n",
      "Epoch 478\n",
      "Step 0: loss = 0.15325382351875305, recon_loss = 0.14796440303325653, 0.0052733952179551125, kl_loss = 0.008011696860194206\n",
      "\n",
      "Epoch 479\n",
      "Step 0: loss = 0.1455463171005249, recon_loss = 0.14148803055286407, 0.004043831489980221, kl_loss = 0.0072293756529688835\n",
      "\n",
      "Epoch 480\n",
      "Step 0: loss = 0.15023893117904663, recon_loss = 0.1471053808927536, 0.003125600516796112, kl_loss = 0.003976740874350071\n",
      "\n",
      "Epoch 481\n",
      "Step 0: loss = 0.15658341348171234, recon_loss = 0.15101909637451172, 0.005549302324652672, kl_loss = 0.0075108082965016365\n",
      "\n",
      "Epoch 482\n",
      "Step 0: loss = 0.14218670129776, recon_loss = 0.13752999901771545, 0.004646880552172661, kl_loss = 0.004906663671135902\n",
      "\n",
      "Epoch 483\n",
      "Step 0: loss = 0.14382871985435486, recon_loss = 0.13953834772109985, 0.004280568100512028, kl_loss = 0.004899045452475548\n",
      "\n",
      "Epoch 484\n",
      "Step 0: loss = 0.14512896537780762, recon_loss = 0.14045464992523193, 0.004661490209400654, kl_loss = 0.006418250501155853\n",
      "\n",
      "Epoch 485\n",
      "Step 0: loss = 0.14858894050121307, recon_loss = 0.14473941922187805, 0.0038382972124964, kl_loss = 0.005612661130726337\n",
      "\n",
      "Epoch 486\n",
      "Step 0: loss = 0.15446743369102478, recon_loss = 0.1486300826072693, 0.005831967107951641, kl_loss = 0.0026865405961871147\n",
      "\n",
      "Epoch 487\n",
      "Step 0: loss = 0.14678668975830078, recon_loss = 0.14331576228141785, 0.0034612207673490047, kl_loss = 0.004853649064898491\n",
      "\n",
      "Epoch 488\n",
      "Step 0: loss = 0.14289119839668274, recon_loss = 0.1389058381319046, 0.003972799051553011, kl_loss = 0.006282228045165539\n",
      "\n",
      "Epoch 489\n",
      "Step 0: loss = 0.14316734671592712, recon_loss = 0.13995489478111267, 0.0032031273003667593, kl_loss = 0.004661647602915764\n",
      "\n",
      "Epoch 490\n",
      "Step 0: loss = 0.14544013142585754, recon_loss = 0.14024361968040466, 0.005176849663257599, kl_loss = 0.009837927296757698\n",
      "\n",
      "Epoch 491\n",
      "Step 0: loss = 0.14688706398010254, recon_loss = 0.1431487500667572, 0.003728054463863373, kl_loss = 0.005124855786561966\n",
      "\n",
      "Epoch 492\n",
      "Step 0: loss = 0.14596347510814667, recon_loss = 0.14358791708946228, 0.00236617773771286, kl_loss = 0.004686123691499233\n",
      "\n",
      "Epoch 493\n",
      "Step 0: loss = 0.14530017971992493, recon_loss = 0.14107921719551086, 0.004212954081594944, kl_loss = 0.0039994120597839355\n",
      "\n",
      "Epoch 494\n",
      "Step 0: loss = 0.14106620848178864, recon_loss = 0.13787971436977386, 0.0031704488210380077, kl_loss = 0.008026461116969585\n",
      "\n",
      "Epoch 495\n",
      "Step 0: loss = 0.14184430241584778, recon_loss = 0.13841992616653442, 0.0034129712730646133, kl_loss = 0.005700680427253246\n",
      "\n",
      "Epoch 496\n",
      "Step 0: loss = 0.14484725892543793, recon_loss = 0.14170584082603455, 0.003134514205157757, kl_loss = 0.003447134979069233\n",
      "\n",
      "Epoch 497\n",
      "Step 0: loss = 0.14552414417266846, recon_loss = 0.1415053755044937, 0.004010919947177172, kl_loss = 0.003923259675502777\n",
      "\n",
      "Epoch 498\n",
      "Step 0: loss = 0.14574174582958221, recon_loss = 0.14230215549468994, 0.0034320452250540257, kl_loss = 0.0037669409066438675\n",
      "\n",
      "Epoch 499\n",
      "Step 0: loss = 0.14246846735477448, recon_loss = 0.1385207176208496, 0.003939357586205006, kl_loss = 0.004195809364318848\n",
      "\n",
      "Epoch 500\n",
      "Step 0: loss = 0.14615139365196228, recon_loss = 0.14073829352855682, 0.005401655565947294, kl_loss = 0.005721148103475571\n",
      "\n",
      "Epoch 501\n",
      "Step 0: loss = 0.13848324120044708, recon_loss = 0.13428151607513428, 0.004192307125777006, kl_loss = 0.004710352048277855\n",
      "\n",
      "Epoch 502\n",
      "Step 0: loss = 0.13898342847824097, recon_loss = 0.13547849655151367, 0.003498088102787733, kl_loss = 0.0034179119393229485\n",
      "\n",
      "Epoch 503\n",
      "Step 0: loss = 0.14158490300178528, recon_loss = 0.13715818524360657, 0.004415054805576801, kl_loss = 0.005832649767398834\n",
      "\n",
      "Epoch 504\n",
      "Step 0: loss = 0.14508575201034546, recon_loss = 0.13824275135993958, 0.006833639927208424, kl_loss = 0.004677160643041134\n",
      "\n",
      "Epoch 505\n",
      "Step 0: loss = 0.1394275277853012, recon_loss = 0.13511505722999573, 0.004304999485611916, kl_loss = 0.003735817037522793\n",
      "\n",
      "Epoch 506\n",
      "Step 0: loss = 0.13957124948501587, recon_loss = 0.13498803973197937, 0.004569678567349911, kl_loss = 0.006767979823052883\n",
      "\n",
      "Epoch 507\n",
      "Step 0: loss = 0.14245231449604034, recon_loss = 0.13774603605270386, 0.004694162402302027, kl_loss = 0.006057250313460827\n",
      "\n",
      "Epoch 508\n",
      "Step 0: loss = 0.13982734084129333, recon_loss = 0.1364087462425232, 0.0034070522524416447, kl_loss = 0.005775262601673603\n",
      "\n",
      "Epoch 509\n",
      "Step 0: loss = 0.14280810952186584, recon_loss = 0.13920283317565918, 0.0035990881733596325, kl_loss = 0.0030955616384744644\n",
      "\n",
      "Epoch 510\n",
      "Step 0: loss = 0.14254230260849, recon_loss = 0.13850989937782288, 0.004025511909276247, kl_loss = 0.0034388015046715736\n",
      "\n",
      "Epoch 511\n",
      "Step 0: loss = 0.13752007484436035, recon_loss = 0.13183239102363586, 0.005681486800312996, kl_loss = 0.0031002741307020187\n",
      "\n",
      "Epoch 512\n",
      "Step 0: loss = 0.14132586121559143, recon_loss = 0.13663142919540405, 0.004686137195676565, kl_loss = 0.004149342887103558\n",
      "\n",
      "Epoch 513\n",
      "Step 0: loss = 0.13925586640834808, recon_loss = 0.13551324605941772, 0.0037364100571721792, kl_loss = 0.003106064163148403\n",
      "\n",
      "Epoch 514\n",
      "Step 0: loss = 0.14526958763599396, recon_loss = 0.14104428887367249, 0.004208574071526527, kl_loss = 0.00835915096104145\n",
      "\n",
      "Epoch 515\n",
      "Step 0: loss = 0.14166823029518127, recon_loss = 0.13623109459877014, 0.0054275598376989365, kl_loss = 0.0047944169491529465\n",
      "\n",
      "Epoch 516\n",
      "Step 0: loss = 0.14389830827713013, recon_loss = 0.1389051079750061, 0.004984466824680567, kl_loss = 0.004366427659988403\n",
      "\n",
      "Epoch 517\n",
      "Step 0: loss = 0.13959963619709015, recon_loss = 0.1353910267353058, 0.0041896263137459755, kl_loss = 0.00949188508093357\n",
      "\n",
      "Epoch 518\n",
      "Step 0: loss = 0.14083562791347504, recon_loss = 0.13568946719169617, 0.005138956941664219, kl_loss = 0.003595372661948204\n",
      "\n",
      "Epoch 519\n",
      "Step 0: loss = 0.13482043147087097, recon_loss = 0.1291346549987793, 0.005674744490534067, kl_loss = 0.0055147940292954445\n",
      "\n",
      "Epoch 520\n",
      "Step 0: loss = 0.14007964730262756, recon_loss = 0.13498526811599731, 0.005085383541882038, kl_loss = 0.004499349743127823\n",
      "\n",
      "Epoch 521\n",
      "Step 0: loss = 0.13115566968917847, recon_loss = 0.12518556416034698, 0.005955921486020088, kl_loss = 0.007093907333910465\n",
      "\n",
      "Epoch 522\n",
      "Step 0: loss = 0.13386499881744385, recon_loss = 0.12942057847976685, 0.004432108253240585, kl_loss = 0.0061529818922281265\n",
      "\n",
      "Epoch 523\n",
      "Step 0: loss = 0.13653412461280823, recon_loss = 0.132738396525383, 0.003788325469940901, kl_loss = 0.003705820068717003\n",
      "\n",
      "Epoch 524\n",
      "Step 0: loss = 0.13444682955741882, recon_loss = 0.12947990000247955, 0.0049604689702391624, kl_loss = 0.0032334355637431145\n",
      "\n",
      "Epoch 525\n",
      "Step 0: loss = 0.1314207762479782, recon_loss = 0.12697917222976685, 0.004433289170265198, kl_loss = 0.004157150164246559\n",
      "\n",
      "Epoch 526\n",
      "Step 0: loss = 0.13460451364517212, recon_loss = 0.13088828325271606, 0.0037016859278082848, kl_loss = 0.007273842580616474\n",
      "\n",
      "Epoch 527\n",
      "Step 0: loss = 0.1315890997648239, recon_loss = 0.1259518265724182, 0.005620962008833885, kl_loss = 0.008157567121088505\n",
      "\n",
      "Epoch 528\n",
      "Step 0: loss = 0.14189179241657257, recon_loss = 0.1361982673406601, 0.005683222785592079, kl_loss = 0.005148779600858688\n",
      "\n",
      "Epoch 529\n",
      "Step 0: loss = 0.13468199968338013, recon_loss = 0.12963923811912537, 0.005035521928220987, kl_loss = 0.0036240704357624054\n",
      "\n",
      "Epoch 530\n",
      "Step 0: loss = 0.1312185525894165, recon_loss = 0.12673421204090118, 0.004475608002394438, kl_loss = 0.004369598813354969\n",
      "\n",
      "Epoch 531\n",
      "Step 0: loss = 0.13197316229343414, recon_loss = 0.12744054198265076, 0.004516568500548601, kl_loss = 0.008020807057619095\n",
      "\n",
      "Epoch 532\n",
      "Step 0: loss = 0.13730721175670624, recon_loss = 0.13125993311405182, 0.00604085810482502, kl_loss = 0.0032142819836735725\n",
      "\n",
      "Epoch 533\n",
      "Step 0: loss = 0.1308702677488327, recon_loss = 0.1266576200723648, 0.00420778151601553, kl_loss = 0.00243949331343174\n",
      "\n",
      "Epoch 534\n",
      "Step 0: loss = 0.13097617030143738, recon_loss = 0.12558947503566742, 0.005376950837671757, kl_loss = 0.004874543286859989\n",
      "\n",
      "Epoch 535\n",
      "Step 0: loss = 0.14030686020851135, recon_loss = 0.1367616504430771, 0.003537744516506791, kl_loss = 0.0037311092019081116\n",
      "\n",
      "Epoch 536\n",
      "Step 0: loss = 0.13512133061885834, recon_loss = 0.13088563084602356, 0.004227064084261656, kl_loss = 0.004318315535783768\n",
      "\n",
      "Epoch 537\n",
      "Step 0: loss = 0.13311244547367096, recon_loss = 0.12788504362106323, 0.005216139368712902, kl_loss = 0.005635242909193039\n",
      "\n",
      "Epoch 538\n",
      "Step 0: loss = 0.13222914934158325, recon_loss = 0.12833938002586365, 0.003884892910718918, kl_loss = 0.0024371687322854996\n",
      "\n",
      "Epoch 539\n",
      "Step 0: loss = 0.13553538918495178, recon_loss = 0.130917489528656, 0.00460840156301856, kl_loss = 0.00474601611495018\n",
      "\n",
      "Epoch 540\n",
      "Step 0: loss = 0.1330442875623703, recon_loss = 0.1270487904548645, 0.0059845722280442715, kl_loss = 0.005460625514388084\n",
      "\n",
      "Epoch 541\n",
      "Step 0: loss = 0.13163670897483826, recon_loss = 0.1274200975894928, 0.004197854548692703, kl_loss = 0.00938299298286438\n",
      "\n",
      "Epoch 542\n",
      "Step 0: loss = 0.1330152153968811, recon_loss = 0.12744367122650146, 0.005562550388276577, kl_loss = 0.004502623341977596\n",
      "\n",
      "Epoch 543\n",
      "Step 0: loss = 0.12988674640655518, recon_loss = 0.12439118325710297, 0.005485743749886751, kl_loss = 0.004910181276500225\n",
      "\n",
      "Epoch 544\n",
      "Step 0: loss = 0.122858926653862, recon_loss = 0.11859821528196335, 0.004254555329680443, kl_loss = 0.0030769603326916695\n",
      "\n",
      "Epoch 545\n",
      "Step 0: loss = 0.1278487890958786, recon_loss = 0.12357306480407715, 0.004265873692929745, kl_loss = 0.004925123415887356\n",
      "\n",
      "Epoch 546\n",
      "Step 0: loss = 0.13485804200172424, recon_loss = 0.13054202497005463, 0.0042860060930252075, kl_loss = 0.015008866786956787\n",
      "\n",
      "Epoch 547\n",
      "Step 0: loss = 0.13010303676128387, recon_loss = 0.12520933151245117, 0.004886902868747711, kl_loss = 0.003407946787774563\n",
      "\n",
      "Epoch 548\n",
      "Step 0: loss = 0.1289772242307663, recon_loss = 0.12507264316082, 0.0038994853384792805, kl_loss = 0.002548651769757271\n",
      "\n",
      "Epoch 549\n",
      "Step 0: loss = 0.13117188215255737, recon_loss = 0.12575246393680573, 0.005413952749222517, kl_loss = 0.0027361176908016205\n",
      "\n",
      "Epoch 550\n",
      "Step 0: loss = 0.12929637730121613, recon_loss = 0.12470368295907974, 0.004586056340485811, kl_loss = 0.0033169426023960114\n",
      "\n",
      "Epoch 551\n",
      "Step 0: loss = 0.12365707755088806, recon_loss = 0.11789596080780029, 0.005745375528931618, kl_loss = 0.00786980427801609\n",
      "\n",
      "Epoch 552\n",
      "Step 0: loss = 0.12990595400333405, recon_loss = 0.12372500449419022, 0.006159638054668903, kl_loss = 0.010656407102942467\n",
      "\n",
      "Epoch 553\n",
      "Step 0: loss = 0.12500526010990143, recon_loss = 0.11922942101955414, 0.0057660238817334175, kl_loss = 0.004908710718154907\n",
      "\n",
      "Epoch 554\n",
      "Step 0: loss = 0.13262182474136353, recon_loss = 0.1276213824748993, 0.004992901347577572, kl_loss = 0.0037734657526016235\n",
      "\n",
      "Epoch 555\n",
      "Step 0: loss = 0.12472023069858551, recon_loss = 0.11987683176994324, 0.004823695868253708, kl_loss = 0.009851345792412758\n",
      "\n",
      "Epoch 556\n",
      "Step 0: loss = 0.1283283531665802, recon_loss = 0.12431678920984268, 0.003993355669081211, kl_loss = 0.009107092395424843\n",
      "\n",
      "Epoch 557\n",
      "Step 0: loss = 0.12933899462223053, recon_loss = 0.12415590137243271, 0.005173849873244762, kl_loss = 0.004617385566234589\n",
      "\n",
      "Epoch 558\n",
      "Step 0: loss = 0.1316039115190506, recon_loss = 0.1259383261203766, 0.005654447246342897, kl_loss = 0.005568636581301689\n",
      "\n",
      "Epoch 559\n",
      "Step 0: loss = 0.1311945915222168, recon_loss = 0.12525726854801178, 0.005915907211601734, kl_loss = 0.010703508742153645\n",
      "\n",
      "Epoch 560\n",
      "Step 0: loss = 0.13174407184123993, recon_loss = 0.12672781944274902, 0.00499742291867733, kl_loss = 0.009415822103619576\n",
      "\n",
      "Epoch 561\n",
      "Step 0: loss = 0.1311020702123642, recon_loss = 0.1248234361410141, 0.006256905384361744, kl_loss = 0.010861184448003769\n",
      "\n",
      "Epoch 562\n",
      "Step 0: loss = 0.12695728242397308, recon_loss = 0.12211814522743225, 0.004824909847229719, kl_loss = 0.007113851606845856\n",
      "\n",
      "Epoch 563\n",
      "Step 0: loss = 0.11966323852539062, recon_loss = 0.11524489521980286, 0.004407060332596302, kl_loss = 0.005641879513859749\n",
      "\n",
      "Epoch 564\n",
      "Step 0: loss = 0.1236787810921669, recon_loss = 0.11957740783691406, 0.00409548357129097, kl_loss = 0.002948485314846039\n",
      "\n",
      "Epoch 565\n",
      "Step 0: loss = 0.12271807342767715, recon_loss = 0.1183476522564888, 0.004351753741502762, kl_loss = 0.009333217516541481\n",
      "\n",
      "Epoch 566\n",
      "Step 0: loss = 0.11945603787899017, recon_loss = 0.1144586130976677, 0.004977963864803314, kl_loss = 0.00972956232726574\n",
      "\n",
      "Epoch 567\n",
      "Step 0: loss = 0.12741507589817047, recon_loss = 0.12359801679849625, 0.0038097465876489878, kl_loss = 0.0036580227315425873\n",
      "\n",
      "Epoch 568\n",
      "Step 0: loss = 0.11872932314872742, recon_loss = 0.11461222916841507, 0.00410978589206934, kl_loss = 0.003652779385447502\n",
      "\n",
      "Epoch 569\n",
      "Step 0: loss = 0.12282254546880722, recon_loss = 0.11747512221336365, 0.005338676273822784, kl_loss = 0.004375035874545574\n",
      "\n",
      "Epoch 570\n",
      "Step 0: loss = 0.1275622546672821, recon_loss = 0.12139520049095154, 0.00615714630112052, kl_loss = 0.004953909665346146\n",
      "\n",
      "Epoch 571\n",
      "Step 0: loss = 0.12650160491466522, recon_loss = 0.12148399651050568, 0.00501080509275198, kl_loss = 0.003405144438147545\n",
      "\n",
      "Epoch 572\n",
      "Step 0: loss = 0.12796185910701752, recon_loss = 0.1235317811369896, 0.004420502111315727, kl_loss = 0.004791475832462311\n",
      "\n",
      "Epoch 573\n",
      "Step 0: loss = 0.1265030950307846, recon_loss = 0.12132056802511215, 0.005172979086637497, kl_loss = 0.004774010740220547\n",
      "\n",
      "Epoch 574\n",
      "Step 0: loss = 0.12336646765470505, recon_loss = 0.11660850048065186, 0.00674941623583436, kl_loss = 0.004275145940482616\n",
      "\n",
      "Epoch 575\n",
      "Step 0: loss = 0.12624184787273407, recon_loss = 0.12053968012332916, 0.005691462196409702, kl_loss = 0.005348232574760914\n",
      "\n",
      "Epoch 576\n",
      "Step 0: loss = 0.12120717763900757, recon_loss = 0.11565277725458145, 0.0055433413945138454, kl_loss = 0.00552965234965086\n",
      "\n",
      "Epoch 577\n",
      "Step 0: loss = 0.12171662598848343, recon_loss = 0.1146014854311943, 0.00711057148873806, kl_loss = 0.002283932641148567\n",
      "\n",
      "Epoch 578\n",
      "Step 0: loss = 0.12452033907175064, recon_loss = 0.11976521462202072, 0.0047490838915109634, kl_loss = 0.003019767813384533\n",
      "\n",
      "Epoch 579\n",
      "Step 0: loss = 0.12295861542224884, recon_loss = 0.11809590458869934, 0.0048550767824053764, kl_loss = 0.003818127326667309\n",
      "\n",
      "Epoch 580\n",
      "Step 0: loss = 0.11827600747346878, recon_loss = 0.1128661036491394, 0.005384982563555241, kl_loss = 0.012460453435778618\n",
      "\n",
      "Epoch 581\n",
      "Step 0: loss = 0.12316162139177322, recon_loss = 0.11768743395805359, 0.005456962622702122, kl_loss = 0.0086135882884264\n",
      "\n",
      "Epoch 582\n",
      "Step 0: loss = 0.11929729580879211, recon_loss = 0.11362363398075104, 0.005659096874296665, kl_loss = 0.007281698286533356\n",
      "\n",
      "Epoch 583\n",
      "Step 0: loss = 0.12202227860689163, recon_loss = 0.11756567656993866, 0.004445935133844614, kl_loss = 0.005334420129656792\n",
      "\n",
      "Epoch 584\n",
      "Step 0: loss = 0.1304207146167755, recon_loss = 0.12336081266403198, 0.007039458490908146, kl_loss = 0.010224106721580029\n",
      "\n",
      "Epoch 585\n",
      "Step 0: loss = 0.12469140440225601, recon_loss = 0.12055230140686035, 0.004129660315811634, kl_loss = 0.004718628711998463\n",
      "\n",
      "Epoch 586\n",
      "Step 0: loss = 0.1221831887960434, recon_loss = 0.11752074956893921, 0.004647558555006981, kl_loss = 0.007440967485308647\n",
      "\n",
      "Epoch 587\n",
      "Step 0: loss = 0.1200781911611557, recon_loss = 0.11529859900474548, 0.00476166931912303, kl_loss = 0.008964200504124165\n",
      "\n",
      "Epoch 588\n",
      "Step 0: loss = 0.1242767795920372, recon_loss = 0.11909732967615128, 0.0051653687842190266, kl_loss = 0.007039823569357395\n",
      "\n",
      "Epoch 589\n",
      "Step 0: loss = 0.12179599702358246, recon_loss = 0.1165638267993927, 0.0052117579616606236, kl_loss = 0.010205669328570366\n",
      "\n",
      "Epoch 590\n",
      "Step 0: loss = 0.12516933679580688, recon_loss = 0.11992116272449493, 0.005222023464739323, kl_loss = 0.013074449263513088\n",
      "\n",
      "Epoch 591\n",
      "Step 0: loss = 0.12072411924600601, recon_loss = 0.11529023200273514, 0.005425739102065563, kl_loss = 0.004076801240444183\n",
      "\n",
      "Epoch 592\n",
      "Step 0: loss = 0.11700280010700226, recon_loss = 0.11089316755533218, 0.006104224361479282, kl_loss = 0.0027046287432312965\n",
      "\n",
      "Epoch 593\n",
      "Step 0: loss = 0.11898521333932877, recon_loss = 0.11388776451349258, 0.0050918543711304665, kl_loss = 0.0027986131608486176\n",
      "\n",
      "Epoch 594\n",
      "Step 0: loss = 0.12013518065214157, recon_loss = 0.11354263871908188, 0.006581000052392483, kl_loss = 0.005771718919277191\n",
      "\n",
      "Epoch 595\n",
      "Step 0: loss = 0.12623994052410126, recon_loss = 0.12039287388324738, 0.0058401478454470634, kl_loss = 0.0034602973610162735\n",
      "\n",
      "Epoch 596\n",
      "Step 0: loss = 0.11303223669528961, recon_loss = 0.1071443036198616, 0.00588252441957593, kl_loss = 0.002702942118048668\n",
      "\n",
      "Epoch 597\n",
      "Step 0: loss = 0.1160794273018837, recon_loss = 0.11148993670940399, 0.004577662795782089, kl_loss = 0.005910918116569519\n",
      "\n",
      "Epoch 598\n",
      "Step 0: loss = 0.1222924217581749, recon_loss = 0.1170446127653122, 0.0052330512553453445, kl_loss = 0.0073807984590530396\n",
      "\n",
      "Epoch 599\n",
      "Step 0: loss = 0.11638791859149933, recon_loss = 0.1129845380783081, 0.003388306125998497, kl_loss = 0.007537761703133583\n",
      "\n",
      "Epoch 600\n",
      "Step 0: loss = 0.11280311644077301, recon_loss = 0.10691552609205246, 0.005879485048353672, kl_loss = 0.004052230156958103\n",
      "\n",
      "Epoch 601\n",
      "Step 0: loss = 0.11506862938404083, recon_loss = 0.10951191931962967, 0.005550933536142111, kl_loss = 0.002888447605073452\n",
      "\n",
      "Epoch 602\n",
      "Step 0: loss = 0.11822517216205597, recon_loss = 0.1132490336894989, 0.004970823880285025, kl_loss = 0.002657252363860607\n",
      "\n",
      "Epoch 603\n",
      "Step 0: loss = 0.11554247885942459, recon_loss = 0.1108526885509491, 0.004678331781178713, kl_loss = 0.005727733485400677\n",
      "\n",
      "Epoch 604\n",
      "Step 0: loss = 0.1179603710770607, recon_loss = 0.11222624778747559, 0.005726765841245651, kl_loss = 0.0036756135523319244\n",
      "\n",
      "Epoch 605\n",
      "Step 0: loss = 0.11147205531597137, recon_loss = 0.10755380988121033, 0.003905180376023054, kl_loss = 0.0065344469621777534\n",
      "\n",
      "Epoch 606\n",
      "Step 0: loss = 0.11733514070510864, recon_loss = 0.11251459270715714, 0.004801041912287474, kl_loss = 0.009754289872944355\n",
      "\n",
      "Epoch 607\n",
      "Step 0: loss = 0.11644195020198822, recon_loss = 0.11068161576986313, 0.0057428935542702675, kl_loss = 0.008721482008695602\n",
      "\n",
      "Epoch 608\n",
      "Step 0: loss = 0.11606648564338684, recon_loss = 0.11052960157394409, 0.005522309802472591, kl_loss = 0.007285729981958866\n",
      "\n",
      "Epoch 609\n",
      "Step 0: loss = 0.1182178407907486, recon_loss = 0.11236955225467682, 0.005840430036187172, kl_loss = 0.003929526545107365\n",
      "\n",
      "Epoch 610\n",
      "Step 0: loss = 0.11415590345859528, recon_loss = 0.10863211750984192, 0.005512399598956108, kl_loss = 0.005690588615834713\n",
      "\n",
      "Epoch 611\n",
      "Step 0: loss = 0.12378545105457306, recon_loss = 0.11434490978717804, 0.009427745826542377, kl_loss = 0.006396135315299034\n",
      "\n",
      "Epoch 612\n",
      "Step 0: loss = 0.11826365441083908, recon_loss = 0.11064831167459488, 0.007584678940474987, kl_loss = 0.015331435017287731\n",
      "\n",
      "Epoch 613\n",
      "Step 0: loss = 0.11067558825016022, recon_loss = 0.10472694039344788, 0.0059344833716750145, kl_loss = 0.007081079296767712\n",
      "\n",
      "Epoch 614\n",
      "Step 0: loss = 0.11731848865747452, recon_loss = 0.11079863458871841, 0.006508167367428541, kl_loss = 0.005845045670866966\n",
      "\n",
      "Epoch 615\n",
      "Step 0: loss = 0.12295600771903992, recon_loss = 0.11481248587369919, 0.008127613924443722, kl_loss = 0.007953967899084091\n",
      "\n",
      "Epoch 616\n",
      "Step 0: loss = 0.11315088719129562, recon_loss = 0.10819456726312637, 0.004941241350024939, kl_loss = 0.007541723549365997\n",
      "\n",
      "Epoch 617\n",
      "Step 0: loss = 0.11218811571598053, recon_loss = 0.10704493522644043, 0.005133307538926601, kl_loss = 0.004935239441692829\n",
      "\n",
      "Epoch 618\n",
      "Step 0: loss = 0.11512525379657745, recon_loss = 0.10764555633068085, 0.007466338574886322, kl_loss = 0.006679150275886059\n",
      "\n",
      "Epoch 619\n",
      "Step 0: loss = 0.11408710479736328, recon_loss = 0.10710509121417999, 0.00697123259305954, kl_loss = 0.0053901346400380135\n",
      "\n",
      "Epoch 620\n",
      "Step 0: loss = 0.11163987964391708, recon_loss = 0.10533370077610016, 0.0062920404598116875, kl_loss = 0.007070406340062618\n",
      "\n",
      "Epoch 621\n",
      "Step 0: loss = 0.11115063726902008, recon_loss = 0.10570350289344788, 0.005432258825749159, kl_loss = 0.007438888773322105\n",
      "\n",
      "Epoch 622\n",
      "Step 0: loss = 0.1133737862110138, recon_loss = 0.10732297599315643, 0.006040220148861408, kl_loss = 0.005294434726238251\n",
      "\n",
      "Epoch 623\n",
      "Step 0: loss = 0.11085669696331024, recon_loss = 0.10272698104381561, 0.008118042722344398, kl_loss = 0.005836158059537411\n",
      "\n",
      "Epoch 624\n",
      "Step 0: loss = 0.12068666517734528, recon_loss = 0.11445048451423645, 0.006220692303031683, kl_loss = 0.0077455900609493256\n",
      "\n",
      "Epoch 625\n",
      "Step 0: loss = 0.10987824946641922, recon_loss = 0.10517942905426025, 0.004687069449573755, kl_loss = 0.005873595364391804\n",
      "\n",
      "Epoch 626\n",
      "Step 0: loss = 0.10934629291296005, recon_loss = 0.1039445698261261, 0.00539455795660615, kl_loss = 0.0035845553502440453\n",
      "\n",
      "Epoch 627\n",
      "Step 0: loss = 0.11076939105987549, recon_loss = 0.10393718630075455, 0.0068202996626496315, kl_loss = 0.005951853469014168\n",
      "\n",
      "Epoch 628\n",
      "Step 0: loss = 0.11014680564403534, recon_loss = 0.10567401349544525, 0.004465999081730843, kl_loss = 0.0033986959606409073\n",
      "\n",
      "Epoch 629\n",
      "Step 0: loss = 0.1159944236278534, recon_loss = 0.10930492728948593, 0.006681906990706921, kl_loss = 0.003796640783548355\n",
      "\n",
      "Epoch 630\n",
      "Step 0: loss = 0.10781922191381454, recon_loss = 0.10147327929735184, 0.00633675791323185, kl_loss = 0.004594405181705952\n",
      "\n",
      "Epoch 631\n",
      "Step 0: loss = 0.11420544981956482, recon_loss = 0.10856672376394272, 0.005630754865705967, kl_loss = 0.003985274583101273\n",
      "\n",
      "Epoch 632\n",
      "Step 0: loss = 0.11233455687761307, recon_loss = 0.10566136986017227, 0.006660571787506342, kl_loss = 0.006308551877737045\n",
      "\n",
      "Epoch 633\n",
      "Step 0: loss = 0.11330738663673401, recon_loss = 0.10460127145051956, 0.008696412667632103, kl_loss = 0.0048500364646315575\n",
      "\n",
      "Epoch 634\n",
      "Step 0: loss = 0.11691562086343765, recon_loss = 0.10950138419866562, 0.007395179942250252, kl_loss = 0.009530783630907536\n",
      "\n",
      "Epoch 635\n",
      "Step 0: loss = 0.11616434901952744, recon_loss = 0.10954827070236206, 0.006598923355340958, kl_loss = 0.008579890243709087\n",
      "\n",
      "Epoch 636\n",
      "Step 0: loss = 0.10785093903541565, recon_loss = 0.10044682025909424, 0.007384057156741619, kl_loss = 0.010033906437456608\n",
      "\n",
      "Epoch 637\n",
      "Step 0: loss = 0.10990116745233536, recon_loss = 0.10328349471092224, 0.006606207229197025, kl_loss = 0.005733148194849491\n",
      "\n",
      "Epoch 638\n",
      "Step 0: loss = 0.10569097846746445, recon_loss = 0.09929579496383667, 0.006384510546922684, kl_loss = 0.005338676273822784\n",
      "\n",
      "Epoch 639\n",
      "Step 0: loss = 0.10681714117527008, recon_loss = 0.10161292552947998, 0.005193063523620367, kl_loss = 0.005575682036578655\n",
      "\n",
      "Epoch 640\n",
      "Step 0: loss = 0.11183611303567886, recon_loss = 0.10797633230686188, 0.0038446388207376003, kl_loss = 0.007570693269371986\n",
      "\n",
      "Epoch 641\n",
      "Step 0: loss = 0.10464387387037277, recon_loss = 0.09863719344139099, 0.005995917599648237, kl_loss = 0.005383145064115524\n",
      "\n",
      "Epoch 642\n",
      "Step 0: loss = 0.10596107691526413, recon_loss = 0.09933938086032867, 0.00661473348736763, kl_loss = 0.0034827785566449165\n",
      "\n",
      "Epoch 643\n",
      "Step 0: loss = 0.10851925611495972, recon_loss = 0.10189526528120041, 0.006612411700189114, kl_loss = 0.0057881735265254974\n",
      "\n",
      "Epoch 644\n",
      "Step 0: loss = 0.10281923413276672, recon_loss = 0.09780901670455933, 0.005003097467124462, kl_loss = 0.0035625947639346123\n",
      "\n",
      "Epoch 645\n",
      "Step 0: loss = 0.107649065554142, recon_loss = 0.10223530232906342, 0.005408434197306633, kl_loss = 0.0026642270386219025\n",
      "\n",
      "Epoch 646\n",
      "Step 0: loss = 0.1057884618639946, recon_loss = 0.09973184019327164, 0.006049641408026218, kl_loss = 0.003490002825856209\n",
      "\n",
      "Epoch 647\n",
      "Step 0: loss = 0.11344479769468307, recon_loss = 0.10746254771947861, 0.005973038263618946, kl_loss = 0.004604228772222996\n",
      "\n",
      "Epoch 648\n",
      "Step 0: loss = 0.11247871816158295, recon_loss = 0.10403016209602356, 0.008443029597401619, kl_loss = 0.002764195203781128\n",
      "\n",
      "Epoch 649\n",
      "Step 0: loss = 0.11128433048725128, recon_loss = 0.1036313995718956, 0.007641103584319353, kl_loss = 0.005911669693887234\n",
      "\n",
      "Epoch 650\n",
      "Step 0: loss = 0.10701901465654373, recon_loss = 0.09793488681316376, 0.009074827656149864, kl_loss = 0.004648471251130104\n",
      "\n",
      "Epoch 651\n",
      "Step 0: loss = 0.11205518990755081, recon_loss = 0.10446515679359436, 0.007576159201562405, kl_loss = 0.006936999969184399\n",
      "\n",
      "Epoch 652\n",
      "Step 0: loss = 0.10489591956138611, recon_loss = 0.09835084527730942, 0.006537296809256077, kl_loss = 0.0038891853764653206\n",
      "\n",
      "Epoch 653\n",
      "Step 0: loss = 0.10414595901966095, recon_loss = 0.0985211580991745, 0.005618572235107422, kl_loss = 0.0031139040365815163\n",
      "\n",
      "Epoch 654\n",
      "Step 0: loss = 0.11254826188087463, recon_loss = 0.10421555489301682, 0.008325858041644096, kl_loss = 0.003421945497393608\n",
      "\n",
      "Epoch 655\n",
      "Step 0: loss = 0.10123012214899063, recon_loss = 0.09573760628700256, 0.005481535568833351, kl_loss = 0.005491514690220356\n",
      "\n",
      "Epoch 656\n",
      "Step 0: loss = 0.10703960806131363, recon_loss = 0.09990472346544266, 0.007126099430024624, kl_loss = 0.004392178729176521\n",
      "\n",
      "Epoch 657\n",
      "Step 0: loss = 0.10334121435880661, recon_loss = 0.09689003974199295, 0.006442376412451267, kl_loss = 0.004398948512971401\n",
      "\n",
      "Epoch 658\n",
      "Step 0: loss = 0.10772062093019485, recon_loss = 0.09953464567661285, 0.008175930008292198, kl_loss = 0.005023217760026455\n",
      "\n",
      "Epoch 659\n",
      "Step 0: loss = 0.10698725283145905, recon_loss = 0.10077522695064545, 0.006203405559062958, kl_loss = 0.004309283569455147\n",
      "\n",
      "Epoch 660\n",
      "Step 0: loss = 0.10523428022861481, recon_loss = 0.09937823563814163, 0.0058492692187428474, kl_loss = 0.0033878162503242493\n",
      "\n",
      "Epoch 661\n",
      "Step 0: loss = 0.10593321174383163, recon_loss = 0.10039912909269333, 0.0055240001529455185, kl_loss = 0.005042004399001598\n",
      "\n",
      "Epoch 662\n",
      "Step 0: loss = 0.10872256755828857, recon_loss = 0.1013278067111969, 0.007376393303275108, kl_loss = 0.009183695539832115\n",
      "\n",
      "Epoch 663\n",
      "Step 0: loss = 0.1032751277089119, recon_loss = 0.09402383863925934, 0.009233243763446808, kl_loss = 0.009022550657391548\n",
      "\n",
      "Epoch 664\n",
      "Step 0: loss = 0.10972107201814651, recon_loss = 0.10321494936943054, 0.0064951153472065926, kl_loss = 0.005502516403794289\n",
      "\n",
      "Epoch 665\n",
      "Step 0: loss = 0.1036376953125, recon_loss = 0.09609788656234741, 0.007534215226769447, kl_loss = 0.0027958713471889496\n",
      "\n",
      "Epoch 666\n",
      "Step 0: loss = 0.1059102714061737, recon_loss = 0.09818747639656067, 0.007689458318054676, kl_loss = 0.0166656244546175\n",
      "\n",
      "Epoch 667\n",
      "Step 0: loss = 0.10827645659446716, recon_loss = 0.1014622151851654, 0.006804490461945534, kl_loss = 0.004876008257269859\n",
      "\n",
      "Epoch 668\n",
      "Step 0: loss = 0.10927015542984009, recon_loss = 0.10240302979946136, 0.006855242419987917, kl_loss = 0.005943678319454193\n",
      "\n",
      "Epoch 669\n",
      "Step 0: loss = 0.10300604999065399, recon_loss = 0.09609426558017731, 0.006898427847772837, kl_loss = 0.006678842008113861\n",
      "\n",
      "Epoch 670\n",
      "Step 0: loss = 0.10976828634738922, recon_loss = 0.10094105452299118, 0.008813844993710518, kl_loss = 0.006692529655992985\n",
      "\n",
      "Epoch 671\n",
      "Step 0: loss = 0.10258032381534576, recon_loss = 0.0973547101020813, 0.0052195023745298386, kl_loss = 0.0030529722571372986\n",
      "\n",
      "Epoch 672\n",
      "Step 0: loss = 0.10169962048530579, recon_loss = 0.09635283797979355, 0.005342060700058937, kl_loss = 0.0023630401119589806\n",
      "\n",
      "Epoch 673\n",
      "Step 0: loss = 0.09913262724876404, recon_loss = 0.09290029108524323, 0.006224900484085083, kl_loss = 0.0037170397117733955\n",
      "\n",
      "Epoch 674\n",
      "Step 0: loss = 0.10649854689836502, recon_loss = 0.10080002248287201, 0.005690748803317547, kl_loss = 0.0038903430104255676\n",
      "\n",
      "Epoch 675\n",
      "Step 0: loss = 0.10515403747558594, recon_loss = 0.09774738550186157, 0.007393141742795706, kl_loss = 0.006755116395652294\n",
      "\n",
      "Epoch 676\n",
      "Step 0: loss = 0.102482870221138, recon_loss = 0.09611659497022629, 0.006355213932693005, kl_loss = 0.005530396476387978\n",
      "\n",
      "Epoch 677\n",
      "Step 0: loss = 0.09865251183509827, recon_loss = 0.09175307303667068, 0.006888850592076778, kl_loss = 0.005295328795909882\n",
      "\n",
      "Epoch 678\n",
      "Step 0: loss = 0.10008610039949417, recon_loss = 0.09435775130987167, 0.005716896150261164, kl_loss = 0.00572634395211935\n",
      "\n",
      "Epoch 679\n",
      "Step 0: loss = 0.10041970759630203, recon_loss = 0.09292857348918915, 0.00747949443757534, kl_loss = 0.005818088538944721\n",
      "\n",
      "Epoch 680\n",
      "Step 0: loss = 0.10384970903396606, recon_loss = 0.09609699249267578, 0.007740640547126532, kl_loss = 0.006037572398781776\n",
      "\n",
      "Epoch 681\n",
      "Step 0: loss = 0.10429201275110245, recon_loss = 0.09706657379865646, 0.007219164166599512, kl_loss = 0.0031356969848275185\n",
      "\n",
      "Epoch 682\n",
      "Step 0: loss = 0.09733834862709045, recon_loss = 0.0895594134926796, 0.0077713048085570335, kl_loss = 0.0038146227598190308\n",
      "\n",
      "Epoch 683\n",
      "Step 0: loss = 0.10221640765666962, recon_loss = 0.09467156231403351, 0.007537882775068283, kl_loss = 0.0034782811999320984\n",
      "\n",
      "Epoch 684\n",
      "Step 0: loss = 0.10414458066225052, recon_loss = 0.09745959937572479, 0.006677309051156044, kl_loss = 0.003835262730717659\n",
      "\n",
      "Epoch 685\n",
      "Step 0: loss = 0.104957215487957, recon_loss = 0.0972478911280632, 0.0077017080038785934, kl_loss = 0.003805425949394703\n",
      "\n",
      "Epoch 686\n",
      "Step 0: loss = 0.10256273299455643, recon_loss = 0.09562750160694122, 0.006925438530743122, kl_loss = 0.004896553233265877\n",
      "\n",
      "Epoch 687\n",
      "Step 0: loss = 0.09768066555261612, recon_loss = 0.09000182151794434, 0.0076680975034832954, kl_loss = 0.0053717102855443954\n",
      "\n",
      "Epoch 688\n",
      "Step 0: loss = 0.10061663389205933, recon_loss = 0.09396767616271973, 0.006636441685259342, kl_loss = 0.006259506568312645\n",
      "\n",
      "Epoch 689\n",
      "Step 0: loss = 0.10201434791088104, recon_loss = 0.09502105414867401, 0.006984289735555649, kl_loss = 0.004499678499996662\n",
      "\n",
      "Epoch 690\n",
      "Step 0: loss = 0.10418754070997238, recon_loss = 0.09751024842262268, 0.006668263114988804, kl_loss = 0.004513971507549286\n",
      "\n",
      "Epoch 691\n",
      "Step 0: loss = 0.101458840072155, recon_loss = 0.09392538666725159, 0.0075258249416947365, kl_loss = 0.00381611380726099\n",
      "\n",
      "Epoch 692\n",
      "Step 0: loss = 0.10155188292264938, recon_loss = 0.09358018636703491, 0.007962491363286972, kl_loss = 0.00460013747215271\n",
      "\n",
      "Epoch 693\n",
      "Step 0: loss = 0.1014566496014595, recon_loss = 0.09549307823181152, 0.005953322164714336, kl_loss = 0.005127063021063805\n",
      "\n",
      "Epoch 694\n",
      "Step 0: loss = 0.09926064312458038, recon_loss = 0.09391164779663086, 0.0053419992327690125, kl_loss = 0.003497549332678318\n",
      "\n",
      "Epoch 695\n",
      "Step 0: loss = 0.09993552416563034, recon_loss = 0.09311263263225555, 0.0068166060373187065, kl_loss = 0.0031432127580046654\n",
      "\n",
      "Epoch 696\n",
      "Step 0: loss = 0.0988556444644928, recon_loss = 0.09214421361684799, 0.006705150939524174, kl_loss = 0.0031420253217220306\n",
      "\n",
      "Epoch 697\n",
      "Step 0: loss = 0.09112613648176193, recon_loss = 0.08427653461694717, 0.006842735689133406, kl_loss = 0.003436489962041378\n",
      "\n",
      "Epoch 698\n",
      "Step 0: loss = 0.09912938624620438, recon_loss = 0.09210073947906494, 0.0070195370353758335, kl_loss = 0.004555658437311649\n",
      "\n",
      "Epoch 699\n",
      "Step 0: loss = 0.0984693393111229, recon_loss = 0.09186027199029922, 0.006602061912417412, kl_loss = 0.0035030487924814224\n",
      "\n",
      "Epoch 700\n",
      "Step 0: loss = 0.09767093509435654, recon_loss = 0.08993248641490936, 0.007729285396635532, kl_loss = 0.004580584354698658\n",
      "\n",
      "Epoch 701\n",
      "Step 0: loss = 0.09796682745218277, recon_loss = 0.09089915454387665, 0.007049700245261192, kl_loss = 0.008983809500932693\n",
      "\n",
      "Epoch 702\n",
      "Step 0: loss = 0.0962497740983963, recon_loss = 0.08812282234430313, 0.008116740733385086, kl_loss = 0.005103209987282753\n",
      "\n",
      "Epoch 703\n",
      "Step 0: loss = 0.0981336161494255, recon_loss = 0.0902816504240036, 0.007843032479286194, kl_loss = 0.004467024467885494\n",
      "\n",
      "Epoch 704\n",
      "Step 0: loss = 0.09175101667642593, recon_loss = 0.082949198782444, 0.008789020590484142, kl_loss = 0.006398878991603851\n",
      "\n",
      "Epoch 705\n",
      "Step 0: loss = 0.10307005047798157, recon_loss = 0.0916866809129715, 0.011370167136192322, kl_loss = 0.006601080298423767\n",
      "\n",
      "Epoch 706\n",
      "Step 0: loss = 0.09512009471654892, recon_loss = 0.08634042739868164, 0.008760437369346619, kl_loss = 0.009615405462682247\n",
      "\n",
      "Epoch 707\n",
      "Step 0: loss = 0.09138158708810806, recon_loss = 0.08239967375993729, 0.008971039205789566, kl_loss = 0.005434354767203331\n",
      "\n",
      "Epoch 708\n",
      "Step 0: loss = 0.09810620546340942, recon_loss = 0.08882357180118561, 0.009265268221497536, kl_loss = 0.008682924322783947\n",
      "\n",
      "Epoch 709\n",
      "Step 0: loss = 0.09626813232898712, recon_loss = 0.08804575353860855, 0.008200965821743011, kl_loss = 0.010704963468015194\n",
      "\n",
      "Epoch 710\n",
      "Step 0: loss = 0.09955673664808273, recon_loss = 0.0875670462846756, 0.011977285146713257, kl_loss = 0.00620432011783123\n",
      "\n",
      "Epoch 711\n",
      "Step 0: loss = 0.09555660933256149, recon_loss = 0.0870976448059082, 0.008440989069640636, kl_loss = 0.008988033048808575\n",
      "\n",
      "Epoch 712\n",
      "Step 0: loss = 0.09531805664300919, recon_loss = 0.08781721442937851, 0.00748264417052269, kl_loss = 0.009100141935050488\n",
      "\n",
      "Epoch 713\n",
      "Step 0: loss = 0.09804810583591461, recon_loss = 0.08846528083086014, 0.009569846093654633, kl_loss = 0.0064910901710391045\n",
      "\n",
      "Epoch 714\n",
      "Step 0: loss = 0.0983143001794815, recon_loss = 0.09052596241235733, 0.007771098054945469, kl_loss = 0.00861916784197092\n",
      "\n",
      "Epoch 715\n",
      "Step 0: loss = 0.0927392914891243, recon_loss = 0.08777343481779099, 0.004956003278493881, kl_loss = 0.004927115514874458\n",
      "\n",
      "Epoch 716\n",
      "Step 0: loss = 0.0934443399310112, recon_loss = 0.08521086722612381, 0.00822436809539795, kl_loss = 0.004550560377538204\n",
      "\n",
      "Epoch 717\n",
      "Step 0: loss = 0.09596620500087738, recon_loss = 0.08851877599954605, 0.007435695268213749, kl_loss = 0.005865518935024738\n",
      "\n",
      "Epoch 718\n",
      "Step 0: loss = 0.09895870834589005, recon_loss = 0.0915675163269043, 0.00738434586673975, kl_loss = 0.0034231506288051605\n",
      "\n",
      "Epoch 719\n",
      "Step 0: loss = 0.09601827710866928, recon_loss = 0.08718059957027435, 0.008826738223433495, kl_loss = 0.005467267706990242\n",
      "\n",
      "Epoch 720\n",
      "Step 0: loss = 0.09728068113327026, recon_loss = 0.08841317892074585, 0.008854065090417862, kl_loss = 0.006719172932207584\n",
      "\n",
      "Epoch 721\n",
      "Step 0: loss = 0.09664683043956757, recon_loss = 0.08783689886331558, 0.00880067702382803, kl_loss = 0.0046251313760876656\n",
      "\n",
      "Epoch 722\n",
      "Step 0: loss = 0.08994396775960922, recon_loss = 0.08241850882768631, 0.007512450218200684, kl_loss = 0.006503969430923462\n",
      "\n",
      "Epoch 723\n",
      "Step 0: loss = 0.0939684584736824, recon_loss = 0.08518056571483612, 0.008771073073148727, kl_loss = 0.008408792316913605\n",
      "\n",
      "Epoch 724\n",
      "Step 0: loss = 0.09538876265287399, recon_loss = 0.08859877288341522, 0.006779918447136879, kl_loss = 0.005035038106143475\n",
      "\n",
      "Epoch 725\n",
      "Step 0: loss = 0.10148076713085175, recon_loss = 0.09219847619533539, 0.009270450100302696, kl_loss = 0.005920544266700745\n",
      "\n",
      "Epoch 726\n",
      "Step 0: loss = 0.10082167387008667, recon_loss = 0.09005484730005264, 0.010749013163149357, kl_loss = 0.008908962830901146\n",
      "\n",
      "Epoch 727\n",
      "Step 0: loss = 0.09863341599702835, recon_loss = 0.08799289911985397, 0.010625706985592842, kl_loss = 0.007405523210763931\n",
      "\n",
      "Epoch 728\n",
      "Step 0: loss = 0.0998746007680893, recon_loss = 0.08962211012840271, 0.010240936651825905, kl_loss = 0.005777244456112385\n",
      "\n",
      "Epoch 729\n",
      "Step 0: loss = 0.0984974056482315, recon_loss = 0.0879686176776886, 0.01051641907542944, kl_loss = 0.006183965131640434\n",
      "\n",
      "Epoch 730\n",
      "Step 0: loss = 0.09814861416816711, recon_loss = 0.08825428038835526, 0.009885106235742569, kl_loss = 0.004610673524439335\n",
      "\n",
      "Epoch 731\n",
      "Step 0: loss = 0.09679082781076431, recon_loss = 0.08714619278907776, 0.009631129913032055, kl_loss = 0.006754469126462936\n",
      "\n",
      "Epoch 732\n",
      "Step 0: loss = 0.09424445778131485, recon_loss = 0.08580227196216583, 0.00842191930860281, kl_loss = 0.010134224779903889\n",
      "\n",
      "Epoch 733\n",
      "Step 0: loss = 0.09489468485116959, recon_loss = 0.08497157692909241, 0.009905559942126274, kl_loss = 0.00877364818006754\n",
      "\n",
      "Epoch 734\n",
      "Step 0: loss = 0.09179205447435379, recon_loss = 0.08117834478616714, 0.010597538203001022, kl_loss = 0.008089440874755383\n",
      "\n",
      "Epoch 735\n",
      "Step 0: loss = 0.09333274513483047, recon_loss = 0.08292567729949951, 0.010398677550256252, kl_loss = 0.0041961828246712685\n",
      "\n",
      "Epoch 736\n",
      "Step 0: loss = 0.09226376563310623, recon_loss = 0.08250914514064789, 0.009748061187565327, kl_loss = 0.003278736025094986\n",
      "\n",
      "Epoch 737\n",
      "Step 0: loss = 0.09508147835731506, recon_loss = 0.08395485579967499, 0.011120995506644249, kl_loss = 0.0028141122311353683\n",
      "\n",
      "Epoch 738\n",
      "Step 0: loss = 0.0880010575056076, recon_loss = 0.07852569967508316, 0.009464744478464127, kl_loss = 0.005304742604494095\n",
      "\n",
      "Epoch 739\n",
      "Step 0: loss = 0.09714113175868988, recon_loss = 0.08812188357114792, 0.009007605724036694, kl_loss = 0.0058210063725709915\n",
      "\n",
      "Epoch 740\n",
      "Step 0: loss = 0.09489739686250687, recon_loss = 0.08533431589603424, 0.009554914198815823, kl_loss = 0.0040824199095368385\n",
      "\n",
      "Epoch 741\n",
      "Step 0: loss = 0.08714935928583145, recon_loss = 0.07839351892471313, 0.00874612107872963, kl_loss = 0.004859890788793564\n",
      "\n",
      "Epoch 742\n",
      "Step 0: loss = 0.09076225012540817, recon_loss = 0.08140053600072861, 0.00935453176498413, kl_loss = 0.0035902513191103935\n",
      "\n",
      "Epoch 743\n",
      "Step 0: loss = 0.0936865359544754, recon_loss = 0.0836741030216217, 0.010003981180489063, kl_loss = 0.004225864075124264\n",
      "\n",
      "Epoch 744\n",
      "Step 0: loss = 0.08978684991598129, recon_loss = 0.082207590341568, 0.00757159199565649, kl_loss = 0.003834674134850502\n",
      "\n",
      "Epoch 745\n",
      "Step 0: loss = 0.08468210697174072, recon_loss = 0.0776432529091835, 0.00703249778598547, kl_loss = 0.003176281228661537\n",
      "\n",
      "Epoch 746\n",
      "Step 0: loss = 0.09768776595592499, recon_loss = 0.08599912375211716, 0.011680549941956997, kl_loss = 0.004044524393975735\n",
      "\n",
      "Epoch 747\n",
      "Step 0: loss = 0.09296241402626038, recon_loss = 0.08371667563915253, 0.00923648476600647, kl_loss = 0.004625585861504078\n",
      "\n",
      "Epoch 748\n",
      "Step 0: loss = 0.08743944019079208, recon_loss = 0.0794573500752449, 0.007964559830725193, kl_loss = 0.008765073493123055\n",
      "\n",
      "Epoch 749\n",
      "Step 0: loss = 0.08524128794670105, recon_loss = 0.07822068780660629, 0.007010902278125286, kl_loss = 0.004850667901337147\n",
      "\n",
      "Epoch 750\n",
      "Step 0: loss = 0.08720949292182922, recon_loss = 0.07935565710067749, 0.007840894162654877, kl_loss = 0.0064704809337854385\n",
      "\n",
      "Epoch 751\n",
      "Step 0: loss = 0.08158449083566666, recon_loss = 0.07620218396186829, 0.005377043038606644, kl_loss = 0.0026334431022405624\n",
      "\n",
      "Epoch 752\n",
      "Step 0: loss = 0.08721865713596344, recon_loss = 0.07984336465597153, 0.007368524558842182, kl_loss = 0.0033836103975772858\n",
      "\n",
      "Epoch 753\n",
      "Step 0: loss = 0.08813715726137161, recon_loss = 0.07839620858430862, 0.00972709245979786, kl_loss = 0.006928001530468464\n",
      "\n",
      "Epoch 754\n",
      "Step 0: loss = 0.0954524427652359, recon_loss = 0.08259893208742142, 0.012828215956687927, kl_loss = 0.012646154500544071\n",
      "\n",
      "Epoch 755\n",
      "Step 0: loss = 0.08657043427228928, recon_loss = 0.07826623320579529, 0.008289061486721039, kl_loss = 0.00757080502808094\n",
      "\n",
      "Epoch 756\n",
      "Step 0: loss = 0.0964469313621521, recon_loss = 0.08297271281480789, 0.013452976942062378, kl_loss = 0.010621007531881332\n",
      "\n",
      "Epoch 757\n",
      "Step 0: loss = 0.09382236003875732, recon_loss = 0.08157865703105927, 0.012228675186634064, kl_loss = 0.007513705641031265\n",
      "\n",
      "Epoch 758\n",
      "Step 0: loss = 0.08571653068065643, recon_loss = 0.07672934979200363, 0.008974450640380383, kl_loss = 0.006365369074046612\n",
      "\n",
      "Epoch 759\n",
      "Step 0: loss = 0.09114335477352142, recon_loss = 0.078314408659935, 0.012817805632948875, kl_loss = 0.005567672662436962\n",
      "\n",
      "Epoch 760\n",
      "Step 0: loss = 0.08714798092842102, recon_loss = 0.07752233743667603, 0.009615101851522923, kl_loss = 0.005272228270769119\n",
      "\n",
      "Epoch 761\n",
      "Step 0: loss = 0.08890996128320694, recon_loss = 0.08140139281749725, 0.007501483429223299, kl_loss = 0.0035416968166828156\n",
      "\n",
      "Epoch 762\n",
      "Step 0: loss = 0.08683627843856812, recon_loss = 0.07686729729175568, 0.009950334206223488, kl_loss = 0.009325884282588959\n",
      "\n",
      "Epoch 763\n",
      "Step 0: loss = 0.07910963147878647, recon_loss = 0.07023823261260986, 0.00885814055800438, kl_loss = 0.006626124493777752\n",
      "\n",
      "Epoch 764\n",
      "Step 0: loss = 0.08600665628910065, recon_loss = 0.07853013277053833, 0.007470768876373768, kl_loss = 0.0028766971081495285\n",
      "\n",
      "Epoch 765\n",
      "Step 0: loss = 0.08404158055782318, recon_loss = 0.075906902551651, 0.008127443492412567, kl_loss = 0.003615463152527809\n",
      "\n",
      "Epoch 766\n",
      "Step 0: loss = 0.08336081355810165, recon_loss = 0.073113813996315, 0.01023105438798666, kl_loss = 0.007971184328198433\n",
      "\n",
      "Epoch 767\n",
      "Step 0: loss = 0.08680983632802963, recon_loss = 0.07564055174589157, 0.011156335473060608, kl_loss = 0.006475567817687988\n",
      "\n",
      "Epoch 768\n",
      "Step 0: loss = 0.08341395109891891, recon_loss = 0.07322519272565842, 0.010170241817831993, kl_loss = 0.00925622321665287\n",
      "\n",
      "Epoch 769\n",
      "Step 0: loss = 0.08035826683044434, recon_loss = 0.07260435074567795, 0.007731024641543627, kl_loss = 0.011445196345448494\n",
      "\n",
      "Epoch 770\n",
      "Step 0: loss = 0.08481758832931519, recon_loss = 0.07396459579467773, 0.010835828259587288, kl_loss = 0.008583098649978638\n",
      "\n",
      "Epoch 771\n",
      "Step 0: loss = 0.0834183469414711, recon_loss = 0.07354864478111267, 0.00985799077898264, kl_loss = 0.005857439711689949\n",
      "\n",
      "Epoch 772\n",
      "Step 0: loss = 0.08291123807430267, recon_loss = 0.07452318072319031, 0.008379259146749973, kl_loss = 0.004398331046104431\n",
      "\n",
      "Epoch 773\n",
      "Step 0: loss = 0.08288493752479553, recon_loss = 0.07302646338939667, 0.009850772097706795, kl_loss = 0.0038509834557771683\n",
      "\n",
      "Epoch 774\n",
      "Step 0: loss = 0.08644076436758041, recon_loss = 0.07761013507843018, 0.00881286896765232, kl_loss = 0.008882763795554638\n",
      "\n",
      "Epoch 775\n",
      "Step 0: loss = 0.08612001687288284, recon_loss = 0.07248639315366745, 0.013626594096422195, kl_loss = 0.0035126274451613426\n",
      "\n",
      "Epoch 776\n",
      "Step 0: loss = 0.09059101343154907, recon_loss = 0.0789419561624527, 0.011632136069238186, kl_loss = 0.008461673744022846\n",
      "\n",
      "Epoch 777\n",
      "Step 0: loss = 0.08520026504993439, recon_loss = 0.07724012434482574, 0.007942772470414639, kl_loss = 0.008681921288371086\n",
      "\n",
      "Epoch 778\n",
      "Step 0: loss = 0.0841427594423294, recon_loss = 0.07273294031620026, 0.011396899819374084, kl_loss = 0.00646052323281765\n",
      "\n",
      "Epoch 779\n",
      "Step 0: loss = 0.08661212027072906, recon_loss = 0.07634927332401276, 0.01025441475212574, kl_loss = 0.004216698929667473\n",
      "\n",
      "Epoch 780\n",
      "Step 0: loss = 0.08685348927974701, recon_loss = 0.07673542201519012, 0.010109344497323036, kl_loss = 0.004362867213785648\n",
      "\n",
      "Epoch 781\n",
      "Step 0: loss = 0.07916200906038284, recon_loss = 0.06807541847229004, 0.011069722473621368, kl_loss = 0.008435503579676151\n",
      "\n",
      "Epoch 782\n",
      "Step 0: loss = 0.08013950288295746, recon_loss = 0.0708279013633728, 0.009283479303121567, kl_loss = 0.014058204367756844\n",
      "\n",
      "Epoch 783\n",
      "Step 0: loss = 0.07942668348550797, recon_loss = 0.0719231516122818, 0.007485237903892994, kl_loss = 0.009144668467342854\n",
      "\n",
      "Epoch 784\n",
      "Step 0: loss = 0.08236297219991684, recon_loss = 0.07362289726734161, 0.008722690865397453, kl_loss = 0.008689839392900467\n",
      "\n",
      "Epoch 785\n",
      "Step 0: loss = 0.08865959197282791, recon_loss = 0.07662323117256165, 0.012014266103506088, kl_loss = 0.011045360937714577\n",
      "\n",
      "Epoch 786\n",
      "Step 0: loss = 0.0850188285112381, recon_loss = 0.07520114630460739, 0.009809701703488827, kl_loss = 0.003988070413470268\n",
      "\n",
      "Epoch 787\n",
      "Step 0: loss = 0.08271247893571854, recon_loss = 0.07201086729764938, 0.01068365853279829, kl_loss = 0.008979419246315956\n",
      "\n",
      "Epoch 788\n",
      "Step 0: loss = 0.0794818177819252, recon_loss = 0.07130759954452515, 0.0081632100045681, kl_loss = 0.00550332386046648\n",
      "\n",
      "Epoch 789\n",
      "Step 0: loss = 0.0741296336054802, recon_loss = 0.06326942145824432, 0.010851038619875908, kl_loss = 0.004584872163832188\n",
      "\n",
      "Epoch 790\n",
      "Step 0: loss = 0.08436670154333115, recon_loss = 0.07181835174560547, 0.012533633038401604, kl_loss = 0.00735897570848465\n",
      "\n",
      "Epoch 791\n",
      "Step 0: loss = 0.08220043033361435, recon_loss = 0.07006263732910156, 0.012112364172935486, kl_loss = 0.012713462114334106\n",
      "\n",
      "Epoch 792\n",
      "Step 0: loss = 0.07958994060754776, recon_loss = 0.06985729932785034, 0.00971677340567112, kl_loss = 0.007934107445180416\n",
      "\n",
      "Epoch 793\n",
      "Step 0: loss = 0.07870012521743774, recon_loss = 0.06796321272850037, 0.010719073936343193, kl_loss = 0.00891662947833538\n",
      "\n",
      "Epoch 794\n",
      "Step 0: loss = 0.07786758989095688, recon_loss = 0.06590279936790466, 0.011946180835366249, kl_loss = 0.009305335581302643\n",
      "\n",
      "Epoch 795\n",
      "Step 0: loss = 0.07608778774738312, recon_loss = 0.06678487360477448, 0.0092870919033885, kl_loss = 0.00791352428495884\n",
      "\n",
      "Epoch 796\n",
      "Step 0: loss = 0.08511892706155777, recon_loss = 0.07040351629257202, 0.014678001403808594, kl_loss = 0.018704913556575775\n",
      "\n",
      "Epoch 797\n",
      "Step 0: loss = 0.083338163793087, recon_loss = 0.07338026165962219, 0.009935451671481133, kl_loss = 0.011223207227885723\n",
      "\n",
      "Epoch 798\n",
      "Step 0: loss = 0.08220823854207993, recon_loss = 0.07054053992033005, 0.0116529930382967, kl_loss = 0.007354379631578922\n",
      "\n",
      "Epoch 799\n",
      "Step 0: loss = 0.07720094919204712, recon_loss = 0.06922843307256699, 0.007954999804496765, kl_loss = 0.008758870884776115\n",
      "\n",
      "Epoch 800\n",
      "Step 0: loss = 0.07824848592281342, recon_loss = 0.06672908365726471, 0.011495819315314293, kl_loss = 0.01179222296923399\n",
      "\n",
      "Epoch 801\n",
      "Step 0: loss = 0.08095411211252213, recon_loss = 0.07119806110858917, 0.009736571460962296, kl_loss = 0.009741617366671562\n",
      "\n",
      "Epoch 802\n",
      "Step 0: loss = 0.07715246826410294, recon_loss = 0.06908576190471649, 0.008051796816289425, kl_loss = 0.0074540358036756516\n",
      "\n",
      "Epoch 803\n",
      "Step 0: loss = 0.07717477530241013, recon_loss = 0.06382890045642853, 0.013332823291420937, kl_loss = 0.006528005003929138\n",
      "\n",
      "Epoch 804\n",
      "Step 0: loss = 0.07162676006555557, recon_loss = 0.06410137563943863, 0.007516343612223864, kl_loss = 0.004519493319094181\n",
      "\n",
      "Epoch 805\n",
      "Step 0: loss = 0.07671292871236801, recon_loss = 0.0643882006406784, 0.012315643951296806, kl_loss = 0.004540352150797844\n",
      "\n",
      "Epoch 806\n",
      "Step 0: loss = 0.07628583163022995, recon_loss = 0.06558869779109955, 0.010684400796890259, kl_loss = 0.006365600973367691\n",
      "\n",
      "Epoch 807\n",
      "Step 0: loss = 0.08028160035610199, recon_loss = 0.06763672828674316, 0.012633712962269783, kl_loss = 0.005579077638685703\n",
      "\n",
      "Epoch 808\n",
      "Step 0: loss = 0.0736810714006424, recon_loss = 0.0625196099281311, 0.011142273433506489, kl_loss = 0.009593620896339417\n",
      "\n",
      "Epoch 809\n",
      "Step 0: loss = 0.0785687267780304, recon_loss = 0.06427597999572754, 0.01425144448876381, kl_loss = 0.0206521637737751\n",
      "\n",
      "Epoch 810\n",
      "Step 0: loss = 0.075095534324646, recon_loss = 0.06482373178005219, 0.010231520980596542, kl_loss = 0.020137935876846313\n",
      "\n",
      "Epoch 811\n",
      "Step 0: loss = 0.07797500491142273, recon_loss = 0.06526856124401093, 0.012692327611148357, kl_loss = 0.0070590125396847725\n",
      "\n",
      "Epoch 812\n",
      "Step 0: loss = 0.07313275337219238, recon_loss = 0.06243479996919632, 0.010673703625798225, kl_loss = 0.012127178721129894\n",
      "\n",
      "Epoch 813\n",
      "Step 0: loss = 0.07549387216567993, recon_loss = 0.06357122957706451, 0.011889075860381126, kl_loss = 0.01678338460624218\n",
      "\n",
      "Epoch 814\n",
      "Step 0: loss = 0.07203318178653717, recon_loss = 0.06603983044624329, 0.005982362665235996, kl_loss = 0.005494129844009876\n",
      "\n",
      "Epoch 815\n",
      "Step 0: loss = 0.07591377198696136, recon_loss = 0.06353501975536346, 0.012369303032755852, kl_loss = 0.0047232285141944885\n",
      "\n",
      "Epoch 816\n",
      "Step 0: loss = 0.07160182297229767, recon_loss = 0.060998864471912384, 0.010590730234980583, kl_loss = 0.006112788803875446\n",
      "\n",
      "Epoch 817\n",
      "Step 0: loss = 0.0791432186961174, recon_loss = 0.06452736258506775, 0.014603639952838421, kl_loss = 0.006110647693276405\n",
      "\n",
      "Epoch 818\n",
      "Step 0: loss = 0.07329412549734116, recon_loss = 0.0623525008559227, 0.010925529524683952, kl_loss = 0.008046695031225681\n",
      "\n",
      "Epoch 819\n",
      "Step 0: loss = 0.06891495734453201, recon_loss = 0.0576111376285553, 0.011290915310382843, kl_loss = 0.006451898254454136\n",
      "\n",
      "Epoch 820\n",
      "Step 0: loss = 0.07153746485710144, recon_loss = 0.06176675111055374, 0.00975954532623291, kl_loss = 0.005585073493421078\n",
      "\n",
      "Epoch 821\n",
      "Step 0: loss = 0.0685112252831459, recon_loss = 0.057465750724077225, 0.011036781594157219, kl_loss = 0.004345736466348171\n",
      "\n",
      "Epoch 822\n",
      "Step 0: loss = 0.06712132692337036, recon_loss = 0.0578279085457325, 0.009280161000788212, kl_loss = 0.006627259775996208\n",
      "\n",
      "Epoch 823\n",
      "Step 0: loss = 0.07005704939365387, recon_loss = 0.058454833924770355, 0.011579427868127823, kl_loss = 0.011392238549888134\n",
      "\n",
      "Epoch 824\n",
      "Step 0: loss = 0.07289301604032516, recon_loss = 0.061488959938287735, 0.011393232271075249, kl_loss = 0.005411701276898384\n",
      "\n",
      "Epoch 825\n",
      "Step 0: loss = 0.07108466327190399, recon_loss = 0.06422264873981476, 0.006855368614196777, kl_loss = 0.0033218329772353172\n",
      "\n",
      "Epoch 826\n",
      "Step 0: loss = 0.07548881322145462, recon_loss = 0.061580367386341095, 0.01389849279075861, kl_loss = 0.00497748889029026\n",
      "\n",
      "Epoch 827\n",
      "Step 0: loss = 0.07428937405347824, recon_loss = 0.0623648576438427, 0.011910201981663704, kl_loss = 0.0071547552943229675\n",
      "\n",
      "Epoch 828\n",
      "Step 0: loss = 0.07813052833080292, recon_loss = 0.06330110132694244, 0.014815711416304111, kl_loss = 0.006859305314719677\n",
      "\n",
      "Epoch 829\n",
      "Step 0: loss = 0.0709940567612648, recon_loss = 0.05970623344182968, 0.011279573664069176, kl_loss = 0.004123806953430176\n",
      "\n",
      "Epoch 830\n",
      "Step 0: loss = 0.06839005649089813, recon_loss = 0.055761612951755524, 0.012617912143468857, kl_loss = 0.005268293432891369\n",
      "\n",
      "Epoch 831\n",
      "Step 0: loss = 0.0689726322889328, recon_loss = 0.05551635101437569, 0.01344926655292511, kl_loss = 0.00350795965641737\n",
      "\n",
      "Epoch 832\n",
      "Step 0: loss = 0.07056813687086105, recon_loss = 0.06082576885819435, 0.009728852659463882, kl_loss = 0.006756857968866825\n",
      "\n",
      "Epoch 833\n",
      "Step 0: loss = 0.07230842113494873, recon_loss = 0.062348783016204834, 0.009951243177056313, kl_loss = 0.004197996109724045\n",
      "\n",
      "Epoch 834\n",
      "Step 0: loss = 0.07240914553403854, recon_loss = 0.06197219341993332, 0.010424597188830376, kl_loss = 0.006177959032356739\n",
      "\n",
      "Epoch 835\n",
      "Step 0: loss = 0.07492324709892273, recon_loss = 0.06325433403253555, 0.011661836877465248, kl_loss = 0.00353799294680357\n",
      "\n",
      "Epoch 836\n",
      "Step 0: loss = 0.07109413295984268, recon_loss = 0.059255246073007584, 0.01183117926120758, kl_loss = 0.003857407718896866\n",
      "\n",
      "Epoch 837\n",
      "Step 0: loss = 0.07034876942634583, recon_loss = 0.05786065384745598, 0.01247941330075264, kl_loss = 0.004350805655121803\n",
      "\n",
      "Epoch 838\n",
      "Step 0: loss = 0.0720406100153923, recon_loss = 0.061089061200618744, 0.01094130240380764, kl_loss = 0.005123144015669823\n",
      "\n",
      "Epoch 839\n",
      "Step 0: loss = 0.07154136896133423, recon_loss = 0.05995745211839676, 0.011572625488042831, kl_loss = 0.005645722150802612\n",
      "\n",
      "Epoch 840\n",
      "Step 0: loss = 0.06575517356395721, recon_loss = 0.05592891573905945, 0.009816458448767662, kl_loss = 0.004897383041679859\n",
      "\n",
      "Epoch 841\n",
      "Step 0: loss = 0.06808662414550781, recon_loss = 0.05626947432756424, 0.011808205395936966, kl_loss = 0.004471170715987682\n",
      "\n",
      "Epoch 842\n",
      "Step 0: loss = 0.07047680765390396, recon_loss = 0.059674687683582306, 0.010782599449157715, kl_loss = 0.00976060051470995\n",
      "\n",
      "Epoch 843\n",
      "Step 0: loss = 0.06654442101716995, recon_loss = 0.055919207632541656, 0.010612218640744686, kl_loss = 0.006495433859527111\n",
      "\n",
      "Epoch 844\n",
      "Step 0: loss = 0.067537322640419, recon_loss = 0.056938715279102325, 0.010578766465187073, kl_loss = 0.009920376352965832\n",
      "\n",
      "Epoch 845\n",
      "Step 0: loss = 0.07093589752912521, recon_loss = 0.05867314338684082, 0.012252962216734886, kl_loss = 0.004894501529633999\n",
      "\n",
      "Epoch 846\n",
      "Step 0: loss = 0.0674973800778389, recon_loss = 0.05532009154558182, 0.012164260260760784, kl_loss = 0.006515837274491787\n",
      "\n",
      "Epoch 847\n",
      "Step 0: loss = 0.08028022199869156, recon_loss = 0.0647776648402214, 0.015491197817027569, kl_loss = 0.005680742673575878\n",
      "\n",
      "Epoch 848\n",
      "Step 0: loss = 0.06455200910568237, recon_loss = 0.049933865666389465, 0.014590168371796608, kl_loss = 0.013989674858748913\n",
      "\n",
      "Epoch 849\n",
      "Step 0: loss = 0.07055487483739853, recon_loss = 0.059184480458498, 0.011332385241985321, kl_loss = 0.019002914428710938\n",
      "\n",
      "Epoch 850\n",
      "Step 0: loss = 0.06635267287492752, recon_loss = 0.05708290636539459, 0.009252186864614487, kl_loss = 0.00878755934536457\n",
      "\n",
      "Epoch 851\n",
      "Step 0: loss = 0.0629454255104065, recon_loss = 0.05384400859475136, 0.009083773009479046, kl_loss = 0.008821425959467888\n",
      "\n",
      "Epoch 852\n",
      "Step 0: loss = 0.06628120690584183, recon_loss = 0.05568286031484604, 0.010588904842734337, kl_loss = 0.004718656651675701\n",
      "\n",
      "Epoch 853\n",
      "Step 0: loss = 0.06504903733730316, recon_loss = 0.0539809912443161, 0.01105586253106594, kl_loss = 0.0060914047062397\n",
      "\n",
      "Epoch 854\n",
      "Step 0: loss = 0.07253339141607285, recon_loss = 0.05759201943874359, 0.014926368370652199, kl_loss = 0.007503248751163483\n",
      "\n",
      "Epoch 855\n",
      "Step 0: loss = 0.06711357831954956, recon_loss = 0.05235976725816727, 0.014742283150553703, kl_loss = 0.005764247849583626\n",
      "\n",
      "Epoch 856\n",
      "Step 0: loss = 0.06721290200948715, recon_loss = 0.05192994326353073, 0.015230311080813408, kl_loss = 0.026323068886995316\n",
      "\n",
      "Epoch 857\n",
      "Step 0: loss = 0.0694684088230133, recon_loss = 0.05477268248796463, 0.014664172194898129, kl_loss = 0.01577596738934517\n",
      "\n",
      "Epoch 858\n",
      "Step 0: loss = 0.07247308641672134, recon_loss = 0.06074178218841553, 0.011716499924659729, kl_loss = 0.007400873117148876\n",
      "\n",
      "Epoch 859\n",
      "Step 0: loss = 0.06376246362924576, recon_loss = 0.052785519510507584, 0.010967289097607136, kl_loss = 0.004828552715480328\n",
      "\n",
      "Epoch 860\n",
      "Step 0: loss = 0.06597084552049637, recon_loss = 0.050510771572589874, 0.015444835647940636, kl_loss = 0.0076182326301932335\n",
      "\n",
      "Epoch 861\n",
      "Step 0: loss = 0.06308352947235107, recon_loss = 0.05138716101646423, 0.0116746686398983, kl_loss = 0.01084691658616066\n",
      "\n",
      "Epoch 862\n",
      "Step 0: loss = 0.06604258716106415, recon_loss = 0.054639942944049835, 0.011372599750757217, kl_loss = 0.015019003301858902\n",
      "\n",
      "Epoch 863\n",
      "Step 0: loss = 0.06667207926511765, recon_loss = 0.05248662084341049, 0.014157610945403576, kl_loss = 0.013924960978329182\n",
      "\n",
      "Epoch 864\n",
      "Step 0: loss = 0.06478830426931381, recon_loss = 0.05256686732172966, 0.012199563905596733, kl_loss = 0.010939085856080055\n",
      "\n",
      "Epoch 865\n",
      "Step 0: loss = 0.06846040487289429, recon_loss = 0.054027676582336426, 0.014416441321372986, kl_loss = 0.008142729289829731\n",
      "\n",
      "Epoch 866\n",
      "Step 0: loss = 0.06396878510713577, recon_loss = 0.048049017786979675, 0.015908289700746536, kl_loss = 0.00574070867151022\n",
      "\n",
      "Epoch 867\n",
      "Step 0: loss = 0.05899513512849808, recon_loss = 0.0505935400724411, 0.008370647206902504, kl_loss = 0.015474163927137852\n",
      "\n",
      "Epoch 868\n",
      "Step 0: loss = 0.06478726863861084, recon_loss = 0.053127966821193695, 0.01164095476269722, kl_loss = 0.009170875884592533\n",
      "\n",
      "Epoch 869\n",
      "Step 0: loss = 0.06100154295563698, recon_loss = 0.0512295663356781, 0.009758234024047852, kl_loss = 0.006871606223285198\n",
      "\n",
      "Epoch 870\n",
      "Step 0: loss = 0.06193600222468376, recon_loss = 0.04950165003538132, 0.012421965599060059, kl_loss = 0.006194175221025944\n",
      "\n",
      "Epoch 871\n",
      "Step 0: loss = 0.062002815306186676, recon_loss = 0.05073755234479904, 0.011258484795689583, kl_loss = 0.0033898670226335526\n",
      "\n",
      "Epoch 872\n",
      "Step 0: loss = 0.06198772042989731, recon_loss = 0.05146865174174309, 0.010509438812732697, kl_loss = 0.004815211519598961\n",
      "\n",
      "Epoch 873\n",
      "Step 0: loss = 0.06410994380712509, recon_loss = 0.052054062485694885, 0.012029251083731651, kl_loss = 0.013312586583197117\n",
      "\n",
      "Epoch 874\n",
      "Step 0: loss = 0.06498325616121292, recon_loss = 0.050368696451187134, 0.014593414962291718, kl_loss = 0.010572166182100773\n",
      "\n",
      "Epoch 875\n",
      "Step 0: loss = 0.05948621407151222, recon_loss = 0.04736660048365593, 0.012094110250473022, kl_loss = 0.012751121073961258\n",
      "\n",
      "Epoch 876\n",
      "Step 0: loss = 0.06007006764411926, recon_loss = 0.048269495368003845, 0.011765921488404274, kl_loss = 0.017326049506664276\n",
      "\n",
      "Epoch 877\n",
      "Step 0: loss = 0.0626768097281456, recon_loss = 0.05044002830982208, 0.012208241969347, kl_loss = 0.014270438812673092\n",
      "\n",
      "Epoch 878\n",
      "Step 0: loss = 0.05937207490205765, recon_loss = 0.048046357929706573, 0.011318229138851166, kl_loss = 0.003743949346244335\n",
      "\n",
      "Epoch 879\n",
      "Step 0: loss = 0.06320863217115402, recon_loss = 0.049282193183898926, 0.013894252479076385, kl_loss = 0.016092725098133087\n",
      "\n",
      "Epoch 880\n",
      "Step 0: loss = 0.06397300958633423, recon_loss = 0.04880008101463318, 0.01515241153538227, kl_loss = 0.010257997550070286\n",
      "\n",
      "Epoch 881\n",
      "Step 0: loss = 0.06164748594164848, recon_loss = 0.04790695384144783, 0.013729967176914215, kl_loss = 0.005282723344862461\n",
      "\n",
      "Epoch 882\n",
      "Step 0: loss = 0.058427710086107254, recon_loss = 0.04760317504405975, 0.010804474353790283, kl_loss = 0.010030104778707027\n",
      "\n",
      "Epoch 883\n",
      "Step 0: loss = 0.06155243515968323, recon_loss = 0.04753497987985611, 0.014004938304424286, kl_loss = 0.006259237416088581\n",
      "\n",
      "Epoch 884\n",
      "Step 0: loss = 0.057796452194452286, recon_loss = 0.04541800171136856, 0.012361427769064903, kl_loss = 0.008510776795446873\n",
      "\n",
      "Epoch 885\n",
      "Step 0: loss = 0.059493500739336014, recon_loss = 0.04519665986299515, 0.014267152175307274, kl_loss = 0.014844122342765331\n",
      "\n",
      "Epoch 886\n",
      "Step 0: loss = 0.05609521642327309, recon_loss = 0.04211663827300072, 0.013955695554614067, kl_loss = 0.01144286710768938\n",
      "\n",
      "Epoch 887\n",
      "Step 0: loss = 0.056544892489910126, recon_loss = 0.04479324817657471, 0.01173874456435442, kl_loss = 0.006451006047427654\n",
      "\n",
      "Epoch 888\n",
      "Step 0: loss = 0.05531817302107811, recon_loss = 0.042953066527843475, 0.012358109466731548, kl_loss = 0.003498668782413006\n",
      "\n",
      "Epoch 889\n",
      "Step 0: loss = 0.059892866760492325, recon_loss = 0.045023735612630844, 0.014864033088088036, kl_loss = 0.0025492245331406593\n",
      "\n",
      "Epoch 890\n",
      "Step 0: loss = 0.05331500992178917, recon_loss = 0.042433612048625946, 0.010865570046007633, kl_loss = 0.007914511486887932\n",
      "\n",
      "Epoch 891\n",
      "Step 0: loss = 0.05935918912291527, recon_loss = 0.0446835532784462, 0.014661810360848904, kl_loss = 0.0069128284230828285\n",
      "\n",
      "Epoch 892\n",
      "Step 0: loss = 0.06217513605952263, recon_loss = 0.04390578716993332, 0.018260831013321877, kl_loss = 0.004260584712028503\n",
      "\n",
      "Epoch 893\n",
      "Step 0: loss = 0.05910872668027878, recon_loss = 0.041780419647693634, 0.017309606075286865, kl_loss = 0.009349657222628593\n",
      "\n",
      "Epoch 894\n",
      "Step 0: loss = 0.05470619723200798, recon_loss = 0.0414397269487381, 0.013237114064395428, kl_loss = 0.014677741564810276\n",
      "\n",
      "Epoch 895\n",
      "Step 0: loss = 0.05453076586127281, recon_loss = 0.043903522193431854, 0.0105636827647686, kl_loss = 0.03178076073527336\n",
      "\n",
      "Epoch 896\n",
      "Step 0: loss = 0.06511788815259933, recon_loss = 0.04630691558122635, 0.018785741180181503, kl_loss = 0.012617516331374645\n",
      "\n",
      "Epoch 897\n",
      "Step 0: loss = 0.05620522424578667, recon_loss = 0.045292243361473083, 0.010886412113904953, kl_loss = 0.013284471817314625\n",
      "\n",
      "Epoch 898\n",
      "Step 0: loss = 0.055318064987659454, recon_loss = 0.04395272955298424, 0.011348677799105644, kl_loss = 0.008329035714268684\n",
      "\n",
      "Epoch 899\n",
      "Step 0: loss = 0.07105452567338943, recon_loss = 0.04766400158405304, 0.023361561819911003, kl_loss = 0.014479964040219784\n",
      "\n",
      "Epoch 900\n",
      "Step 0: loss = 0.059521377086639404, recon_loss = 0.04599883407354355, 0.01346589531749487, kl_loss = 0.028323575854301453\n",
      "\n",
      "Epoch 901\n",
      "Step 0: loss = 0.05490498244762421, recon_loss = 0.04397076740860939, 0.0109037384390831, kl_loss = 0.015238460153341293\n",
      "\n",
      "Epoch 902\n",
      "Step 0: loss = 0.05489000305533409, recon_loss = 0.04222971200942993, 0.012638469226658344, kl_loss = 0.010912125930190086\n",
      "\n",
      "Epoch 903\n",
      "Step 0: loss = 0.05475618317723274, recon_loss = 0.04494069516658783, 0.009799227118492126, kl_loss = 0.008130785077810287\n",
      "\n",
      "Epoch 904\n",
      "Step 0: loss = 0.0527598038315773, recon_loss = 0.04077901691198349, 0.011974098160862923, kl_loss = 0.0033451803028583527\n",
      "\n",
      "Epoch 905\n",
      "Step 0: loss = 0.06227657571434975, recon_loss = 0.04454994201660156, 0.017719024792313576, kl_loss = 0.003804462030529976\n",
      "\n",
      "Epoch 906\n",
      "Step 0: loss = 0.05038264021277428, recon_loss = 0.03974955528974533, 0.0106273228302598, kl_loss = 0.0028807679191231728\n",
      "\n",
      "Epoch 907\n",
      "Step 0: loss = 0.05045505613088608, recon_loss = 0.03829246014356613, 0.012158218771219254, kl_loss = 0.002188204787671566\n",
      "\n",
      "Epoch 908\n",
      "Step 0: loss = 0.05008409917354584, recon_loss = 0.04079136252403259, 0.009277774021029472, kl_loss = 0.007480372674763203\n",
      "\n",
      "Epoch 909\n",
      "Step 0: loss = 0.050350792706012726, recon_loss = 0.03841663524508476, 0.011924538761377335, kl_loss = 0.004809454083442688\n",
      "\n",
      "Epoch 910\n",
      "Step 0: loss = 0.04920102283358574, recon_loss = 0.039134833961725235, 0.010058113373816013, kl_loss = 0.004038505256175995\n",
      "\n",
      "Epoch 911\n",
      "Step 0: loss = 0.05610701069235802, recon_loss = 0.03959047794342041, 0.016492007300257683, kl_loss = 0.012261192314326763\n",
      "\n",
      "Epoch 912\n",
      "Step 0: loss = 0.05967630818486214, recon_loss = 0.042569518089294434, 0.017090987414121628, kl_loss = 0.007901987992227077\n",
      "\n",
      "Epoch 913\n",
      "Step 0: loss = 0.05196347087621689, recon_loss = 0.040854670107364655, 0.011100828647613525, kl_loss = 0.00398563127964735\n",
      "\n",
      "Epoch 914\n",
      "Step 0: loss = 0.049720801413059235, recon_loss = 0.03896120935678482, 0.010755237191915512, kl_loss = 0.002178001217544079\n",
      "\n",
      "Epoch 915\n",
      "Step 0: loss = 0.05815184488892555, recon_loss = 0.04250749200582504, 0.015639785677194595, kl_loss = 0.0022839298471808434\n",
      "\n",
      "Epoch 916\n",
      "Step 0: loss = 0.05463079363107681, recon_loss = 0.043397754430770874, 0.011217169463634491, kl_loss = 0.007935135625302792\n",
      "\n",
      "Epoch 917\n",
      "Step 0: loss = 0.05149136856198311, recon_loss = 0.03942873328924179, 0.012052914127707481, kl_loss = 0.0048604365438222885\n",
      "\n",
      "Epoch 918\n",
      "Step 0: loss = 0.05216887593269348, recon_loss = 0.038706302642822266, 0.013454067520797253, kl_loss = 0.004252307116985321\n",
      "\n",
      "Epoch 919\n",
      "Step 0: loss = 0.050272975116968155, recon_loss = 0.03706701472401619, 0.013194873929023743, kl_loss = 0.0055428771302104\n",
      "\n",
      "Epoch 920\n",
      "Step 0: loss = 0.050602272152900696, recon_loss = 0.037823084741830826, 0.012766899541020393, kl_loss = 0.00614317599684\n",
      "\n",
      "Epoch 921\n",
      "Step 0: loss = 0.053102750331163406, recon_loss = 0.03840150311589241, 0.014683499000966549, kl_loss = 0.00887297373265028\n",
      "\n",
      "Epoch 922\n",
      "Step 0: loss = 0.054432135075330734, recon_loss = 0.03795747831463814, 0.016451828181743622, kl_loss = 0.011414443142712116\n",
      "\n",
      "Epoch 923\n",
      "Step 0: loss = 0.05659462884068489, recon_loss = 0.03792908787727356, 0.01864670403301716, kl_loss = 0.009419835172593594\n",
      "\n",
      "Epoch 924\n",
      "Step 0: loss = 0.05079043656587601, recon_loss = 0.03681079298257828, 0.013937486335635185, kl_loss = 0.02107812650501728\n",
      "\n",
      "Epoch 925\n",
      "Step 0: loss = 0.0539950467646122, recon_loss = 0.040328316390514374, 0.01360880397260189, kl_loss = 0.028962496668100357\n",
      "\n",
      "Epoch 926\n",
      "Step 0: loss = 0.057343870401382446, recon_loss = 0.0387490838766098, 0.018572844564914703, kl_loss = 0.010971790179610252\n",
      "\n",
      "Epoch 927\n",
      "Step 0: loss = 0.05249286815524101, recon_loss = 0.03889387845993042, 0.01356634683907032, kl_loss = 0.01632228121161461\n",
      "\n",
      "Epoch 928\n",
      "Step 0: loss = 0.04786379262804985, recon_loss = 0.03477588668465614, 0.013069204986095428, kl_loss = 0.009349772706627846\n",
      "\n",
      "Epoch 929\n",
      "Step 0: loss = 0.05306543409824371, recon_loss = 0.03980650752782822, 0.013241026550531387, kl_loss = 0.008949443697929382\n",
      "\n",
      "Epoch 930\n",
      "Step 0: loss = 0.05108378827571869, recon_loss = 0.04003867134451866, 0.011000595055520535, kl_loss = 0.02226131781935692\n",
      "\n",
      "Epoch 931\n",
      "Step 0: loss = 0.04904880374670029, recon_loss = 0.03763381764292717, 0.011400071904063225, kl_loss = 0.007458267733454704\n",
      "\n",
      "Epoch 932\n",
      "Step 0: loss = 0.0484863817691803, recon_loss = 0.035499416291713715, 0.012969184666872025, kl_loss = 0.008890070952475071\n",
      "\n",
      "Epoch 933\n",
      "Step 0: loss = 0.051395077258348465, recon_loss = 0.03590582311153412, 0.015477350912988186, kl_loss = 0.005951757542788982\n",
      "\n",
      "Epoch 934\n",
      "Step 0: loss = 0.0441754013299942, recon_loss = 0.03370366245508194, 0.01046270877122879, kl_loss = 0.004515014588832855\n",
      "\n",
      "Epoch 935\n",
      "Step 0: loss = 0.047019556164741516, recon_loss = 0.03389335796236992, 0.01311523001641035, kl_loss = 0.005483563058078289\n",
      "\n",
      "Epoch 936\n",
      "Step 0: loss = 0.047328922897577286, recon_loss = 0.03731788322329521, 0.010002346709370613, kl_loss = 0.00434565544128418\n",
      "\n",
      "Epoch 937\n",
      "Step 0: loss = 0.047003839164972305, recon_loss = 0.03481245040893555, 0.012182867154479027, kl_loss = 0.004260075278580189\n",
      "\n",
      "Epoch 938\n",
      "Step 0: loss = 0.05424287170171738, recon_loss = 0.031864605844020844, 0.022366773337125778, kl_loss = 0.0057464540004730225\n",
      "\n",
      "Epoch 939\n",
      "Step 0: loss = 0.054592832922935486, recon_loss = 0.03693971782922745, 0.017635207623243332, kl_loss = 0.008953520096838474\n",
      "\n",
      "Epoch 940\n",
      "Step 0: loss = 0.04519738256931305, recon_loss = 0.032943613827228546, 0.012240711599588394, kl_loss = 0.0065285395830869675\n",
      "\n",
      "Epoch 941\n",
      "Step 0: loss = 0.045732177793979645, recon_loss = 0.034736499190330505, 0.010979468002915382, kl_loss = 0.008106446824967861\n",
      "\n",
      "Epoch 942\n",
      "Step 0: loss = 0.052504993975162506, recon_loss = 0.03439725190401077, 0.01808941178023815, kl_loss = 0.009163906797766685\n",
      "\n",
      "Epoch 943\n",
      "Step 0: loss = 0.04559465870261192, recon_loss = 0.036766767501831055, 0.008805998601019382, kl_loss = 0.01094595342874527\n",
      "\n",
      "Epoch 944\n",
      "Step 0: loss = 0.04091055691242218, recon_loss = 0.030729003250598907, 0.010164197534322739, kl_loss = 0.008677425794303417\n",
      "\n",
      "Epoch 945\n",
      "Step 0: loss = 0.04605574533343315, recon_loss = 0.0355696976184845, 0.010473533533513546, kl_loss = 0.006256909109652042\n",
      "\n",
      "Epoch 946\n",
      "Step 0: loss = 0.043150100857019424, recon_loss = 0.0331689715385437, 0.009971162304282188, kl_loss = 0.0049822162836790085\n",
      "\n",
      "Epoch 947\n",
      "Step 0: loss = 0.05014099180698395, recon_loss = 0.03479135408997536, 0.015332337468862534, kl_loss = 0.008649304509162903\n",
      "\n",
      "Epoch 948\n",
      "Step 0: loss = 0.04325515776872635, recon_loss = 0.02978401631116867, 0.013456232845783234, kl_loss = 0.007453497499227524\n",
      "\n",
      "Epoch 949\n",
      "Step 0: loss = 0.04436281695961952, recon_loss = 0.03072173334658146, 0.013631949201226234, kl_loss = 0.004566728137433529\n",
      "\n",
      "Epoch 950\n",
      "Step 0: loss = 0.04726210609078407, recon_loss = 0.030969470739364624, 0.01628522202372551, kl_loss = 0.0037060165777802467\n",
      "\n",
      "Epoch 951\n",
      "Step 0: loss = 0.043670039623975754, recon_loss = 0.03190043568611145, 0.011760532855987549, kl_loss = 0.004535616375505924\n",
      "\n",
      "Epoch 952\n",
      "Step 0: loss = 0.046652358025312424, recon_loss = 0.03511591628193855, 0.011522484943270683, kl_loss = 0.0069790976122021675\n",
      "\n",
      "Epoch 953\n",
      "Step 0: loss = 0.04680419713258743, recon_loss = 0.033531852066516876, 0.013260288164019585, kl_loss = 0.006026851013302803\n",
      "\n",
      "Epoch 954\n",
      "Step 0: loss = 0.04002049192786217, recon_loss = 0.030760617926716805, 0.00925419107079506, kl_loss = 0.0028406642377376556\n",
      "\n",
      "Epoch 955\n",
      "Step 0: loss = 0.046659208834171295, recon_loss = 0.032542865723371506, 0.014103371649980545, kl_loss = 0.00648601446300745\n",
      "\n",
      "Epoch 956\n",
      "Step 0: loss = 0.04580404981970787, recon_loss = 0.030937019735574722, 0.014856543391942978, kl_loss = 0.005243568681180477\n",
      "\n",
      "Epoch 957\n",
      "Step 0: loss = 0.0426541268825531, recon_loss = 0.03008583001792431, 0.012560904957354069, kl_loss = 0.003696148283779621\n",
      "\n",
      "Epoch 958\n",
      "Step 0: loss = 0.04532870277762413, recon_loss = 0.030364682897925377, 0.014955147169530392, kl_loss = 0.004436926916241646\n",
      "\n",
      "Epoch 959\n",
      "Step 0: loss = 0.04513132572174072, recon_loss = 0.03147011995315552, 0.013637141324579716, kl_loss = 0.012033097445964813\n",
      "\n",
      "Epoch 960\n",
      "Step 0: loss = 0.04910908639431, recon_loss = 0.03257814794778824, 0.01651119627058506, kl_loss = 0.009872913360595703\n",
      "\n",
      "Epoch 961\n",
      "Step 0: loss = 0.04916347935795784, recon_loss = 0.03031625971198082, 0.01881750300526619, kl_loss = 0.014858531765639782\n",
      "\n",
      "Epoch 962\n",
      "Step 0: loss = 0.04190969094634056, recon_loss = 0.03103673830628395, 0.010852726176381111, kl_loss = 0.01011224091053009\n",
      "\n",
      "Epoch 963\n",
      "Step 0: loss = 0.04867833852767944, recon_loss = 0.034238532185554504, 0.014426985755562782, kl_loss = 0.006412087939679623\n",
      "\n",
      "Epoch 964\n",
      "Step 0: loss = 0.0460132472217083, recon_loss = 0.031202539801597595, 0.014779170975089073, kl_loss = 0.015766501426696777\n",
      "\n",
      "Epoch 965\n",
      "Step 0: loss = 0.04824256896972656, recon_loss = 0.03360453248023987, 0.014590666629374027, kl_loss = 0.023684736341238022\n",
      "\n",
      "Epoch 966\n",
      "Step 0: loss = 0.04100235924124718, recon_loss = 0.030544651672244072, 0.010443056002259254, kl_loss = 0.007325630635023117\n",
      "\n",
      "Epoch 967\n",
      "Step 0: loss = 0.04437289386987686, recon_loss = 0.030263280496001244, 0.014102339744567871, kl_loss = 0.003635190427303314\n",
      "\n",
      "Epoch 968\n",
      "Step 0: loss = 0.041831228882074356, recon_loss = 0.030221974477171898, 0.011601962149143219, kl_loss = 0.0036451155319809914\n",
      "\n",
      "Epoch 969\n",
      "Step 0: loss = 0.04426788538694382, recon_loss = 0.03229041397571564, 0.011955082416534424, kl_loss = 0.011193967424333096\n",
      "\n",
      "Epoch 970\n",
      "Step 0: loss = 0.040707655251026154, recon_loss = 0.027885479852557182, 0.01281212642788887, kl_loss = 0.005025508813560009\n",
      "\n",
      "Epoch 971\n",
      "Step 0: loss = 0.04657759889960289, recon_loss = 0.028482960537075996, 0.01804206520318985, kl_loss = 0.026287227869033813\n",
      "\n",
      "Epoch 972\n",
      "Step 0: loss = 0.04610000550746918, recon_loss = 0.027116255834698677, 0.018949013203382492, kl_loss = 0.017368223518133163\n",
      "\n",
      "Epoch 973\n",
      "Step 0: loss = 0.04558319225907326, recon_loss = 0.03172757476568222, 0.013838552869856358, kl_loss = 0.008532155305147171\n",
      "\n",
      "Epoch 974\n",
      "Step 0: loss = 0.04612995684146881, recon_loss = 0.03145395964384079, 0.014666295610368252, kl_loss = 0.0048498474061489105\n",
      "\n",
      "Epoch 975\n",
      "Step 0: loss = 0.04604972153902054, recon_loss = 0.029370605945587158, 0.01667376607656479, kl_loss = 0.0026746271178126335\n",
      "\n",
      "Epoch 976\n",
      "Step 0: loss = 0.04039842262864113, recon_loss = 0.027721503749489784, 0.012664005160331726, kl_loss = 0.006458386778831482\n",
      "\n",
      "Epoch 977\n",
      "Step 0: loss = 0.03764137253165245, recon_loss = 0.027343321591615677, 0.010285832919180393, kl_loss = 0.006110384128987789\n",
      "\n",
      "Epoch 978\n",
      "Step 0: loss = 0.03880106285214424, recon_loss = 0.027063531801104546, 0.01172766461968422, kl_loss = 0.004934555850923061\n",
      "\n",
      "Epoch 979\n",
      "Step 0: loss = 0.04477052018046379, recon_loss = 0.029937246814370155, 0.014823537319898605, kl_loss = 0.004866397939622402\n",
      "\n",
      "Epoch 980\n",
      "Step 0: loss = 0.04022417962551117, recon_loss = 0.026871221140027046, 0.013329742476344109, kl_loss = 0.011607621796429157\n",
      "\n",
      "Epoch 981\n",
      "Step 0: loss = 0.040757548063993454, recon_loss = 0.028314856812357903, 0.012433095835149288, kl_loss = 0.004797616973519325\n",
      "\n",
      "Epoch 982\n",
      "Step 0: loss = 0.03662930428981781, recon_loss = 0.025491155683994293, 0.011127670295536518, kl_loss = 0.005238693207502365\n",
      "\n",
      "Epoch 983\n",
      "Step 0: loss = 0.04525909945368767, recon_loss = 0.028558380901813507, 0.01666538417339325, kl_loss = 0.017666708678007126\n",
      "\n",
      "Epoch 984\n",
      "Step 0: loss = 0.041355375200510025, recon_loss = 0.0290279071778059, 0.012286028824746609, kl_loss = 0.020719371736049652\n",
      "\n",
      "Epoch 985\n",
      "Step 0: loss = 0.03987531363964081, recon_loss = 0.027660472318530083, 0.012195896357297897, kl_loss = 0.009473815560340881\n",
      "\n",
      "Epoch 986\n",
      "Step 0: loss = 0.0426553376019001, recon_loss = 0.030469762161374092, 0.012174615636467934, kl_loss = 0.005479501560330391\n",
      "\n",
      "Epoch 987\n",
      "Step 0: loss = 0.039883337914943695, recon_loss = 0.026276681572198868, 0.013599536381661892, kl_loss = 0.003560015931725502\n",
      "\n",
      "Epoch 988\n",
      "Step 0: loss = 0.0383797362446785, recon_loss = 0.02642117813229561, 0.01195300742983818, kl_loss = 0.0027749277651309967\n",
      "\n",
      "Epoch 989\n",
      "Step 0: loss = 0.039506684988737106, recon_loss = 0.025986753404140472, 0.013513540849089622, kl_loss = 0.0031943097710609436\n",
      "\n",
      "Epoch 990\n",
      "Step 0: loss = 0.045314669609069824, recon_loss = 0.02795599400997162, 0.017349643632769585, kl_loss = 0.004514558240771294\n",
      "\n",
      "Epoch 991\n",
      "Step 0: loss = 0.044208139181137085, recon_loss = 0.02834813855588436, 0.015843193978071213, kl_loss = 0.00840404536575079\n",
      "\n",
      "Epoch 992\n",
      "Step 0: loss = 0.03956378623843193, recon_loss = 0.026232387870550156, 0.013318255543708801, kl_loss = 0.00657130591571331\n",
      "\n",
      "Epoch 993\n",
      "Step 0: loss = 0.03879883140325546, recon_loss = 0.024584054946899414, 0.014205314218997955, kl_loss = 0.004731741733849049\n",
      "\n",
      "Epoch 994\n",
      "Step 0: loss = 0.03814998269081116, recon_loss = 0.024702537804841995, 0.013436791487038136, kl_loss = 0.005327874794602394\n",
      "\n",
      "Epoch 995\n",
      "Step 0: loss = 0.03823018819093704, recon_loss = 0.02694706805050373, 0.01127268373966217, kl_loss = 0.0052182553336024284\n",
      "\n",
      "Epoch 996\n",
      "Step 0: loss = 0.03817478194832802, recon_loss = 0.02510474994778633, 0.013062852434813976, kl_loss = 0.003589734435081482\n",
      "\n",
      "Epoch 997\n",
      "Step 0: loss = 0.04771946743130684, recon_loss = 0.028164437040686607, 0.019547805190086365, kl_loss = 0.0036116410046815872\n",
      "\n",
      "Epoch 998\n",
      "Step 0: loss = 0.03669954836368561, recon_loss = 0.02539008855819702, 0.011301758699119091, kl_loss = 0.003850819543004036\n",
      "\n",
      "Epoch 999\n",
      "Step 0: loss = 0.038794130086898804, recon_loss = 0.02656739018857479, 0.012213755398988724, kl_loss = 0.006493138149380684\n",
      "\n",
      "Epoch 1000\n",
      "Step 0: loss = 0.04086737334728241, recon_loss = 0.02608288824558258, 0.01476537436246872, kl_loss = 0.009554757736623287\n",
      "\n",
      "Epoch 1001\n",
      "Step 0: loss = 0.040967803448438644, recon_loss = 0.027396705001592636, 0.01356002688407898, kl_loss = 0.00553539115935564\n",
      "\n",
      "Epoch 1002\n",
      "Step 0: loss = 0.04494837298989296, recon_loss = 0.027739500626921654, 0.017197053879499435, kl_loss = 0.005910166539251804\n",
      "\n",
      "Epoch 1003\n",
      "Step 0: loss = 0.039517421275377274, recon_loss = 0.02609354816377163, 0.013412410393357277, kl_loss = 0.005732095800340176\n",
      "\n",
      "Epoch 1004\n",
      "Step 0: loss = 0.036395493894815445, recon_loss = 0.023152323439717293, 0.013236267492175102, kl_loss = 0.003451530821621418\n",
      "\n",
      "Epoch 1005\n",
      "Step 0: loss = 0.039268217980861664, recon_loss = 0.023483306169509888, 0.015778524801135063, kl_loss = 0.0031922785565257072\n",
      "\n",
      "Epoch 1006\n",
      "Step 0: loss = 0.04144066572189331, recon_loss = 0.02454747073352337, 0.0168844573199749, kl_loss = 0.0043694814667105675\n",
      "\n",
      "Epoch 1007\n",
      "Step 0: loss = 0.045351527631282806, recon_loss = 0.024110672995448112, 0.021203266456723213, kl_loss = 0.01879419945180416\n",
      "\n",
      "Epoch 1008\n",
      "Step 0: loss = 0.04728546366095543, recon_loss = 0.02600565366446972, 0.02125108242034912, kl_loss = 0.014363533817231655\n",
      "\n",
      "Epoch 1009\n",
      "Step 0: loss = 0.03891671448945999, recon_loss = 0.023943817242980003, 0.014953423291444778, kl_loss = 0.009738759137690067\n",
      "\n",
      "Epoch 1010\n",
      "Step 0: loss = 0.036947447806596756, recon_loss = 0.024468552321195602, 0.012437500059604645, kl_loss = 0.020697467029094696\n",
      "\n",
      "Epoch 1011\n",
      "Step 0: loss = 0.04303785040974617, recon_loss = 0.025821927934885025, 0.01720133237540722, kl_loss = 0.007296649739146233\n",
      "\n",
      "Epoch 1012\n",
      "Step 0: loss = 0.041220277547836304, recon_loss = 0.025262221693992615, 0.01593267172574997, kl_loss = 0.01269205566495657\n",
      "\n",
      "Epoch 1013\n",
      "Step 0: loss = 0.041322916746139526, recon_loss = 0.026503000408411026, 0.014770998619496822, kl_loss = 0.024458318948745728\n",
      "\n",
      "Epoch 1014\n",
      "Step 0: loss = 0.03909694403409958, recon_loss = 0.023372625932097435, 0.015709001570940018, kl_loss = 0.007658430375158787\n",
      "\n",
      "Epoch 1015\n",
      "Step 0: loss = 0.04358605667948723, recon_loss = 0.025924911722540855, 0.017639733850955963, kl_loss = 0.01070548314601183\n",
      "\n",
      "Epoch 1016\n",
      "Step 0: loss = 0.040036749094724655, recon_loss = 0.02513212524354458, 0.014886184595525265, kl_loss = 0.00922006368637085\n",
      "\n",
      "Epoch 1017\n",
      "Step 0: loss = 0.042702533304691315, recon_loss = 0.026944953948259354, 0.015732338652014732, kl_loss = 0.012620693072676659\n",
      "\n",
      "Epoch 1018\n",
      "Step 0: loss = 0.031131237745285034, recon_loss = 0.022476742044091225, 0.008645856752991676, kl_loss = 0.004319815896451473\n",
      "\n",
      "Epoch 1019\n",
      "Step 0: loss = 0.034879419952631, recon_loss = 0.022698191925883293, 0.01216140203177929, kl_loss = 0.009913314133882523\n",
      "\n",
      "Epoch 1020\n",
      "Step 0: loss = 0.03746038302779198, recon_loss = 0.022172942757606506, 0.015277420170605183, kl_loss = 0.005009882152080536\n",
      "\n",
      "Epoch 1021\n",
      "Step 0: loss = 0.03960128873586655, recon_loss = 0.02236766926944256, 0.017225949093699455, kl_loss = 0.0038356035947799683\n",
      "\n",
      "Epoch 1022\n",
      "Step 0: loss = 0.03681516274809837, recon_loss = 0.02107987180352211, 0.0157165564596653, kl_loss = 0.009367245249450207\n",
      "\n",
      "Epoch 1023\n",
      "Step 0: loss = 0.03342560678720474, recon_loss = 0.022733112797141075, 0.01067647896707058, kl_loss = 0.008007188327610493\n",
      "\n",
      "Epoch 1024\n",
      "Step 0: loss = 0.042482610791921616, recon_loss = 0.02237500064074993, 0.02009781077504158, kl_loss = 0.0049013178795576096\n",
      "\n",
      "Epoch 1025\n",
      "Step 0: loss = 0.03782336786389351, recon_loss = 0.022030385211110115, 0.015776535496115685, kl_loss = 0.008224181830883026\n",
      "\n",
      "Epoch 1026\n",
      "Step 0: loss = 0.03705477714538574, recon_loss = 0.022838328033685684, 0.014204777777194977, kl_loss = 0.005836376920342445\n",
      "\n",
      "Epoch 1027\n",
      "Step 0: loss = 0.03739992901682854, recon_loss = 0.021456612274050713, 0.01591208577156067, kl_loss = 0.015613782219588757\n",
      "\n",
      "Epoch 1028\n",
      "Step 0: loss = 0.037110693752765656, recon_loss = 0.022474125027656555, 0.014622904360294342, kl_loss = 0.00683186762034893\n",
      "\n",
      "Epoch 1029\n",
      "Step 0: loss = 0.04665956646203995, recon_loss = 0.02262653410434723, 0.024012643843889236, kl_loss = 0.010193604975938797\n",
      "\n",
      "Epoch 1030\n",
      "Step 0: loss = 0.039625946432352066, recon_loss = 0.021763218566775322, 0.017840828746557236, kl_loss = 0.0109510887414217\n",
      "\n",
      "Epoch 1031\n",
      "Step 0: loss = 0.03564918413758278, recon_loss = 0.023800037801265717, 0.011820919811725616, kl_loss = 0.014113737270236015\n",
      "\n",
      "Epoch 1032\n",
      "Step 0: loss = 0.03234945610165596, recon_loss = 0.019644254818558693, 0.012685875408351421, kl_loss = 0.009663756005465984\n",
      "\n",
      "Epoch 1033\n",
      "Step 0: loss = 0.030406875535845757, recon_loss = 0.020136183127760887, 0.010265992023050785, kl_loss = 0.0023496942594647408\n",
      "\n",
      "Epoch 1034\n",
      "Step 0: loss = 0.03236947953701019, recon_loss = 0.02075512334704399, 0.011608470231294632, kl_loss = 0.0029435018077492714\n",
      "\n",
      "Epoch 1035\n",
      "Step 0: loss = 0.03056875430047512, recon_loss = 0.020424367859959602, 0.010129747912287712, kl_loss = 0.007319305092096329\n",
      "\n",
      "Epoch 1036\n",
      "Step 0: loss = 0.036499958485364914, recon_loss = 0.022035565227270126, 0.014444964937865734, kl_loss = 0.00971413217484951\n",
      "\n",
      "Epoch 1037\n",
      "Step 0: loss = 0.031066671013832092, recon_loss = 0.021807918325066566, 0.009241587482392788, kl_loss = 0.008582890965044498\n",
      "\n",
      "Epoch 1038\n",
      "Step 0: loss = 0.033462557941675186, recon_loss = 0.02279484085738659, 0.010649682953953743, kl_loss = 0.009017806500196457\n",
      "\n",
      "Epoch 1039\n",
      "Step 0: loss = 0.03505552560091019, recon_loss = 0.020195508375763893, 0.014852916821837425, kl_loss = 0.0035496633499860764\n",
      "\n",
      "Epoch 1040\n",
      "Step 0: loss = 0.032170724123716354, recon_loss = 0.02144225500524044, 0.010721717961132526, kl_loss = 0.0033743251115083694\n",
      "\n",
      "Epoch 1041\n",
      "Step 0: loss = 0.03398662805557251, recon_loss = 0.021041302010416985, 0.01293354481458664, kl_loss = 0.005889564752578735\n",
      "\n",
      "Epoch 1042\n",
      "Step 0: loss = 0.034812942147254944, recon_loss = 0.021675659343600273, 0.013118872418999672, kl_loss = 0.009204459376633167\n",
      "\n",
      "Epoch 1043\n",
      "Step 0: loss = 0.036162540316581726, recon_loss = 0.021619679406285286, 0.01451872568577528, kl_loss = 0.01206809002906084\n",
      "\n",
      "Epoch 1044\n",
      "Step 0: loss = 0.0377267487347126, recon_loss = 0.020806489512324333, 0.016907580196857452, kl_loss = 0.006338454782962799\n",
      "\n",
      "Epoch 1045\n",
      "Step 0: loss = 0.035501424223184586, recon_loss = 0.018498098477721214, 0.016991768032312393, kl_loss = 0.005779603496193886\n",
      "\n",
      "Epoch 1046\n",
      "Step 0: loss = 0.039105191826820374, recon_loss = 0.02027040161192417, 0.018826304003596306, kl_loss = 0.00424343254417181\n",
      "\n",
      "Epoch 1047\n",
      "Step 0: loss = 0.03413717448711395, recon_loss = 0.01996123045682907, 0.014168732799589634, kl_loss = 0.0036061638966202736\n",
      "\n",
      "Epoch 1048\n",
      "Step 0: loss = 0.037552591413259506, recon_loss = 0.02344062738120556, 0.014102155342698097, kl_loss = 0.004904303699731827\n",
      "\n",
      "Epoch 1049\n",
      "Step 0: loss = 0.03394188731908798, recon_loss = 0.01962333731353283, 0.014310047961771488, kl_loss = 0.004249866120517254\n",
      "\n",
      "Epoch 1050\n",
      "Step 0: loss = 0.03125247731804848, recon_loss = 0.021026674658060074, 0.010215748101472855, kl_loss = 0.005027420818805695\n",
      "\n",
      "Epoch 1051\n",
      "Step 0: loss = 0.030622921884059906, recon_loss = 0.019151123240590096, 0.011463712900876999, kl_loss = 0.004042689688503742\n",
      "\n",
      "Epoch 1052\n",
      "Step 0: loss = 0.032510992139577866, recon_loss = 0.019482091069221497, 0.013024704530835152, kl_loss = 0.002098808065056801\n",
      "\n",
      "Epoch 1053\n",
      "Step 0: loss = 0.03178412839770317, recon_loss = 0.01732810214161873, 0.01445208303630352, kl_loss = 0.001971796154975891\n",
      "\n",
      "Epoch 1054\n",
      "Step 0: loss = 0.03409459441900253, recon_loss = 0.017607280984520912, 0.016482053324580193, kl_loss = 0.0026308847591280937\n",
      "\n",
      "Epoch 1055\n",
      "Step 0: loss = 0.02908019721508026, recon_loss = 0.018330134451389313, 0.010746808722615242, kl_loss = 0.001626778393983841\n",
      "\n",
      "Epoch 1056\n",
      "Step 0: loss = 0.030532652512192726, recon_loss = 0.01840151660144329, 0.012127439491450787, kl_loss = 0.0018487954512238503\n",
      "\n",
      "Epoch 1057\n",
      "Step 0: loss = 0.032614272087812424, recon_loss = 0.01894783228635788, 0.013661541044712067, kl_loss = 0.0024490375071763992\n",
      "\n",
      "Epoch 1058\n",
      "Step 0: loss = 0.02889556996524334, recon_loss = 0.016394056379795074, 0.012494121678173542, kl_loss = 0.0036966437473893166\n",
      "\n",
      "Epoch 1059\n",
      "Step 0: loss = 0.029368776828050613, recon_loss = 0.019114648923277855, 0.01024673506617546, kl_loss = 0.003696742467582226\n",
      "\n",
      "Epoch 1060\n",
      "Step 0: loss = 0.03556821122765541, recon_loss = 0.019087310880422592, 0.016471032053232193, kl_loss = 0.0049335407093167305\n",
      "\n",
      "Epoch 1061\n",
      "Step 0: loss = 0.03371283784508705, recon_loss = 0.01999281719326973, 0.013693468645215034, kl_loss = 0.013275932520627975\n",
      "\n",
      "Epoch 1062\n",
      "Step 0: loss = 0.03444341942667961, recon_loss = 0.020206265151500702, 0.014180673286318779, kl_loss = 0.028240112587809563\n",
      "\n",
      "Epoch 1063\n",
      "Step 0: loss = 0.02751937508583069, recon_loss = 0.01858500763773918, 0.008917749859392643, kl_loss = 0.00830899178981781\n",
      "\n",
      "Epoch 1064\n",
      "Step 0: loss = 0.03028068318963051, recon_loss = 0.018289921805262566, 0.011977782472968102, kl_loss = 0.0064897965639829636\n",
      "\n",
      "Epoch 1065\n",
      "Step 0: loss = 0.02571931481361389, recon_loss = 0.01669439859688282, 0.009016862139105797, kl_loss = 0.004027490504086018\n",
      "\n",
      "Epoch 1066\n",
      "Step 0: loss = 0.026363670825958252, recon_loss = 0.016531508415937424, 0.009825976565480232, kl_loss = 0.0030925124883651733\n",
      "\n",
      "Epoch 1067\n",
      "Step 0: loss = 0.026555487886071205, recon_loss = 0.01638551987707615, 0.010163553059101105, kl_loss = 0.003207308240234852\n",
      "\n",
      "Epoch 1068\n",
      "Step 0: loss = 0.0326165072619915, recon_loss = 0.018258284777402878, 0.014343809336423874, kl_loss = 0.007205691188573837\n",
      "\n",
      "Epoch 1069\n",
      "Step 0: loss = 0.03114873543381691, recon_loss = 0.018017061054706573, 0.013125332072377205, kl_loss = 0.0031707165762782097\n",
      "\n",
      "Epoch 1070\n",
      "Step 0: loss = 0.031332362443208694, recon_loss = 0.01681022346019745, 0.014514446258544922, kl_loss = 0.003846983425319195\n",
      "\n",
      "Epoch 1071\n",
      "Step 0: loss = 0.0319099947810173, recon_loss = 0.018358075991272926, 0.013547804206609726, kl_loss = 0.0020568473264575005\n",
      "\n",
      "Epoch 1072\n",
      "Step 0: loss = 0.027424242347478867, recon_loss = 0.01724131405353546, 0.010167501866817474, kl_loss = 0.007712766528129578\n",
      "\n",
      "Epoch 1073\n",
      "Step 0: loss = 0.028775017708539963, recon_loss = 0.0164774302393198, 0.012290487065911293, kl_loss = 0.0035497620701789856\n",
      "\n",
      "Epoch 1074\n",
      "Step 0: loss = 0.03310396894812584, recon_loss = 0.01778402365744114, 0.01531537901610136, kl_loss = 0.0022837240248918533\n",
      "\n",
      "Epoch 1075\n",
      "Step 0: loss = 0.03762046620249748, recon_loss = 0.017278006300330162, 0.020306099206209183, kl_loss = 0.018181007355451584\n",
      "\n",
      "Epoch 1076\n",
      "Step 0: loss = 0.02952929213643074, recon_loss = 0.017875606194138527, 0.01162552461028099, kl_loss = 0.014080895110964775\n",
      "\n",
      "Epoch 1077\n",
      "Step 0: loss = 0.030500048771500587, recon_loss = 0.016311248764395714, 0.014178633689880371, kl_loss = 0.005083049647510052\n",
      "\n",
      "Epoch 1078\n",
      "Step 0: loss = 0.03084373101592064, recon_loss = 0.016397781670093536, 0.014438221231102943, kl_loss = 0.0038641951978206635\n",
      "\n",
      "Epoch 1079\n",
      "Step 0: loss = 0.027071209624409676, recon_loss = 0.015476793050765991, 0.011584142223000526, kl_loss = 0.0051369378343224525\n",
      "\n",
      "Epoch 1080\n",
      "Step 0: loss = 0.029111934825778008, recon_loss = 0.01716218702495098, 0.011940566822886467, kl_loss = 0.004590515047311783\n",
      "\n",
      "Epoch 1081\n",
      "Step 0: loss = 0.031811900436878204, recon_loss = 0.018225496634840965, 0.013578609563410282, kl_loss = 0.0038961851969361305\n",
      "\n",
      "Epoch 1082\n",
      "Step 0: loss = 0.03051045536994934, recon_loss = 0.016382183879613876, 0.01411561667919159, kl_loss = 0.006327438168227673\n",
      "\n",
      "Epoch 1083\n",
      "Step 0: loss = 0.02809060551226139, recon_loss = 0.01719704084098339, 0.01088622584939003, kl_loss = 0.0036690905690193176\n",
      "\n",
      "Epoch 1084\n",
      "Step 0: loss = 0.03611687570810318, recon_loss = 0.019951609894633293, 0.016145428642630577, kl_loss = 0.009918870404362679\n",
      "\n",
      "Epoch 1085\n",
      "Step 0: loss = 0.028182055801153183, recon_loss = 0.01617121323943138, 0.01199942547827959, kl_loss = 0.005708563141524792\n",
      "\n",
      "Epoch 1086\n",
      "Step 0: loss = 0.03290019556879997, recon_loss = 0.016713419929146767, 0.016169119626283646, kl_loss = 0.008827945217490196\n",
      "\n",
      "Epoch 1087\n",
      "Step 0: loss = 0.030386241152882576, recon_loss = 0.01725800894200802, 0.013117363676428795, kl_loss = 0.005434667691588402\n",
      "\n",
      "Epoch 1088\n",
      "Step 0: loss = 0.03009985014796257, recon_loss = 0.01808139681816101, 0.012011907994747162, kl_loss = 0.003272787667810917\n",
      "\n",
      "Epoch 1089\n",
      "Step 0: loss = 0.03102020174264908, recon_loss = 0.017268143594264984, 0.013745228759944439, kl_loss = 0.0034142015501856804\n",
      "\n",
      "Epoch 1090\n",
      "Step 0: loss = 0.03043522872030735, recon_loss = 0.017640968784689903, 0.01278494019061327, kl_loss = 0.004659494385123253\n",
      "\n",
      "Epoch 1091\n",
      "Step 0: loss = 0.03230554983019829, recon_loss = 0.017428353428840637, 0.014868982136249542, kl_loss = 0.00410689041018486\n",
      "\n",
      "Epoch 1092\n",
      "Step 0: loss = 0.03433305025100708, recon_loss = 0.01670297048985958, 0.017620310187339783, kl_loss = 0.004884703084826469\n",
      "\n",
      "Epoch 1093\n",
      "Step 0: loss = 0.031062299385666847, recon_loss = 0.01782449521124363, 0.013227524235844612, kl_loss = 0.005140234716236591\n",
      "\n",
      "Epoch 1094\n",
      "Step 0: loss = 0.027444126084446907, recon_loss = 0.015856709331274033, 0.011577012948691845, kl_loss = 0.005201139487326145\n",
      "\n",
      "Epoch 1095\n",
      "Step 0: loss = 0.02837374061346054, recon_loss = 0.014282261952757835, 0.01408853754401207, kl_loss = 0.0014704009518027306\n",
      "\n",
      "Epoch 1096\n",
      "Step 0: loss = 0.02909255586564541, recon_loss = 0.016401616856455803, 0.012688394635915756, kl_loss = 0.001272355206310749\n",
      "\n",
      "Epoch 1097\n",
      "Step 0: loss = 0.028811637312173843, recon_loss = 0.015255609527230263, 0.013542886823415756, kl_loss = 0.006570229306817055\n",
      "\n",
      "Epoch 1098\n",
      "Step 0: loss = 0.03295482322573662, recon_loss = 0.017342431470751762, 0.015598543919622898, kl_loss = 0.006922531872987747\n",
      "\n",
      "Epoch 1099\n",
      "Step 0: loss = 0.03312550112605095, recon_loss = 0.016155585646629333, 0.016959741711616516, kl_loss = 0.005087347701191902\n",
      "\n",
      "Epoch 1100\n",
      "Step 0: loss = 0.029549051076173782, recon_loss = 0.01647951267659664, 0.013055559247732162, kl_loss = 0.006989257410168648\n",
      "\n",
      "Epoch 1101\n",
      "Step 0: loss = 0.02762591652572155, recon_loss = 0.014999844133853912, 0.012618843466043472, kl_loss = 0.003614836372435093\n",
      "\n",
      "Epoch 1102\n",
      "Step 0: loss = 0.029018878936767578, recon_loss = 0.016880804672837257, 0.012126754969358444, kl_loss = 0.005659616552293301\n",
      "\n",
      "Epoch 1103\n",
      "Step 0: loss = 0.02919289655983448, recon_loss = 0.015919558703899384, 0.013262687250971794, kl_loss = 0.0053251152858138084\n",
      "\n",
      "Epoch 1104\n",
      "Step 0: loss = 0.03043072298169136, recon_loss = 0.014954498037695885, 0.015468618832528591, kl_loss = 0.003803105093538761\n",
      "\n",
      "Epoch 1105\n",
      "Step 0: loss = 0.02428116463124752, recon_loss = 0.013827657327055931, 0.01044065784662962, kl_loss = 0.00642518512904644\n",
      "\n",
      "Epoch 1106\n",
      "Step 0: loss = 0.02631170116364956, recon_loss = 0.014119701460003853, 0.01218440756201744, kl_loss = 0.0037961872294545174\n",
      "\n",
      "Epoch 1107\n",
      "Step 0: loss = 0.03068380244076252, recon_loss = 0.013905122876167297, 0.016770631074905396, kl_loss = 0.004024364985525608\n",
      "\n",
      "Epoch 1108\n",
      "Step 0: loss = 0.0286109559237957, recon_loss = 0.01682300493121147, 0.011776608414947987, kl_loss = 0.005671925842761993\n",
      "\n",
      "Epoch 1109\n",
      "Step 0: loss = 0.036501385271549225, recon_loss = 0.015966134145855904, 0.02051970735192299, kl_loss = 0.007771383039653301\n",
      "\n",
      "Epoch 1110\n",
      "Step 0: loss = 0.030303772538900375, recon_loss = 0.015276877209544182, 0.01501996349543333, kl_loss = 0.0034665344282984734\n",
      "\n",
      "Epoch 1111\n",
      "Step 0: loss = 0.03011159598827362, recon_loss = 0.016502976417541504, 0.013597608543932438, kl_loss = 0.005505869165062904\n",
      "\n",
      "Epoch 1112\n",
      "Step 0: loss = 0.03169601783156395, recon_loss = 0.0155121348798275, 0.016170330345630646, kl_loss = 0.006776675581932068\n",
      "\n",
      "Epoch 1113\n",
      "Step 0: loss = 0.032200586050748825, recon_loss = 0.01549544557929039, 0.01669655553996563, kl_loss = 0.004293961450457573\n",
      "\n",
      "Epoch 1114\n",
      "Step 0: loss = 0.03406071290373802, recon_loss = 0.01626662351191044, 0.01773284189403057, kl_loss = 0.030624572187662125\n",
      "\n",
      "Epoch 1115\n",
      "Step 0: loss = 0.0246321652084589, recon_loss = 0.015731798484921455, 0.00888018123805523, kl_loss = 0.01009228453040123\n",
      "\n",
      "Epoch 1116\n",
      "Step 0: loss = 0.0296118576079607, recon_loss = 0.015674229711294174, 0.013929473236203194, kl_loss = 0.004077139310538769\n",
      "\n",
      "Epoch 1117\n",
      "Step 0: loss = 0.02679373137652874, recon_loss = 0.015409963205456734, 0.0113780302926898, kl_loss = 0.002869231626391411\n",
      "\n",
      "Epoch 1118\n",
      "Step 0: loss = 0.034599658101797104, recon_loss = 0.014871211722493172, 0.01971563510596752, kl_loss = 0.00640577357262373\n",
      "\n",
      "Epoch 1119\n",
      "Step 0: loss = 0.026174942031502724, recon_loss = 0.01390354335308075, 0.012262064963579178, kl_loss = 0.004666702821850777\n",
      "\n",
      "Epoch 1120\n",
      "Step 0: loss = 0.029124196618795395, recon_loss = 0.014404380694031715, 0.014710990712046623, kl_loss = 0.004413032904267311\n",
      "\n",
      "Epoch 1121\n",
      "Step 0: loss = 0.023165961727499962, recon_loss = 0.013676276430487633, 0.009484166279435158, kl_loss = 0.0027599558234214783\n",
      "\n",
      "Epoch 1122\n",
      "Step 0: loss = 0.030991321429610252, recon_loss = 0.0144709013402462, 0.016514074057340622, kl_loss = 0.003173215314745903\n",
      "\n",
      "Epoch 1123\n",
      "Step 0: loss = 0.026592673733830452, recon_loss = 0.014079751446843147, 0.01250087283551693, kl_loss = 0.006024372763931751\n",
      "\n",
      "Epoch 1124\n",
      "Step 0: loss = 0.032588109374046326, recon_loss = 0.014066953212022781, 0.01849902793765068, kl_loss = 0.011063577607274055\n",
      "\n",
      "Epoch 1125\n",
      "Step 0: loss = 0.028043068945407867, recon_loss = 0.014247657731175423, 0.013777105137705803, kl_loss = 0.009153309278190136\n",
      "\n",
      "Epoch 1126\n",
      "Step 0: loss = 0.02705448493361473, recon_loss = 0.016409406438469887, 0.01063492801040411, kl_loss = 0.005075360648334026\n",
      "\n",
      "Epoch 1127\n",
      "Step 0: loss = 0.025025896728038788, recon_loss = 0.015161683782935143, 0.009855266660451889, kl_loss = 0.0044731637462973595\n",
      "\n",
      "Epoch 1128\n",
      "Step 0: loss = 0.02704749070107937, recon_loss = 0.014146197587251663, 0.012891371734440327, kl_loss = 0.004960810765624046\n",
      "\n",
      "Epoch 1129\n",
      "Step 0: loss = 0.02854643203318119, recon_loss = 0.013697901740670204, 0.014844678342342377, kl_loss = 0.0019259490072727203\n",
      "\n",
      "Epoch 1130\n",
      "Step 0: loss = 0.03102010302245617, recon_loss = 0.013833006843924522, 0.017170749604701996, kl_loss = 0.008173639886081219\n",
      "\n",
      "Epoch 1131\n",
      "Step 0: loss = 0.029298730194568634, recon_loss = 0.015084238722920418, 0.014183989726006985, kl_loss = 0.01525135338306427\n",
      "\n",
      "Epoch 1132\n",
      "Step 0: loss = 0.030715372413396835, recon_loss = 0.013962522149085999, 0.016719374805688858, kl_loss = 0.016738111153244972\n",
      "\n",
      "Epoch 1133\n",
      "Step 0: loss = 0.028986690565943718, recon_loss = 0.014468714594841003, 0.014501497149467468, kl_loss = 0.008239510469138622\n",
      "\n",
      "Epoch 1134\n",
      "Step 0: loss = 0.02969042956829071, recon_loss = 0.01261681504547596, 0.017063962295651436, kl_loss = 0.0048264069482684135\n",
      "\n",
      "Epoch 1135\n",
      "Step 0: loss = 0.027539312839508057, recon_loss = 0.013494515791535378, 0.014029206708073616, kl_loss = 0.007794995792210102\n",
      "\n",
      "Epoch 1136\n",
      "Step 0: loss = 0.029616842046380043, recon_loss = 0.014808550477027893, 0.014792682603001595, kl_loss = 0.007804286666214466\n",
      "\n",
      "Epoch 1137\n",
      "Step 0: loss = 0.031291261315345764, recon_loss = 0.014577534049749374, 0.01670445315539837, kl_loss = 0.004637694917619228\n",
      "\n",
      "Epoch 1138\n",
      "Step 0: loss = 0.036956507712602615, recon_loss = 0.015151707455515862, 0.02179459109902382, kl_loss = 0.0051057590171694756\n",
      "\n",
      "Epoch 1139\n",
      "Step 0: loss = 0.028830312192440033, recon_loss = 0.013140218332409859, 0.01566370576620102, kl_loss = 0.013193830847740173\n",
      "\n",
      "Epoch 1140\n",
      "Step 0: loss = 0.024721717461943626, recon_loss = 0.013486465439200401, 0.011222188360989094, kl_loss = 0.006531070917844772\n",
      "\n",
      "Epoch 1141\n",
      "Step 0: loss = 0.026815714314579964, recon_loss = 0.013706350699067116, 0.013095540925860405, kl_loss = 0.006911757402122021\n",
      "\n",
      "Epoch 1142\n",
      "Step 0: loss = 0.027049195021390915, recon_loss = 0.013664845377206802, 0.013378007337450981, kl_loss = 0.0031712260097265244\n",
      "\n",
      "Epoch 1143\n",
      "Step 0: loss = 0.026499683037400246, recon_loss = 0.013670329004526138, 0.012820491567254066, kl_loss = 0.004430897533893585\n",
      "\n",
      "Epoch 1144\n",
      "Step 0: loss = 0.028324533253908157, recon_loss = 0.01359921507537365, 0.014713946729898453, kl_loss = 0.005685609765350819\n",
      "\n",
      "Epoch 1145\n",
      "Step 0: loss = 0.03697267919778824, recon_loss = 0.014676123857498169, 0.02228332869708538, kl_loss = 0.0066121360287070274\n",
      "\n",
      "Epoch 1146\n",
      "Step 0: loss = 0.028732692822813988, recon_loss = 0.01428934559226036, 0.014431783929467201, kl_loss = 0.00578154344111681\n",
      "\n",
      "Epoch 1147\n",
      "Step 0: loss = 0.02692822739481926, recon_loss = 0.012436175718903542, 0.014462282881140709, kl_loss = 0.014884538017213345\n",
      "\n",
      "Epoch 1148\n",
      "Step 0: loss = 0.026469139382243156, recon_loss = 0.011714000254869461, 0.014746405184268951, kl_loss = 0.004366950131952763\n",
      "\n",
      "Epoch 1149\n",
      "Step 0: loss = 0.03148806095123291, recon_loss = 0.012681001797318459, 0.018797893077135086, kl_loss = 0.004582361318171024\n",
      "\n",
      "Epoch 1150\n",
      "Step 0: loss = 0.03001435287296772, recon_loss = 0.013334246352314949, 0.01666124537587166, kl_loss = 0.00943051464855671\n",
      "\n",
      "Epoch 1151\n",
      "Step 0: loss = 0.026107128709554672, recon_loss = 0.013167597353458405, 0.012929154559969902, kl_loss = 0.005187991075217724\n",
      "\n",
      "Epoch 1152\n",
      "Step 0: loss = 0.035954397171735764, recon_loss = 0.012582020834088326, 0.023356560617685318, kl_loss = 0.007907142862677574\n",
      "\n",
      "Epoch 1153\n",
      "Step 0: loss = 0.027979061007499695, recon_loss = 0.0136738121509552, 0.01429205946624279, kl_loss = 0.006594907492399216\n",
      "\n",
      "Epoch 1154\n",
      "Step 0: loss = 0.024366166442632675, recon_loss = 0.013539118692278862, 0.010818690061569214, kl_loss = 0.004178486764431\n",
      "\n",
      "Epoch 1155\n",
      "Step 0: loss = 0.026835350319743156, recon_loss = 0.011997189372777939, 0.014829074963927269, kl_loss = 0.00454254075884819\n",
      "\n",
      "Epoch 1156\n",
      "Step 0: loss = 0.02724517323076725, recon_loss = 0.013332003727555275, 0.013905186206102371, kl_loss = 0.003991343080997467\n",
      "\n",
      "Epoch 1157\n",
      "Step 0: loss = 0.021710151806473732, recon_loss = 0.012418251484632492, 0.009282245300710201, kl_loss = 0.004826853983104229\n",
      "\n",
      "Epoch 1158\n",
      "Step 0: loss = 0.026717770844697952, recon_loss = 0.012680310755968094, 0.014027927070856094, kl_loss = 0.004766951315104961\n",
      "\n",
      "Epoch 1159\n",
      "Step 0: loss = 0.03164592757821083, recon_loss = 0.013032505288720131, 0.018603552132844925, kl_loss = 0.004934150725603104\n",
      "\n",
      "Epoch 1160\n",
      "Step 0: loss = 0.023975349962711334, recon_loss = 0.01209348812699318, 0.011872419156134129, kl_loss = 0.004722160287201405\n",
      "\n",
      "Epoch 1161\n",
      "Step 0: loss = 0.025897620245814323, recon_loss = 0.013060936704277992, 0.012828998267650604, kl_loss = 0.003842294216156006\n",
      "\n",
      "Epoch 1162\n",
      "Step 0: loss = 0.026138529181480408, recon_loss = 0.012211063876748085, 0.013922040350735188, kl_loss = 0.002712443470954895\n",
      "\n",
      "Epoch 1163\n",
      "Step 0: loss = 0.025355558842420578, recon_loss = 0.011379310861229897, 0.013963066972792149, kl_loss = 0.0065903933718800545\n",
      "\n",
      "Epoch 1164\n",
      "Step 0: loss = 0.025196677073836327, recon_loss = 0.011961104348301888, 0.013226930052042007, kl_loss = 0.004321327432990074\n",
      "\n",
      "Epoch 1165\n",
      "Step 0: loss = 0.026349568739533424, recon_loss = 0.011662092059850693, 0.014679096639156342, kl_loss = 0.004189822822809219\n",
      "\n",
      "Epoch 1166\n",
      "Step 0: loss = 0.026630958542227745, recon_loss = 0.011957567185163498, 0.014660671353340149, kl_loss = 0.006359751336276531\n",
      "\n",
      "Epoch 1167\n",
      "Step 0: loss = 0.02516472525894642, recon_loss = 0.011299028992652893, 0.013851884752511978, kl_loss = 0.006906129419803619\n",
      "\n",
      "Epoch 1168\n",
      "Step 0: loss = 0.02336413413286209, recon_loss = 0.011974573135375977, 0.011367550119757652, kl_loss = 0.011005164124071598\n",
      "\n",
      "Epoch 1169\n",
      "Step 0: loss = 0.023538410663604736, recon_loss = 0.0108192078769207, 0.012708332389593124, kl_loss = 0.0054352134466171265\n",
      "\n",
      "Epoch 1170\n",
      "Step 0: loss = 0.025480765849351883, recon_loss = 0.010731423273682594, 0.014740800485014915, kl_loss = 0.004271229729056358\n",
      "\n",
      "Epoch 1171\n",
      "Step 0: loss = 0.022448454052209854, recon_loss = 0.010424220934510231, 0.01201397180557251, kl_loss = 0.005130590870976448\n",
      "\n",
      "Epoch 1172\n",
      "Step 0: loss = 0.02255057543516159, recon_loss = 0.011436166241765022, 0.011108454316854477, kl_loss = 0.002977541647851467\n",
      "\n",
      "Epoch 1173\n",
      "Step 0: loss = 0.024476131424307823, recon_loss = 0.01056014932692051, 0.013906456530094147, kl_loss = 0.004762817174196243\n",
      "\n",
      "Epoch 1174\n",
      "Step 0: loss = 0.027776092290878296, recon_loss = 0.011536335572600365, 0.016230691224336624, kl_loss = 0.004533091560006142\n",
      "\n",
      "Epoch 1175\n",
      "Step 0: loss = 0.020750615745782852, recon_loss = 0.010823968797922134, 0.009920821525156498, kl_loss = 0.0029131807386875153\n",
      "\n",
      "Epoch 1176\n",
      "Step 0: loss = 0.021778950467705727, recon_loss = 0.010526981204748154, 0.011248264461755753, kl_loss = 0.001852543093264103\n",
      "\n",
      "Epoch 1177\n",
      "Step 0: loss = 0.024107947945594788, recon_loss = 0.01084214448928833, 0.013261530548334122, kl_loss = 0.002136014401912689\n",
      "\n",
      "Epoch 1178\n",
      "Step 0: loss = 0.022047819569706917, recon_loss = 0.01163577102124691, 0.010405967943370342, kl_loss = 0.003040626645088196\n",
      "\n",
      "Epoch 1179\n",
      "Step 0: loss = 0.026770731434226036, recon_loss = 0.011128665879368782, 0.015634575858712196, kl_loss = 0.003744392655789852\n",
      "\n",
      "Epoch 1180\n",
      "Step 0: loss = 0.030321022495627403, recon_loss = 0.013568155467510223, 0.016731150448322296, kl_loss = 0.010858682915568352\n",
      "\n",
      "Epoch 1181\n",
      "Step 0: loss = 0.022922148928046227, recon_loss = 0.010867970064282417, 0.012036513537168503, kl_loss = 0.00883280485868454\n",
      "\n",
      "Epoch 1182\n",
      "Step 0: loss = 0.0243193581700325, recon_loss = 0.010840538889169693, 0.013470874167978764, kl_loss = 0.003973209299147129\n",
      "\n",
      "Epoch 1183\n",
      "Step 0: loss = 0.023483004420995712, recon_loss = 0.011929057538509369, 0.01153995469212532, kl_loss = 0.0069959405809640884\n",
      "\n",
      "Epoch 1184\n",
      "Step 0: loss = 0.029953567311167717, recon_loss = 0.013276929035782814, 0.016661040484905243, kl_loss = 0.007798528298735619\n",
      "\n",
      "Epoch 1185\n",
      "Step 0: loss = 0.029350291937589645, recon_loss = 0.013989437371492386, 0.015338950790464878, kl_loss = 0.010952630080282688\n",
      "\n",
      "Epoch 1186\n",
      "Step 0: loss = 0.02660413458943367, recon_loss = 0.012742972001433372, 0.013853078708052635, kl_loss = 0.004041640087962151\n",
      "\n",
      "Epoch 1187\n",
      "Step 0: loss = 0.02442106604576111, recon_loss = 0.012315545231103897, 0.012098575942218304, kl_loss = 0.0034719835966825485\n",
      "\n",
      "Epoch 1188\n",
      "Step 0: loss = 0.024745874106884003, recon_loss = 0.011484760791063309, 0.013242607936263084, kl_loss = 0.009252246469259262\n",
      "\n",
      "Epoch 1189\n",
      "Step 0: loss = 0.024597186595201492, recon_loss = 0.011691639199852943, 0.012890415266156197, kl_loss = 0.007566048763692379\n",
      "\n",
      "Epoch 1190\n",
      "Step 0: loss = 0.021889429539442062, recon_loss = 0.010650597512722015, 0.011224987916648388, kl_loss = 0.0069217374548316\n",
      "\n",
      "Epoch 1191\n",
      "Step 0: loss = 0.028446165844798088, recon_loss = 0.01208018884062767, 0.016354000195860863, kl_loss = 0.005988736636936665\n",
      "\n",
      "Epoch 1192\n",
      "Step 0: loss = 0.01931764930486679, recon_loss = 0.010574955493211746, 0.008735725656151772, kl_loss = 0.0034838924184441566\n",
      "\n",
      "Epoch 1193\n",
      "Step 0: loss = 0.02738078683614731, recon_loss = 0.011389771476387978, 0.015982983633875847, kl_loss = 0.004015563055872917\n",
      "\n",
      "Epoch 1194\n",
      "Step 0: loss = 0.022920912131667137, recon_loss = 0.01100800558924675, 0.011907689273357391, kl_loss = 0.0026084957644343376\n",
      "\n",
      "Epoch 1195\n",
      "Step 0: loss = 0.026425914838910103, recon_loss = 0.00999690406024456, 0.016421224921941757, kl_loss = 0.0038933847099542618\n",
      "\n",
      "Epoch 1196\n",
      "Step 0: loss = 0.02771303988993168, recon_loss = 0.010998284444212914, 0.016702739521861076, kl_loss = 0.006007718853652477\n",
      "\n",
      "Epoch 1197\n",
      "Step 0: loss = 0.026495713740587234, recon_loss = 0.010381326079368591, 0.01610606163740158, kl_loss = 0.004162873141467571\n",
      "\n",
      "Epoch 1198\n",
      "Step 0: loss = 0.024970505386590958, recon_loss = 0.010856810957193375, 0.014095927588641644, kl_loss = 0.00888283085078001\n",
      "\n",
      "Epoch 1199\n",
      "Step 0: loss = 0.02488728240132332, recon_loss = 0.01078559085726738, 0.014064920134842396, kl_loss = 0.01838662475347519\n",
      "\n",
      "Epoch 1200\n",
      "Step 0: loss = 0.027376921847462654, recon_loss = 0.011686328798532486, 0.01568194478750229, kl_loss = 0.0043244631960988045\n",
      "\n",
      "Epoch 1201\n",
      "Step 0: loss = 0.023127330467104912, recon_loss = 0.011800076812505722, 0.011317417956888676, kl_loss = 0.004917890764772892\n",
      "\n",
      "Epoch 1202\n",
      "Step 0: loss = 0.02693086303770542, recon_loss = 0.0107846949249506, 0.016130421310663223, kl_loss = 0.007873686961829662\n",
      "\n",
      "Epoch 1203\n",
      "Step 0: loss = 0.03156999126076698, recon_loss = 0.012075185775756836, 0.01948477327823639, kl_loss = 0.0050169602036476135\n",
      "\n",
      "Epoch 1204\n",
      "Step 0: loss = 0.02482481859624386, recon_loss = 0.01012762077152729, 0.014673324301838875, kl_loss = 0.01193717960268259\n",
      "\n",
      "Epoch 1205\n",
      "Step 0: loss = 0.022166630253195763, recon_loss = 0.011008504778146744, 0.0111452117562294, kl_loss = 0.006457170471549034\n",
      "\n",
      "Epoch 1206\n",
      "Step 0: loss = 0.02349427156150341, recon_loss = 0.011746615171432495, 0.011738013476133347, kl_loss = 0.004821453243494034\n",
      "\n",
      "Epoch 1207\n",
      "Step 0: loss = 0.024083875119686127, recon_loss = 0.010066907852888107, 0.014009279198944569, kl_loss = 0.0038448795676231384\n",
      "\n",
      "Epoch 1208\n",
      "Step 0: loss = 0.018895847722887993, recon_loss = 0.009362561628222466, 0.00952603854238987, kl_loss = 0.0036235321313142776\n",
      "\n",
      "Epoch 1209\n",
      "Step 0: loss = 0.02087714895606041, recon_loss = 0.009969376027584076, 0.010898696258664131, kl_loss = 0.004538561217486858\n",
      "\n",
      "Epoch 1210\n",
      "Step 0: loss = 0.029285579919815063, recon_loss = 0.010877024382352829, 0.018397031351923943, kl_loss = 0.005761750973761082\n",
      "\n",
      "Epoch 1211\n",
      "Step 0: loss = 0.02578616701066494, recon_loss = 0.010883770883083344, 0.014889414422214031, kl_loss = 0.006490297615528107\n",
      "\n",
      "Epoch 1212\n",
      "Step 0: loss = 0.02428564243018627, recon_loss = 0.010033827275037766, 0.014241538010537624, kl_loss = 0.005138522945344448\n",
      "\n",
      "Epoch 1213\n",
      "Step 0: loss = 0.022515403106808662, recon_loss = 0.01035035215318203, 0.012156100943684578, kl_loss = 0.004475198686122894\n",
      "\n",
      "Epoch 1214\n",
      "Step 0: loss = 0.02426137775182724, recon_loss = 0.011502759531140327, 0.012739498168230057, kl_loss = 0.009560237638652325\n",
      "\n",
      "Epoch 1215\n",
      "Step 0: loss = 0.02201465703547001, recon_loss = 0.009663050994277, 0.01233983226120472, kl_loss = 0.005887036211788654\n",
      "\n",
      "Epoch 1216\n",
      "Step 0: loss = 0.02513197995722294, recon_loss = 0.011110290884971619, 0.014009503647685051, kl_loss = 0.006092616356909275\n",
      "\n",
      "Epoch 1217\n",
      "Step 0: loss = 0.025570645928382874, recon_loss = 0.009682981297373772, 0.01586856134235859, kl_loss = 0.009551999159157276\n",
      "\n",
      "Epoch 1218\n",
      "Step 0: loss = 0.026967059820890427, recon_loss = 0.009960208088159561, 0.016998054459691048, kl_loss = 0.0043986886739730835\n",
      "\n",
      "Epoch 1219\n",
      "Step 0: loss = 0.022028600797057152, recon_loss = 0.010836182162165642, 0.011174561455845833, kl_loss = 0.00892842561006546\n",
      "\n",
      "Epoch 1220\n",
      "Step 0: loss = 0.01859070174396038, recon_loss = 0.009554194286465645, 0.009025546722114086, kl_loss = 0.005480455234646797\n",
      "\n",
      "Epoch 1221\n",
      "Step 0: loss = 0.023411236703395844, recon_loss = 0.009741036221385002, 0.013660320080816746, kl_loss = 0.004940127022564411\n",
      "\n",
      "Epoch 1222\n",
      "Step 0: loss = 0.029190093278884888, recon_loss = 0.009899044409394264, 0.0192846842110157, kl_loss = 0.003182421438395977\n",
      "\n",
      "Epoch 1223\n",
      "Step 0: loss = 0.024812810122966766, recon_loss = 0.011433009058237076, 0.013368412852287292, kl_loss = 0.005693846382200718\n",
      "\n",
      "Epoch 1224\n",
      "Step 0: loss = 0.023382872343063354, recon_loss = 0.010457830503582954, 0.012909172102808952, kl_loss = 0.00793446321040392\n",
      "\n",
      "Epoch 1225\n",
      "Step 0: loss = 0.025851521641016006, recon_loss = 0.011404788121581078, 0.014440339989960194, kl_loss = 0.0031965477392077446\n",
      "\n",
      "Epoch 1226\n",
      "Step 0: loss = 0.024891646578907967, recon_loss = 0.009872689843177795, 0.01501106470823288, kl_loss = 0.003945925273001194\n",
      "\n",
      "Epoch 1227\n",
      "Step 0: loss = 0.02389644645154476, recon_loss = 0.01019054651260376, 0.013691987842321396, kl_loss = 0.006955812685191631\n",
      "\n",
      "Epoch 1228\n",
      "Step 0: loss = 0.026372194290161133, recon_loss = 0.01054757833480835, 0.01581377536058426, kl_loss = 0.005420653149485588\n",
      "\n",
      "Epoch 1229\n",
      "Step 0: loss = 0.02247174084186554, recon_loss = 0.010001551359891891, 0.012457141652703285, kl_loss = 0.006523660384118557\n",
      "\n",
      "Epoch 1230\n",
      "Step 0: loss = 0.022534765303134918, recon_loss = 0.009750181809067726, 0.012775390408933163, kl_loss = 0.004597024992108345\n",
      "\n",
      "Epoch 1231\n",
      "Step 0: loss = 0.022486967965960503, recon_loss = 0.010140405967831612, 0.012340639717876911, kl_loss = 0.0029603643342852592\n",
      "\n",
      "Epoch 1232\n",
      "Step 0: loss = 0.02051742933690548, recon_loss = 0.009448682889342308, 0.011062782257795334, kl_loss = 0.00298184622079134\n",
      "\n",
      "Epoch 1233\n",
      "Step 0: loss = 0.022577427327632904, recon_loss = 0.009623195976018906, 0.012949792668223381, kl_loss = 0.002219683490693569\n",
      "\n",
      "Epoch 1234\n",
      "Step 0: loss = 0.03226475790143013, recon_loss = 0.009942419826984406, 0.02231171354651451, kl_loss = 0.005313125438988209\n",
      "\n",
      "Epoch 1235\n",
      "Step 0: loss = 0.021039918065071106, recon_loss = 0.008367221802473068, 0.012662418186664581, kl_loss = 0.005138713866472244\n",
      "\n",
      "Epoch 1236\n",
      "Step 0: loss = 0.022233325988054276, recon_loss = 0.008418772369623184, 0.013807786628603935, kl_loss = 0.0033839019015431404\n",
      "\n",
      "Epoch 1237\n",
      "Step 0: loss = 0.021728625521063805, recon_loss = 0.008838558569550514, 0.012882339768111706, kl_loss = 0.0038642287254333496\n",
      "\n",
      "Epoch 1238\n",
      "Step 0: loss = 0.020650997757911682, recon_loss = 0.008850568905472755, 0.011790352873504162, kl_loss = 0.005038712173700333\n",
      "\n",
      "Epoch 1239\n",
      "Step 0: loss = 0.02838278003036976, recon_loss = 0.009285785257816315, 0.019088149070739746, kl_loss = 0.004423120990395546\n",
      "\n",
      "Epoch 1240\n",
      "Step 0: loss = 0.02228415571153164, recon_loss = 0.009914886206388474, 0.012359704822301865, kl_loss = 0.004782427102327347\n",
      "\n",
      "Epoch 1241\n",
      "Step 0: loss = 0.024690458551049232, recon_loss = 0.008534446358680725, 0.016141727566719055, kl_loss = 0.007142580114305019\n",
      "\n",
      "Epoch 1242\n",
      "Step 0: loss = 0.03652988746762276, recon_loss = 0.011613503098487854, 0.02490057609975338, kl_loss = 0.007903059013187885\n",
      "\n",
      "Epoch 1243\n",
      "Step 0: loss = 0.020607883110642433, recon_loss = 0.008687956258654594, 0.011894896626472473, kl_loss = 0.012514711357653141\n",
      "\n",
      "Epoch 1244\n",
      "Step 0: loss = 0.02211521938443184, recon_loss = 0.009732313454151154, 0.012367352843284607, kl_loss = 0.00777620542794466\n",
      "\n",
      "Epoch 1245\n",
      "Step 0: loss = 0.019213082268834114, recon_loss = 0.009021855890750885, 0.010183881968259811, kl_loss = 0.003672521561384201\n",
      "\n",
      "Epoch 1246\n",
      "Step 0: loss = 0.018085092306137085, recon_loss = 0.008681809529662132, 0.009399458765983582, kl_loss = 0.0019123218953609467\n",
      "\n",
      "Epoch 1247\n",
      "Step 0: loss = 0.020281130447983742, recon_loss = 0.007948178797960281, 0.012326235882937908, kl_loss = 0.0033571701496839523\n",
      "\n",
      "Epoch 1248\n",
      "Step 0: loss = 0.020321102812886238, recon_loss = 0.008298462256789207, 0.012011205777525902, kl_loss = 0.005717788822948933\n",
      "\n",
      "Epoch 1249\n",
      "Step 0: loss = 0.02035399153828621, recon_loss = 0.008323654532432556, 0.012022854760289192, kl_loss = 0.0037415344268083572\n",
      "\n",
      "Epoch 1250\n",
      "Step 0: loss = 0.02371508628129959, recon_loss = 0.009168228134512901, 0.014542069286108017, kl_loss = 0.002394423820078373\n",
      "\n",
      "Epoch 1251\n",
      "Step 0: loss = 0.019845418632030487, recon_loss = 0.009748205542564392, 0.010091559961438179, kl_loss = 0.002826295793056488\n",
      "\n",
      "Epoch 1252\n",
      "Step 0: loss = 0.02406938560307026, recon_loss = 0.008870488032698631, 0.015183931216597557, kl_loss = 0.0074833352118730545\n",
      "\n",
      "Epoch 1253\n",
      "Step 0: loss = 0.022571440786123276, recon_loss = 0.009335841983556747, 0.013222891837358475, kl_loss = 0.006353395991027355\n",
      "\n",
      "Epoch 1254\n",
      "Step 0: loss = 0.027353042736649513, recon_loss = 0.00907520018517971, 0.018257446587085724, kl_loss = 0.010198414325714111\n",
      "\n",
      "Epoch 1255\n",
      "Step 0: loss = 0.021992294117808342, recon_loss = 0.010050518438220024, 0.01190171018242836, kl_loss = 0.02003311738371849\n",
      "\n",
      "Epoch 1256\n",
      "Step 0: loss = 0.021660316735506058, recon_loss = 0.010075097903609276, 0.011569848284125328, kl_loss = 0.007685631513595581\n",
      "\n",
      "Epoch 1257\n",
      "Step 0: loss = 0.02299053594470024, recon_loss = 0.010234946385025978, 0.012734701856970787, kl_loss = 0.010443535633385181\n",
      "\n",
      "Epoch 1258\n",
      "Step 0: loss = 0.02386198192834854, recon_loss = 0.010075831785798073, 0.013772955164313316, kl_loss = 0.006597472354769707\n",
      "\n",
      "Epoch 1259\n",
      "Step 0: loss = 0.027194436639547348, recon_loss = 0.010076301172375679, 0.017105288803577423, kl_loss = 0.006423346698284149\n",
      "\n",
      "Epoch 1260\n",
      "Step 0: loss = 0.02127966843545437, recon_loss = 0.009716972708702087, 0.011551681905984879, kl_loss = 0.005506972782313824\n",
      "\n",
      "Epoch 1261\n",
      "Step 0: loss = 0.018340714275836945, recon_loss = 0.008973747491836548, 0.009356863796710968, kl_loss = 0.005051302723586559\n",
      "\n",
      "Epoch 1262\n",
      "Step 0: loss = 0.021189583465456963, recon_loss = 0.008356647565960884, 0.012824634090065956, kl_loss = 0.004150801338255405\n",
      "\n",
      "Epoch 1263\n",
      "Step 0: loss = 0.025575580075383186, recon_loss = 0.00932791456580162, 0.016238221898674965, kl_loss = 0.004721854813396931\n",
      "\n",
      "Epoch 1264\n",
      "Step 0: loss = 0.01873299665749073, recon_loss = 0.008549883961677551, 0.010172028094530106, kl_loss = 0.005542656406760216\n",
      "\n",
      "Epoch 1265\n",
      "Step 0: loss = 0.023552177473902702, recon_loss = 0.008343007415533066, 0.015194233506917953, kl_loss = 0.007468667812645435\n",
      "\n",
      "Epoch 1266\n",
      "Step 0: loss = 0.020909054204821587, recon_loss = 0.00826479122042656, 0.012633915059268475, kl_loss = 0.005173938348889351\n",
      "\n",
      "Epoch 1267\n",
      "Step 0: loss = 0.02202351577579975, recon_loss = 0.008641798049211502, 0.013375403359532356, kl_loss = 0.0031569385901093483\n",
      "\n",
      "Epoch 1268\n",
      "Step 0: loss = 0.0223828237503767, recon_loss = 0.008390463888645172, 0.013984693214297295, kl_loss = 0.0038329148665070534\n",
      "\n",
      "Epoch 1269\n",
      "Step 0: loss = 0.02392389625310898, recon_loss = 0.008556932210922241, 0.01535932719707489, kl_loss = 0.0038180435076355934\n",
      "\n",
      "Epoch 1270\n",
      "Step 0: loss = 0.018504414707422256, recon_loss = 0.007684158161282539, 0.01081074308604002, kl_loss = 0.004757574759423733\n",
      "\n",
      "Epoch 1271\n",
      "Step 0: loss = 0.026144329458475113, recon_loss = 0.009509086608886719, 0.016629889607429504, kl_loss = 0.0026769889518618584\n",
      "\n",
      "Epoch 1272\n",
      "Step 0: loss = 0.02252536453306675, recon_loss = 0.008092604577541351, 0.014416459016501904, kl_loss = 0.008149554952979088\n",
      "\n",
      "Epoch 1273\n",
      "Step 0: loss = 0.01994175836443901, recon_loss = 0.009371539577841759, 0.010555367916822433, kl_loss = 0.0074256956577301025\n",
      "\n",
      "Epoch 1274\n",
      "Step 0: loss = 0.021596180275082588, recon_loss = 0.008996343240141869, 0.012594117783010006, kl_loss = 0.0028601577505469322\n",
      "\n",
      "Epoch 1275\n",
      "Step 0: loss = 0.024884723126888275, recon_loss = 0.008955441415309906, 0.015911342576146126, kl_loss = 0.008969491347670555\n",
      "\n",
      "Epoch 1276\n",
      "Step 0: loss = 0.02375273033976555, recon_loss = 0.008261067792773247, 0.015477166511118412, kl_loss = 0.0072478605434298515\n",
      "\n",
      "Epoch 1277\n",
      "Step 0: loss = 0.0222467053681612, recon_loss = 0.008557207882404327, 0.013678462244570255, kl_loss = 0.005517974495887756\n",
      "\n",
      "Epoch 1278\n",
      "Step 0: loss = 0.023344064131379128, recon_loss = 0.008312210440635681, 0.015025924891233444, kl_loss = 0.002964390441775322\n",
      "\n",
      "Epoch 1279\n",
      "Step 0: loss = 0.021501872688531876, recon_loss = 0.008026627823710442, 0.013469519093632698, kl_loss = 0.0028625428676605225\n",
      "\n",
      "Epoch 1280\n",
      "Step 0: loss = 0.02190522290766239, recon_loss = 0.008045265451073647, 0.013855285942554474, kl_loss = 0.002336018718779087\n",
      "\n",
      "Epoch 1281\n",
      "Step 0: loss = 0.02622750774025917, recon_loss = 0.008384183049201965, 0.01783301681280136, kl_loss = 0.005153869278728962\n",
      "\n",
      "Epoch 1282\n",
      "Step 0: loss = 0.021259626373648643, recon_loss = 0.007826747372746468, 0.013424718752503395, kl_loss = 0.004080391488969326\n",
      "\n",
      "Epoch 1283\n",
      "Step 0: loss = 0.023412736132740974, recon_loss = 0.008670790120959282, 0.014734551310539246, kl_loss = 0.003697766922414303\n",
      "\n",
      "Epoch 1284\n",
      "Step 0: loss = 0.019318843260407448, recon_loss = 0.008103789761662483, 0.011208131909370422, kl_loss = 0.003460800275206566\n",
      "\n",
      "Epoch 1285\n",
      "Step 0: loss = 0.020658070221543312, recon_loss = 0.007852993905544281, 0.01279849000275135, kl_loss = 0.0032931743189692497\n",
      "\n",
      "Epoch 1286\n",
      "Step 0: loss = 0.022337060421705246, recon_loss = 0.007779061794281006, 0.01453900896012783, kl_loss = 0.00949479453265667\n",
      "\n",
      "Epoch 1287\n",
      "Step 0: loss = 0.02665487863123417, recon_loss = 0.00805051252245903, 0.01859050616621971, kl_loss = 0.006929941475391388\n",
      "\n",
      "Epoch 1288\n",
      "Step 0: loss = 0.024900954216718674, recon_loss = 0.00897393561899662, 0.015917276963591576, kl_loss = 0.004871201701462269\n",
      "\n",
      "Epoch 1289\n",
      "Step 0: loss = 0.02312019094824791, recon_loss = 0.0089577566832304, 0.014151664450764656, kl_loss = 0.0053852396085858345\n",
      "\n",
      "Epoch 1290\n",
      "Step 0: loss = 0.022311443462967873, recon_loss = 0.009226188063621521, 0.013074750080704689, kl_loss = 0.005252198316156864\n",
      "\n",
      "Epoch 1291\n",
      "Step 0: loss = 0.02232745848596096, recon_loss = 0.008099846541881561, 0.014216776937246323, kl_loss = 0.005417903885245323\n",
      "\n",
      "Epoch 1292\n",
      "Step 0: loss = 0.019117141142487526, recon_loss = 0.008180063217878342, 0.010930121876299381, kl_loss = 0.0034786406904459\n",
      "\n",
      "Epoch 1293\n",
      "Step 0: loss = 0.024536089971661568, recon_loss = 0.009164934977889061, 0.015361422672867775, kl_loss = 0.004865830764174461\n",
      "\n",
      "Epoch 1294\n",
      "Step 0: loss = 0.023555854335427284, recon_loss = 0.008148590102791786, 0.015396567061543465, kl_loss = 0.005348337814211845\n",
      "\n",
      "Epoch 1295\n",
      "Step 0: loss = 0.01736563630402088, recon_loss = 0.007474349811673164, 0.00988319143652916, kl_loss = 0.004047941416501999\n",
      "\n",
      "Epoch 1296\n",
      "Step 0: loss = 0.021534550935029984, recon_loss = 0.007315210998058319, 0.014208824373781681, kl_loss = 0.005258471705019474\n",
      "\n",
      "Epoch 1297\n",
      "Step 0: loss = 0.023648584261536598, recon_loss = 0.008222958073019981, 0.015402723103761673, kl_loss = 0.01145133189857006\n",
      "\n",
      "Epoch 1298\n",
      "Step 0: loss = 0.023658959195017815, recon_loss = 0.007939599454402924, 0.015700165182352066, kl_loss = 0.00959765911102295\n",
      "\n",
      "Epoch 1299\n",
      "Step 0: loss = 0.0318559892475605, recon_loss = 0.008941041305661201, 0.022894443944096565, kl_loss = 0.010251755826175213\n",
      "\n",
      "Epoch 1300\n",
      "Step 0: loss = 0.022732041776180267, recon_loss = 0.009951315820217133, 0.012766735628247261, kl_loss = 0.006995054893195629\n",
      "\n",
      "Epoch 1301\n",
      "Step 0: loss = 0.019955433905124664, recon_loss = 0.008792437613010406, 0.01115058921277523, kl_loss = 0.006203712895512581\n",
      "\n",
      "Epoch 1302\n",
      "Step 0: loss = 0.01817401684820652, recon_loss = 0.007841764017939568, 0.010325628332793713, kl_loss = 0.0033131660893559456\n",
      "\n",
      "Epoch 1303\n",
      "Step 0: loss = 0.02728085219860077, recon_loss = 0.008507387712597847, 0.018761126324534416, kl_loss = 0.006168819963932037\n",
      "\n",
      "Epoch 1304\n",
      "Step 0: loss = 0.025314494967460632, recon_loss = 0.009098971262574196, 0.016177939251065254, kl_loss = 0.018792299553751945\n",
      "\n",
      "Epoch 1305\n",
      "Step 0: loss = 0.024380654096603394, recon_loss = 0.008729441091418266, 0.015620582737028599, kl_loss = 0.015314534306526184\n",
      "\n",
      "Epoch 1306\n",
      "Step 0: loss = 0.018744613975286484, recon_loss = 0.008297216147184372, 0.010437628254294395, kl_loss = 0.004884505644440651\n",
      "\n",
      "Epoch 1307\n",
      "Step 0: loss = 0.020207494497299194, recon_loss = 0.0074381642043590546, 0.012762781232595444, kl_loss = 0.003274412825703621\n",
      "\n",
      "Epoch 1308\n",
      "Step 0: loss = 0.020976562052965164, recon_loss = 0.007905805483460426, 0.013063985854387283, kl_loss = 0.0033851591870188713\n",
      "\n",
      "Epoch 1309\n",
      "Step 0: loss = 0.022027060389518738, recon_loss = 0.007002651691436768, 0.015000407584011555, kl_loss = 0.012000693939626217\n",
      "\n",
      "Epoch 1310\n",
      "Step 0: loss = 0.021803326904773712, recon_loss = 0.007946837693452835, 0.01383970770984888, kl_loss = 0.008390815928578377\n",
      "\n",
      "Epoch 1311\n",
      "Step 0: loss = 0.019311292096972466, recon_loss = 0.00799199752509594, 0.011308279819786549, kl_loss = 0.005507185123860836\n",
      "\n",
      "Epoch 1312\n",
      "Step 0: loss = 0.017165370285511017, recon_loss = 0.007273310795426369, 0.009877335280179977, kl_loss = 0.007362299598753452\n",
      "\n",
      "Epoch 1313\n",
      "Step 0: loss = 0.018732214346528053, recon_loss = 0.006936056539416313, 0.011785533279180527, kl_loss = 0.00531263742595911\n",
      "\n",
      "Epoch 1314\n",
      "Step 0: loss = 0.02015228383243084, recon_loss = 0.007664235308766365, 0.012475134804844856, kl_loss = 0.00645681656897068\n",
      "\n",
      "Epoch 1315\n",
      "Step 0: loss = 0.019390033558011055, recon_loss = 0.008206019178032875, 0.011173877865076065, kl_loss = 0.0050681037828326225\n",
      "\n",
      "Epoch 1316\n",
      "Step 0: loss = 0.021691730245947838, recon_loss = 0.008576778694987297, 0.013108305633068085, kl_loss = 0.0033228741958737373\n",
      "\n",
      "Epoch 1317\n",
      "Step 0: loss = 0.021464047953486443, recon_loss = 0.00847548246383667, 0.012977355159819126, kl_loss = 0.005605344660580158\n",
      "\n",
      "Epoch 1318\n",
      "Step 0: loss = 0.017129790037870407, recon_loss = 0.007349278777837753, 0.009773429483175278, kl_loss = 0.0035405727103352547\n",
      "\n",
      "Epoch 1319\n",
      "Step 0: loss = 0.01811933144927025, recon_loss = 0.00675877183675766, 0.011353842914104462, kl_loss = 0.003357985056936741\n",
      "\n",
      "Epoch 1320\n",
      "Step 0: loss = 0.025199828669428825, recon_loss = 0.0069730039685964584, 0.018215682357549667, kl_loss = 0.005571039393544197\n",
      "\n",
      "Epoch 1321\n",
      "Step 0: loss = 0.01900702901184559, recon_loss = 0.006858834996819496, 0.012137003242969513, kl_loss = 0.005594968795776367\n",
      "\n",
      "Epoch 1322\n",
      "Step 0: loss = 0.020284637808799744, recon_loss = 0.006733715534210205, 0.013546797446906567, kl_loss = 0.002062215469777584\n",
      "\n",
      "Epoch 1323\n",
      "Step 0: loss = 0.01948871836066246, recon_loss = 0.007224021479487419, 0.0122563187032938, kl_loss = 0.004188690334558487\n",
      "\n",
      "Epoch 1324\n",
      "Step 0: loss = 0.018204104155302048, recon_loss = 0.006828799843788147, 0.011368424631655216, kl_loss = 0.00344059057533741\n",
      "\n",
      "Epoch 1325\n",
      "Step 0: loss = 0.022303758189082146, recon_loss = 0.007605466991662979, 0.014681538566946983, kl_loss = 0.008376091718673706\n",
      "\n",
      "Epoch 1326\n",
      "Step 0: loss = 0.022672446444630623, recon_loss = 0.006943376734852791, 0.015721092000603676, kl_loss = 0.0039887139573693275\n",
      "\n",
      "Epoch 1327\n",
      "Step 0: loss = 0.020576294511556625, recon_loss = 0.00733431801199913, 0.013237003237009048, kl_loss = 0.002486259676516056\n",
      "\n",
      "Epoch 1328\n",
      "Step 0: loss = 0.01616244949400425, recon_loss = 0.006601417437195778, 0.009555928409099579, kl_loss = 0.002551375888288021\n",
      "\n",
      "Epoch 1329\n",
      "Step 0: loss = 0.023434430360794067, recon_loss = 0.0079808309674263, 0.015444990247488022, kl_loss = 0.004304783418774605\n",
      "\n",
      "Epoch 1330\n",
      "Step 0: loss = 0.02117144502699375, recon_loss = 0.007359884679317474, 0.013799391686916351, kl_loss = 0.006084009073674679\n",
      "\n",
      "Epoch 1331\n",
      "Step 0: loss = 0.027544738724827766, recon_loss = 0.00801599957048893, 0.019518103450536728, kl_loss = 0.005317531526088715\n",
      "\n",
      "Epoch 1332\n",
      "Step 0: loss = 0.026852726936340332, recon_loss = 0.008586389943957329, 0.018210090696811676, kl_loss = 0.028123486787080765\n",
      "\n",
      "Epoch 1333\n",
      "Step 0: loss = 0.023195821791887283, recon_loss = 0.008387131616473198, 0.01477470900863409, kl_loss = 0.01699080504477024\n",
      "\n",
      "Epoch 1334\n",
      "Step 0: loss = 0.0195828378200531, recon_loss = 0.007765820249915123, 0.011801647022366524, kl_loss = 0.007685068063437939\n",
      "\n",
      "Epoch 1335\n",
      "Step 0: loss = 0.018856117501854897, recon_loss = 0.007393158972263336, 0.011456310749053955, kl_loss = 0.0033242693170905113\n",
      "\n",
      "Epoch 1336\n",
      "Step 0: loss = 0.01942792534828186, recon_loss = 0.006990684196352959, 0.01243063434958458, kl_loss = 0.0033036768436431885\n",
      "\n",
      "Epoch 1337\n",
      "Step 0: loss = 0.017269669100642204, recon_loss = 0.006998114287853241, 0.010254236869513988, kl_loss = 0.00865887850522995\n",
      "\n",
      "Epoch 1338\n",
      "Step 0: loss = 0.02103884145617485, recon_loss = 0.006999418139457703, 0.014029320329427719, kl_loss = 0.005051678977906704\n",
      "\n",
      "Epoch 1339\n",
      "Step 0: loss = 0.015326748602092266, recon_loss = 0.006444141268730164, 0.008873826824128628, kl_loss = 0.004390253685414791\n",
      "\n",
      "Epoch 1340\n",
      "Step 0: loss = 0.017795858904719353, recon_loss = 0.006670305505394936, 0.011117082089185715, kl_loss = 0.0042357370257377625\n",
      "\n",
      "Epoch 1341\n",
      "Step 0: loss = 0.021440234035253525, recon_loss = 0.008609121665358543, 0.012821333482861519, kl_loss = 0.004889906384050846\n",
      "\n",
      "Epoch 1342\n",
      "Step 0: loss = 0.021826745942234993, recon_loss = 0.007962418720126152, 0.013856373727321625, kl_loss = 0.003976774401962757\n",
      "\n",
      "Epoch 1343\n",
      "Step 0: loss = 0.02159406617283821, recon_loss = 0.007653564214706421, 0.013934205286204815, kl_loss = 0.0031480975449085236\n",
      "\n",
      "Epoch 1344\n",
      "Step 0: loss = 0.016839686781167984, recon_loss = 0.007089577615261078, 0.009743740782141685, kl_loss = 0.003184176981449127\n",
      "\n",
      "Epoch 1345\n",
      "Step 0: loss = 0.02094126120209694, recon_loss = 0.00663280114531517, 0.014300926588475704, kl_loss = 0.0037665748968720436\n",
      "\n",
      "Epoch 1346\n",
      "Step 0: loss = 0.020580682903528214, recon_loss = 0.006843714043498039, 0.013728184625506401, kl_loss = 0.004391859285533428\n",
      "\n",
      "Epoch 1347\n",
      "Step 0: loss = 0.020031945779919624, recon_loss = 0.006862131878733635, 0.013161025941371918, kl_loss = 0.00439442228525877\n",
      "\n",
      "Epoch 1348\n",
      "Step 0: loss = 0.019688673317432404, recon_loss = 0.00632062740623951, 0.01336115412414074, kl_loss = 0.003445529378950596\n",
      "\n",
      "Epoch 1349\n",
      "Step 0: loss = 0.01894066110253334, recon_loss = 0.006864892318844795, 0.012068812735378742, kl_loss = 0.0034779002889990807\n",
      "\n",
      "Epoch 1350\n",
      "Step 0: loss = 0.021401982754468918, recon_loss = 0.006933685392141342, 0.014462972059845924, kl_loss = 0.002662437967956066\n",
      "\n",
      "Epoch 1351\n",
      "Step 0: loss = 0.016614705324172974, recon_loss = 0.0071242209523916245, 0.009486522525548935, kl_loss = 0.0019809864461421967\n",
      "\n",
      "Epoch 1352\n",
      "Step 0: loss = 0.020380273461341858, recon_loss = 0.0076363906264305115, 0.012728018686175346, kl_loss = 0.007932109758257866\n",
      "\n",
      "Epoch 1353\n",
      "Step 0: loss = 0.02180940844118595, recon_loss = 0.007773224264383316, 0.014010955579578876, kl_loss = 0.012615064159035683\n",
      "\n",
      "Epoch 1354\n",
      "Step 0: loss = 0.019539786502718925, recon_loss = 0.007643047720193863, 0.011883966624736786, kl_loss = 0.006385916844010353\n",
      "\n",
      "Epoch 1355\n",
      "Step 0: loss = 0.01454989891499281, recon_loss = 0.0067294891923666, 0.007811439223587513, kl_loss = 0.004485363140702248\n",
      "\n",
      "Epoch 1356\n",
      "Step 0: loss = 0.015628695487976074, recon_loss = 0.005978364497423172, 0.009646209888160229, kl_loss = 0.0020608771592378616\n",
      "\n",
      "Epoch 1357\n",
      "Step 0: loss = 0.016868239268660545, recon_loss = 0.006683574989438057, 0.010177494958043098, kl_loss = 0.0035843346267938614\n",
      "\n",
      "Epoch 1358\n",
      "Step 0: loss = 0.022073252126574516, recon_loss = 0.006514010950922966, 0.01555009838193655, kl_loss = 0.004571426659822464\n",
      "\n",
      "Epoch 1359\n",
      "Step 0: loss = 0.024557461962103844, recon_loss = 0.006847953423857689, 0.017698723822832108, kl_loss = 0.005392588675022125\n",
      "\n",
      "Epoch 1360\n",
      "Step 0: loss = 0.025309111922979355, recon_loss = 0.008370038121938705, 0.016923194751143456, kl_loss = 0.007939865812659264\n",
      "\n",
      "Epoch 1361\n",
      "Step 0: loss = 0.02187575027346611, recon_loss = 0.006913060322403908, 0.014949782751500607, kl_loss = 0.006454433314502239\n",
      "\n",
      "Epoch 1362\n",
      "Step 0: loss = 0.02304636500775814, recon_loss = 0.007928641512989998, 0.015110686421394348, kl_loss = 0.003518523648381233\n",
      "\n",
      "Epoch 1363\n",
      "Step 0: loss = 0.02340378426015377, recon_loss = 0.008584324270486832, 0.014809987507760525, kl_loss = 0.00473586842417717\n",
      "\n",
      "Epoch 1364\n",
      "Step 0: loss = 0.01960878260433674, recon_loss = 0.007791092619299889, 0.011809080839157104, kl_loss = 0.004304565489292145\n",
      "\n",
      "Epoch 1365\n",
      "Step 0: loss = 0.026995273306965828, recon_loss = 0.007474452257156372, 0.019514186307787895, kl_loss = 0.0033176979050040245\n",
      "\n",
      "Epoch 1366\n",
      "Step 0: loss = 0.021764136850833893, recon_loss = 0.007547220215201378, 0.014207417145371437, kl_loss = 0.004749649204313755\n",
      "\n",
      "Epoch 1367\n",
      "Step 0: loss = 0.026454757899045944, recon_loss = 0.007140345871448517, 0.019292911514639854, kl_loss = 0.010749831795692444\n",
      "\n",
      "Epoch 1368\n",
      "Step 0: loss = 0.020871497690677643, recon_loss = 0.007958313450217247, 0.01290206890553236, kl_loss = 0.005558013916015625\n",
      "\n",
      "Epoch 1369\n",
      "Step 0: loss = 0.01804414577782154, recon_loss = 0.005989750847220421, 0.012043364346027374, kl_loss = 0.005515478551387787\n",
      "\n",
      "Epoch 1370\n",
      "Step 0: loss = 0.019086794927716255, recon_loss = 0.006657058373093605, 0.012422148138284683, kl_loss = 0.0037939995527267456\n",
      "\n",
      "Epoch 1371\n",
      "Step 0: loss = 0.01935083419084549, recon_loss = 0.006019888445734978, 0.01332366093993187, kl_loss = 0.003642532043159008\n",
      "\n",
      "Epoch 1372\n",
      "Step 0: loss = 0.02235368639230728, recon_loss = 0.007443074136972427, 0.014903392642736435, kl_loss = 0.003609767183661461\n",
      "\n",
      "Epoch 1373\n",
      "Step 0: loss = 0.02094428427517414, recon_loss = 0.006677659228444099, 0.014259346760809422, kl_loss = 0.003638775087893009\n",
      "\n",
      "Epoch 1374\n",
      "Step 0: loss = 0.020504485815763474, recon_loss = 0.006822381168603897, 0.013675756752490997, kl_loss = 0.0031736884266138077\n",
      "\n",
      "Epoch 1375\n",
      "Step 0: loss = 0.0171144250780344, recon_loss = 0.0067835524678230286, 0.010326027870178223, kl_loss = 0.0024227099493145943\n",
      "\n",
      "Epoch 1376\n",
      "Step 0: loss = 0.0183052197098732, recon_loss = 0.006919067353010178, 0.011380927637219429, kl_loss = 0.002612648531794548\n",
      "\n",
      "Epoch 1377\n",
      "Step 0: loss = 0.02664867788553238, recon_loss = 0.006676992401480675, 0.019964220002293587, kl_loss = 0.003732520155608654\n",
      "\n",
      "Epoch 1378\n",
      "Step 0: loss = 0.025206351652741432, recon_loss = 0.00777936726808548, 0.017412740737199783, kl_loss = 0.007122100330889225\n",
      "\n",
      "Epoch 1379\n",
      "Step 0: loss = 0.02803543023765087, recon_loss = 0.007095012813806534, 0.020928973332047462, kl_loss = 0.005721726454794407\n",
      "\n",
      "Epoch 1380\n",
      "Step 0: loss = 0.01678525283932686, recon_loss = 0.00710584782063961, 0.009659802541136742, kl_loss = 0.009801624342799187\n",
      "\n",
      "Epoch 1381\n",
      "Step 0: loss = 0.022844934836030006, recon_loss = 0.007358398288488388, 0.015478739514946938, kl_loss = 0.0038983728736639023\n",
      "\n",
      "Epoch 1382\n",
      "Step 0: loss = 0.023975027725100517, recon_loss = 0.00792069360613823, 0.016040340065956116, kl_loss = 0.006996733136475086\n",
      "\n",
      "Epoch 1383\n",
      "Step 0: loss = 0.02396351844072342, recon_loss = 0.007745454087853432, 0.01620294712483883, kl_loss = 0.007558348588645458\n",
      "\n",
      "Epoch 1384\n",
      "Step 0: loss = 0.020335040986537933, recon_loss = 0.0072492677718400955, 0.013063625432550907, kl_loss = 0.011073155328631401\n",
      "\n",
      "Epoch 1385\n",
      "Step 0: loss = 0.02335519529879093, recon_loss = 0.00824086181819439, 0.015106357634067535, kl_loss = 0.003988285548985004\n",
      "\n",
      "Epoch 1386\n",
      "Step 0: loss = 0.025841744616627693, recon_loss = 0.007916180416941643, 0.017917785793542862, kl_loss = 0.003889092244207859\n",
      "\n",
      "Epoch 1387\n",
      "Step 0: loss = 0.023119665682315826, recon_loss = 0.006598271429538727, 0.016514811664819717, kl_loss = 0.0032911179587244987\n",
      "\n",
      "Epoch 1388\n",
      "Step 0: loss = 0.020696014165878296, recon_loss = 0.0069495756179094315, 0.013739212416112423, kl_loss = 0.0036137839779257774\n",
      "\n",
      "Epoch 1389\n",
      "Step 0: loss = 0.0242275670170784, recon_loss = 0.007602334022521973, 0.016617801040410995, kl_loss = 0.0037158019840717316\n",
      "\n",
      "Epoch 1390\n",
      "Step 0: loss = 0.01905575953423977, recon_loss = 0.007863994687795639, 0.011178062297403812, kl_loss = 0.006851400248706341\n",
      "\n",
      "Epoch 1391\n",
      "Step 0: loss = 0.022295372560620308, recon_loss = 0.007777348160743713, 0.014507211744785309, kl_loss = 0.005405924282968044\n",
      "\n",
      "Epoch 1392\n",
      "Step 0: loss = 0.02345537580549717, recon_loss = 0.0066498201340436935, 0.016793955117464066, kl_loss = 0.005800592713057995\n",
      "\n",
      "Epoch 1393\n",
      "Step 0: loss = 0.016232749447226524, recon_loss = 0.006718635559082031, 0.009504759684205055, kl_loss = 0.004676791839301586\n",
      "\n",
      "Epoch 1394\n",
      "Step 0: loss = 0.020335571840405464, recon_loss = 0.006545698270201683, 0.013785069808363914, kl_loss = 0.0024015242233872414\n",
      "\n",
      "Epoch 1395\n",
      "Step 0: loss = 0.01604868285357952, recon_loss = 0.006612127646803856, 0.00941738672554493, kl_loss = 0.009584372863173485\n",
      "\n",
      "Epoch 1396\n",
      "Step 0: loss = 0.022452836856245995, recon_loss = 0.007524378597736359, 0.01491520181298256, kl_loss = 0.006628002040088177\n",
      "\n",
      "Epoch 1397\n",
      "Step 0: loss = 0.019265765324234962, recon_loss = 0.006977614015340805, 0.012279655784368515, kl_loss = 0.004247346892952919\n",
      "\n",
      "Epoch 1398\n",
      "Step 0: loss = 0.023512788116931915, recon_loss = 0.006854236125946045, 0.016644638031721115, kl_loss = 0.00695707555860281\n",
      "\n",
      "Epoch 1399\n",
      "Step 0: loss = 0.019528847187757492, recon_loss = 0.007153693586587906, 0.012363705784082413, kl_loss = 0.005724083632230759\n",
      "\n",
      "Epoch 1400\n",
      "Step 0: loss = 0.025228580459952354, recon_loss = 0.007229644805192947, 0.017989765852689743, kl_loss = 0.004585042595863342\n",
      "\n",
      "Epoch 1401\n",
      "Step 0: loss = 0.016859162598848343, recon_loss = 0.00651315413415432, 0.01033703237771988, kl_loss = 0.004488068632781506\n",
      "\n",
      "Epoch 1402\n",
      "Step 0: loss = 0.01954624429345131, recon_loss = 0.006396317854523659, 0.013137958012521267, kl_loss = 0.005984742194414139\n",
      "\n",
      "Epoch 1403\n",
      "Step 0: loss = 0.02045818231999874, recon_loss = 0.0068056900054216385, 0.013647708110511303, kl_loss = 0.0023926561698317528\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2278638/1832901265.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_betas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dirs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_betas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2278638/603419946.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, inputs, dir_inputs, epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# outputs = model.decoder([z, dir_inputs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1108\u001b[0m               output_gradients))\n\u001b[1;32m   1109\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_MeanGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m   \u001b[0;34m\"\"\"Gradient for Mean.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m   \u001b[0msum_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SumGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m   \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   if (input_shape is not None and output_shape is not None and\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_0_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m           \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_0_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m           \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_0_shape\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# The shape and reduction indices are statically known, so we use a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# graph-level cache to avoid recomputing `reduced_shape()` for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, multiples, name)\u001b[0m\n\u001b[1;32m  11731\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11732\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11733\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11734\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11735\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11736\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11737\u001b[0m       _result = _dispatcher_for_tile(\n\u001b[1;32m  11738\u001b[0m           (input, multiples, name,), None)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs, i)\n",
    "    losses.append(loss_vals)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon_loss = {loss_vals[1].numpy()}, {loss_vals[2].numpy()}, kl_loss = {loss_vals[3].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"loss_vae_32_0.005\", np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHF0P4eISAH5"
   },
   "outputs": [],
   "source": [
    "vae.save_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JkrFRN7uOlf",
    "outputId": "90f21636-cdec-4d1e-eb4f-405a557a43b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7dfda7a63c70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS8sKCRfgUoH",
    "outputId": "b7f487ba-8083-4757-b9ea-250fd343a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  chem-checkpoint.zip\n",
      "   creating: my_checkpoint/\n",
      "  inflating: my_checkpoint/chekpont.index  \n",
      "  inflating: my_checkpoint/.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/checkpoint  \n",
      "  inflating: my_checkpoint/chekpont.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/.index    \n"
     ]
    }
   ],
   "source": [
    "# !zip -r my_checkpoint.zip my_checkpoint/\n",
    "!unzip chem-checkpoint.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EbHImoN2QaKX"
   },
   "outputs": [],
   "source": [
    "drawn_random_dirs = np.random.randn(50_000, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "\n",
    "def posterior_sampling(model, betas, random_dirs, num_samples=1):\n",
    "  zm, zlv, z = model.encoder((betas, random_dirs))\n",
    "  if num_samples == 1:\n",
    "      return model.decoder((z, random_dirs))\n",
    "  else:\n",
    "      samples = [sampling((zm, zlv)) for _ in range(num_samples)]\n",
    "      return tf.concat([model.decoder((sm, random_dirs))[:, None, :] for sm in samples], axis=1)\n",
    "\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs1 = np.random.randn(num_samples, d)\n",
    "  random_dirs2 = np.random.randn(num_samples, d)\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))    \n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [],
   "source": [
    "# drawn_betas, dir1, drawn_betas2 = generate_new_betas(vae, 50_000)\n",
    "# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\n",
    "drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\n",
    "# generate_new_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 10, 4081])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_X = (external_X-mu_x)/sigma_x\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 200\n",
    "X_sub, X_ids = project_and_filter(external_X, random_dirs[sample], 40)\n",
    "Y_sub = external_Y[X_ids]\n",
    "prd1 = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, X_ids = project_and_filter(external_X, random_dirs[sample], 40)\n",
    "Y_sub = external_Y[X_ids]\n",
    "# prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample][3])\n",
    "prd2 = np.mean(np.stack([softmax_prob(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample][ix]) for ix in range(10)], axis=0), axis=0) > 0.5\n",
    "# prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((375,), (375,))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prd1.shape, prd2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.99977815>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0], drawn_betas2[0]\n",
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas[:1], drawn_betas2[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([-0.00631047,  0.12106761,  0.46212596, ...,  0.47387064,\n",
       "        -0.281489  , -0.05550657], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check agreement between vae and training samples\n",
    "from sklearn.metrics import jaccard_score\n",
    "def agreement(y_pred1, y_pred2, y_true):\n",
    "    tp1 = np.float32(y_pred1==1) * np.float32(y_true==1)\n",
    "    fp1 = np.float32(y_pred1==1) * np.float32(y_true==0)\n",
    "    tn1 = np.float32(y_pred1==0) * np.float32(y_true==0)\n",
    "    fn1 = np.float32(y_pred1==0) * np.float32(y_true==1)\n",
    "\n",
    "    tp2 = np.float32(y_pred2==1) * np.float32(y_true==1)\n",
    "    fp2 = np.float32(y_pred2==1) * np.float32(y_true==0)\n",
    "    tn2 = np.float32(y_pred2==0) * np.float32(y_true==0)\n",
    "    fn2 = np.float32(y_pred2==0) * np.float32(y_true==1)\n",
    "    print(np.sum(tp1)/len(tp1), np.sum(fp1)/len(tp1), np.sum(tn1)/len(tp1), np.sum(fn1)/len(tp1))\n",
    "    print(np.sum(tp2)/len(tp1), np.sum(fp2)/len(tp1), np.sum(tn2)/len(tp1), np.sum(fn2)/len(tp1))\n",
    "    print(np.sum(tp1==tp2)/len(tp1), np.sum(tn1==tn2)/len(tp1), np.sum(fp1==fp2)/len(tp1), np.sum(fn1==fn2)/len(tp1))\n",
    "    print(np.sum(tp1), np.sum(tn1), np.sum(fp1), np.sum(fn1))\n",
    "    return jaccard_score(tp1, tp2), jaccard_score(fp1, fp2), jaccard_score(tn1, tn2), jaccard_score(fn1, fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26666666666666666 0.27466666666666667 0.26666666666666666 0.192\n",
      "0.2693333333333333 0.264 0.2773333333333333 0.18933333333333333\n",
      "0.992 0.968 0.968 0.992\n",
      "100.0 100.0 103.0 72.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9705882352941176, 0.8878504672897196, 0.8888888888888888, 0.958904109589041)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement(prd1, prd2, Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 10, 4081])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0054231044>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps= betas\n",
    "oups = drawn_betas\n",
    "tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(tf.expand_dims(inps, axis=1)[:, :, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(oups[:, :, 1:], axis=-1)[0], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.012491584>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.abs(tf.cast(tf.expand_dims(inps, axis=1)[:, :, :1], dtype=tf.float32)-oups[:, :, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([ 0.01558006,  0.08848727,  0.02163708, ...,  0.32644653,\n",
       "        -0.05755342,  0.21104604], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IHicI0RhbDe",
    "outputId": "32426975-705a-4668-f061-08acd65bc48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.99948734>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas, drawn_betas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOFI9xq2b7_3",
    "outputId": "a11e5724-0e2a-428d-a65a-28064209e6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4081), dtype=float32, numpy=\n",
       "array([[ 3.5679474e-02, -8.5116491e-02, -2.7868379e-02, ...,\n",
       "         3.8362685e-01,  6.8785306e-03,  5.8818167e-01],\n",
       "       [ 2.0887703e-02,  5.4998733e-02,  2.2995186e-01, ...,\n",
       "         1.0724969e-01,  3.8565136e-04,  3.8573799e-01],\n",
       "       [-6.5227631e-03,  2.0269346e-01,  4.0428180e-01, ...,\n",
       "        -4.8758507e-01, -8.2871839e-02,  3.4979448e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFCgGG6jgkLL",
    "outputId": "5e799c27-fff1-41fc-d71f-3fc7ea88538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13627878, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "var = tf.math.reduce_variance(drawn_betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxnzX5lJQtjX"
   },
   "outputs": [],
   "source": [
    "np.mean(drawn_betas1 @ tf.transpose(drawn_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "TfAej5fqsfHh",
    "outputId": "fd888e0d-c076-4577-be0d-4de29be77dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ood_val_features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_data_to_features\u001b[49m(ood_val_data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#ood_test_features = convert_data_to_features(ood_test_data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ood_val_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m ood_val_data])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "#ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U07oDyUdsl1u"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ood_val_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m external_X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mood_val_features\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m external_Y \u001b[38;5;241m=\u001b[39m ood_val_labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ood_val_features' is not defined"
     ]
    }
   ],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "external_Y = ood_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "external_X = (external_X-mu_x)/sigma_x\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "1a114f68-6f8b-4ede-b395-df1c2dfa2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      " -0.03134901]\n",
      "tf.Tensor(\n",
      "[-0.13125554  0.30866534  0.63319725 -0.5780234  -0.21648076 -0.3846287\n",
      "  0.99320394 -0.1799166   0.678208    0.62888056], shape=(10,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "5ec19308-4552-4326-ec47-b86470a2f7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 1024)\n",
      "(937, 2040)\n",
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "ae104348-bf03-48d8-a6c3-d3112b1dcb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " ...\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W32O3S_gKnEp",
    "outputId": "8797bdae-deb8-43ed-ef0c-cbf43fb4aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 512) (937, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred > 0.5), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 100, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  # wghts = np.ones_like(projs > thresh[:, None])\n",
    "  wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return *aggregate_preds(preds), preds\n",
    "\n",
    "\n",
    "# drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs, betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, dir1, drawn_betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas1) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, betas) # 0.634\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, drawn_betas) # 0.85\n",
    "# ext_preds, mp_rand, sp_rand, pred = get_preds_and_aggregate(external_randfeats_X, betas) # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOyhZXJBrhaj",
    "outputId": "98cc9eb0-2e98-4b38-e823-337411f61e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0], tf.transpose(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "a9702828-7bea-436e-de46-89ca03c30b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.767455  ,  0.1001012 , -0.54182774, ..., -6.8707843 ,\n",
       "         7.257238  , -0.5050444 ], dtype=float32),\n",
       " array([-0.0186294 ,  0.06830448, -0.27256313, ..., -0.72351612,\n",
       "         0.81710885,  0.05805234]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "2fac824c-8637-4a35-8dfb-7b60045b10b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mext_preds\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ext_preds' is not defined"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "06eb05c6-f5f0-4d58-a8a2-ad76ee40b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Total Positive Preds:  435.0\n",
      "Total Preds:  937\n",
      "% Positive Preds:  0.464247598719317\n",
      "\n",
      "First 10 Ground Truth:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Total Positive Ground Truth:  421.0\n",
      "Total Ground Truth:  937\n",
      "% Positive Ground Truth:  0.44930629669156885\n",
      "\n",
      "Accuracy:  0.6328708644610459\n"
     ]
    }
   ],
   "source": [
    "# testing_Y = Y\n",
    "# ext_preds = ext_probs > 0.5\n",
    "testing_Y = external_Y\n",
    "\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", testing_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(testing_Y))\n",
    "print(\"Total Ground Truth: \", len(testing_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(testing_Y) / len(testing_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == testing_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "L7kl6J3wLped",
    "outputId": "a5d20ac1-f339-47e4-cf12-18073bb4b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9a014d0220>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.2, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8571428571428571, 0.8536585365853658, 0.7857142857142857, 0.81, 0.7898550724637681, 0.7909604519774012, 0.7813953488372093, 0.7649402390438247, 0.738831615120275, 0.7284345047923323, 0.7308781869688386, 0.7222222222222222, 0.7126696832579186, 0.7034764826175869, 0.6950998185117967, 0.6829268292682927, 0.6651917404129793, 0.6557591623036649, 0.6211312700106724]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbc16418e80>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_probs)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
