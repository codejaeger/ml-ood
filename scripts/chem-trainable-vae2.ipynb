{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:44:32.242265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 19:44:33.243723: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-29 19:44:33.243847: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-04-29 19:44:33.243861: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "20637ffb-f819-4f0a-b926-d24784746a9e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkF1N4olXTtZ",
    "outputId": "de7467ba-69c5-42b4-a918-755fe87765d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "weKR7jCuMvQl"
   },
   "outputs": [],
   "source": [
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataX = np.float32(np.array([line.strip().split(',')[2:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/train.csv', 'r') as f:\n",
    "  dataY = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "X = dataX\n",
    "Y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('./chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "2948628d-0085-48ba-8e9b-4bbe93f27e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5997, 1024)\n",
      "(5997,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, d, D=128):\n",
    "\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3S8unT73bEtM"
   },
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print((prd[:, 0]==Y).mean())\n",
    "  return wgts\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    beta = linear_coefs(get_rand_feats(X_sub@pca_projs, model), Y_sub)\n",
    "    # print(beta.shape)\n",
    "      \n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:45:00.617524: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-29 19:45:00.764184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:0a:00.0, compute capability: 7.5\n",
      "2024-04-29 19:45:00.967746: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x8a0dfe0\n"
     ]
    }
   ],
   "source": [
    "s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(232.08453, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [0.05, 0.2, 0.3, 0.4, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5997, 1024), TensorShape([1024, 204]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, pca_projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "f3d09ad5-4e02-44a5-e001-fe83e3d99938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m random_dirs \u001b[38;5;241m=\u001b[39m random_dirs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(random_dirs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m betas \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_projs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m, in \u001b[0;36mget_models\u001b[0;34m(X, Y, pca_projs, dirs, model, percentile)\u001b[0m\n\u001b[1;32m     10\u001b[0m Y_sub \u001b[38;5;241m=\u001b[39m Y[X_ids]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# print((X_sub@pca_projs).shape)\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_coefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_rand_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;129;43m@pca_projs\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(beta.shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m   \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#X_subsets.append(X_sub)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#data_ids.append(X_ids)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Y_subsets.append(Y_sub)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m betas\u001b[38;5;241m.\u001b[39mappend(beta)\n",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m, in \u001b[0;36mlinear_coefs\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlinear_coefs\u001b[39m(X, Y):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    X: N x d matrix of input features\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Beta: d x 1 matrix of linear coefficients\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m   clf \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28mprint\u001b[39m(clf\u001b[38;5;241m.\u001b[39mscore(X, Y))\n\u001b[1;32m     12\u001b[0m   wgts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((clf\u001b[38;5;241m.\u001b[39mintercept_[:,\u001b[38;5;28;01mNone\u001b[39;00m], clf\u001b[38;5;241m.\u001b[39mcoef_))\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1354\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1351\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1352\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1353\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)))\n\u001b[0;32m-> 1354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([n_iter_])\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:964\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m    960\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X,\n\u001b[1;32m    961\u001b[0m                                      dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    963\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m--> 964\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misspmatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m    972\u001b[0m n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "\n",
    "N = 2**9    # ~ 8k\n",
    "# N = 2**2    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas = get_models(X, Y, pca_projs, random_dirs, model, percentile=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "# np.save('random_dirs-chem2.npy', random_dirs)\n",
    "# np.save('betas-chem2.npy', betas)\n",
    "# np.save('Ws-chem2.npy', model.Ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "CnxEkcWwWlwn"
   },
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-chem2.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./betas-chem2.npy')))\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "model.Ws = tf.constant(np.load('./Ws-chem2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "BCHvO4qeupNQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mrandom_dirs1\u001b[49m, random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.allclose(random_dirs1, random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mrandom_dirs1\u001b[49m \u001b[38;5;241m-\u001b[39m random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(random_dirs1 - random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "264b9e19-74f8-4d13-ee50-bb4796f0d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 4081)\n",
      "(512, 1024)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "15119544-cde9-4462-b565-7deb6dd61daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.01111806548776194, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "b44e65f3-b44b-40fd-e564-63e35d43997c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9958333333333333"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "def softmax_prob(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) ) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0NZNorQW0vg",
    "outputId": "afcb4822-0cda-4c13-ea68-8454473bb529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6204268292682927"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n",
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[sample], 70)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "perds = np.zeros((ex_X.shape[0],))\n",
    "cnt = np.zeros((ex_X.shape[0],))\n",
    "print(perds.shape)\n",
    "for sm in range(512):\n",
    "    X_sub, X_ids = project_and_filter(ex_X, random_dirs[sm], 10)\n",
    "    Y_sub = ex_Y[X_ids]\n",
    "    prd = softmax_prob(get_rand_feats(X_sub@pca_projs, model), betas[sm])\n",
    "    # print(prd.shape, X_ids, ex_X.shape)\n",
    "    perds[X_ids] += prd\n",
    "    cnt[X_ids] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5036899112150593"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zer_ids = np.argwhere(cnt!=0)\n",
    "veX = perds[zer_ids]\n",
    "veX /= cnt[zer_ids]\n",
    "((veX>0.5)==Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Pack_N_512_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [843] != values[12].shape = [844] [Op:Pack] name: stack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m((np\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m==\u001b[39mY_sub)\u001b[38;5;241m.\u001b[39mmean())\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Pack_N_512_device_/job:localhost/replica:0/task:0/device:GPU:0}} Shapes of all inputs must match: values[0].shape = [843] != values[12].shape = [844] [Op:Pack] name: stack"
     ]
    }
   ],
   "source": [
    "print((np.mean(tf.stack(perds, axis=0), axis=0)==Y_sub).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "beta_input = layers.Input(shape=(beta_dim,))\n",
    "beta_x = layers.Dense(512, activation=tf.nn.elu)(beta_input)\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([beta_x, dir_input])\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(8, activation=tf.nn.elu)(x)\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# x = layers.Dense(8, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([beta_input, dir_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs, decoder_dir_input], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([beta_input, dir_input])[2], dir_input])\n",
    "vae = models.Model([beta_input, dir_input], outputs, name=\"vae\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = vae((betas[:1], random_dirs[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, z_mean, z_log_var, reg=0.02):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  intercp_loss = tf.reduce_mean(tf.abs(tf.cast(inputs[:, :1], dtype=tf.float32) - outputs[:, :1]))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs[:, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs[:, 1:], axis=-1)[0], axis=-1))\n",
    "  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = recon_loss + intercp_loss + reg * kl_loss\n",
    "  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.5716181>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0056375>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.016130272>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=27.492508>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_loss(betas[:1], ots[:1], tf.ones((1, 32)), tf.ones((1, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# def regul(epoch):\n",
    "#     if epoch%10<=5:\n",
    "#         return epoch*0.2\n",
    "\n",
    "def train_step(model, inputs, dir_inputs, epoch=None):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z_mean, z_log_var, z = model.encoder([inputs, dir_inputs])\n",
    "    # outputs = model.decoder([z, dir_inputs])\n",
    "    outputs = model.decoder([z, dir_inputs])\n",
    "    \n",
    "    total_loss, recon_loss, intercp_loss, kl_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0652798>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0007908>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.04532135>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.9583828>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(vae, betas[:32], random_dirs[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(betas, dirs, batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield tf.constant(betas[i:i+batch_size]), tf.constant(dirs[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch at 0x7feeac11f6d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch(betas, random_dirs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "a6e1ccc1-2cd0-4549-e61a-6dae8aa1062a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 1.2715390920639038, recon_loss = 1.0202349424362183, 0.05596787855029106, kl_loss = 9.76681137084961\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.5499029159545898, recon_loss = 0.4177165627479553, 0.03750542923808098, kl_loss = 4.734047889709473\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.5920256972312927, recon_loss = 0.5037596225738525, 0.04003147780895233, kl_loss = 2.411728858947754\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.48603788018226624, recon_loss = 0.4304544925689697, 0.024667274206876755, kl_loss = 1.5458062887191772\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.45737770199775696, recon_loss = 0.40820738673210144, 0.02475924789905548, kl_loss = 1.220552682876587\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.43935710191726685, recon_loss = 0.402736634016037, 0.01383404340595007, kl_loss = 1.1393215656280518\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.42661741375923157, recon_loss = 0.4049603343009949, 0.01445953268557787, kl_loss = 0.3598778247833252\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.420422226190567, recon_loss = 0.40147632360458374, 0.016054682433605194, kl_loss = 0.144561305642128\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.41682690382003784, recon_loss = 0.40365976095199585, 0.011023741215467453, kl_loss = 0.10717051476240158\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.41352900862693787, recon_loss = 0.40045619010925293, 0.01151935663074255, kl_loss = 0.07767358422279358\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.40849751234054565, recon_loss = 0.40018486976623535, 0.0068845609202980995, kl_loss = 0.0714038833975792\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.4188769459724426, recon_loss = 0.40463197231292725, 0.01280885562300682, kl_loss = 0.07180634140968323\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.4153831899166107, recon_loss = 0.40028324723243713, 0.01346190832555294, kl_loss = 0.08190086483955383\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.4159278869628906, recon_loss = 0.39692050218582153, 0.016122231259942055, kl_loss = 0.14425858855247498\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.4053516089916229, recon_loss = 0.3913576006889343, 0.010889158584177494, kl_loss = 0.15524180233478546\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.4108586311340332, recon_loss = 0.3937233090400696, 0.013280509039759636, kl_loss = 0.1927412450313568\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.40448102355003357, recon_loss = 0.3906494975090027, 0.009386219084262848, kl_loss = 0.2222656011581421\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.4003452658653259, recon_loss = 0.38379570841789246, 0.011740206740796566, kl_loss = 0.24046805500984192\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.4056667387485504, recon_loss = 0.39072203636169434, 0.009260343387722969, kl_loss = 0.28421786427497864\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.39084622263908386, recon_loss = 0.37824976444244385, 0.008749542757868767, kl_loss = 0.19234511256217957\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.3942820131778717, recon_loss = 0.3816349506378174, 0.009574427269399166, kl_loss = 0.15363243222236633\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.3832913339138031, recon_loss = 0.3723905086517334, 0.008592627011239529, kl_loss = 0.11541012674570084\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.37833359837532043, recon_loss = 0.36774420738220215, 0.008524265140295029, kl_loss = 0.10325546562671661\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.37492939829826355, recon_loss = 0.36483269929885864, 0.008768372237682343, kl_loss = 0.06641598045825958\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.36845993995666504, recon_loss = 0.3566584289073944, 0.010653791017830372, kl_loss = 0.0573866106569767\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.3751962184906006, recon_loss = 0.36831992864608765, 0.006096744444221258, kl_loss = 0.03897639364004135\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.3526252210140228, recon_loss = 0.3442767858505249, 0.007690933533012867, kl_loss = 0.03287458047270775\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.36833006143569946, recon_loss = 0.36130860447883606, 0.0061982073821127415, kl_loss = 0.04116355627775192\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.3614780306816101, recon_loss = 0.3547224998474121, 0.006024317815899849, kl_loss = 0.03656074404716492\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.3561190962791443, recon_loss = 0.3509409427642822, 0.004520747810602188, kl_loss = 0.03287062048912048\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.3522273302078247, recon_loss = 0.3421330153942108, 0.00959017500281334, kl_loss = 0.02520664408802986\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.33950290083885193, recon_loss = 0.33368074893951416, 0.005245535168796778, kl_loss = 0.028831135481595993\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.3540988862514496, recon_loss = 0.3477283716201782, 0.005942093208432198, kl_loss = 0.02141984924674034\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.3422250747680664, recon_loss = 0.33625125885009766, 0.0055544497445225716, kl_loss = 0.020969104021787643\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.32548102736473083, recon_loss = 0.3196154534816742, 0.005518713966012001, kl_loss = 0.01734359748661518\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.3261682093143463, recon_loss = 0.3197072744369507, 0.0061026159673929214, kl_loss = 0.017915409058332443\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.31279727816581726, recon_loss = 0.3076966106891632, 0.004763010889291763, kl_loss = 0.016883568838238716\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.31831666827201843, recon_loss = 0.3127673268318176, 0.00526913907378912, kl_loss = 0.01400995347648859\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.31101271510124207, recon_loss = 0.3049599230289459, 0.005777864716947079, kl_loss = 0.013746066950261593\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.30856260657310486, recon_loss = 0.3039475679397583, 0.00438842223957181, kl_loss = 0.01133033074438572\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.30943208932876587, recon_loss = 0.30511632561683655, 0.004043661989271641, kl_loss = 0.0136046651750803\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.30244123935699463, recon_loss = 0.2975417375564575, 0.004615426994860172, kl_loss = 0.014203919097781181\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.2959500849246979, recon_loss = 0.2915247976779938, 0.004204259719699621, kl_loss = 0.01105121336877346\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.29823413491249084, recon_loss = 0.2935086488723755, 0.0045113228261470795, kl_loss = 0.010707405395805836\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.2786948084831238, recon_loss = 0.27449485659599304, 0.0038978704251348972, kl_loss = 0.015103267505764961\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.2855909764766693, recon_loss = 0.2815074324607849, 0.003843447659164667, kl_loss = 0.012004747055470943\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.2802862524986267, recon_loss = 0.2764228582382202, 0.0036765076220035553, kl_loss = 0.009344528429210186\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.2815415859222412, recon_loss = 0.2775280177593231, 0.003849491011351347, kl_loss = 0.008204633370041847\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.26839736104011536, recon_loss = 0.2650074362754822, 0.003233114490285516, kl_loss = 0.007840855047106743\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.27539175748825073, recon_loss = 0.271872878074646, 0.003384375013411045, kl_loss = 0.006725446321070194\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.2659044861793518, recon_loss = 0.2625274956226349, 0.0031816756818443537, kl_loss = 0.00976556446403265\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.27503374218940735, recon_loss = 0.2707127332687378, 0.004171375185251236, kl_loss = 0.007482342422008514\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.26413530111312866, recon_loss = 0.2612230181694031, 0.002800620160996914, kl_loss = 0.005583117716014385\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.26978418231010437, recon_loss = 0.2655981481075287, 0.004078144207596779, kl_loss = 0.00539491418749094\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.2664628326892853, recon_loss = 0.2623250186443329, 0.004025743808597326, kl_loss = 0.0056028468534350395\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.26038703322410583, recon_loss = 0.2572665214538574, 0.003020846750587225, kl_loss = 0.004982479847967625\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.26996442675590515, recon_loss = 0.26647600531578064, 0.003387368516996503, kl_loss = 0.005052441731095314\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.25355616211891174, recon_loss = 0.25100624561309814, 0.0024452952202409506, kl_loss = 0.005232177674770355\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.25733426213264465, recon_loss = 0.2540181875228882, 0.003231574082747102, kl_loss = 0.0042237648740410805\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.25582680106163025, recon_loss = 0.25267112255096436, 0.0030599150341004133, kl_loss = 0.004788395017385483\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.25251129269599915, recon_loss = 0.24969258904457092, 0.0027385400608181953, kl_loss = 0.004007692448794842\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.24998095631599426, recon_loss = 0.24729667603969574, 0.0025860671885311604, kl_loss = 0.004910922609269619\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.2482733130455017, recon_loss = 0.24453717470169067, 0.0036529111675918102, kl_loss = 0.004161356948316097\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.2486991435289383, recon_loss = 0.24568548798561096, 0.002947887871414423, kl_loss = 0.00328852329403162\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.25063344836235046, recon_loss = 0.2477453500032425, 0.002818866167217493, kl_loss = 0.0034618396311998367\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.2427564561367035, recon_loss = 0.24000045657157898, 0.0026674948167055845, kl_loss = 0.004424752667546272\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.2452917844057083, recon_loss = 0.2428237348794937, 0.002399555640295148, kl_loss = 0.0034249527379870415\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.2478209286928177, recon_loss = 0.24547351896762848, 0.002288057468831539, kl_loss = 0.002967679873108864\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.24134257435798645, recon_loss = 0.23863643407821655, 0.0026430017314851284, kl_loss = 0.003156641498208046\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.24088875949382782, recon_loss = 0.23785006999969482, 0.0029743416234850883, kl_loss = 0.003216993063688278\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.2401825338602066, recon_loss = 0.23763823509216309, 0.0024831811897456646, kl_loss = 0.0030564935877919197\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.24116583168506622, recon_loss = 0.23866555094718933, 0.002444211859256029, kl_loss = 0.002803407609462738\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.2398352324962616, recon_loss = 0.2375728189945221, 0.002209245692938566, kl_loss = 0.002658161334693432\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.231155663728714, recon_loss = 0.22927385568618774, 0.0018255411414429545, kl_loss = 0.002813424915075302\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.23601944744586945, recon_loss = 0.23396942019462585, 0.002004976850003004, kl_loss = 0.0022519780322909355\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.23775136470794678, recon_loss = 0.2359066605567932, 0.001794965472072363, kl_loss = 0.002486967481672764\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.2350524365901947, recon_loss = 0.23206007480621338, 0.0029473458416759968, kl_loss = 0.0022511333227157593\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.2345629632472992, recon_loss = 0.23207053542137146, 0.0024470812641084194, kl_loss = 0.002267579548060894\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.23223896324634552, recon_loss = 0.22994495928287506, 0.002253263955935836, kl_loss = 0.0020366311073303223\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.23076294362545013, recon_loss = 0.228231742978096, 0.0024983282200992107, kl_loss = 0.001643480733036995\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.22853241860866547, recon_loss = 0.2256171703338623, 0.0028666709549725056, kl_loss = 0.002429167740046978\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.22889047861099243, recon_loss = 0.2257913500070572, 0.003059015143662691, kl_loss = 0.00200576800853014\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.22831030189990997, recon_loss = 0.22566138207912445, 0.0025975946336984634, kl_loss = 0.0025656260550022125\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.22819028794765472, recon_loss = 0.22635343670845032, 0.0017930882750079036, kl_loss = 0.0021884115412831306\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.22981414198875427, recon_loss = 0.2269296944141388, 0.002833261853083968, kl_loss = 0.002559264190495014\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.22696883976459503, recon_loss = 0.22518353164196014, 0.0017455816268920898, kl_loss = 0.0019865790382027626\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.2278738021850586, recon_loss = 0.22604690492153168, 0.001790716778486967, kl_loss = 0.0018086796626448631\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.2284742295742035, recon_loss = 0.22603201866149902, 0.0024118609726428986, kl_loss = 0.0015174346044659615\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.22502213716506958, recon_loss = 0.22276262938976288, 0.0022285757586359978, kl_loss = 0.0015466902405023575\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.2237904667854309, recon_loss = 0.22188574075698853, 0.001873913686722517, kl_loss = 0.0015410883352160454\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.2239391952753067, recon_loss = 0.22202014923095703, 0.0018914598040282726, kl_loss = 0.0013787727802991867\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.22174079716205597, recon_loss = 0.21950387954711914, 0.002209988422691822, kl_loss = 0.001346411183476448\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.22492210566997528, recon_loss = 0.2225061058998108, 0.0023913346230983734, kl_loss = 0.001232776790857315\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.22518441081047058, recon_loss = 0.22329168021678925, 0.0018649082630872726, kl_loss = 0.0013910755515098572\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.2238101363182068, recon_loss = 0.22210238873958588, 0.001677114050835371, kl_loss = 0.0015318244695663452\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.22432982921600342, recon_loss = 0.22235429286956787, 0.0019469821127131581, kl_loss = 0.0014272350817918777\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.2197342962026596, recon_loss = 0.21789707243442535, 0.001803114078938961, kl_loss = 0.0017056092619895935\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.21768507361412048, recon_loss = 0.2160065770149231, 0.0016476931050419807, kl_loss = 0.001539972610771656\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.2222973257303238, recon_loss = 0.22003185749053955, 0.0022305860184133053, kl_loss = 0.0017441688105463982\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.21602216362953186, recon_loss = 0.2143552303314209, 0.0016418781597167253, kl_loss = 0.00125226192176342\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.2218627780675888, recon_loss = 0.22001183032989502, 0.0018010083585977554, kl_loss = 0.002497022971510887\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.21495197713375092, recon_loss = 0.21325691044330597, 0.0016650785692036152, kl_loss = 0.0014993678778409958\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.2189137488603592, recon_loss = 0.21702325344085693, 0.0018672406440600753, kl_loss = 0.0011630980297923088\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.2169814556837082, recon_loss = 0.21535778045654297, 0.001598157687112689, kl_loss = 0.0012757685035467148\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.21690119802951813, recon_loss = 0.2149224430322647, 0.0019524763338267803, kl_loss = 0.0013142768293619156\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.2185385674238205, recon_loss = 0.21618352830410004, 0.0023296806029975414, kl_loss = 0.0012681325897574425\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.2172190397977829, recon_loss = 0.2149425745010376, 0.0022532620932906866, kl_loss = 0.001160215586423874\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.21350964903831482, recon_loss = 0.21184760332107544, 0.0016427573282271624, kl_loss = 0.0009641917422413826\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.21780917048454285, recon_loss = 0.21606740355491638, 0.0017240878660231829, kl_loss = 0.000883420929312706\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.21351198852062225, recon_loss = 0.2119760662317276, 0.0015147882513701916, kl_loss = 0.0010564317926764488\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.2138173133134842, recon_loss = 0.2124040573835373, 0.001395425875671208, kl_loss = 0.0008921073749661446\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.21695704758167267, recon_loss = 0.21535445749759674, 0.001584677491337061, kl_loss = 0.0008952757343649864\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.21554945409297943, recon_loss = 0.2134772539138794, 0.0020522652193903923, kl_loss = 0.0009970683604478836\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.2133205384016037, recon_loss = 0.21174868941307068, 0.0015543977497145534, kl_loss = 0.000872819684445858\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.21148955821990967, recon_loss = 0.20990139245986938, 0.0015649747801944613, kl_loss = 0.0011595319956541061\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.21251915395259857, recon_loss = 0.21089552342891693, 0.0016030704136937857, kl_loss = 0.0010282117873430252\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.21548129618167877, recon_loss = 0.2136145532131195, 0.0018394245998933911, kl_loss = 0.0013659540563821793\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.21073150634765625, recon_loss = 0.20873431861400604, 0.0019680955447256565, kl_loss = 0.001454479992389679\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.21212390065193176, recon_loss = 0.20970305800437927, 0.00239853304810822, kl_loss = 0.0011150427162647247\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.211835578083992, recon_loss = 0.21021658182144165, 0.0015992105472832918, kl_loss = 0.000989684835076332\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.2066304236650467, recon_loss = 0.20509646832942963, 0.0015166299417614937, kl_loss = 0.0008663274347782135\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.21209777891635895, recon_loss = 0.21035514771938324, 0.0017247312935069203, kl_loss = 0.0008946675807237625\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.2108016163110733, recon_loss = 0.2087322473526001, 0.0020448199938982725, kl_loss = 0.0012268424034118652\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.20689436793327332, recon_loss = 0.2055489718914032, 0.0013177270302549005, kl_loss = 0.0013834722340106964\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.20842257142066956, recon_loss = 0.20714890956878662, 0.0012509406078606844, kl_loss = 0.0011362796649336815\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.20955030620098114, recon_loss = 0.20803514122962952, 0.0014873233158141375, kl_loss = 0.0013917358592152596\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.21020933985710144, recon_loss = 0.20848238468170166, 0.0016954843886196613, kl_loss = 0.001573784276843071\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.2123231142759323, recon_loss = 0.21040402352809906, 0.0019007192458957434, kl_loss = 0.0009187208488583565\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.21212507784366608, recon_loss = 0.20983052253723145, 0.002272526966407895, kl_loss = 0.001101447269320488\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.20827314257621765, recon_loss = 0.20659798383712769, 0.00165236322209239, kl_loss = 0.0011399909853935242\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.2060745358467102, recon_loss = 0.2046894133090973, 0.0013686235761269927, kl_loss = 0.0008250009268522263\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.20718815922737122, recon_loss = 0.20545139908790588, 0.0017192240338772535, kl_loss = 0.0008772034198045731\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.2041853815317154, recon_loss = 0.20183658599853516, 0.0023316850420087576, kl_loss = 0.0008555594831705093\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.20815587043762207, recon_loss = 0.20603883266448975, 0.002094586845487356, kl_loss = 0.0011231452226638794\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.20033317804336548, recon_loss = 0.19871927797794342, 0.0015844200970605016, kl_loss = 0.00147349014878273\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.20945081114768982, recon_loss = 0.20778551697731018, 0.0016450471011921763, kl_loss = 0.0010126959532499313\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.20835919678211212, recon_loss = 0.20695644617080688, 0.0013820210006088018, kl_loss = 0.0010366709902882576\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.20263934135437012, recon_loss = 0.2011469602584839, 0.0014639119617640972, kl_loss = 0.0014239251613616943\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.2065018266439438, recon_loss = 0.20468425750732422, 0.0017876941710710526, kl_loss = 0.0014939093962311745\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.20481717586517334, recon_loss = 0.2035081386566162, 0.0012923195026814938, kl_loss = 0.000836225226521492\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.20531833171844482, recon_loss = 0.20354875922203064, 0.0017541968263685703, kl_loss = 0.0007688971236348152\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.20641767978668213, recon_loss = 0.20435476303100586, 0.0020443033427000046, kl_loss = 0.0009305831044912338\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.20104697346687317, recon_loss = 0.19933849573135376, 0.0016890631522983313, kl_loss = 0.0009709727019071579\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.204167902469635, recon_loss = 0.20264548063278198, 0.00150071841198951, kl_loss = 0.0010844571515917778\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.20763953030109406, recon_loss = 0.2060844600200653, 0.0015035145916044712, kl_loss = 0.002578040584921837\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.20531629025936127, recon_loss = 0.2036382406949997, 0.0016542843077331781, kl_loss = 0.0011886367574334145\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.20438487827777863, recon_loss = 0.20237678289413452, 0.0019742692820727825, kl_loss = 0.0016915583983063698\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.20701076090335846, recon_loss = 0.2050405740737915, 0.0019403771730139852, kl_loss = 0.0014898888766765594\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.2037007063627243, recon_loss = 0.20209185779094696, 0.0015805112197995186, kl_loss = 0.001416909508407116\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.20437608659267426, recon_loss = 0.2029009759426117, 0.0014447423163801432, kl_loss = 0.0015182392671704292\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.2051703780889511, recon_loss = 0.20329821109771729, 0.0018438436090946198, kl_loss = 0.0014163888990879059\n",
      "\n",
      "Epoch 151\n",
      "Step 0: loss = 0.2032179981470108, recon_loss = 0.2016901671886444, 0.0014685285277664661, kl_loss = 0.0029655229300260544\n",
      "\n",
      "Epoch 152\n",
      "Step 0: loss = 0.20565587282180786, recon_loss = 0.20450058579444885, 0.0011241722386330366, kl_loss = 0.00155552476644516\n",
      "\n",
      "Epoch 153\n",
      "Step 0: loss = 0.2073913812637329, recon_loss = 0.20621518790721893, 0.0011558753903955221, kl_loss = 0.0010161474347114563\n",
      "\n",
      "Epoch 154\n",
      "Step 0: loss = 0.20395174622535706, recon_loss = 0.20204485952854156, 0.0018834004877135158, kl_loss = 0.0011740699410438538\n",
      "\n",
      "Epoch 155\n",
      "Step 0: loss = 0.2046341896057129, recon_loss = 0.20307207107543945, 0.0015266877599060535, kl_loss = 0.0017715329304337502\n",
      "\n",
      "Epoch 156\n",
      "Step 0: loss = 0.20769821107387543, recon_loss = 0.2058870792388916, 0.0017899398226290941, kl_loss = 0.001059671863913536\n",
      "\n",
      "Epoch 157\n",
      "Step 0: loss = 0.20275114476680756, recon_loss = 0.20116716623306274, 0.0015607182867825031, kl_loss = 0.001163099892437458\n",
      "\n",
      "Epoch 158\n",
      "Step 0: loss = 0.20237968862056732, recon_loss = 0.2008424997329712, 0.0015216449974104762, kl_loss = 0.0007771952077746391\n",
      "\n",
      "Epoch 159\n",
      "Step 0: loss = 0.20405080914497375, recon_loss = 0.20233261585235596, 0.0017034339252859354, kl_loss = 0.0007373113185167313\n",
      "\n",
      "Epoch 160\n",
      "Step 0: loss = 0.2045246958732605, recon_loss = 0.2025582194328308, 0.0019476754823699594, kl_loss = 0.0009402288123965263\n",
      "\n",
      "Epoch 161\n",
      "Step 0: loss = 0.20179174840450287, recon_loss = 0.2000722736120224, 0.0016880705952644348, kl_loss = 0.0015699155628681183\n",
      "\n",
      "Epoch 162\n",
      "Step 0: loss = 0.2032168060541153, recon_loss = 0.20160940289497375, 0.001582621829584241, kl_loss = 0.0012388061732053757\n",
      "\n",
      "Epoch 163\n",
      "Step 0: loss = 0.20536769926548004, recon_loss = 0.20355801284313202, 0.0017951992340385914, kl_loss = 0.0007241629064083099\n",
      "\n",
      "Epoch 164\n",
      "Step 0: loss = 0.20345538854599, recon_loss = 0.20171606540679932, 0.0017238506115972996, kl_loss = 0.0007731262594461441\n",
      "\n",
      "Epoch 165\n",
      "Step 0: loss = 0.19991733133792877, recon_loss = 0.19830955564975739, 0.001585015095770359, kl_loss = 0.0011380361393094063\n",
      "\n",
      "Epoch 166\n",
      "Step 0: loss = 0.2073996365070343, recon_loss = 0.20586451888084412, 0.0015231373254209757, kl_loss = 0.0005990155041217804\n",
      "\n",
      "Epoch 167\n",
      "Step 0: loss = 0.20540165901184082, recon_loss = 0.20388559997081757, 0.0014902092516422272, kl_loss = 0.0012925630435347557\n",
      "\n",
      "Epoch 168\n",
      "Step 0: loss = 0.2027011513710022, recon_loss = 0.20070114731788635, 0.0019812355749309063, kl_loss = 0.00093843974173069\n",
      "\n",
      "Epoch 169\n",
      "Step 0: loss = 0.20346713066101074, recon_loss = 0.20163118839263916, 0.001815437339246273, kl_loss = 0.001024843193590641\n",
      "\n",
      "Epoch 170\n",
      "Step 0: loss = 0.20318053662776947, recon_loss = 0.20092715322971344, 0.002240316476672888, kl_loss = 0.0006536524742841721\n",
      "\n",
      "Epoch 171\n",
      "Step 0: loss = 0.2032279670238495, recon_loss = 0.2013886570930481, 0.0018195512238889933, kl_loss = 0.0009882273152470589\n",
      "\n",
      "Epoch 172\n",
      "Step 0: loss = 0.2011951357126236, recon_loss = 0.19931238889694214, 0.0018676167819648981, kl_loss = 0.0007562944665551186\n",
      "\n",
      "Epoch 173\n",
      "Step 0: loss = 0.2024809569120407, recon_loss = 0.20110984146595, 0.0013503298396244645, kl_loss = 0.001039513386785984\n",
      "\n",
      "Epoch 174\n",
      "Step 0: loss = 0.2020363211631775, recon_loss = 0.20049495995044708, 0.001523089362308383, kl_loss = 0.0009134653955698013\n",
      "\n",
      "Epoch 175\n",
      "Step 0: loss = 0.2025216519832611, recon_loss = 0.20084445178508759, 0.0016525790560990572, kl_loss = 0.0012309495359659195\n",
      "\n",
      "Epoch 176\n",
      "Step 0: loss = 0.20491638779640198, recon_loss = 0.20316308736801147, 0.0017192335799336433, kl_loss = 0.0017035342752933502\n",
      "\n",
      "Epoch 177\n",
      "Step 0: loss = 0.20408181846141815, recon_loss = 0.20259690284729004, 0.0014519477263092995, kl_loss = 0.0016478579491376877\n",
      "\n",
      "Epoch 178\n",
      "Step 0: loss = 0.2025776356458664, recon_loss = 0.20080572366714478, 0.001745379064232111, kl_loss = 0.00132701825350523\n",
      "\n",
      "Epoch 179\n",
      "Step 0: loss = 0.2032695710659027, recon_loss = 0.2008923590183258, 0.0023334939032793045, kl_loss = 0.0021863188594579697\n",
      "\n",
      "Epoch 180\n",
      "Step 0: loss = 0.20366069674491882, recon_loss = 0.20153066515922546, 0.0020810780115425587, kl_loss = 0.002447303384542465\n",
      "\n",
      "Epoch 181\n",
      "Step 0: loss = 0.2028719186782837, recon_loss = 0.20171113312244415, 0.0011184753384441137, kl_loss = 0.0021155141294002533\n",
      "\n",
      "Epoch 182\n",
      "Step 0: loss = 0.20374254882335663, recon_loss = 0.2022401988506317, 0.0014726307708770037, kl_loss = 0.0014855097979307175\n",
      "\n",
      "Epoch 183\n",
      "Step 0: loss = 0.20366217195987701, recon_loss = 0.20202983915805817, 0.001589437946677208, kl_loss = 0.0021449970081448555\n",
      "\n",
      "Epoch 184\n",
      "Step 0: loss = 0.20209062099456787, recon_loss = 0.20026344060897827, 0.0017951158806681633, kl_loss = 0.0016033584251999855\n",
      "\n",
      "Epoch 185\n",
      "Step 0: loss = 0.2062014639377594, recon_loss = 0.20440593361854553, 0.0017578804399818182, kl_loss = 0.001883024349808693\n",
      "\n",
      "Epoch 186\n",
      "Step 0: loss = 0.20037828385829926, recon_loss = 0.19909434020519257, 0.0012440956197679043, kl_loss = 0.001992523670196533\n",
      "\n",
      "Epoch 187\n",
      "Step 0: loss = 0.19973823428153992, recon_loss = 0.19783398509025574, 0.0018397702369838953, kl_loss = 0.003223690204322338\n",
      "\n",
      "Epoch 188\n",
      "Step 0: loss = 0.20434153079986572, recon_loss = 0.20257258415222168, 0.0017240812303498387, kl_loss = 0.0022436799481511116\n",
      "\n",
      "Epoch 189\n",
      "Step 0: loss = 0.20215623080730438, recon_loss = 0.20054638385772705, 0.0015837210230529308, kl_loss = 0.0013060010969638824\n",
      "\n",
      "Epoch 190\n",
      "Step 0: loss = 0.20339587330818176, recon_loss = 0.20187079906463623, 0.0015091546811163425, kl_loss = 0.0007955869659781456\n",
      "\n",
      "Epoch 191\n",
      "Step 0: loss = 0.19893519580364227, recon_loss = 0.19734328985214233, 0.001564580830745399, kl_loss = 0.0013667279854416847\n",
      "\n",
      "Epoch 192\n",
      "Step 0: loss = 0.20021440088748932, recon_loss = 0.19860824942588806, 0.001578881754539907, kl_loss = 0.001363503746688366\n",
      "\n",
      "Epoch 193\n",
      "Step 0: loss = 0.19969770312309265, recon_loss = 0.1975797861814499, 0.0021021426655352116, kl_loss = 0.0007890276610851288\n",
      "\n",
      "Epoch 194\n",
      "Step 0: loss = 0.19613614678382874, recon_loss = 0.19444188475608826, 0.001667817821726203, kl_loss = 0.0013226782903075218\n",
      "\n",
      "Epoch 195\n",
      "Step 0: loss = 0.20250661671161652, recon_loss = 0.20090916752815247, 0.0015575943980365992, kl_loss = 0.001993386074900627\n",
      "\n",
      "Epoch 196\n",
      "Step 0: loss = 0.20073620975017548, recon_loss = 0.1995522528886795, 0.0011448375880718231, kl_loss = 0.001955866813659668\n",
      "\n",
      "Epoch 197\n",
      "Step 0: loss = 0.19964851438999176, recon_loss = 0.19758643209934235, 0.002031149575486779, kl_loss = 0.0015468187630176544\n",
      "\n",
      "Epoch 198\n",
      "Step 0: loss = 0.20048685371875763, recon_loss = 0.19901150465011597, 0.0014365820679813623, kl_loss = 0.00193891953676939\n",
      "\n",
      "Epoch 199\n",
      "Step 0: loss = 0.200345978140831, recon_loss = 0.19808053970336914, 0.002246858086436987, kl_loss = 0.0009293192997574806\n",
      "\n",
      "Epoch 200\n",
      "Step 0: loss = 0.20157241821289062, recon_loss = 0.199855238199234, 0.0016835134010761976, kl_loss = 0.0016830544918775558\n",
      "\n",
      "Epoch 201\n",
      "Step 0: loss = 0.20057687163352966, recon_loss = 0.19903139770030975, 0.0015088008949533105, kl_loss = 0.0018337815999984741\n",
      "\n",
      "Epoch 202\n",
      "Step 0: loss = 0.19829893112182617, recon_loss = 0.19678142666816711, 0.0014634127728641033, kl_loss = 0.0027045495808124542\n",
      "\n",
      "Epoch 203\n",
      "Step 0: loss = 0.20006515085697174, recon_loss = 0.1981181800365448, 0.0019178225193172693, kl_loss = 0.0014569945633411407\n",
      "\n",
      "Epoch 204\n",
      "Step 0: loss = 0.19392140209674835, recon_loss = 0.19240587949752808, 0.0014832918532192707, kl_loss = 0.0016112234443426132\n",
      "\n",
      "Epoch 205\n",
      "Step 0: loss = 0.20181214809417725, recon_loss = 0.19994190335273743, 0.0018052214290946722, kl_loss = 0.0032513896003365517\n",
      "\n",
      "Epoch 206\n",
      "Step 0: loss = 0.19385510683059692, recon_loss = 0.19216254353523254, 0.0016705822199583054, kl_loss = 0.0010991273447871208\n",
      "\n",
      "Epoch 207\n",
      "Step 0: loss = 0.19996222853660583, recon_loss = 0.1978500932455063, 0.002055987250059843, kl_loss = 0.002807444892823696\n",
      "\n",
      "Epoch 208\n",
      "Step 0: loss = 0.19869165122509003, recon_loss = 0.19713759422302246, 0.0014981022104620934, kl_loss = 0.002797858789563179\n",
      "\n",
      "Epoch 209\n",
      "Step 0: loss = 0.19982701539993286, recon_loss = 0.1977006196975708, 0.0020826063118875027, kl_loss = 0.002190046012401581\n",
      "\n",
      "Epoch 210\n",
      "Step 0: loss = 0.1935722678899765, recon_loss = 0.19188445806503296, 0.0016509854467585683, kl_loss = 0.001841099001467228\n",
      "\n",
      "Epoch 211\n",
      "Step 0: loss = 0.19760411977767944, recon_loss = 0.19600239396095276, 0.0015421969583258033, kl_loss = 0.0029765311628580093\n",
      "\n",
      "Epoch 212\n",
      "Step 0: loss = 0.19738072156906128, recon_loss = 0.19553828239440918, 0.001808656845241785, kl_loss = 0.0016891388222575188\n",
      "\n",
      "Epoch 213\n",
      "Step 0: loss = 0.19879548251628876, recon_loss = 0.19703590869903564, 0.0017084998544305563, kl_loss = 0.0025537628680467606\n",
      "\n",
      "Epoch 214\n",
      "Step 0: loss = 0.19710025191307068, recon_loss = 0.19520580768585205, 0.0018187857931479812, kl_loss = 0.0037828395143151283\n",
      "\n",
      "Epoch 215\n",
      "Step 0: loss = 0.1934284120798111, recon_loss = 0.19194677472114563, 0.0014540943084284663, kl_loss = 0.0013771848753094673\n",
      "\n",
      "Epoch 216\n",
      "Step 0: loss = 0.19643373787403107, recon_loss = 0.1946275234222412, 0.0017725115176290274, kl_loss = 0.0016852952539920807\n",
      "\n",
      "Epoch 217\n",
      "Step 0: loss = 0.19447089731693268, recon_loss = 0.1923411637544632, 0.0020641935989260674, kl_loss = 0.0032764794304966927\n",
      "\n",
      "Epoch 218\n",
      "Step 0: loss = 0.19634132087230682, recon_loss = 0.194391667842865, 0.001899875234812498, kl_loss = 0.0024893097579479218\n",
      "\n",
      "Epoch 219\n",
      "Step 0: loss = 0.1931920349597931, recon_loss = 0.19168192148208618, 0.001475038705393672, kl_loss = 0.0017541376873850822\n",
      "\n",
      "Epoch 220\n",
      "Step 0: loss = 0.1911035031080246, recon_loss = 0.1891489177942276, 0.001915307017043233, kl_loss = 0.0019643381237983704\n",
      "\n",
      "Epoch 221\n",
      "Step 0: loss = 0.19359445571899414, recon_loss = 0.19220051169395447, 0.0013562042731791735, kl_loss = 0.0018873456865549088\n",
      "\n",
      "Epoch 222\n",
      "Step 0: loss = 0.1951911747455597, recon_loss = 0.19354216754436493, 0.0016181059181690216, kl_loss = 0.0015453649684786797\n",
      "\n",
      "Epoch 223\n",
      "Step 0: loss = 0.19873948395252228, recon_loss = 0.19697734713554382, 0.0017315223813056946, kl_loss = 0.0015307338908314705\n",
      "\n",
      "Epoch 224\n",
      "Step 0: loss = 0.193727046251297, recon_loss = 0.19124621152877808, 0.0024553220719099045, kl_loss = 0.001275641843676567\n",
      "\n",
      "Epoch 225\n",
      "Step 0: loss = 0.19740062952041626, recon_loss = 0.19556689262390137, 0.0017837387276813388, kl_loss = 0.0024999789893627167\n",
      "\n",
      "Epoch 226\n",
      "Step 0: loss = 0.19193553924560547, recon_loss = 0.19083227217197418, 0.0010561360977590084, kl_loss = 0.002356479875743389\n",
      "\n",
      "Epoch 227\n",
      "Step 0: loss = 0.18585671484470367, recon_loss = 0.18361318111419678, 0.002202908508479595, kl_loss = 0.002031145617365837\n",
      "\n",
      "Epoch 228\n",
      "Step 0: loss = 0.19140976667404175, recon_loss = 0.18954840302467346, 0.0018227286636829376, kl_loss = 0.0019316300749778748\n",
      "\n",
      "Epoch 229\n",
      "Step 0: loss = 0.19032414257526398, recon_loss = 0.18801262974739075, 0.0022534928284585476, kl_loss = 0.0029013371095061302\n",
      "\n",
      "Epoch 230\n",
      "Step 0: loss = 0.18990164995193481, recon_loss = 0.1884310245513916, 0.0014269191306084394, kl_loss = 0.002185497432947159\n",
      "\n",
      "Epoch 231\n",
      "Step 0: loss = 0.1905316412448883, recon_loss = 0.18800967931747437, 0.002469153143465519, kl_loss = 0.0026403656229376793\n",
      "\n",
      "Epoch 232\n",
      "Step 0: loss = 0.1831655651330948, recon_loss = 0.18161341547966003, 0.0015181169146671891, kl_loss = 0.001702011562883854\n",
      "\n",
      "Epoch 233\n",
      "Step 0: loss = 0.19048188626766205, recon_loss = 0.18826797604560852, 0.0021748447325080633, kl_loss = 0.0019533736631274223\n",
      "\n",
      "Epoch 234\n",
      "Step 0: loss = 0.18999940156936646, recon_loss = 0.18824800848960876, 0.0017232761019840837, kl_loss = 0.0014059348031878471\n",
      "\n",
      "Epoch 235\n",
      "Step 0: loss = 0.18846264481544495, recon_loss = 0.18656332790851593, 0.0018366100266575813, kl_loss = 0.003135177306830883\n",
      "\n",
      "Epoch 236\n",
      "Step 0: loss = 0.18546123802661896, recon_loss = 0.18381111323833466, 0.001616830937564373, kl_loss = 0.0016646655276417732\n",
      "\n",
      "Epoch 237\n",
      "Step 0: loss = 0.18645073473453522, recon_loss = 0.18442440032958984, 0.001963928807526827, kl_loss = 0.0031203655526041985\n",
      "\n",
      "Epoch 238\n",
      "Step 0: loss = 0.18299537897109985, recon_loss = 0.1811041384935379, 0.0018479296704754233, kl_loss = 0.0021661389619112015\n",
      "\n",
      "Epoch 239\n",
      "Step 0: loss = 0.18280282616615295, recon_loss = 0.18015597760677338, 0.002587628783658147, kl_loss = 0.002960607409477234\n",
      "\n",
      "Epoch 240\n",
      "Step 0: loss = 0.18562687933444977, recon_loss = 0.18362610042095184, 0.0019522757502272725, kl_loss = 0.0024255169555544853\n",
      "\n",
      "Epoch 241\n",
      "Step 0: loss = 0.1831938922405243, recon_loss = 0.18089959025382996, 0.0022533272858709097, kl_loss = 0.002048864960670471\n",
      "\n",
      "Epoch 242\n",
      "Step 0: loss = 0.18204675614833832, recon_loss = 0.17973946034908295, 0.002264746930450201, kl_loss = 0.0021274928003549576\n",
      "\n",
      "Epoch 243\n",
      "Step 0: loss = 0.1848715990781784, recon_loss = 0.18295493721961975, 0.0018637161701917648, kl_loss = 0.0026474911719560623\n",
      "\n",
      "Epoch 244\n",
      "Step 0: loss = 0.18484064936637878, recon_loss = 0.18283256888389587, 0.0019604964181780815, kl_loss = 0.0023790188133716583\n",
      "\n",
      "Epoch 245\n",
      "Step 0: loss = 0.18509089946746826, recon_loss = 0.18225312232971191, 0.0027792411856353283, kl_loss = 0.0029267435893416405\n",
      "\n",
      "Epoch 246\n",
      "Step 0: loss = 0.18395692110061646, recon_loss = 0.18118442595005035, 0.0027289048302918673, kl_loss = 0.002179572358727455\n",
      "\n",
      "Epoch 247\n",
      "Step 0: loss = 0.17567579448223114, recon_loss = 0.17248505353927612, 0.00313421618193388, kl_loss = 0.002825835719704628\n",
      "\n",
      "Epoch 248\n",
      "Step 0: loss = 0.1849830001592636, recon_loss = 0.18263575434684753, 0.0022944901138544083, kl_loss = 0.0026373565196990967\n",
      "\n",
      "Epoch 249\n",
      "Step 0: loss = 0.17758050560951233, recon_loss = 0.17510905861854553, 0.0024098814465105534, kl_loss = 0.003078347072005272\n",
      "\n",
      "Epoch 250\n",
      "Step 0: loss = 0.177218958735466, recon_loss = 0.17504435777664185, 0.0021297186613082886, kl_loss = 0.0022444669157266617\n",
      "\n",
      "Epoch 251\n",
      "Step 0: loss = 0.17863932251930237, recon_loss = 0.17653390765190125, 0.0020730914548039436, kl_loss = 0.001616068184375763\n",
      "\n",
      "Epoch 252\n",
      "Step 0: loss = 0.1767052710056305, recon_loss = 0.17427612841129303, 0.0023699067533016205, kl_loss = 0.0029617836698889732\n",
      "\n",
      "Epoch 253\n",
      "Step 0: loss = 0.17719344794750214, recon_loss = 0.1748400181531906, 0.002281379420310259, kl_loss = 0.0036023948341608047\n",
      "\n",
      "Epoch 254\n",
      "Step 0: loss = 0.17758473753929138, recon_loss = 0.17496924102306366, 0.002538342960178852, kl_loss = 0.003858208656311035\n",
      "\n",
      "Epoch 255\n",
      "Step 0: loss = 0.17146551609039307, recon_loss = 0.1678570955991745, 0.003562507452443242, kl_loss = 0.0022957464680075645\n",
      "\n",
      "Epoch 256\n",
      "Step 0: loss = 0.17196984589099884, recon_loss = 0.16963301599025726, 0.002284018788486719, kl_loss = 0.002640295773744583\n",
      "\n",
      "Epoch 257\n",
      "Step 0: loss = 0.17503748834133148, recon_loss = 0.1720721572637558, 0.002881839405745268, kl_loss = 0.004174753092229366\n",
      "\n",
      "Epoch 258\n",
      "Step 0: loss = 0.16805456578731537, recon_loss = 0.1648092269897461, 0.003212022827938199, kl_loss = 0.0016662226989865303\n",
      "\n",
      "Epoch 259\n",
      "Step 0: loss = 0.1702929586172104, recon_loss = 0.16836416721343994, 0.0018948217621073127, kl_loss = 0.0016989931464195251\n",
      "\n",
      "Epoch 260\n",
      "Step 0: loss = 0.16953302919864655, recon_loss = 0.1666570007801056, 0.002838442800566554, kl_loss = 0.0018786974251270294\n",
      "\n",
      "Epoch 261\n",
      "Step 0: loss = 0.16550083458423615, recon_loss = 0.16324812173843384, 0.0021967808715999126, kl_loss = 0.0027969293296337128\n",
      "\n",
      "Epoch 262\n",
      "Step 0: loss = 0.1735430657863617, recon_loss = 0.17157995700836182, 0.0019211191684007645, kl_loss = 0.002099316567182541\n",
      "\n",
      "Epoch 263\n",
      "Step 0: loss = 0.1702425628900528, recon_loss = 0.16804683208465576, 0.0021640202030539513, kl_loss = 0.001585368998348713\n",
      "\n",
      "Epoch 264\n",
      "Step 0: loss = 0.16395370662212372, recon_loss = 0.1615498960018158, 0.0023788008838891983, kl_loss = 0.00125033687800169\n",
      "\n",
      "Epoch 265\n",
      "Step 0: loss = 0.16311830282211304, recon_loss = 0.16074466705322266, 0.0023439957294613123, kl_loss = 0.0014819120988249779\n",
      "\n",
      "Epoch 266\n",
      "Step 0: loss = 0.16838331520557404, recon_loss = 0.16560786962509155, 0.0027417573146522045, kl_loss = 0.001684257760643959\n",
      "\n",
      "Epoch 267\n",
      "Step 0: loss = 0.1651933789253235, recon_loss = 0.16268549859523773, 0.002447203267365694, kl_loss = 0.003033917397260666\n",
      "\n",
      "Epoch 268\n",
      "Step 0: loss = 0.16559578478336334, recon_loss = 0.16406099498271942, 0.0015039086574688554, kl_loss = 0.0015436047688126564\n",
      "\n",
      "Epoch 269\n",
      "Step 0: loss = 0.1661287397146225, recon_loss = 0.16340267658233643, 0.002660843078047037, kl_loss = 0.003261122852563858\n",
      "\n",
      "Epoch 270\n",
      "Step 0: loss = 0.16192002594470978, recon_loss = 0.1578773558139801, 0.003977111540734768, kl_loss = 0.003278507851064205\n",
      "\n",
      "Epoch 271\n",
      "Step 0: loss = 0.1644306778907776, recon_loss = 0.16075719892978668, 0.0036442538257688284, kl_loss = 0.001461041159927845\n",
      "\n",
      "Epoch 272\n",
      "Step 0: loss = 0.15948978066444397, recon_loss = 0.15713772177696228, 0.0023085493594408035, kl_loss = 0.0021752389147877693\n",
      "\n",
      "Epoch 273\n",
      "Step 0: loss = 0.15957292914390564, recon_loss = 0.15711015462875366, 0.0024284590035676956, kl_loss = 0.001715654507279396\n",
      "\n",
      "Epoch 274\n",
      "Step 0: loss = 0.15704357624053955, recon_loss = 0.1547289788722992, 0.002286301925778389, kl_loss = 0.0014145448803901672\n",
      "\n",
      "Epoch 275\n",
      "Step 0: loss = 0.15244469046592712, recon_loss = 0.14959225058555603, 0.002824331633746624, kl_loss = 0.0014053164049983025\n",
      "\n",
      "Epoch 276\n",
      "Step 0: loss = 0.16117842495441437, recon_loss = 0.15792450308799744, 0.0032172983046621084, kl_loss = 0.0018313145264983177\n",
      "\n",
      "Epoch 277\n",
      "Step 0: loss = 0.1556617170572281, recon_loss = 0.15395930409431458, 0.0016600710805505514, kl_loss = 0.002117220312356949\n",
      "\n",
      "Epoch 278\n",
      "Step 0: loss = 0.147360697388649, recon_loss = 0.1448463797569275, 0.0024919509887695312, kl_loss = 0.0011181971058249474\n",
      "\n",
      "Epoch 279\n",
      "Step 0: loss = 0.15535107254981995, recon_loss = 0.15299083292484283, 0.002338509075343609, kl_loss = 0.0010866494849324226\n",
      "\n",
      "Epoch 280\n",
      "Step 0: loss = 0.14779427647590637, recon_loss = 0.14534957706928253, 0.0024240261409431696, kl_loss = 0.0010333266109228134\n",
      "\n",
      "Epoch 281\n",
      "Step 0: loss = 0.15236592292785645, recon_loss = 0.14977066218852997, 0.002578531624749303, kl_loss = 0.0008367486298084259\n",
      "\n",
      "Epoch 282\n",
      "Step 0: loss = 0.15395553410053253, recon_loss = 0.1515679955482483, 0.002359212376177311, kl_loss = 0.0014161765575408936\n",
      "\n",
      "Epoch 283\n",
      "Step 0: loss = 0.15048688650131226, recon_loss = 0.14739862084388733, 0.0030675071757286787, kl_loss = 0.0010375073179602623\n",
      "\n",
      "Epoch 284\n",
      "Step 0: loss = 0.1539473533630371, recon_loss = 0.15032023191452026, 0.0036097776610404253, kl_loss = 0.0008671628311276436\n",
      "\n",
      "Epoch 285\n",
      "Step 0: loss = 0.1446927785873413, recon_loss = 0.14177045226097107, 0.002905747387558222, kl_loss = 0.0008291788399219513\n",
      "\n",
      "Epoch 286\n",
      "Step 0: loss = 0.14555208384990692, recon_loss = 0.142873615026474, 0.0026522292755544186, kl_loss = 0.001312345266342163\n",
      "\n",
      "Epoch 287\n",
      "Step 0: loss = 0.14547573029994965, recon_loss = 0.14279145002365112, 0.0026386906392872334, kl_loss = 0.0022792741656303406\n",
      "\n",
      "Epoch 288\n",
      "Step 0: loss = 0.15124444663524628, recon_loss = 0.148494690656662, 0.002721155062317848, kl_loss = 0.001429770141839981\n",
      "\n",
      "Epoch 289\n",
      "Step 0: loss = 0.14450959861278534, recon_loss = 0.14204606413841248, 0.0024428043980151415, kl_loss = 0.001036291942000389\n",
      "\n",
      "Epoch 290\n",
      "Step 0: loss = 0.14097799360752106, recon_loss = 0.13830684125423431, 0.002649259753525257, kl_loss = 0.0010947594419121742\n",
      "\n",
      "Epoch 291\n",
      "Step 0: loss = 0.1416090577840805, recon_loss = 0.13868635892868042, 0.0028937198221683502, kl_loss = 0.0014490904286503792\n",
      "\n",
      "Epoch 292\n",
      "Step 0: loss = 0.13942070305347443, recon_loss = 0.1361081898212433, 0.0032814880833029747, kl_loss = 0.0015510143712162971\n",
      "\n",
      "Epoch 293\n",
      "Step 0: loss = 0.14186914265155792, recon_loss = 0.13938388228416443, 0.002453397959470749, kl_loss = 0.0015925737097859383\n",
      "\n",
      "Epoch 294\n",
      "Step 0: loss = 0.14536495506763458, recon_loss = 0.14193940162658691, 0.0034016252029687166, kl_loss = 0.0011967122554779053\n",
      "\n",
      "Epoch 295\n",
      "Step 0: loss = 0.13932263851165771, recon_loss = 0.13629376888275146, 0.0029982784762978554, kl_loss = 0.0015297504141926765\n",
      "\n",
      "Epoch 296\n",
      "Step 0: loss = 0.13876031339168549, recon_loss = 0.1354747861623764, 0.0032597677782177925, kl_loss = 0.0012881746515631676\n",
      "\n",
      "Epoch 297\n",
      "Step 0: loss = 0.1433734893798828, recon_loss = 0.1402679830789566, 0.0030715386383235455, kl_loss = 0.0016984054818749428\n",
      "\n",
      "Epoch 298\n",
      "Step 0: loss = 0.14203974604606628, recon_loss = 0.13843505084514618, 0.0035694409161806107, kl_loss = 0.0017624786123633385\n",
      "\n",
      "Epoch 299\n",
      "Step 0: loss = 0.137231707572937, recon_loss = 0.13400399684906006, 0.0031899160239845514, kl_loss = 0.001889641396701336\n",
      "\n",
      "Epoch 300\n",
      "Step 0: loss = 0.14132268726825714, recon_loss = 0.13870137929916382, 0.0025802268646657467, kl_loss = 0.0020543066784739494\n",
      "\n",
      "Epoch 301\n",
      "Step 0: loss = 0.13711746037006378, recon_loss = 0.1339658796787262, 0.0030922165606170893, kl_loss = 0.0029682982712984085\n",
      "\n",
      "Epoch 302\n",
      "Step 0: loss = 0.14081145823001862, recon_loss = 0.13748596608638763, 0.0032734465785324574, kl_loss = 0.0026025045663118362\n",
      "\n",
      "Epoch 303\n",
      "Step 0: loss = 0.13863946497440338, recon_loss = 0.13548816740512848, 0.003113239537924528, kl_loss = 0.0019031306728720665\n",
      "\n",
      "Epoch 304\n",
      "Step 0: loss = 0.13718801736831665, recon_loss = 0.13410791754722595, 0.0030340063385665417, kl_loss = 0.00230436772108078\n",
      "\n",
      "Epoch 305\n",
      "Step 0: loss = 0.12634097039699554, recon_loss = 0.12255340069532394, 0.0037577906623482704, kl_loss = 0.0014886092394590378\n",
      "\n",
      "Epoch 306\n",
      "Step 0: loss = 0.1348796933889389, recon_loss = 0.1309707909822464, 0.0038863334339112043, kl_loss = 0.0011284071952104568\n",
      "\n",
      "Epoch 307\n",
      "Step 0: loss = 0.13100533187389374, recon_loss = 0.12804296612739563, 0.0029221377335488796, kl_loss = 0.002011633478105068\n",
      "\n",
      "Epoch 308\n",
      "Step 0: loss = 0.1279100626707077, recon_loss = 0.12427830696105957, 0.003564037848263979, kl_loss = 0.0033855270594358444\n",
      "\n",
      "Epoch 309\n",
      "Step 0: loss = 0.13146619498729706, recon_loss = 0.12708522379398346, 0.004349255934357643, kl_loss = 0.0015852469950914383\n",
      "\n",
      "Epoch 310\n",
      "Step 0: loss = 0.1256425678730011, recon_loss = 0.1220182552933693, 0.003565605729818344, kl_loss = 0.0029354356229305267\n",
      "\n",
      "Epoch 311\n",
      "Step 0: loss = 0.12619230151176453, recon_loss = 0.12278378009796143, 0.0033510010689496994, kl_loss = 0.0028762053698301315\n",
      "\n",
      "Epoch 312\n",
      "Step 0: loss = 0.13099108636379242, recon_loss = 0.12834987044334412, 0.0025871822144836187, kl_loss = 0.0027013057842850685\n",
      "\n",
      "Epoch 313\n",
      "Step 0: loss = 0.12609568238258362, recon_loss = 0.12297574430704117, 0.003070402890443802, kl_loss = 0.0024768682196736336\n",
      "\n",
      "Epoch 314\n",
      "Step 0: loss = 0.1311720460653305, recon_loss = 0.12760472297668457, 0.0035366725642234087, kl_loss = 0.0015323460102081299\n",
      "\n",
      "Epoch 315\n",
      "Step 0: loss = 0.1306336522102356, recon_loss = 0.12692570686340332, 0.0036815693601965904, kl_loss = 0.0013188961893320084\n",
      "\n",
      "Epoch 316\n",
      "Step 0: loss = 0.12734311819076538, recon_loss = 0.12366046011447906, 0.0036564632318913937, kl_loss = 0.001310167834162712\n",
      "\n",
      "Epoch 317\n",
      "Step 0: loss = 0.12887431681156158, recon_loss = 0.12490439414978027, 0.003937860950827599, kl_loss = 0.0016037151217460632\n",
      "\n",
      "Epoch 318\n",
      "Step 0: loss = 0.12813043594360352, recon_loss = 0.12344594299793243, 0.004631164949387312, kl_loss = 0.0026663383468985558\n",
      "\n",
      "Epoch 319\n",
      "Step 0: loss = 0.13252317905426025, recon_loss = 0.12794139981269836, 0.00453543197363615, kl_loss = 0.0023169368505477905\n",
      "\n",
      "Epoch 320\n",
      "Step 0: loss = 0.13152571022510529, recon_loss = 0.1275327205657959, 0.003965187817811966, kl_loss = 0.0013900510966777802\n",
      "\n",
      "Epoch 321\n",
      "Step 0: loss = 0.12391162663698196, recon_loss = 0.12081007659435272, 0.0030676587484776974, kl_loss = 0.0016945404931902885\n",
      "\n",
      "Epoch 322\n",
      "Step 0: loss = 0.12722040712833405, recon_loss = 0.12342974543571472, 0.0037542348727583885, kl_loss = 0.0018216874450445175\n",
      "\n",
      "Epoch 323\n",
      "Step 0: loss = 0.12760105729103088, recon_loss = 0.12376917898654938, 0.0037948910612612963, kl_loss = 0.0018494734540581703\n",
      "\n",
      "Epoch 324\n",
      "Step 0: loss = 0.12534554302692413, recon_loss = 0.12172422558069229, 0.0035926245618611574, kl_loss = 0.0014347080141305923\n",
      "\n",
      "Epoch 325\n",
      "Step 0: loss = 0.12615789473056793, recon_loss = 0.12271517515182495, 0.003413193393498659, kl_loss = 0.0014760149642825127\n",
      "\n",
      "Epoch 326\n",
      "Step 0: loss = 0.11603370308876038, recon_loss = 0.11116066575050354, 0.004822477698326111, kl_loss = 0.0025278665125370026\n",
      "\n",
      "Epoch 327\n",
      "Step 0: loss = 0.12737725675106049, recon_loss = 0.1238560751080513, 0.0034927590750157833, kl_loss = 0.001420796848833561\n",
      "\n",
      "Epoch 328\n",
      "Step 0: loss = 0.11620546877384186, recon_loss = 0.11215905845165253, 0.004017829895019531, kl_loss = 0.0014289366081357002\n",
      "\n",
      "Epoch 329\n",
      "Step 0: loss = 0.11140646785497665, recon_loss = 0.10759805142879486, 0.0037887259386479855, kl_loss = 0.0009847339242696762\n",
      "\n",
      "Epoch 330\n",
      "Step 0: loss = 0.11997110396623611, recon_loss = 0.11573125422000885, 0.004193022847175598, kl_loss = 0.0023414287716150284\n",
      "\n",
      "Epoch 331\n",
      "Step 0: loss = 0.12361107021570206, recon_loss = 0.1193288117647171, 0.004239726811647415, kl_loss = 0.00212673656642437\n",
      "\n",
      "Epoch 332\n",
      "Step 0: loss = 0.11517923325300217, recon_loss = 0.11117952316999435, 0.003963073715567589, kl_loss = 0.0018316786736249924\n",
      "\n",
      "Epoch 333\n",
      "Step 0: loss = 0.12298788875341415, recon_loss = 0.11893963813781738, 0.003991394769400358, kl_loss = 0.002842625603079796\n",
      "\n",
      "Epoch 334\n",
      "Step 0: loss = 0.12175185978412628, recon_loss = 0.11848416179418564, 0.0032312851399183273, kl_loss = 0.0018206769600510597\n",
      "\n",
      "Epoch 335\n",
      "Step 0: loss = 0.10721959173679352, recon_loss = 0.10320118069648743, 0.0039763678796589375, kl_loss = 0.0021023284643888474\n",
      "\n",
      "Epoch 336\n",
      "Step 0: loss = 0.11655514687299728, recon_loss = 0.11212923377752304, 0.004363400861620903, kl_loss = 0.0031255539506673813\n",
      "\n",
      "Epoch 337\n",
      "Step 0: loss = 0.11729123443365097, recon_loss = 0.11381006985902786, 0.0034419046714901924, kl_loss = 0.001963019371032715\n",
      "\n",
      "Epoch 338\n",
      "Step 0: loss = 0.12049385905265808, recon_loss = 0.1161455288529396, 0.004327575676143169, kl_loss = 0.0010376833379268646\n",
      "\n",
      "Epoch 339\n",
      "Step 0: loss = 0.11659130454063416, recon_loss = 0.1128641739487648, 0.0036930651403963566, kl_loss = 0.0017031151801347733\n",
      "\n",
      "Epoch 340\n",
      "Step 0: loss = 0.1229596734046936, recon_loss = 0.11821173131465912, 0.00471390038728714, kl_loss = 0.001702241599559784\n",
      "\n",
      "Epoch 341\n",
      "Step 0: loss = 0.11926941573619843, recon_loss = 0.11432003229856491, 0.004926811903715134, kl_loss = 0.0011288737878203392\n",
      "\n",
      "Epoch 342\n",
      "Step 0: loss = 0.11726732552051544, recon_loss = 0.11325497925281525, 0.00398009130731225, kl_loss = 0.0016127396374940872\n",
      "\n",
      "Epoch 343\n",
      "Step 0: loss = 0.119418665766716, recon_loss = 0.11601322144269943, 0.0033757274504750967, kl_loss = 0.001485869288444519\n",
      "\n",
      "Epoch 344\n",
      "Step 0: loss = 0.1150943711400032, recon_loss = 0.11086209118366241, 0.004199841525405645, kl_loss = 0.001621842384338379\n",
      "\n",
      "Epoch 345\n",
      "Step 0: loss = 0.11955404281616211, recon_loss = 0.11421167850494385, 0.0053214398212730885, kl_loss = 0.001046009361743927\n",
      "\n",
      "Epoch 346\n",
      "Step 0: loss = 0.11252232640981674, recon_loss = 0.10783752799034119, 0.004659085068851709, kl_loss = 0.0012856675311923027\n",
      "\n",
      "Epoch 347\n",
      "Step 0: loss = 0.12044177204370499, recon_loss = 0.11699498444795609, 0.003429359756410122, kl_loss = 0.0008711628615856171\n",
      "\n",
      "Epoch 348\n",
      "Step 0: loss = 0.11541826277971268, recon_loss = 0.111087866127491, 0.004306415561586618, kl_loss = 0.0011991504579782486\n",
      "\n",
      "Epoch 349\n",
      "Step 0: loss = 0.11003046482801437, recon_loss = 0.10518182814121246, 0.004826502874493599, kl_loss = 0.0011068331077694893\n",
      "\n",
      "Epoch 350\n",
      "Step 0: loss = 0.11009635776281357, recon_loss = 0.10544852167367935, 0.004624184221029282, kl_loss = 0.0011827191337943077\n",
      "\n",
      "Epoch 351\n",
      "Step 0: loss = 0.11237189918756485, recon_loss = 0.1072278544306755, 0.005122858565300703, kl_loss = 0.0010596318170428276\n",
      "\n",
      "Epoch 352\n",
      "Step 0: loss = 0.11073312908411026, recon_loss = 0.10625220835208893, 0.004445692524313927, kl_loss = 0.0017614653334021568\n",
      "\n",
      "Epoch 353\n",
      "Step 0: loss = 0.11667852848768234, recon_loss = 0.11310960352420807, 0.003551797941327095, kl_loss = 0.000856298953294754\n",
      "\n",
      "Epoch 354\n",
      "Step 0: loss = 0.11043259501457214, recon_loss = 0.1059069037437439, 0.004490131512284279, kl_loss = 0.001777907833456993\n",
      "\n",
      "Epoch 355\n",
      "Step 0: loss = 0.11024671792984009, recon_loss = 0.1056191623210907, 0.004601416178047657, kl_loss = 0.0013068011030554771\n",
      "\n",
      "Epoch 356\n",
      "Step 0: loss = 0.1136147528886795, recon_loss = 0.10953100025653839, 0.004042389336973429, kl_loss = 0.002068127505481243\n",
      "\n",
      "Epoch 357\n",
      "Step 0: loss = 0.10469664633274078, recon_loss = 0.1002025455236435, 0.004467528313398361, kl_loss = 0.001328272745013237\n",
      "\n",
      "Epoch 358\n",
      "Step 0: loss = 0.11281335353851318, recon_loss = 0.10784146189689636, 0.004941851831972599, kl_loss = 0.0015021217986941338\n",
      "\n",
      "Epoch 359\n",
      "Step 0: loss = 0.11102934181690216, recon_loss = 0.10623873025178909, 0.004723948426544666, kl_loss = 0.0033330032601952553\n",
      "\n",
      "Epoch 360\n",
      "Step 0: loss = 0.107204370200634, recon_loss = 0.10293882340192795, 0.004221854731440544, kl_loss = 0.0021845437586307526\n",
      "\n",
      "Epoch 361\n",
      "Step 0: loss = 0.11010278761386871, recon_loss = 0.10480404645204544, 0.005249544978141785, kl_loss = 0.0024597160518169403\n",
      "\n",
      "Epoch 362\n",
      "Step 0: loss = 0.11585891246795654, recon_loss = 0.1119769811630249, 0.003825253574177623, kl_loss = 0.0028339605778455734\n",
      "\n",
      "Epoch 363\n",
      "Step 0: loss = 0.11515254527330399, recon_loss = 0.10931237787008286, 0.005755924154073, kl_loss = 0.004212137311697006\n",
      "\n",
      "Epoch 364\n",
      "Step 0: loss = 0.10986459255218506, recon_loss = 0.10574246197938919, 0.004066647496074438, kl_loss = 0.0027741407975554466\n",
      "\n",
      "Epoch 365\n",
      "Step 0: loss = 0.10649843513965607, recon_loss = 0.10233697295188904, 0.0041222525760531425, kl_loss = 0.0019607963040471077\n",
      "\n",
      "Epoch 366\n",
      "Step 0: loss = 0.10406261682510376, recon_loss = 0.10007388889789581, 0.003940640948712826, kl_loss = 0.002404412254691124\n",
      "\n",
      "Epoch 367\n",
      "Step 0: loss = 0.10537897795438766, recon_loss = 0.10139176994562149, 0.003958746790885925, kl_loss = 0.0014229007065296173\n",
      "\n",
      "Epoch 368\n",
      "Step 0: loss = 0.10927922278642654, recon_loss = 0.10488493740558624, 0.0043532829731702805, kl_loss = 0.0020498819649219513\n",
      "\n",
      "Epoch 369\n",
      "Step 0: loss = 0.10543301701545715, recon_loss = 0.10029539465904236, 0.005101826041936874, kl_loss = 0.0017896834760904312\n",
      "\n",
      "Epoch 370\n",
      "Step 0: loss = 0.11003605276346207, recon_loss = 0.1057203859090805, 0.004279118962585926, kl_loss = 0.0018272269517183304\n",
      "\n",
      "Epoch 371\n",
      "Step 0: loss = 0.1053052544593811, recon_loss = 0.10134710371494293, 0.0038883350789546967, kl_loss = 0.0034904256463050842\n",
      "\n",
      "Epoch 372\n",
      "Step 0: loss = 0.10753394663333893, recon_loss = 0.10319609194993973, 0.004294787533581257, kl_loss = 0.002153240144252777\n",
      "\n",
      "Epoch 373\n",
      "Step 0: loss = 0.10962239652872086, recon_loss = 0.10505732148885727, 0.004523864947259426, kl_loss = 0.0020603155717253685\n",
      "\n",
      "Epoch 374\n",
      "Step 0: loss = 0.10589472949504852, recon_loss = 0.1002637967467308, 0.005582141689956188, kl_loss = 0.0024395938962697983\n",
      "\n",
      "Epoch 375\n",
      "Step 0: loss = 0.1050356775522232, recon_loss = 0.10032618045806885, 0.004675108008086681, kl_loss = 0.001719444990158081\n",
      "\n",
      "Epoch 376\n",
      "Step 0: loss = 0.11246653646230698, recon_loss = 0.10687509179115295, 0.005572278052568436, kl_loss = 0.000958533026278019\n",
      "\n",
      "Epoch 377\n",
      "Step 0: loss = 0.10506394505500793, recon_loss = 0.09972299635410309, 0.005317138507962227, kl_loss = 0.0011906297877430916\n",
      "\n",
      "Epoch 378\n",
      "Step 0: loss = 0.10693682730197906, recon_loss = 0.10327334702014923, 0.003643383737653494, kl_loss = 0.0010048681870102882\n",
      "\n",
      "Epoch 379\n",
      "Step 0: loss = 0.10812876373529434, recon_loss = 0.10393525660037994, 0.0041549550369381905, kl_loss = 0.0019276002421975136\n",
      "\n",
      "Epoch 380\n",
      "Step 0: loss = 0.10004959255456924, recon_loss = 0.09441322833299637, 0.005602752324193716, kl_loss = 0.0016806237399578094\n",
      "\n",
      "Epoch 381\n",
      "Step 0: loss = 0.10535237938165665, recon_loss = 0.09896024316549301, 0.006351442541927099, kl_loss = 0.0020347414538264275\n",
      "\n",
      "Epoch 382\n",
      "Step 0: loss = 0.10954853147268295, recon_loss = 0.10412061959505081, 0.005371525418013334, kl_loss = 0.0028193145990371704\n",
      "\n",
      "Epoch 383\n",
      "Step 0: loss = 0.10348381102085114, recon_loss = 0.09984564781188965, 0.0035732409451156855, kl_loss = 0.003246331587433815\n",
      "\n",
      "Epoch 384\n",
      "Step 0: loss = 0.10445831716060638, recon_loss = 0.09979020059108734, 0.00461818091571331, kl_loss = 0.0024967752397060394\n",
      "\n",
      "Epoch 385\n",
      "Step 0: loss = 0.10100214183330536, recon_loss = 0.09519485384225845, 0.005741211585700512, kl_loss = 0.00330386683344841\n",
      "\n",
      "Epoch 386\n",
      "Step 0: loss = 0.10025908052921295, recon_loss = 0.09509751200675964, 0.00513075478374958, kl_loss = 0.0015407511964440346\n",
      "\n",
      "Epoch 387\n",
      "Step 0: loss = 0.10647471994161606, recon_loss = 0.10167409479618073, 0.004761348478496075, kl_loss = 0.0019640959799289703\n",
      "\n",
      "Epoch 388\n",
      "Step 0: loss = 0.10585001111030579, recon_loss = 0.1007549911737442, 0.005060827359557152, kl_loss = 0.0017093578353524208\n",
      "\n",
      "Epoch 389\n",
      "Step 0: loss = 0.10285282880067825, recon_loss = 0.09725946187973022, 0.00555883115157485, kl_loss = 0.001726735383272171\n",
      "\n",
      "Epoch 390\n",
      "Step 0: loss = 0.10203993320465088, recon_loss = 0.09754624962806702, 0.0044754985719919205, kl_loss = 0.0009093331173062325\n",
      "\n",
      "Epoch 391\n",
      "Step 0: loss = 0.09469747543334961, recon_loss = 0.08967075496912003, 0.004992134869098663, kl_loss = 0.0017293598502874374\n",
      "\n",
      "Epoch 392\n",
      "Step 0: loss = 0.10493844002485275, recon_loss = 0.09872447699308395, 0.006186327897012234, kl_loss = 0.0013817129656672478\n",
      "\n",
      "Epoch 393\n",
      "Step 0: loss = 0.09903992712497711, recon_loss = 0.09449554234743118, 0.0045196786522865295, kl_loss = 0.0012351833283901215\n",
      "\n",
      "Epoch 394\n",
      "Step 0: loss = 0.10295584797859192, recon_loss = 0.09821085631847382, 0.004677880089730024, kl_loss = 0.00335569866001606\n",
      "\n",
      "Epoch 395\n",
      "Step 0: loss = 0.10590217262506485, recon_loss = 0.09916691482067108, 0.006705292500555515, kl_loss = 0.0014984318986535072\n",
      "\n",
      "Epoch 396\n",
      "Step 0: loss = 0.09948379546403885, recon_loss = 0.09418182075023651, 0.005258219316601753, kl_loss = 0.0021878806874156\n",
      "\n",
      "Epoch 397\n",
      "Step 0: loss = 0.10473024845123291, recon_loss = 0.09999677538871765, 0.00467823026701808, kl_loss = 0.0027621565386652946\n",
      "\n",
      "Epoch 398\n",
      "Step 0: loss = 0.09909896552562714, recon_loss = 0.09509783983230591, 0.003965959884226322, kl_loss = 0.001758369617164135\n",
      "\n",
      "Epoch 399\n",
      "Step 0: loss = 0.10095865279436111, recon_loss = 0.09638030081987381, 0.004551242105662823, kl_loss = 0.0013555549085140228\n",
      "\n",
      "Epoch 400\n",
      "Step 0: loss = 0.10366930067539215, recon_loss = 0.09725676476955414, 0.006375819910317659, kl_loss = 0.0018357262015342712\n",
      "\n",
      "Epoch 401\n",
      "Step 0: loss = 0.09576431661844254, recon_loss = 0.0901557058095932, 0.005581969395279884, kl_loss = 0.0013321815058588982\n",
      "\n",
      "Epoch 402\n",
      "Step 0: loss = 0.10238940268754959, recon_loss = 0.09670639783143997, 0.005655328743159771, kl_loss = 0.0013837683945894241\n",
      "\n",
      "Epoch 403\n",
      "Step 0: loss = 0.09663888812065125, recon_loss = 0.09084770083427429, 0.005760119296610355, kl_loss = 0.0015534739941358566\n",
      "\n",
      "Epoch 404\n",
      "Step 0: loss = 0.09793955087661743, recon_loss = 0.09339268505573273, 0.0045255874283611774, kl_loss = 0.0010638488456606865\n",
      "\n",
      "Epoch 405\n",
      "Step 0: loss = 0.09692677110433578, recon_loss = 0.0920182466506958, 0.004892040975391865, kl_loss = 0.0008241701871156693\n",
      "\n",
      "Epoch 406\n",
      "Step 0: loss = 0.10043676942586899, recon_loss = 0.09339455515146255, 0.0070127760991454124, kl_loss = 0.0014717811718583107\n",
      "\n",
      "Epoch 407\n",
      "Step 0: loss = 0.1001615971326828, recon_loss = 0.09532241523265839, 0.004794546868652105, kl_loss = 0.0022317059338092804\n",
      "\n",
      "Epoch 408\n",
      "Step 0: loss = 0.10353387892246246, recon_loss = 0.09855479747056961, 0.004959431476891041, kl_loss = 0.0009823916479945183\n",
      "\n",
      "Epoch 409\n",
      "Step 0: loss = 0.10076577961444855, recon_loss = 0.09514083713293076, 0.005586792249232531, kl_loss = 0.001907462254166603\n",
      "\n",
      "Epoch 410\n",
      "Step 0: loss = 0.10186562687158585, recon_loss = 0.0961710587143898, 0.005657850299030542, kl_loss = 0.0018356656655669212\n",
      "\n",
      "Epoch 411\n",
      "Step 0: loss = 0.09552588313817978, recon_loss = 0.0903504490852356, 0.005148603115230799, kl_loss = 0.0013413066044449806\n",
      "\n",
      "Epoch 412\n",
      "Step 0: loss = 0.09345157444477081, recon_loss = 0.0887230634689331, 0.004682027269154787, kl_loss = 0.002324373461306095\n",
      "\n",
      "Epoch 413\n",
      "Step 0: loss = 0.09489990025758743, recon_loss = 0.0900784581899643, 0.004780714400112629, kl_loss = 0.0020361831411719322\n",
      "\n",
      "Epoch 414\n",
      "Step 0: loss = 0.09548704326152802, recon_loss = 0.09003846347332001, 0.005419193767011166, kl_loss = 0.0014690719544887543\n",
      "\n",
      "Epoch 415\n",
      "Step 0: loss = 0.09798213094472885, recon_loss = 0.09246855974197388, 0.005473828874528408, kl_loss = 0.0019869860261678696\n",
      "\n",
      "Epoch 416\n",
      "Step 0: loss = 0.1006854698061943, recon_loss = 0.09464900195598602, 0.0059862397611141205, kl_loss = 0.0025113411247730255\n",
      "\n",
      "Epoch 417\n",
      "Step 0: loss = 0.10297419130802155, recon_loss = 0.09736994653940201, 0.0055555421859025955, kl_loss = 0.002435203641653061\n",
      "\n",
      "Epoch 418\n",
      "Step 0: loss = 0.09504368901252747, recon_loss = 0.08998338878154755, 0.005024716258049011, kl_loss = 0.0017792647704482079\n",
      "\n",
      "Epoch 419\n",
      "Step 0: loss = 0.09573213756084442, recon_loss = 0.0895906388759613, 0.006091964431107044, kl_loss = 0.002476431429386139\n",
      "\n",
      "Epoch 420\n",
      "Step 0: loss = 0.09659790247678757, recon_loss = 0.09025707840919495, 0.006306026596575975, kl_loss = 0.0017396854236721992\n",
      "\n",
      "Epoch 421\n",
      "Step 0: loss = 0.09699978679418564, recon_loss = 0.09205830842256546, 0.0049093347042799, kl_loss = 0.0016069328412413597\n",
      "\n",
      "Epoch 422\n",
      "Step 0: loss = 0.09190607815980911, recon_loss = 0.08612994849681854, 0.005752845201641321, kl_loss = 0.001163979060947895\n",
      "\n",
      "Epoch 423\n",
      "Step 0: loss = 0.0935124009847641, recon_loss = 0.08792857825756073, 0.005555214360356331, kl_loss = 0.0014305515214800835\n",
      "\n",
      "Epoch 424\n",
      "Step 0: loss = 0.09714719653129578, recon_loss = 0.09008027613162994, 0.007044609636068344, kl_loss = 0.0011154105886816978\n",
      "\n",
      "Epoch 425\n",
      "Step 0: loss = 0.0972437858581543, recon_loss = 0.09177974611520767, 0.005434813443571329, kl_loss = 0.0014614034444093704\n",
      "\n",
      "Epoch 426\n",
      "Step 0: loss = 0.09799124300479889, recon_loss = 0.09092147648334503, 0.007038527633994818, kl_loss = 0.0015621054917573929\n",
      "\n",
      "Epoch 427\n",
      "Step 0: loss = 0.0955108180642128, recon_loss = 0.09007470309734344, 0.00540277361869812, kl_loss = 0.0016670292243361473\n",
      "\n",
      "Epoch 428\n",
      "Step 0: loss = 0.0969943106174469, recon_loss = 0.09098458290100098, 0.005986960604786873, kl_loss = 0.0011382931843400002\n",
      "\n",
      "Epoch 429\n",
      "Step 0: loss = 0.09592942148447037, recon_loss = 0.08954255282878876, 0.006355513818562031, kl_loss = 0.0015675975009799004\n",
      "\n",
      "Epoch 430\n",
      "Step 0: loss = 0.09866093844175339, recon_loss = 0.09149272739887238, 0.007112536579370499, kl_loss = 0.0027837343513965607\n",
      "\n",
      "Epoch 431\n",
      "Step 0: loss = 0.0969560518860817, recon_loss = 0.09039928764104843, 0.006507723592221737, kl_loss = 0.0024520186707377434\n",
      "\n",
      "Epoch 432\n",
      "Step 0: loss = 0.09400865435600281, recon_loss = 0.08913975954055786, 0.0048158736899495125, kl_loss = 0.002650911919772625\n",
      "\n",
      "Epoch 433\n",
      "Step 0: loss = 0.09799762070178986, recon_loss = 0.09005364775657654, 0.007910683751106262, kl_loss = 0.0016642836853861809\n",
      "\n",
      "Epoch 434\n",
      "Step 0: loss = 0.09283215552568436, recon_loss = 0.08728480339050293, 0.0055204699747264385, kl_loss = 0.0013441340997815132\n",
      "\n",
      "Epoch 435\n",
      "Step 0: loss = 0.0945601686835289, recon_loss = 0.08860722929239273, 0.0059180790558457375, kl_loss = 0.0017429227009415627\n",
      "\n",
      "Epoch 436\n",
      "Step 0: loss = 0.09340112656354904, recon_loss = 0.08537082374095917, 0.007995856925845146, kl_loss = 0.0017221439629793167\n",
      "\n",
      "Epoch 437\n",
      "Step 0: loss = 0.09261760115623474, recon_loss = 0.08848311007022858, 0.004106379114091396, kl_loss = 0.0014054551720619202\n",
      "\n",
      "Epoch 438\n",
      "Step 0: loss = 0.09163199365139008, recon_loss = 0.08778443932533264, 0.0038288477808237076, kl_loss = 0.0009355535730719566\n",
      "\n",
      "Epoch 439\n",
      "Step 0: loss = 0.09337925910949707, recon_loss = 0.08860364556312561, 0.0047432975843548775, kl_loss = 0.001615620218217373\n",
      "\n",
      "Epoch 440\n",
      "Step 0: loss = 0.09491650015115738, recon_loss = 0.08838790655136108, 0.006457879673689604, kl_loss = 0.003535578027367592\n",
      "\n",
      "Epoch 441\n",
      "Step 0: loss = 0.09392847120761871, recon_loss = 0.08730854839086533, 0.0065819500014185905, kl_loss = 0.001898912712931633\n",
      "\n",
      "Epoch 442\n",
      "Step 0: loss = 0.09252665936946869, recon_loss = 0.0866856500506401, 0.005805063992738724, kl_loss = 0.001797206699848175\n",
      "\n",
      "Epoch 443\n",
      "Step 0: loss = 0.09354028105735779, recon_loss = 0.08715450763702393, 0.0063424077816307545, kl_loss = 0.002168132923543453\n",
      "\n",
      "Epoch 444\n",
      "Step 0: loss = 0.095447838306427, recon_loss = 0.08804985880851746, 0.007361860945820808, kl_loss = 0.0018061185255646706\n",
      "\n",
      "Epoch 445\n",
      "Step 0: loss = 0.09375045448541641, recon_loss = 0.0876222550868988, 0.006098951678723097, kl_loss = 0.0014625247567892075\n",
      "\n",
      "Epoch 446\n",
      "Step 0: loss = 0.08828797936439514, recon_loss = 0.08244998008012772, 0.0058191800490021706, kl_loss = 0.0009408248588442802\n",
      "\n",
      "Epoch 447\n",
      "Step 0: loss = 0.09383101761341095, recon_loss = 0.08867165446281433, 0.005117220804095268, kl_loss = 0.0021071303635835648\n",
      "\n",
      "Epoch 448\n",
      "Step 0: loss = 0.09663192927837372, recon_loss = 0.08998585492372513, 0.0066270651295781136, kl_loss = 0.0009502321481704712\n",
      "\n",
      "Epoch 449\n",
      "Step 0: loss = 0.08792222291231155, recon_loss = 0.08180499076843262, 0.006079098209738731, kl_loss = 0.00190659798681736\n",
      "\n",
      "Epoch 450\n",
      "Step 0: loss = 0.09292641282081604, recon_loss = 0.08770065009593964, 0.005191545933485031, kl_loss = 0.0017108339816331863\n",
      "\n",
      "Epoch 451\n",
      "Step 0: loss = 0.09074685722589493, recon_loss = 0.0842994675040245, 0.006412399001419544, kl_loss = 0.001749388873577118\n",
      "\n",
      "Epoch 452\n",
      "Step 0: loss = 0.09299765527248383, recon_loss = 0.08642595261335373, 0.006523317191749811, kl_loss = 0.002419363707304001\n",
      "\n",
      "Epoch 453\n",
      "Step 0: loss = 0.09242334216833115, recon_loss = 0.08818712085485458, 0.0042116777040064335, kl_loss = 0.0012271273881196976\n",
      "\n",
      "Epoch 454\n",
      "Step 0: loss = 0.09038562327623367, recon_loss = 0.08597591519355774, 0.004385996609926224, kl_loss = 0.0011857831850647926\n",
      "\n",
      "Epoch 455\n",
      "Step 0: loss = 0.09170567989349365, recon_loss = 0.08616708219051361, 0.005514142103493214, kl_loss = 0.0012228190898895264\n",
      "\n",
      "Epoch 456\n",
      "Step 0: loss = 0.09530465304851532, recon_loss = 0.08751602470874786, 0.007766456808894873, kl_loss = 0.001108814962208271\n",
      "\n",
      "Epoch 457\n",
      "Step 0: loss = 0.09151852130889893, recon_loss = 0.08500957489013672, 0.00647064670920372, kl_loss = 0.0019146399572491646\n",
      "\n",
      "Epoch 458\n",
      "Step 0: loss = 0.09239381551742554, recon_loss = 0.08396346867084503, 0.008381707593798637, kl_loss = 0.0024319766089320183\n",
      "\n",
      "Epoch 459\n",
      "Step 0: loss = 0.09180034697055817, recon_loss = 0.08639641106128693, 0.005360564216971397, kl_loss = 0.002168586477637291\n",
      "\n",
      "Epoch 460\n",
      "Step 0: loss = 0.09459895640611649, recon_loss = 0.08617614209651947, 0.008369112387299538, kl_loss = 0.002685200423002243\n",
      "\n",
      "Epoch 461\n",
      "Step 0: loss = 0.0906769335269928, recon_loss = 0.08492054045200348, 0.005722985137254, kl_loss = 0.0016702907159924507\n",
      "\n",
      "Epoch 462\n",
      "Step 0: loss = 0.09496133774518967, recon_loss = 0.08846147358417511, 0.0064719608053565025, kl_loss = 0.0013950644060969353\n",
      "\n",
      "Epoch 463\n",
      "Step 0: loss = 0.09485676139593124, recon_loss = 0.08753424882888794, 0.007292670197784901, kl_loss = 0.0014919638633728027\n",
      "\n",
      "Epoch 464\n",
      "Step 0: loss = 0.0890246033668518, recon_loss = 0.08169662952423096, 0.007278705947101116, kl_loss = 0.0024636434391140938\n",
      "\n",
      "Epoch 465\n",
      "Step 0: loss = 0.09334533661603928, recon_loss = 0.08506973832845688, 0.008245717734098434, kl_loss = 0.0014941850677132607\n",
      "\n",
      "Epoch 466\n",
      "Step 0: loss = 0.08747262507677078, recon_loss = 0.08207046985626221, 0.005382393021136522, kl_loss = 0.000988079234957695\n",
      "\n",
      "Epoch 467\n",
      "Step 0: loss = 0.08705481886863708, recon_loss = 0.08102886378765106, 0.0059938738122582436, kl_loss = 0.0016039302572607994\n",
      "\n",
      "Epoch 468\n",
      "Step 0: loss = 0.08488437533378601, recon_loss = 0.07761819660663605, 0.0072343843057751656, kl_loss = 0.0015896661207079887\n",
      "\n",
      "Epoch 469\n",
      "Step 0: loss = 0.08819342404603958, recon_loss = 0.08252517133951187, 0.005643532611429691, kl_loss = 0.0012359702959656715\n",
      "\n",
      "Epoch 470\n",
      "Step 0: loss = 0.09397812187671661, recon_loss = 0.08768603950738907, 0.006265981122851372, kl_loss = 0.0013050297275185585\n",
      "\n",
      "Epoch 471\n",
      "Step 0: loss = 0.09114125370979309, recon_loss = 0.08420335501432419, 0.006910651922225952, kl_loss = 0.001362362876534462\n",
      "\n",
      "Epoch 472\n",
      "Step 0: loss = 0.09012564271688461, recon_loss = 0.08286064118146896, 0.0072381459176540375, kl_loss = 0.0013431087136268616\n",
      "\n",
      "Epoch 473\n",
      "Step 0: loss = 0.08851581811904907, recon_loss = 0.08193355053663254, 0.006545598618686199, kl_loss = 0.0018335683271288872\n",
      "\n",
      "Epoch 474\n",
      "Step 0: loss = 0.09098448604345322, recon_loss = 0.08268161118030548, 0.008253907784819603, kl_loss = 0.0024484163150191307\n",
      "\n",
      "Epoch 475\n",
      "Step 0: loss = 0.09115353971719742, recon_loss = 0.08422033488750458, 0.006904808338731527, kl_loss = 0.0014196280390024185\n",
      "\n",
      "Epoch 476\n",
      "Step 0: loss = 0.08700019121170044, recon_loss = 0.08112098276615143, 0.005848895758390427, kl_loss = 0.0015152748674154282\n",
      "\n",
      "Epoch 477\n",
      "Step 0: loss = 0.09228314459323883, recon_loss = 0.08463037014007568, 0.0076297977939248085, kl_loss = 0.001148708164691925\n",
      "\n",
      "Epoch 478\n",
      "Step 0: loss = 0.08751377463340759, recon_loss = 0.07988040894269943, 0.007577553391456604, kl_loss = 0.0027906009927392006\n",
      "\n",
      "Epoch 479\n",
      "Step 0: loss = 0.0861039087176323, recon_loss = 0.07854419201612473, 0.007498645689338446, kl_loss = 0.003053623251616955\n",
      "\n",
      "Epoch 480\n",
      "Step 0: loss = 0.08746833354234695, recon_loss = 0.08067779988050461, 0.006747817620635033, kl_loss = 0.0021358439698815346\n",
      "\n",
      "Epoch 481\n",
      "Step 0: loss = 0.08776755630970001, recon_loss = 0.08124883472919464, 0.006476301234215498, kl_loss = 0.002121151424944401\n",
      "\n",
      "Epoch 482\n",
      "Step 0: loss = 0.08720164746046066, recon_loss = 0.07959847897291183, 0.007579498458653688, kl_loss = 0.0011835182085633278\n",
      "\n",
      "Epoch 483\n",
      "Step 0: loss = 0.08180883526802063, recon_loss = 0.07500466704368591, 0.006763810757547617, kl_loss = 0.0020179254934191704\n",
      "\n",
      "Epoch 484\n",
      "Step 0: loss = 0.08255452662706375, recon_loss = 0.07652325928211212, 0.005989027209579945, kl_loss = 0.0021120402961969376\n",
      "\n",
      "Epoch 485\n",
      "Step 0: loss = 0.08590741455554962, recon_loss = 0.07911378145217896, 0.006777267903089523, kl_loss = 0.0008180746808648109\n",
      "\n",
      "Epoch 486\n",
      "Step 0: loss = 0.09037172794342041, recon_loss = 0.08280022442340851, 0.007553969509899616, kl_loss = 0.0008763866499066353\n",
      "\n",
      "Epoch 487\n",
      "Step 0: loss = 0.08507701754570007, recon_loss = 0.07859861105680466, 0.006460833363234997, kl_loss = 0.0008787736296653748\n",
      "\n",
      "Epoch 488\n",
      "Step 0: loss = 0.08326926827430725, recon_loss = 0.0747993141412735, 0.008441122248768806, kl_loss = 0.0014417273923754692\n",
      "\n",
      "Epoch 489\n",
      "Step 0: loss = 0.0847371518611908, recon_loss = 0.07793521881103516, 0.006772918626666069, kl_loss = 0.0014507705345749855\n",
      "\n",
      "Epoch 490\n",
      "Step 0: loss = 0.0816664919257164, recon_loss = 0.07618275284767151, 0.00545712374150753, kl_loss = 0.0013305991888046265\n",
      "\n",
      "Epoch 491\n",
      "Step 0: loss = 0.08183861523866653, recon_loss = 0.07461193203926086, 0.00720305647701025, kl_loss = 0.0011814292520284653\n",
      "\n",
      "Epoch 492\n",
      "Step 0: loss = 0.08084860444068909, recon_loss = 0.0735948383808136, 0.0072275251150131226, kl_loss = 0.001312103122472763\n",
      "\n",
      "Epoch 493\n",
      "Step 0: loss = 0.08513013273477554, recon_loss = 0.07817418873310089, 0.006932600401341915, kl_loss = 0.001167074777185917\n",
      "\n",
      "Epoch 494\n",
      "Step 0: loss = 0.08070605993270874, recon_loss = 0.07630456984043121, 0.00436055101454258, kl_loss = 0.0020471159368753433\n",
      "\n",
      "Epoch 495\n",
      "Step 0: loss = 0.08653514087200165, recon_loss = 0.07522159814834595, 0.011287584900856018, kl_loss = 0.00129783246666193\n",
      "\n",
      "Epoch 496\n",
      "Step 0: loss = 0.08306881785392761, recon_loss = 0.07540213316679001, 0.007627149112522602, kl_loss = 0.001976683735847473\n",
      "\n",
      "Epoch 497\n",
      "Step 0: loss = 0.08717966079711914, recon_loss = 0.07998190075159073, 0.007157275918871164, kl_loss = 0.0020241495221853256\n",
      "\n",
      "Epoch 498\n",
      "Step 0: loss = 0.077067531645298, recon_loss = 0.07227348536252975, 0.004761083051562309, kl_loss = 0.001648196019232273\n",
      "\n",
      "Epoch 499\n",
      "Step 0: loss = 0.08623718470335007, recon_loss = 0.07778657227754593, 0.008434122428297997, kl_loss = 0.000824236311018467\n",
      "\n",
      "Epoch 500\n",
      "Step 0: loss = 0.08297614753246307, recon_loss = 0.07409913092851639, 0.008851706981658936, kl_loss = 0.0012655109167099\n",
      "\n",
      "Epoch 501\n",
      "Step 0: loss = 0.08372955024242401, recon_loss = 0.07571959495544434, 0.0079796826466918, kl_loss = 0.001513681374490261\n",
      "\n",
      "Epoch 502\n",
      "Step 0: loss = 0.0774431824684143, recon_loss = 0.06911541521549225, 0.00828147679567337, kl_loss = 0.0023145638406276703\n",
      "\n",
      "Epoch 503\n",
      "Step 0: loss = 0.08148808777332306, recon_loss = 0.0728742852807045, 0.00857771560549736, kl_loss = 0.0018045473843812943\n",
      "\n",
      "Epoch 504\n",
      "Step 0: loss = 0.07972382754087448, recon_loss = 0.07247085869312286, 0.007228541187942028, kl_loss = 0.001221400685608387\n",
      "\n",
      "Epoch 505\n",
      "Step 0: loss = 0.08093974739313126, recon_loss = 0.07355982065200806, 0.007360643707215786, kl_loss = 0.0009639263153076172\n",
      "\n",
      "Epoch 506\n",
      "Step 0: loss = 0.07584302872419357, recon_loss = 0.06725271791219711, 0.008554067462682724, kl_loss = 0.001812276430428028\n",
      "\n",
      "Epoch 507\n",
      "Step 0: loss = 0.08061578124761581, recon_loss = 0.07320647686719894, 0.007372046820819378, kl_loss = 0.0018629077821969986\n",
      "\n",
      "Epoch 508\n",
      "Step 0: loss = 0.0795716866850853, recon_loss = 0.07123447209596634, 0.008306075818836689, kl_loss = 0.0015566516667604446\n",
      "\n",
      "Epoch 509\n",
      "Step 0: loss = 0.08275151252746582, recon_loss = 0.07260474562644958, 0.010101576335728168, kl_loss = 0.002259347587823868\n",
      "\n",
      "Epoch 510\n",
      "Step 0: loss = 0.08264683187007904, recon_loss = 0.07667070627212524, 0.005945761688053608, kl_loss = 0.0015181517228484154\n",
      "\n",
      "Epoch 511\n",
      "Step 0: loss = 0.08234705030918121, recon_loss = 0.07620048522949219, 0.006110569927841425, kl_loss = 0.0017997203394770622\n",
      "\n",
      "Epoch 512\n",
      "Step 0: loss = 0.08478735387325287, recon_loss = 0.07296280562877655, 0.011778638698160648, kl_loss = 0.0022955453023314476\n",
      "\n",
      "Epoch 513\n",
      "Step 0: loss = 0.08074863255023956, recon_loss = 0.07155527174472809, 0.009152205660939217, kl_loss = 0.002057998441159725\n",
      "\n",
      "Epoch 514\n",
      "Step 0: loss = 0.077671118080616, recon_loss = 0.07126343995332718, 0.0063760243356227875, kl_loss = 0.001582893542945385\n",
      "\n",
      "Epoch 515\n",
      "Step 0: loss = 0.08321823179721832, recon_loss = 0.0743388831615448, 0.008833006024360657, kl_loss = 0.0023170653730630875\n",
      "\n",
      "Epoch 516\n",
      "Step 0: loss = 0.08194137364625931, recon_loss = 0.07384119182825089, 0.00807125959545374, kl_loss = 0.0014462955296039581\n",
      "\n",
      "Epoch 517\n",
      "Step 0: loss = 0.08002069592475891, recon_loss = 0.07007932662963867, 0.00991440936923027, kl_loss = 0.0013477765023708344\n",
      "\n",
      "Epoch 518\n",
      "Step 0: loss = 0.07773946225643158, recon_loss = 0.07113292813301086, 0.006586183328181505, kl_loss = 0.0010173320770263672\n",
      "\n",
      "Epoch 519\n",
      "Step 0: loss = 0.0788651779294014, recon_loss = 0.07199402898550034, 0.006847257260233164, kl_loss = 0.0011945897713303566\n",
      "\n",
      "Epoch 520\n",
      "Step 0: loss = 0.07680411636829376, recon_loss = 0.06879490613937378, 0.007984308525919914, kl_loss = 0.0012449752539396286\n",
      "\n",
      "Epoch 521\n",
      "Step 0: loss = 0.08013004064559937, recon_loss = 0.07120378315448761, 0.008878186345100403, kl_loss = 0.002403685823082924\n",
      "\n",
      "Epoch 522\n",
      "Step 0: loss = 0.07325534522533417, recon_loss = 0.06625671684741974, 0.0069607924669981, kl_loss = 0.00189162977039814\n",
      "\n",
      "Epoch 523\n",
      "Step 0: loss = 0.0803007036447525, recon_loss = 0.07004214823246002, 0.010212603956460953, kl_loss = 0.0022977180778980255\n",
      "\n",
      "Epoch 524\n",
      "Step 0: loss = 0.08357945829629898, recon_loss = 0.0767659842967987, 0.0067596654407680035, kl_loss = 0.0026902686804533005\n",
      "\n",
      "Epoch 525\n",
      "Step 0: loss = 0.07187044620513916, recon_loss = 0.06409332156181335, 0.007757800631225109, kl_loss = 0.0009662890806794167\n",
      "\n",
      "Epoch 526\n",
      "Step 0: loss = 0.07566900551319122, recon_loss = 0.06792213767766953, 0.007717692758888006, kl_loss = 0.0014588488265872002\n",
      "\n",
      "Epoch 527\n",
      "Step 0: loss = 0.07552723586559296, recon_loss = 0.06883618235588074, 0.006640366744250059, kl_loss = 0.0025344956666231155\n",
      "\n",
      "Epoch 528\n",
      "Step 0: loss = 0.07693184912204742, recon_loss = 0.0683942660689354, 0.008512996137142181, kl_loss = 0.0012294361367821693\n",
      "\n",
      "Epoch 529\n",
      "Step 0: loss = 0.0812561884522438, recon_loss = 0.07061033695936203, 0.010618265718221664, kl_loss = 0.0013795476406812668\n",
      "\n",
      "Epoch 530\n",
      "Step 0: loss = 0.07291611284017563, recon_loss = 0.06644725054502487, 0.0064384667202830315, kl_loss = 0.0015197806060314178\n",
      "\n",
      "Epoch 531\n",
      "Step 0: loss = 0.07733865827322006, recon_loss = 0.06868595629930496, 0.008628898300230503, kl_loss = 0.0011903680860996246\n",
      "\n",
      "Epoch 532\n",
      "Step 0: loss = 0.07551374286413193, recon_loss = 0.06614314764738083, 0.00933933351188898, kl_loss = 0.0015630107372999191\n",
      "\n",
      "Epoch 533\n",
      "Step 0: loss = 0.07366039603948593, recon_loss = 0.06435626745223999, 0.009269459173083305, kl_loss = 0.0017335163429379463\n",
      "\n",
      "Epoch 534\n",
      "Step 0: loss = 0.07709968090057373, recon_loss = 0.06899851560592651, 0.008068501017987728, kl_loss = 0.0016332212835550308\n",
      "\n",
      "Epoch 535\n",
      "Step 0: loss = 0.07377946376800537, recon_loss = 0.0649198666214943, 0.008814675733447075, kl_loss = 0.0022460883483290672\n",
      "\n",
      "Epoch 536\n",
      "Step 0: loss = 0.0782528817653656, recon_loss = 0.07009617984294891, 0.008124480955302715, kl_loss = 0.001611039973795414\n",
      "\n",
      "Epoch 537\n",
      "Step 0: loss = 0.07845082134008408, recon_loss = 0.06802476942539215, 0.010397039353847504, kl_loss = 0.001450762152671814\n",
      "\n",
      "Epoch 538\n",
      "Step 0: loss = 0.0763789489865303, recon_loss = 0.06778015196323395, 0.008562760427594185, kl_loss = 0.001801948994398117\n",
      "\n",
      "Epoch 539\n",
      "Step 0: loss = 0.07199978083372116, recon_loss = 0.06306042522192001, 0.008924676105380058, kl_loss = 0.0007339799776673317\n",
      "\n",
      "Epoch 540\n",
      "Step 0: loss = 0.0771569237112999, recon_loss = 0.06850045919418335, 0.008623859845101833, kl_loss = 0.0016302689909934998\n",
      "\n",
      "Epoch 541\n",
      "Step 0: loss = 0.07434025406837463, recon_loss = 0.064883753657341, 0.009431569837033749, kl_loss = 0.0012463582679629326\n",
      "\n",
      "Epoch 542\n",
      "Step 0: loss = 0.07310047000646591, recon_loss = 0.0658382847905159, 0.007230998482555151, kl_loss = 0.001559312455356121\n",
      "\n",
      "Epoch 543\n",
      "Step 0: loss = 0.07548293471336365, recon_loss = 0.06547384709119797, 0.009952671825885773, kl_loss = 0.002820659428834915\n",
      "\n",
      "Epoch 544\n",
      "Step 0: loss = 0.07860815525054932, recon_loss = 0.06710013002157211, 0.011468588374555111, kl_loss = 0.0019716834649443626\n",
      "\n",
      "Epoch 545\n",
      "Step 0: loss = 0.07245597243309021, recon_loss = 0.06364182382822037, 0.008760510012507439, kl_loss = 0.0026818029582500458\n",
      "\n",
      "Epoch 546\n",
      "Step 0: loss = 0.07944883406162262, recon_loss = 0.07037164270877838, 0.009037915617227554, kl_loss = 0.0019639795646071434\n",
      "\n",
      "Epoch 547\n",
      "Step 0: loss = 0.08209295570850372, recon_loss = 0.07070838660001755, 0.011337239295244217, kl_loss = 0.0023663751780986786\n",
      "\n",
      "Epoch 548\n",
      "Step 0: loss = 0.0734526589512825, recon_loss = 0.06498722732067108, 0.008426893502473831, kl_loss = 0.0019270600751042366\n",
      "\n",
      "Epoch 549\n",
      "Step 0: loss = 0.06952410936355591, recon_loss = 0.06123925745487213, 0.008251995779573917, kl_loss = 0.0016427570953965187\n",
      "\n",
      "Epoch 550\n",
      "Step 0: loss = 0.075688935816288, recon_loss = 0.06684494763612747, 0.00881427712738514, kl_loss = 0.001485474407672882\n",
      "\n",
      "Epoch 551\n",
      "Step 0: loss = 0.07035340368747711, recon_loss = 0.06281296908855438, 0.0074976468458771706, kl_loss = 0.0021395105868577957\n",
      "\n",
      "Epoch 552\n",
      "Step 0: loss = 0.07084213942289352, recon_loss = 0.0629073828458786, 0.007905768230557442, kl_loss = 0.001449459232389927\n",
      "\n",
      "Epoch 553\n",
      "Step 0: loss = 0.06874540448188782, recon_loss = 0.060574378818273544, 0.008132009766995907, kl_loss = 0.0019510183483362198\n",
      "\n",
      "Epoch 554\n",
      "Step 0: loss = 0.07088980823755264, recon_loss = 0.06195472925901413, 0.008856011554598808, kl_loss = 0.003953424282371998\n",
      "\n",
      "Epoch 555\n",
      "Step 0: loss = 0.0773734524846077, recon_loss = 0.06723374128341675, 0.010084295645356178, kl_loss = 0.0027706995606422424\n",
      "\n",
      "Epoch 556\n",
      "Step 0: loss = 0.07153390347957611, recon_loss = 0.06048118323087692, 0.010988469235599041, kl_loss = 0.0032127704471349716\n",
      "\n",
      "Epoch 557\n",
      "Step 0: loss = 0.07112331688404083, recon_loss = 0.06001929193735123, 0.011065291240811348, kl_loss = 0.0019366787746548653\n",
      "\n",
      "Epoch 558\n",
      "Step 0: loss = 0.06896640360355377, recon_loss = 0.0623975545167923, 0.00653995294123888, kl_loss = 0.0014447029680013657\n",
      "\n",
      "Epoch 559\n",
      "Step 0: loss = 0.07339216768741608, recon_loss = 0.06077132374048233, 0.012570254504680634, kl_loss = 0.002529548481106758\n",
      "\n",
      "Epoch 560\n",
      "Step 0: loss = 0.06786975264549255, recon_loss = 0.05829799920320511, 0.009531699120998383, kl_loss = 0.0020025335252285004\n",
      "\n",
      "Epoch 561\n",
      "Step 0: loss = 0.07159142941236496, recon_loss = 0.06137007847428322, 0.010175460018217564, kl_loss = 0.002294314093887806\n",
      "\n",
      "Epoch 562\n",
      "Step 0: loss = 0.06707470118999481, recon_loss = 0.05946112424135208, 0.0075893038883805275, kl_loss = 0.001213681884109974\n",
      "\n",
      "Epoch 563\n",
      "Step 0: loss = 0.07058518379926682, recon_loss = 0.0621139332652092, 0.008436181582510471, kl_loss = 0.0017535993829369545\n",
      "\n",
      "Epoch 564\n",
      "Step 0: loss = 0.06991808861494064, recon_loss = 0.05805078148841858, 0.01183690968900919, kl_loss = 0.0015200255438685417\n",
      "\n",
      "Epoch 565\n",
      "Step 0: loss = 0.06555890291929245, recon_loss = 0.05746563524007797, 0.008063762448728085, kl_loss = 0.0014751143753528595\n",
      "\n",
      "Epoch 566\n",
      "Step 0: loss = 0.06483369320631027, recon_loss = 0.05497857183218002, 0.00981291476637125, kl_loss = 0.0021102866157889366\n",
      "\n",
      "Epoch 567\n",
      "Step 0: loss = 0.06719634681940079, recon_loss = 0.058103565126657486, 0.009060661308467388, kl_loss = 0.0016059279441833496\n",
      "\n",
      "Epoch 568\n",
      "Step 0: loss = 0.06723056733608246, recon_loss = 0.05708713084459305, 0.010084155946969986, kl_loss = 0.0029636630788445473\n",
      "\n",
      "Epoch 569\n",
      "Step 0: loss = 0.06830698251724243, recon_loss = 0.056809715926647186, 0.01145957037806511, kl_loss = 0.0018851244822144508\n",
      "\n",
      "Epoch 570\n",
      "Step 0: loss = 0.07018708437681198, recon_loss = 0.06031721085309982, 0.009814714081585407, kl_loss = 0.002757822163403034\n",
      "\n",
      "Epoch 571\n",
      "Step 0: loss = 0.07102206349372864, recon_loss = 0.060854170471429825, 0.01009603962302208, kl_loss = 0.003592836670577526\n",
      "\n",
      "Epoch 572\n",
      "Step 0: loss = 0.07509180903434753, recon_loss = 0.0609893798828125, 0.014046105556190014, kl_loss = 0.0028162384405732155\n",
      "\n",
      "Epoch 573\n",
      "Step 0: loss = 0.07158207148313522, recon_loss = 0.06170450896024704, 0.009800932370126247, kl_loss = 0.003831340931355953\n",
      "\n",
      "Epoch 574\n",
      "Step 0: loss = 0.06784344464540482, recon_loss = 0.060066286474466324, 0.007745010778307915, kl_loss = 0.001607278361916542\n",
      "\n",
      "Epoch 575\n",
      "Step 0: loss = 0.06217831000685692, recon_loss = 0.0548507422208786, 0.00728983711451292, kl_loss = 0.0018865140154957771\n",
      "\n",
      "Epoch 576\n",
      "Step 0: loss = 0.06334824860095978, recon_loss = 0.05427831783890724, 0.009029081091284752, kl_loss = 0.0020425114780664444\n",
      "\n",
      "Epoch 577\n",
      "Step 0: loss = 0.06636913120746613, recon_loss = 0.05670730769634247, 0.009640132077038288, kl_loss = 0.0010844748467206955\n",
      "\n",
      "Epoch 578\n",
      "Step 0: loss = 0.06456203013658524, recon_loss = 0.055734843015670776, 0.008803270757198334, kl_loss = 0.0011959858238697052\n",
      "\n",
      "Epoch 579\n",
      "Step 0: loss = 0.07076521962881088, recon_loss = 0.05791465938091278, 0.01282399520277977, kl_loss = 0.0013280976563692093\n",
      "\n",
      "Epoch 580\n",
      "Step 0: loss = 0.06963576376438141, recon_loss = 0.059494681656360626, 0.01005420833826065, kl_loss = 0.004343857988715172\n",
      "\n",
      "Epoch 581\n",
      "Step 0: loss = 0.060829706490039825, recon_loss = 0.05226067453622818, 0.008519588969647884, kl_loss = 0.0024721650406718254\n",
      "\n",
      "Epoch 582\n",
      "Step 0: loss = 0.0652952492237091, recon_loss = 0.05419852212071419, 0.011056777089834213, kl_loss = 0.0019973721355199814\n",
      "\n",
      "Epoch 583\n",
      "Step 0: loss = 0.07289238274097443, recon_loss = 0.056868262588977814, 0.01592545211315155, kl_loss = 0.0049333395436406136\n",
      "\n",
      "Epoch 584\n",
      "Step 0: loss = 0.06630215793848038, recon_loss = 0.053472138941287994, 0.012743609957396984, kl_loss = 0.0043206969276070595\n",
      "\n",
      "Epoch 585\n",
      "Step 0: loss = 0.07100522518157959, recon_loss = 0.05905180424451828, 0.011834985576570034, kl_loss = 0.005921776406466961\n",
      "\n",
      "Epoch 586\n",
      "Step 0: loss = 0.06909272819757462, recon_loss = 0.05840947479009628, 0.010608858428895473, kl_loss = 0.003719688393175602\n",
      "\n",
      "Epoch 587\n",
      "Step 0: loss = 0.06802887469530106, recon_loss = 0.05801135301589966, 0.009968699887394905, kl_loss = 0.0024413643404841423\n",
      "\n",
      "Epoch 588\n",
      "Step 0: loss = 0.06342244893312454, recon_loss = 0.05395866185426712, 0.009436704218387604, kl_loss = 0.0013539595529437065\n",
      "\n",
      "Epoch 589\n",
      "Step 0: loss = 0.057757120579481125, recon_loss = 0.05167798697948456, 0.006052738521248102, kl_loss = 0.001319599337875843\n",
      "\n",
      "Epoch 590\n",
      "Step 0: loss = 0.06429945677518845, recon_loss = 0.05316412076354027, 0.011110291816294193, kl_loss = 0.0012522386386990547\n",
      "\n",
      "Epoch 591\n",
      "Step 0: loss = 0.061988476663827896, recon_loss = 0.050613440573215485, 0.011296416632831097, kl_loss = 0.003930993378162384\n",
      "\n",
      "Epoch 592\n",
      "Step 0: loss = 0.06761318445205688, recon_loss = 0.052931513637304306, 0.014604207128286362, kl_loss = 0.0038732774555683136\n",
      "\n",
      "Epoch 593\n",
      "Step 0: loss = 0.06398903578519821, recon_loss = 0.054179929196834564, 0.00975845381617546, kl_loss = 0.002532806247472763\n",
      "\n",
      "Epoch 594\n",
      "Step 0: loss = 0.06403873860836029, recon_loss = 0.0520784854888916, 0.011887758038938046, kl_loss = 0.003624800592660904\n",
      "\n",
      "Epoch 595\n",
      "Step 0: loss = 0.06384338438510895, recon_loss = 0.05265381187200546, 0.01114191859960556, kl_loss = 0.002382810227572918\n",
      "\n",
      "Epoch 596\n",
      "Step 0: loss = 0.06880646198987961, recon_loss = 0.056510381400585175, 0.012248240411281586, kl_loss = 0.0023921681568026543\n",
      "\n",
      "Epoch 597\n",
      "Step 0: loss = 0.06625199317932129, recon_loss = 0.051692020148038864, 0.014519786462187767, kl_loss = 0.0020095473155379295\n",
      "\n",
      "Epoch 598\n",
      "Step 0: loss = 0.06262245029211044, recon_loss = 0.05125153064727783, 0.011331358924508095, kl_loss = 0.001977980136871338\n",
      "\n",
      "Epoch 599\n",
      "Step 0: loss = 0.060868825763463974, recon_loss = 0.05163513123989105, 0.009190166369080544, kl_loss = 0.0021765101701021194\n",
      "\n",
      "Epoch 600\n",
      "Step 0: loss = 0.058566778898239136, recon_loss = 0.049337103962898254, 0.009180056862533092, kl_loss = 0.002480865456163883\n",
      "\n",
      "Epoch 601\n",
      "Step 0: loss = 0.05863923951983452, recon_loss = 0.04885007068514824, 0.009756339713931084, kl_loss = 0.0016416171565651894\n",
      "\n",
      "Epoch 602\n",
      "Step 0: loss = 0.05874653160572052, recon_loss = 0.04729267954826355, 0.011436836794018745, kl_loss = 0.0008509093895554543\n",
      "\n",
      "Epoch 603\n",
      "Step 0: loss = 0.05804191902279854, recon_loss = 0.04770014435052872, 0.010293210856616497, kl_loss = 0.00242818146944046\n",
      "\n",
      "Epoch 604\n",
      "Step 0: loss = 0.0596013106405735, recon_loss = 0.049971845000982285, 0.009584151208400726, kl_loss = 0.0022657709196209908\n",
      "\n",
      "Epoch 605\n",
      "Step 0: loss = 0.05988669395446777, recon_loss = 0.0499088317155838, 0.009927701205015182, kl_loss = 0.00250812154263258\n",
      "\n",
      "Epoch 606\n",
      "Step 0: loss = 0.06425175070762634, recon_loss = 0.04751846194267273, 0.016679134219884872, kl_loss = 0.0027076248079538345\n",
      "\n",
      "Epoch 607\n",
      "Step 0: loss = 0.058485206216573715, recon_loss = 0.046454302966594696, 0.011988348327577114, kl_loss = 0.002127755433320999\n",
      "\n",
      "Epoch 608\n",
      "Step 0: loss = 0.059997476637363434, recon_loss = 0.048593707382678986, 0.011373765766620636, kl_loss = 0.0015001492574810982\n",
      "\n",
      "Epoch 609\n",
      "Step 0: loss = 0.0591137558221817, recon_loss = 0.04728173464536667, 0.011799998581409454, kl_loss = 0.0016012070700526237\n",
      "\n",
      "Epoch 610\n",
      "Step 0: loss = 0.05851123109459877, recon_loss = 0.04650957137346268, 0.011942511424422264, kl_loss = 0.002957399934530258\n",
      "\n",
      "Epoch 611\n",
      "Step 0: loss = 0.051181189715862274, recon_loss = 0.04297098517417908, 0.008185544051229954, kl_loss = 0.0012330086901783943\n",
      "\n",
      "Epoch 612\n",
      "Step 0: loss = 0.06188562139868736, recon_loss = 0.049654945731163025, 0.01218015979975462, kl_loss = 0.0025257673114538193\n",
      "\n",
      "Epoch 613\n",
      "Step 0: loss = 0.0625525563955307, recon_loss = 0.04866473004221916, 0.013859232887625694, kl_loss = 0.0014296500012278557\n",
      "\n",
      "Epoch 614\n",
      "Step 0: loss = 0.05542811378836632, recon_loss = 0.04587660729885101, 0.009508905000984669, kl_loss = 0.002130049280822277\n",
      "\n",
      "Epoch 615\n",
      "Step 0: loss = 0.05900972709059715, recon_loss = 0.04622869938611984, 0.012745355255901814, kl_loss = 0.0017836084589362144\n",
      "\n",
      "Epoch 616\n",
      "Step 0: loss = 0.0613364651799202, recon_loss = 0.049612339586019516, 0.011681306175887585, kl_loss = 0.0021409746259450912\n",
      "\n",
      "Epoch 617\n",
      "Step 0: loss = 0.05707292631268501, recon_loss = 0.045572079718112946, 0.011475113220512867, kl_loss = 0.0012867916375398636\n",
      "\n",
      "Epoch 618\n",
      "Step 0: loss = 0.05652754008769989, recon_loss = 0.04436290264129639, 0.012120823375880718, kl_loss = 0.0021906085312366486\n",
      "\n",
      "Epoch 619\n",
      "Step 0: loss = 0.052232999354600906, recon_loss = 0.04444872587919235, 0.007726416923105717, kl_loss = 0.002892959862947464\n",
      "\n",
      "Epoch 620\n",
      "Step 0: loss = 0.05517594888806343, recon_loss = 0.044405728578567505, 0.010721396654844284, kl_loss = 0.0024412041530013084\n",
      "\n",
      "Epoch 621\n",
      "Step 0: loss = 0.048656728118658066, recon_loss = 0.04051751270890236, 0.008079665713012218, kl_loss = 0.0029775043949484825\n",
      "\n",
      "Epoch 622\n",
      "Step 0: loss = 0.055060841143131256, recon_loss = 0.041424792259931564, 0.013605112209916115, kl_loss = 0.0015467209741473198\n",
      "\n",
      "Epoch 623\n",
      "Step 0: loss = 0.053008176386356354, recon_loss = 0.04002201557159424, 0.01291114091873169, kl_loss = 0.003751053474843502\n",
      "\n",
      "Epoch 624\n",
      "Step 0: loss = 0.053243488073349, recon_loss = 0.04233251512050629, 0.010856005363166332, kl_loss = 0.002748338505625725\n",
      "\n",
      "Epoch 625\n",
      "Step 0: loss = 0.0543665811419487, recon_loss = 0.044619232416152954, 0.009711850434541702, kl_loss = 0.0017750049009919167\n",
      "\n",
      "Epoch 626\n",
      "Step 0: loss = 0.05711846426129341, recon_loss = 0.04514522850513458, 0.01192893460392952, kl_loss = 0.002214999869465828\n",
      "\n",
      "Epoch 627\n",
      "Step 0: loss = 0.051347143948078156, recon_loss = 0.041501592844724655, 0.009802369400858879, kl_loss = 0.002159218303859234\n",
      "\n",
      "Epoch 628\n",
      "Step 0: loss = 0.05496467277407646, recon_loss = 0.04306270182132721, 0.011871693655848503, kl_loss = 0.0015138061717152596\n",
      "\n",
      "Epoch 629\n",
      "Step 0: loss = 0.05907493084669113, recon_loss = 0.044504571706056595, 0.014551990665495396, kl_loss = 0.0009185401722788811\n",
      "\n",
      "Epoch 630\n",
      "Step 0: loss = 0.053613871335983276, recon_loss = 0.04223112016916275, 0.011320602148771286, kl_loss = 0.0031074825674295425\n",
      "\n",
      "Epoch 631\n",
      "Step 0: loss = 0.05171108618378639, recon_loss = 0.03864503651857376, 0.013024738058447838, kl_loss = 0.002065543085336685\n",
      "\n",
      "Epoch 632\n",
      "Step 0: loss = 0.052769050002098083, recon_loss = 0.04375655949115753, 0.00898610521107912, kl_loss = 0.001319398172199726\n",
      "\n",
      "Epoch 633\n",
      "Step 0: loss = 0.05481027439236641, recon_loss = 0.04234477877616882, 0.012400315143167973, kl_loss = 0.0032590562477707863\n",
      "\n",
      "Epoch 634\n",
      "Step 0: loss = 0.0531662181019783, recon_loss = 0.041157066822052, 0.011976994574069977, kl_loss = 0.0016078781336545944\n",
      "\n",
      "Epoch 635\n",
      "Step 0: loss = 0.05330135300755501, recon_loss = 0.0402001291513443, 0.013083672150969505, kl_loss = 0.0008775545284152031\n",
      "\n",
      "Epoch 636\n",
      "Step 0: loss = 0.054404117166996, recon_loss = 0.041067734360694885, 0.013321293517947197, kl_loss = 0.0007544578984379768\n",
      "\n",
      "Epoch 637\n",
      "Step 0: loss = 0.0511879101395607, recon_loss = 0.03821954131126404, 0.012925120070576668, kl_loss = 0.002162533812224865\n",
      "\n",
      "Epoch 638\n",
      "Step 0: loss = 0.04984335973858833, recon_loss = 0.04040476307272911, 0.009406684897840023, kl_loss = 0.00159560889005661\n",
      "\n",
      "Epoch 639\n",
      "Step 0: loss = 0.050476495176553726, recon_loss = 0.03873969614505768, 0.011677643284201622, kl_loss = 0.0029577696695923805\n",
      "\n",
      "Epoch 640\n",
      "Step 0: loss = 0.04954764246940613, recon_loss = 0.041429877281188965, 0.008081980049610138, kl_loss = 0.0017893100157380104\n",
      "\n",
      "Epoch 641\n",
      "Step 0: loss = 0.051380813121795654, recon_loss = 0.0394732728600502, 0.01187332533299923, kl_loss = 0.0017107054591178894\n",
      "\n",
      "Epoch 642\n",
      "Step 0: loss = 0.047984529286623, recon_loss = 0.035720061510801315, 0.012202311307191849, kl_loss = 0.0031077824532985687\n",
      "\n",
      "Epoch 643\n",
      "Step 0: loss = 0.05623548477888107, recon_loss = 0.03998641297221184, 0.01618434116244316, kl_loss = 0.0032364754006266594\n",
      "\n",
      "Epoch 644\n",
      "Step 0: loss = 0.052328113466501236, recon_loss = 0.03910885751247406, 0.013172468170523643, kl_loss = 0.002339305356144905\n",
      "\n",
      "Epoch 645\n",
      "Step 0: loss = 0.04851752519607544, recon_loss = 0.03579951822757721, 0.01262371614575386, kl_loss = 0.004714479669928551\n",
      "\n",
      "Epoch 646\n",
      "Step 0: loss = 0.05255206301808357, recon_loss = 0.0409553200006485, 0.011539308354258537, kl_loss = 0.0028715981170535088\n",
      "\n",
      "Epoch 647\n",
      "Step 0: loss = 0.0473782904446125, recon_loss = 0.03558816760778427, 0.01173572987318039, kl_loss = 0.0027197077870368958\n",
      "\n",
      "Epoch 648\n",
      "Step 0: loss = 0.051097363233566284, recon_loss = 0.040181659162044525, 0.01081203855574131, kl_loss = 0.005183428525924683\n",
      "\n",
      "Epoch 649\n",
      "Step 0: loss = 0.04733826965093613, recon_loss = 0.03941570222377777, 0.007878697477281094, kl_loss = 0.0021935123950242996\n",
      "\n",
      "Epoch 650\n",
      "Step 0: loss = 0.046354759484529495, recon_loss = 0.03606080263853073, 0.010265059769153595, kl_loss = 0.001444818452000618\n",
      "\n",
      "Epoch 651\n",
      "Step 0: loss = 0.04971518740057945, recon_loss = 0.03805593028664589, 0.01163962297141552, kl_loss = 0.000981791876256466\n",
      "\n",
      "Epoch 652\n",
      "Step 0: loss = 0.04628912732005119, recon_loss = 0.03607949614524841, 0.010180860757827759, kl_loss = 0.0014384295791387558\n",
      "\n",
      "Epoch 653\n",
      "Step 0: loss = 0.04957466572523117, recon_loss = 0.03361375629901886, 0.01589273288846016, kl_loss = 0.003408791497349739\n",
      "\n",
      "Epoch 654\n",
      "Step 0: loss = 0.04662293195724487, recon_loss = 0.035304244607686996, 0.011268014088273048, kl_loss = 0.0025336360558867455\n",
      "\n",
      "Epoch 655\n",
      "Step 0: loss = 0.05470576509833336, recon_loss = 0.039788760244846344, 0.014883879572153091, kl_loss = 0.0016561932861804962\n",
      "\n",
      "Epoch 656\n",
      "Step 0: loss = 0.05537557229399681, recon_loss = 0.036012448370456696, 0.019285328686237335, kl_loss = 0.003889733925461769\n",
      "\n",
      "Epoch 657\n",
      "Step 0: loss = 0.04376785457134247, recon_loss = 0.03575450927019119, 0.007955826818943024, kl_loss = 0.002875847741961479\n",
      "\n",
      "Epoch 658\n",
      "Step 0: loss = 0.04814504086971283, recon_loss = 0.03477086126804352, 0.013343014754354954, kl_loss = 0.0015582116320729256\n",
      "\n",
      "Epoch 659\n",
      "Step 0: loss = 0.044010039418935776, recon_loss = 0.03353729099035263, 0.01044372096657753, kl_loss = 0.001451350748538971\n",
      "\n",
      "Epoch 660\n",
      "Step 0: loss = 0.04373367503285408, recon_loss = 0.03210901841521263, 0.011599395424127579, kl_loss = 0.0012630745768547058\n",
      "\n",
      "Epoch 661\n",
      "Step 0: loss = 0.04617663472890854, recon_loss = 0.03503215312957764, 0.0110684335231781, kl_loss = 0.0038023237138986588\n",
      "\n",
      "Epoch 662\n",
      "Step 0: loss = 0.04341932758688927, recon_loss = 0.03538147360086441, 0.008009305223822594, kl_loss = 0.001427317038178444\n",
      "\n",
      "Epoch 663\n",
      "Step 0: loss = 0.04592665284872055, recon_loss = 0.036055825650691986, 0.009848684072494507, kl_loss = 0.001107160933315754\n",
      "\n",
      "Epoch 664\n",
      "Step 0: loss = 0.04910779371857643, recon_loss = 0.03257196396589279, 0.016482237726449966, kl_loss = 0.0026796897873282433\n",
      "\n",
      "Epoch 665\n",
      "Step 0: loss = 0.04511714354157448, recon_loss = 0.03461956977844238, 0.01045590452849865, kl_loss = 0.002083348110318184\n",
      "\n",
      "Epoch 666\n",
      "Step 0: loss = 0.04172404110431671, recon_loss = 0.032091379165649414, 0.009550611488521099, kl_loss = 0.0041023846715688705\n",
      "\n",
      "Epoch 667\n",
      "Step 0: loss = 0.044240761548280716, recon_loss = 0.03424123674631119, 0.00996945146471262, kl_loss = 0.0015036286786198616\n",
      "\n",
      "Epoch 668\n",
      "Step 0: loss = 0.04675273597240448, recon_loss = 0.03408535569906235, 0.012623979710042477, kl_loss = 0.002170042134821415\n",
      "\n",
      "Epoch 669\n",
      "Step 0: loss = 0.046137865632772446, recon_loss = 0.03666289150714874, 0.009442783892154694, kl_loss = 0.0016094828024506569\n",
      "\n",
      "Epoch 670\n",
      "Step 0: loss = 0.04596832022070885, recon_loss = 0.03316304087638855, 0.01277213916182518, kl_loss = 0.001656932756304741\n",
      "\n",
      "Epoch 671\n",
      "Step 0: loss = 0.04807925969362259, recon_loss = 0.033389635384082794, 0.014645526185631752, kl_loss = 0.002205008640885353\n",
      "\n",
      "Epoch 672\n",
      "Step 0: loss = 0.04685378819704056, recon_loss = 0.03141026943922043, 0.015383854508399963, kl_loss = 0.00298321433365345\n",
      "\n",
      "Epoch 673\n",
      "Step 0: loss = 0.04388100281357765, recon_loss = 0.03071010112762451, 0.013136915862560272, kl_loss = 0.0016993768513202667\n",
      "\n",
      "Epoch 674\n",
      "Step 0: loss = 0.04391426965594292, recon_loss = 0.03150971606373787, 0.012360806576907635, kl_loss = 0.0021872613579034805\n",
      "\n",
      "Epoch 675\n",
      "Step 0: loss = 0.044273145496845245, recon_loss = 0.0340162068605423, 0.010214242152869701, kl_loss = 0.002134827896952629\n",
      "\n",
      "Epoch 676\n",
      "Step 0: loss = 0.04634696990251541, recon_loss = 0.03359976038336754, 0.012712721712887287, kl_loss = 0.0017244555056095123\n",
      "\n",
      "Epoch 677\n",
      "Step 0: loss = 0.04232519492506981, recon_loss = 0.03016260638833046, 0.012098899111151695, kl_loss = 0.003184579312801361\n",
      "\n",
      "Epoch 678\n",
      "Step 0: loss = 0.04513370990753174, recon_loss = 0.0297591183334589, 0.01520797610282898, kl_loss = 0.008330927230417728\n",
      "\n",
      "Epoch 679\n",
      "Step 0: loss = 0.04844368249177933, recon_loss = 0.03104613535106182, 0.017269965261220932, kl_loss = 0.006379195488989353\n",
      "\n",
      "Epoch 680\n",
      "Step 0: loss = 0.04259394854307175, recon_loss = 0.02935757115483284, 0.01316702738404274, kl_loss = 0.0034675095230340958\n",
      "\n",
      "Epoch 681\n",
      "Step 0: loss = 0.04638446867465973, recon_loss = 0.028767235577106476, 0.017568696290254593, kl_loss = 0.002426845021545887\n",
      "\n",
      "Epoch 682\n",
      "Step 0: loss = 0.03931038826704025, recon_loss = 0.031180409714579582, 0.008097590878605843, kl_loss = 0.0016193510964512825\n",
      "\n",
      "Epoch 683\n",
      "Step 0: loss = 0.04196041449904442, recon_loss = 0.027810947969555855, 0.014081156812608242, kl_loss = 0.003415529616177082\n",
      "\n",
      "Epoch 684\n",
      "Step 0: loss = 0.04368671774864197, recon_loss = 0.030775677412748337, 0.012817748822271824, kl_loss = 0.004664651118218899\n",
      "\n",
      "Epoch 685\n",
      "Step 0: loss = 0.04457036778330803, recon_loss = 0.030833285301923752, 0.013690682128071785, kl_loss = 0.0023199962452054024\n",
      "\n",
      "Epoch 686\n",
      "Step 0: loss = 0.04450231045484543, recon_loss = 0.03123069740831852, 0.013236085884273052, kl_loss = 0.001776318997144699\n",
      "\n",
      "Epoch 687\n",
      "Step 0: loss = 0.04417292773723602, recon_loss = 0.03103109449148178, 0.01311442069709301, kl_loss = 0.0013704616576433182\n",
      "\n",
      "Epoch 688\n",
      "Step 0: loss = 0.03740939497947693, recon_loss = 0.02819114737212658, 0.009160969406366348, kl_loss = 0.002863924019038677\n",
      "\n",
      "Epoch 689\n",
      "Step 0: loss = 0.038587786257267, recon_loss = 0.0283771101385355, 0.010180279612541199, kl_loss = 0.0015198634937405586\n",
      "\n",
      "Epoch 690\n",
      "Step 0: loss = 0.04032247141003609, recon_loss = 0.02947351522743702, 0.010820059105753899, kl_loss = 0.0014449385926127434\n",
      "\n",
      "Epoch 691\n",
      "Step 0: loss = 0.04393685609102249, recon_loss = 0.029687831178307533, 0.01421041414141655, kl_loss = 0.0019304221495985985\n",
      "\n",
      "Epoch 692\n",
      "Step 0: loss = 0.04126191511750221, recon_loss = 0.02786399982869625, 0.013366242870688438, kl_loss = 0.0015835724771022797\n",
      "\n",
      "Epoch 693\n",
      "Step 0: loss = 0.03974268212914467, recon_loss = 0.02866816706955433, 0.011052285321056843, kl_loss = 0.0011114142835140228\n",
      "\n",
      "Epoch 694\n",
      "Step 0: loss = 0.037201497703790665, recon_loss = 0.028371015563607216, 0.008791998960077763, kl_loss = 0.0019241301342844963\n",
      "\n",
      "Epoch 695\n",
      "Step 0: loss = 0.044346291571855545, recon_loss = 0.03012561798095703, 0.01418156549334526, kl_loss = 0.001955319195985794\n",
      "\n",
      "Epoch 696\n",
      "Step 0: loss = 0.03740052133798599, recon_loss = 0.028969204053282738, 0.008404058404266834, kl_loss = 0.0013628723099827766\n",
      "\n",
      "Epoch 697\n",
      "Step 0: loss = 0.03783797100186348, recon_loss = 0.027351900935173035, 0.010466883890330791, kl_loss = 0.0009593432769179344\n",
      "\n",
      "Epoch 698\n",
      "Step 0: loss = 0.036589257419109344, recon_loss = 0.02501143142580986, 0.011528640985488892, kl_loss = 0.0024592652916908264\n",
      "\n",
      "Epoch 699\n",
      "Step 0: loss = 0.04442060366272926, recon_loss = 0.028780635446310043, 0.015591263771057129, kl_loss = 0.0024351440370082855\n",
      "\n",
      "Epoch 700\n",
      "Step 0: loss = 0.03813917934894562, recon_loss = 0.027353009209036827, 0.010741794481873512, kl_loss = 0.0022187046706676483\n",
      "\n",
      "Epoch 701\n",
      "Step 0: loss = 0.03922416642308235, recon_loss = 0.02657860331237316, 0.012626957148313522, kl_loss = 0.000930415466427803\n",
      "\n",
      "Epoch 702\n",
      "Step 0: loss = 0.04305511713027954, recon_loss = 0.027310239151120186, 0.015730159357190132, kl_loss = 0.0007358668372035027\n",
      "\n",
      "Epoch 703\n",
      "Step 0: loss = 0.036556366831064224, recon_loss = 0.025922540575265884, 0.010583065450191498, kl_loss = 0.0025381306186318398\n",
      "\n",
      "Epoch 704\n",
      "Step 0: loss = 0.04065887629985809, recon_loss = 0.025881383568048477, 0.014706827700138092, kl_loss = 0.0035332152619957924\n",
      "\n",
      "Epoch 705\n",
      "Step 0: loss = 0.03986281901597977, recon_loss = 0.025350969284772873, 0.014483680948615074, kl_loss = 0.0014085238799452782\n",
      "\n",
      "Epoch 706\n",
      "Step 0: loss = 0.03721068426966667, recon_loss = 0.025666778907179832, 0.011509524658322334, kl_loss = 0.001718992367386818\n",
      "\n",
      "Epoch 707\n",
      "Step 0: loss = 0.041280776262283325, recon_loss = 0.024874882772564888, 0.016286564990878105, kl_loss = 0.005966356955468655\n",
      "\n",
      "Epoch 708\n",
      "Step 0: loss = 0.04088115692138672, recon_loss = 0.02490639127790928, 0.01592094451189041, kl_loss = 0.0026912204921245575\n",
      "\n",
      "Epoch 709\n",
      "Step 0: loss = 0.035009704530239105, recon_loss = 0.02593870274722576, 0.009002921171486378, kl_loss = 0.0034040044993162155\n",
      "\n",
      "Epoch 710\n",
      "Step 0: loss = 0.03516990318894386, recon_loss = 0.023808196187019348, 0.011339463293552399, kl_loss = 0.0011122748255729675\n",
      "\n",
      "Epoch 711\n",
      "Step 0: loss = 0.03705817461013794, recon_loss = 0.02403569407761097, 0.0129978246986866, kl_loss = 0.0012327004224061966\n",
      "\n",
      "Epoch 712\n",
      "Step 0: loss = 0.03344349190592766, recon_loss = 0.025400957092642784, 0.008016608655452728, kl_loss = 0.0012961989268660545\n",
      "\n",
      "Epoch 713\n",
      "Step 0: loss = 0.038065142929553986, recon_loss = 0.025487495586276054, 0.012538219802081585, kl_loss = 0.001971461810171604\n",
      "\n",
      "Epoch 714\n",
      "Step 0: loss = 0.03782385587692261, recon_loss = 0.024188896641135216, 0.013595318421721458, kl_loss = 0.0019820239394903183\n",
      "\n",
      "Epoch 715\n",
      "Step 0: loss = 0.040554553270339966, recon_loss = 0.025173909962177277, 0.015325929969549179, kl_loss = 0.002735629677772522\n",
      "\n",
      "Epoch 716\n",
      "Step 0: loss = 0.037918977439403534, recon_loss = 0.023172691464424133, 0.014708288013935089, kl_loss = 0.001899823546409607\n",
      "\n",
      "Epoch 717\n",
      "Step 0: loss = 0.03488443046808243, recon_loss = 0.02368892915546894, 0.01115716528147459, kl_loss = 0.0019168267026543617\n",
      "\n",
      "Epoch 718\n",
      "Step 0: loss = 0.035136617720127106, recon_loss = 0.023313988000154495, 0.011806933209300041, kl_loss = 0.0007849065586924553\n",
      "\n",
      "Epoch 719\n",
      "Step 0: loss = 0.036993272602558136, recon_loss = 0.024949945509433746, 0.011992722749710083, kl_loss = 0.002530178055167198\n",
      "\n",
      "Epoch 720\n",
      "Step 0: loss = 0.03380045294761658, recon_loss = 0.02170743979513645, 0.012076279148459435, kl_loss = 0.0008366387337446213\n",
      "\n",
      "Epoch 721\n",
      "Step 0: loss = 0.039082951843738556, recon_loss = 0.02326052263379097, 0.01579643413424492, kl_loss = 0.0012998133897781372\n",
      "\n",
      "Epoch 722\n",
      "Step 0: loss = 0.037532441318035126, recon_loss = 0.02245667204260826, 0.01503141038119793, kl_loss = 0.00221809558570385\n",
      "\n",
      "Epoch 723\n",
      "Step 0: loss = 0.03363294154405594, recon_loss = 0.022357255220413208, 0.011242683976888657, kl_loss = 0.001650194637477398\n",
      "\n",
      "Epoch 724\n",
      "Step 0: loss = 0.03730039298534393, recon_loss = 0.024883223697543144, 0.012398610822856426, kl_loss = 0.0009280480444431305\n",
      "\n",
      "Epoch 725\n",
      "Step 0: loss = 0.04148583486676216, recon_loss = 0.023296087980270386, 0.018165476620197296, kl_loss = 0.0012134741991758347\n",
      "\n",
      "Epoch 726\n",
      "Step 0: loss = 0.03416575863957405, recon_loss = 0.02421698346734047, 0.009917197749018669, kl_loss = 0.0015789521858096123\n",
      "\n",
      "Epoch 727\n",
      "Step 0: loss = 0.03424365073442459, recon_loss = 0.021550342440605164, 0.012647578492760658, kl_loss = 0.002286636270582676\n",
      "\n",
      "Epoch 728\n",
      "Step 0: loss = 0.038434773683547974, recon_loss = 0.021710779517889023, 0.01666312664747238, kl_loss = 0.003043294884264469\n",
      "\n",
      "Epoch 729\n",
      "Step 0: loss = 0.03618290647864342, recon_loss = 0.024775756523013115, 0.011336538940668106, kl_loss = 0.0035306792706251144\n",
      "\n",
      "Epoch 730\n",
      "Step 0: loss = 0.04229890927672386, recon_loss = 0.02416142076253891, 0.018108215183019638, kl_loss = 0.0014637252315878868\n",
      "\n",
      "Epoch 731\n",
      "Step 0: loss = 0.03830918297171593, recon_loss = 0.02355327643454075, 0.014677386730909348, kl_loss = 0.003925804980099201\n",
      "\n",
      "Epoch 732\n",
      "Step 0: loss = 0.036245934665203094, recon_loss = 0.02087106555700302, 0.015283861197531223, kl_loss = 0.004550428129732609\n",
      "\n",
      "Epoch 733\n",
      "Step 0: loss = 0.03283790498971939, recon_loss = 0.02086269110441208, 0.011941840872168541, kl_loss = 0.0016685081645846367\n",
      "\n",
      "Epoch 734\n",
      "Step 0: loss = 0.03504329174757004, recon_loss = 0.02285013347864151, 0.012154030613601208, kl_loss = 0.0019562924280762672\n",
      "\n",
      "Epoch 735\n",
      "Step 0: loss = 0.03179270029067993, recon_loss = 0.019036591053009033, 0.012732546776533127, kl_loss = 0.001178092323243618\n",
      "\n",
      "Epoch 736\n",
      "Step 0: loss = 0.03715340793132782, recon_loss = 0.020800840109586716, 0.016317002475261688, kl_loss = 0.0017782524228096008\n",
      "\n",
      "Epoch 737\n",
      "Step 0: loss = 0.031767286360263824, recon_loss = 0.02123703621327877, 0.01043214462697506, kl_loss = 0.004905341193079948\n",
      "\n",
      "Epoch 738\n",
      "Step 0: loss = 0.03330746293067932, recon_loss = 0.02367948181927204, 0.009578902274370193, kl_loss = 0.0024539362639188766\n",
      "\n",
      "Epoch 739\n",
      "Step 0: loss = 0.033044349402189255, recon_loss = 0.02138446271419525, 0.011640401557087898, kl_loss = 0.0009744223207235336\n",
      "\n",
      "Epoch 740\n",
      "Step 0: loss = 0.03413238003849983, recon_loss = 0.022965237498283386, 0.011145642958581448, kl_loss = 0.0010748514905571938\n",
      "\n",
      "Epoch 741\n",
      "Step 0: loss = 0.03192376717925072, recon_loss = 0.019862394779920578, 0.011997316032648087, kl_loss = 0.0032027428969740868\n",
      "\n",
      "Epoch 742\n",
      "Step 0: loss = 0.03652053698897362, recon_loss = 0.019574908539652824, 0.01691330596804619, kl_loss = 0.0016161659732460976\n",
      "\n",
      "Epoch 743\n",
      "Step 0: loss = 0.030113227665424347, recon_loss = 0.01939721405506134, 0.010673588141798973, kl_loss = 0.00212131068110466\n",
      "\n",
      "Epoch 744\n",
      "Step 0: loss = 0.02930806390941143, recon_loss = 0.01980670727789402, 0.009477523155510426, kl_loss = 0.0011915834620594978\n",
      "\n",
      "Epoch 745\n",
      "Step 0: loss = 0.032489925622940063, recon_loss = 0.02007902041077614, 0.012377328239381313, kl_loss = 0.0016788477078080177\n",
      "\n",
      "Epoch 746\n",
      "Step 0: loss = 0.03634536266326904, recon_loss = 0.02075740322470665, 0.015553376637399197, kl_loss = 0.0017291055992245674\n",
      "\n",
      "Epoch 747\n",
      "Step 0: loss = 0.03497208654880524, recon_loss = 0.021282872185111046, 0.013556400313973427, kl_loss = 0.006640613079071045\n",
      "\n",
      "Epoch 748\n",
      "Step 0: loss = 0.03615262359380722, recon_loss = 0.02176191844046116, 0.01431423332542181, kl_loss = 0.00382367055863142\n",
      "\n",
      "Epoch 749\n",
      "Step 0: loss = 0.030278179794549942, recon_loss = 0.021035509184002876, 0.009195167571306229, kl_loss = 0.0023751091212034225\n",
      "\n",
      "Epoch 750\n",
      "Step 0: loss = 0.03220842406153679, recon_loss = 0.019362008199095726, 0.0128240417689085, kl_loss = 0.001118638552725315\n",
      "\n",
      "Epoch 751\n",
      "Step 0: loss = 0.03507179021835327, recon_loss = 0.019188670441508293, 0.015837594866752625, kl_loss = 0.0022760992869734764\n",
      "\n",
      "Epoch 752\n",
      "Step 0: loss = 0.03238072618842125, recon_loss = 0.0205281563103199, 0.01174510270357132, kl_loss = 0.0053734444081783295\n",
      "\n",
      "Epoch 753\n",
      "Step 0: loss = 0.030758989974856377, recon_loss = 0.019042586907744408, 0.011680630967020988, kl_loss = 0.001788572408258915\n",
      "\n",
      "Epoch 754\n",
      "Step 0: loss = 0.03111444041132927, recon_loss = 0.01829974539577961, 0.012790932320058346, kl_loss = 0.001188160851597786\n",
      "\n",
      "Epoch 755\n",
      "Step 0: loss = 0.03287580981850624, recon_loss = 0.02035076729953289, 0.012492607347667217, kl_loss = 0.0016218461096286774\n",
      "\n",
      "Epoch 756\n",
      "Step 0: loss = 0.030692657455801964, recon_loss = 0.019484898075461388, 0.011179657652974129, kl_loss = 0.0014050891622900963\n",
      "\n",
      "Epoch 757\n",
      "Step 0: loss = 0.033408064395189285, recon_loss = 0.01929602213203907, 0.01409523468464613, kl_loss = 0.0008404236286878586\n",
      "\n",
      "Epoch 758\n",
      "Step 0: loss = 0.03354732319712639, recon_loss = 0.02051296830177307, 0.013011923059821129, kl_loss = 0.00112142413854599\n",
      "\n",
      "Epoch 759\n",
      "Step 0: loss = 0.03173188492655754, recon_loss = 0.018583767116069794, 0.013108260929584503, kl_loss = 0.00199285801500082\n",
      "\n",
      "Epoch 760\n",
      "Step 0: loss = 0.02741938829421997, recon_loss = 0.017943551763892174, 0.009454906918108463, kl_loss = 0.0010463930666446686\n",
      "\n",
      "Epoch 761\n",
      "Step 0: loss = 0.02725387178361416, recon_loss = 0.01758621446788311, 0.00964655913412571, kl_loss = 0.0010549305006861687\n",
      "\n",
      "Epoch 762\n",
      "Step 0: loss = 0.028694968670606613, recon_loss = 0.01781247928738594, 0.010850711725652218, kl_loss = 0.001588813029229641\n",
      "\n",
      "Epoch 763\n",
      "Step 0: loss = 0.030406801030039787, recon_loss = 0.01738743856549263, 0.012934966012835503, kl_loss = 0.004219835624098778\n",
      "\n",
      "Epoch 764\n",
      "Step 0: loss = 0.029989084228873253, recon_loss = 0.018137648701667786, 0.011796263977885246, kl_loss = 0.0027585793286561966\n",
      "\n",
      "Epoch 765\n",
      "Step 0: loss = 0.03751447796821594, recon_loss = 0.01991291716694832, 0.017539577558636665, kl_loss = 0.00309913232922554\n",
      "\n",
      "Epoch 766\n",
      "Step 0: loss = 0.025819066911935806, recon_loss = 0.016803327947854996, 0.008937581442296505, kl_loss = 0.003907863982021809\n",
      "\n",
      "Epoch 767\n",
      "Step 0: loss = 0.029928768053650856, recon_loss = 0.016739195212721825, 0.013156488537788391, kl_loss = 0.0016541704535484314\n",
      "\n",
      "Epoch 768\n",
      "Step 0: loss = 0.030484942719340324, recon_loss = 0.017991676926612854, 0.012474886141717434, kl_loss = 0.0009189145639538765\n",
      "\n",
      "Epoch 769\n",
      "Step 0: loss = 0.028317207470536232, recon_loss = 0.016896666958928108, 0.011403193697333336, kl_loss = 0.0008673742413520813\n",
      "\n",
      "Epoch 770\n",
      "Step 0: loss = 0.03133580461144447, recon_loss = 0.016640111804008484, 0.014618591405451298, kl_loss = 0.003855133429169655\n",
      "\n",
      "Epoch 771\n",
      "Step 0: loss = 0.03063315898180008, recon_loss = 0.016793785616755486, 0.0137307308614254, kl_loss = 0.0054321009665727615\n",
      "\n",
      "Epoch 772\n",
      "Step 0: loss = 0.03653515875339508, recon_loss = 0.01811966486275196, 0.01827181503176689, kl_loss = 0.00718382652848959\n",
      "\n",
      "Epoch 773\n",
      "Step 0: loss = 0.030997801572084427, recon_loss = 0.01766291819512844, 0.013252068310976028, kl_loss = 0.004140731878578663\n",
      "\n",
      "Epoch 774\n",
      "Step 0: loss = 0.032477132976055145, recon_loss = 0.01689232885837555, 0.015554790385067463, kl_loss = 0.0015007881447672844\n",
      "\n",
      "Epoch 775\n",
      "Step 0: loss = 0.027752868831157684, recon_loss = 0.017040928825736046, 0.010677836835384369, kl_loss = 0.0017051873728632927\n",
      "\n",
      "Epoch 776\n",
      "Step 0: loss = 0.029838979244232178, recon_loss = 0.016677891835570335, 0.013138221576809883, kl_loss = 0.0011432524770498276\n",
      "\n",
      "Epoch 777\n",
      "Step 0: loss = 0.028675630688667297, recon_loss = 0.017548389732837677, 0.011096316389739513, kl_loss = 0.0015461798757314682\n",
      "\n",
      "Epoch 778\n",
      "Step 0: loss = 0.030268117785453796, recon_loss = 0.01726587302982807, 0.012954866513609886, kl_loss = 0.002368948422372341\n",
      "\n",
      "Epoch 779\n",
      "Step 0: loss = 0.029599791392683983, recon_loss = 0.015532059594988823, 0.01402673963457346, kl_loss = 0.002049604430794716\n",
      "\n",
      "Epoch 780\n",
      "Step 0: loss = 0.03105820342898369, recon_loss = 0.01678498089313507, 0.01423882506787777, kl_loss = 0.0017199013382196426\n",
      "\n",
      "Epoch 781\n",
      "Step 0: loss = 0.02637450583279133, recon_loss = 0.01695210486650467, 0.009405190125107765, kl_loss = 0.0008605103939771652\n",
      "\n",
      "Epoch 782\n",
      "Step 0: loss = 0.030262712389230728, recon_loss = 0.015924397855997086, 0.0142891276627779, kl_loss = 0.002459355629980564\n",
      "\n",
      "Epoch 783\n",
      "Step 0: loss = 0.027455570176243782, recon_loss = 0.01609647274017334, 0.011309480294585228, kl_loss = 0.002480842173099518\n",
      "\n",
      "Epoch 784\n",
      "Step 0: loss = 0.027201170101761818, recon_loss = 0.016136927530169487, 0.01102230604737997, kl_loss = 0.002096910960972309\n",
      "\n",
      "Epoch 785\n",
      "Step 0: loss = 0.03314974531531334, recon_loss = 0.017759287729859352, 0.01524911168962717, kl_loss = 0.007067164406180382\n",
      "\n",
      "Epoch 786\n",
      "Step 0: loss = 0.039360709488391876, recon_loss = 0.018737472593784332, 0.020489543676376343, kl_loss = 0.006684688851237297\n",
      "\n",
      "Epoch 787\n",
      "Step 0: loss = 0.03166237846016884, recon_loss = 0.01847982220351696, 0.01301849540323019, kl_loss = 0.008203070610761642\n",
      "\n",
      "Epoch 788\n",
      "Step 0: loss = 0.029518673196434975, recon_loss = 0.016984492540359497, 0.012432454153895378, kl_loss = 0.005086316727101803\n",
      "\n",
      "Epoch 789\n",
      "Step 0: loss = 0.037749554961919785, recon_loss = 0.01827809028327465, 0.019404469057917595, kl_loss = 0.003349786624312401\n",
      "\n",
      "Epoch 790\n",
      "Step 0: loss = 0.028126614168286324, recon_loss = 0.015970787033438683, 0.012082505039870739, kl_loss = 0.0036661671474575996\n",
      "\n",
      "Epoch 791\n",
      "Step 0: loss = 0.02849254012107849, recon_loss = 0.016629986464977264, 0.011719075962901115, kl_loss = 0.007173871621489525\n",
      "\n",
      "Epoch 792\n",
      "Step 0: loss = 0.028440015390515327, recon_loss = 0.01578489877283573, 0.012553930282592773, kl_loss = 0.00505934190005064\n",
      "\n",
      "Epoch 793\n",
      "Step 0: loss = 0.03173942118883133, recon_loss = 0.015371795743703842, 0.016295259818434715, kl_loss = 0.0036184024065732956\n",
      "\n",
      "Epoch 794\n",
      "Step 0: loss = 0.030063575133681297, recon_loss = 0.017239943146705627, 0.012775708921253681, kl_loss = 0.0023962268605828285\n",
      "\n",
      "Epoch 795\n",
      "Step 0: loss = 0.025218846276402473, recon_loss = 0.015130458399653435, 0.010066122747957706, kl_loss = 0.0011131735518574715\n",
      "\n",
      "Epoch 796\n",
      "Step 0: loss = 0.026419812813401222, recon_loss = 0.014674883335828781, 0.01171802543103695, kl_loss = 0.0013452256098389626\n",
      "\n",
      "Epoch 797\n",
      "Step 0: loss = 0.03332046791911125, recon_loss = 0.01540796086192131, 0.017859268933534622, kl_loss = 0.002661864273250103\n",
      "\n",
      "Epoch 798\n",
      "Step 0: loss = 0.027105338871479034, recon_loss = 0.014910418540239334, 0.01216860394924879, kl_loss = 0.001315799541771412\n",
      "\n",
      "Epoch 799\n",
      "Step 0: loss = 0.02685420773923397, recon_loss = 0.014461586251854897, 0.012367460876703262, kl_loss = 0.0012580277398228645\n",
      "\n",
      "Epoch 800\n",
      "Step 0: loss = 0.03231948986649513, recon_loss = 0.015385987237095833, 0.01689842902123928, kl_loss = 0.001753712072968483\n",
      "\n",
      "Epoch 801\n",
      "Step 0: loss = 0.026253890246152878, recon_loss = 0.014625024050474167, 0.011576150543987751, kl_loss = 0.002635807730257511\n",
      "\n",
      "Epoch 802\n",
      "Step 0: loss = 0.02754158154129982, recon_loss = 0.015101224184036255, 0.012422957457602024, kl_loss = 0.0008700210601091385\n",
      "\n",
      "Epoch 803\n",
      "Step 0: loss = 0.026684867218136787, recon_loss = 0.01422862894833088, 0.01240072213113308, kl_loss = 0.0027758097276091576\n",
      "\n",
      "Epoch 804\n",
      "Step 0: loss = 0.029463009908795357, recon_loss = 0.015147149562835693, 0.014275295659899712, kl_loss = 0.0020281942561268806\n",
      "\n",
      "Epoch 805\n",
      "Step 0: loss = 0.02674226649105549, recon_loss = 0.014150487259030342, 0.012554740533232689, kl_loss = 0.001851891167461872\n",
      "\n",
      "Epoch 806\n",
      "Step 0: loss = 0.02681533619761467, recon_loss = 0.013184385374188423, 0.01359511073678732, kl_loss = 0.0017920797690749168\n",
      "\n",
      "Epoch 807\n",
      "Step 0: loss = 0.02841867320239544, recon_loss = 0.013628760352730751, 0.014742271974682808, kl_loss = 0.0023819981142878532\n",
      "\n",
      "Epoch 808\n",
      "Step 0: loss = 0.029173148795962334, recon_loss = 0.015528127551078796, 0.013593585230410099, kl_loss = 0.002571878954768181\n",
      "\n",
      "Epoch 809\n",
      "Step 0: loss = 0.027789989486336708, recon_loss = 0.014724934473633766, 0.013011923991143703, kl_loss = 0.002656618133187294\n",
      "\n",
      "Epoch 810\n",
      "Step 0: loss = 0.027502382174134254, recon_loss = 0.014723656699061394, 0.01270090602338314, kl_loss = 0.0038909614086151123\n",
      "\n",
      "Epoch 811\n",
      "Step 0: loss = 0.028243403881788254, recon_loss = 0.0146571584045887, 0.013546664267778397, kl_loss = 0.0019790660589933395\n",
      "\n",
      "Epoch 812\n",
      "Step 0: loss = 0.027754798531532288, recon_loss = 0.014715222641825676, 0.012943975627422333, kl_loss = 0.004780009388923645\n",
      "\n",
      "Epoch 813\n",
      "Step 0: loss = 0.02740732580423355, recon_loss = 0.014418253675103188, 0.01296287402510643, kl_loss = 0.0013099517673254013\n",
      "\n",
      "Epoch 814\n",
      "Step 0: loss = 0.027462230995297432, recon_loss = 0.015329191461205482, 0.012072505429387093, kl_loss = 0.0030267173424363136\n",
      "\n",
      "Epoch 815\n",
      "Step 0: loss = 0.026668807491660118, recon_loss = 0.015803374350070953, 0.010827504098415375, kl_loss = 0.0018964949995279312\n",
      "\n",
      "Epoch 816\n",
      "Step 0: loss = 0.025985023006796837, recon_loss = 0.014128213748335838, 0.01182420365512371, kl_loss = 0.0016302475705742836\n",
      "\n",
      "Epoch 817\n",
      "Step 0: loss = 0.02766246162354946, recon_loss = 0.013472376391291618, 0.014069292694330215, kl_loss = 0.006039583124220371\n",
      "\n",
      "Epoch 818\n",
      "Step 0: loss = 0.027804039418697357, recon_loss = 0.0150446817278862, 0.0126649159938097, kl_loss = 0.00472203828394413\n",
      "\n",
      "Epoch 819\n",
      "Step 0: loss = 0.025224512442946434, recon_loss = 0.014114860445261002, 0.0110624385997653, kl_loss = 0.0023606354370713234\n",
      "\n",
      "Epoch 820\n",
      "Step 0: loss = 0.027039501816034317, recon_loss = 0.01288623921573162, 0.014138363301753998, kl_loss = 0.0007449695840477943\n",
      "\n",
      "Epoch 821\n",
      "Step 0: loss = 0.03178201615810394, recon_loss = 0.014226136729121208, 0.01751650869846344, kl_loss = 0.001968422904610634\n",
      "\n",
      "Epoch 822\n",
      "Step 0: loss = 0.02499665878713131, recon_loss = 0.012217912822961807, 0.01275890413671732, kl_loss = 0.0009921761229634285\n",
      "\n",
      "Epoch 823\n",
      "Step 0: loss = 0.024590305984020233, recon_loss = 0.01239706389605999, 0.012166587635874748, kl_loss = 0.0013326862826943398\n",
      "\n",
      "Epoch 824\n",
      "Step 0: loss = 0.02857518196105957, recon_loss = 0.012319574132561684, 0.016210023313760757, kl_loss = 0.0022792601957917213\n",
      "\n",
      "Epoch 825\n",
      "Step 0: loss = 0.028036776930093765, recon_loss = 0.013126745820045471, 0.014857517555356026, kl_loss = 0.0026257000863552094\n",
      "\n",
      "Epoch 826\n",
      "Step 0: loss = 0.02765032649040222, recon_loss = 0.014830894768238068, 0.01275191642343998, kl_loss = 0.003375740721821785\n",
      "\n",
      "Epoch 827\n",
      "Step 0: loss = 0.02514849789440632, recon_loss = 0.013857243582606316, 0.011256465688347816, kl_loss = 0.0017394470050930977\n",
      "\n",
      "Epoch 828\n",
      "Step 0: loss = 0.024070262908935547, recon_loss = 0.013343030586838722, 0.01066107489168644, kl_loss = 0.003307896666228771\n",
      "\n",
      "Epoch 829\n",
      "Step 0: loss = 0.022649617865681648, recon_loss = 0.013075415045022964, 0.009532101452350616, kl_loss = 0.002105090767145157\n",
      "\n",
      "Epoch 830\n",
      "Step 0: loss = 0.026065969839692116, recon_loss = 0.012056887149810791, 0.013986283913254738, kl_loss = 0.0011398959904909134\n",
      "\n",
      "Epoch 831\n",
      "Step 0: loss = 0.025456272065639496, recon_loss = 0.012449299916625023, 0.012975181452929974, kl_loss = 0.0015895692631602287\n",
      "\n",
      "Epoch 832\n",
      "Step 0: loss = 0.021543383598327637, recon_loss = 0.011840824037790298, 0.00966624915599823, kl_loss = 0.0018154745921492577\n",
      "\n",
      "Epoch 833\n",
      "Step 0: loss = 0.020619608461856842, recon_loss = 0.012227242812514305, 0.008375848643481731, kl_loss = 0.0008258689194917679\n",
      "\n",
      "Epoch 834\n",
      "Step 0: loss = 0.030927501618862152, recon_loss = 0.013961359858512878, 0.016945814713835716, kl_loss = 0.0010163607075810432\n",
      "\n",
      "Epoch 835\n",
      "Step 0: loss = 0.023535780608654022, recon_loss = 0.013026803731918335, 0.010470498353242874, kl_loss = 0.0019239699468016624\n",
      "\n",
      "Epoch 836\n",
      "Step 0: loss = 0.024418780580163002, recon_loss = 0.013050632551312447, 0.011327353306114674, kl_loss = 0.0020397063344717026\n",
      "\n",
      "Epoch 837\n",
      "Step 0: loss = 0.022133391350507736, recon_loss = 0.012411264702677727, 0.009660396724939346, kl_loss = 0.0030865008011460304\n",
      "\n",
      "Epoch 838\n",
      "Step 0: loss = 0.03363511338829994, recon_loss = 0.01401432417333126, 0.01956649124622345, kl_loss = 0.002714752219617367\n",
      "\n",
      "Epoch 839\n",
      "Step 0: loss = 0.02821328118443489, recon_loss = 0.014652125537395477, 0.013506948947906494, kl_loss = 0.0027103684842586517\n",
      "\n",
      "Epoch 840\n",
      "Step 0: loss = 0.027502678334712982, recon_loss = 0.012937013059854507, 0.014511230401694775, kl_loss = 0.0027217091992497444\n",
      "\n",
      "Epoch 841\n",
      "Step 0: loss = 0.028119638562202454, recon_loss = 0.01337558962404728, 0.014715387485921383, kl_loss = 0.0014331517741084099\n",
      "\n",
      "Epoch 842\n",
      "Step 0: loss = 0.02839416265487671, recon_loss = 0.012374944984912872, 0.01600082963705063, kl_loss = 0.000919378362596035\n",
      "\n",
      "Epoch 843\n",
      "Step 0: loss = 0.028569724410772324, recon_loss = 0.013137070462107658, 0.015393955633044243, kl_loss = 0.001934894360601902\n",
      "\n",
      "Epoch 844\n",
      "Step 0: loss = 0.0285547636449337, recon_loss = 0.012858424335718155, 0.015674248337745667, kl_loss = 0.001104532741010189\n",
      "\n",
      "Epoch 845\n",
      "Step 0: loss = 0.029070448130369186, recon_loss = 0.014011090621352196, 0.015015997923910618, kl_loss = 0.002167954109609127\n",
      "\n",
      "Epoch 846\n",
      "Step 0: loss = 0.029181640595197678, recon_loss = 0.013580836355686188, 0.015571516007184982, kl_loss = 0.0014644013717770576\n",
      "\n",
      "Epoch 847\n",
      "Step 0: loss = 0.02857299894094467, recon_loss = 0.012592757120728493, 0.01595314033329487, kl_loss = 0.0013550873845815659\n",
      "\n",
      "Epoch 848\n",
      "Step 0: loss = 0.027148906141519547, recon_loss = 0.013430571183562279, 0.013580648228526115, kl_loss = 0.006884331814944744\n",
      "\n",
      "Epoch 849\n",
      "Step 0: loss = 0.03134693205356598, recon_loss = 0.013094708323478699, 0.01820027455687523, kl_loss = 0.002597406506538391\n",
      "\n",
      "Epoch 850\n",
      "Step 0: loss = 0.026861293241381645, recon_loss = 0.012134559452533722, 0.014705972746014595, kl_loss = 0.0010380372405052185\n",
      "\n",
      "Epoch 851\n",
      "Step 0: loss = 0.03025985136628151, recon_loss = 0.01256544515490532, 0.017669327557086945, kl_loss = 0.0012539727613329887\n",
      "\n",
      "Epoch 852\n",
      "Step 0: loss = 0.022827250882983208, recon_loss = 0.011712564155459404, 0.01108672097325325, kl_loss = 0.001398332417011261\n",
      "\n",
      "Epoch 853\n",
      "Step 0: loss = 0.024881303310394287, recon_loss = 0.011892998591065407, 0.012958843261003494, kl_loss = 0.0014730701223015785\n",
      "\n",
      "Epoch 854\n",
      "Step 0: loss = 0.022096427157521248, recon_loss = 0.011393215507268906, 0.010685752145946026, kl_loss = 0.0008729519322514534\n",
      "\n",
      "Epoch 855\n",
      "Step 0: loss = 0.02934386394917965, recon_loss = 0.012506483122706413, 0.016802135854959488, kl_loss = 0.0017622169107198715\n",
      "\n",
      "Epoch 856\n",
      "Step 0: loss = 0.024375824257731438, recon_loss = 0.01158955879509449, 0.012738137505948544, kl_loss = 0.0024064332246780396\n",
      "\n",
      "Epoch 857\n",
      "Step 0: loss = 0.022101832553744316, recon_loss = 0.01074761152267456, 0.011327400803565979, kl_loss = 0.0013410355895757675\n",
      "\n",
      "Epoch 858\n",
      "Step 0: loss = 0.02041313424706459, recon_loss = 0.010568616911768913, 0.009814731776714325, kl_loss = 0.0014892872422933578\n",
      "\n",
      "Epoch 859\n",
      "Step 0: loss = 0.022315239533782005, recon_loss = 0.012830454856157303, 0.0094566959887743, kl_loss = 0.0014044130221009254\n",
      "\n",
      "Epoch 860\n",
      "Step 0: loss = 0.025027964264154434, recon_loss = 0.012457897886633873, 0.012541383504867554, kl_loss = 0.0014341427013278008\n",
      "\n",
      "Epoch 861\n",
      "Step 0: loss = 0.02498873509466648, recon_loss = 0.011804008856415749, 0.01316162757575512, kl_loss = 0.0011549657210707664\n",
      "\n",
      "Epoch 862\n",
      "Step 0: loss = 0.025548193603754044, recon_loss = 0.010683268308639526, 0.014821238815784454, kl_loss = 0.002184300683438778\n",
      "\n",
      "Epoch 863\n",
      "Step 0: loss = 0.02313927188515663, recon_loss = 0.011853588744997978, 0.01126174908131361, kl_loss = 0.0011967914178967476\n",
      "\n",
      "Epoch 864\n",
      "Step 0: loss = 0.024112751707434654, recon_loss = 0.010612521320581436, 0.013481946662068367, kl_loss = 0.0009141508489847183\n",
      "\n",
      "Epoch 865\n",
      "Step 0: loss = 0.020312121137976646, recon_loss = 0.010933397337794304, 0.009356104768812656, kl_loss = 0.0011309273540973663\n",
      "\n",
      "Epoch 866\n",
      "Step 0: loss = 0.023609915748238564, recon_loss = 0.011475605890154839, 0.012101215310394764, kl_loss = 0.0016546575352549553\n",
      "\n",
      "Epoch 867\n",
      "Step 0: loss = 0.021613709628582, recon_loss = 0.011434756219387054, 0.010159119963645935, kl_loss = 0.0009917104616761208\n",
      "\n",
      "Epoch 868\n",
      "Step 0: loss = 0.02271622233092785, recon_loss = 0.010740233585238457, 0.011931492947041988, kl_loss = 0.0022248420864343643\n",
      "\n",
      "Epoch 869\n",
      "Step 0: loss = 0.02403019554913044, recon_loss = 0.011138573288917542, 0.012854007072746754, kl_loss = 0.0018807677552103996\n",
      "\n",
      "Epoch 870\n",
      "Step 0: loss = 0.024011844769120216, recon_loss = 0.010531272739171982, 0.013449782505631447, kl_loss = 0.001539439894258976\n",
      "\n",
      "Epoch 871\n",
      "Step 0: loss = 0.023260587826371193, recon_loss = 0.010470371693372726, 0.012726763263344765, kl_loss = 0.00317262951284647\n",
      "\n",
      "Epoch 872\n",
      "Step 0: loss = 0.020566793158650398, recon_loss = 0.009992055594921112, 0.010553356260061264, kl_loss = 0.001069067046046257\n",
      "\n",
      "Epoch 873\n",
      "Step 0: loss = 0.024896027520298958, recon_loss = 0.010716622695326805, 0.014130081050097942, kl_loss = 0.00246621947735548\n",
      "\n",
      "Epoch 874\n",
      "Step 0: loss = 0.028685037046670914, recon_loss = 0.012078853324055672, 0.016523178666830063, kl_loss = 0.004150270484387875\n",
      "\n",
      "Epoch 875\n",
      "Step 0: loss = 0.029059268534183502, recon_loss = 0.011385951191186905, 0.017438773065805435, kl_loss = 0.01172718033194542\n",
      "\n",
      "Epoch 876\n",
      "Step 0: loss = 0.023051371797919273, recon_loss = 0.010362045839428902, 0.012631319463253021, kl_loss = 0.0029003359377384186\n",
      "\n",
      "Epoch 877\n",
      "Step 0: loss = 0.02392740547657013, recon_loss = 0.010637382045388222, 0.013245759531855583, kl_loss = 0.0022132303565740585\n",
      "\n",
      "Epoch 878\n",
      "Step 0: loss = 0.0243772491812706, recon_loss = 0.011767912656068802, 0.01253853552043438, kl_loss = 0.0035400139167904854\n",
      "\n",
      "Epoch 879\n",
      "Step 0: loss = 0.023239122703671455, recon_loss = 0.011410577222704887, 0.01178058423101902, kl_loss = 0.0023980913683772087\n",
      "\n",
      "Epoch 880\n",
      "Step 0: loss = 0.02207271195948124, recon_loss = 0.012140292674303055, 0.00989505648612976, kl_loss = 0.0018681567162275314\n",
      "\n",
      "Epoch 881\n",
      "Step 0: loss = 0.02986176870763302, recon_loss = 0.011565892025828362, 0.01826385036110878, kl_loss = 0.0016012787818908691\n",
      "\n",
      "Epoch 882\n",
      "Step 0: loss = 0.027470722794532776, recon_loss = 0.012061571702361107, 0.015344033017754555, kl_loss = 0.003255940042436123\n",
      "\n",
      "Epoch 883\n",
      "Step 0: loss = 0.02414778247475624, recon_loss = 0.01160590909421444, 0.012497199699282646, kl_loss = 0.00223372969776392\n",
      "\n",
      "Epoch 884\n",
      "Step 0: loss = 0.02273556962609291, recon_loss = 0.011162631213665009, 0.011525966227054596, kl_loss = 0.0023486530408263206\n",
      "\n",
      "Epoch 885\n",
      "Step 0: loss = 0.02254023402929306, recon_loss = 0.011393999680876732, 0.011123857460916042, kl_loss = 0.0011189030483365059\n",
      "\n",
      "Epoch 886\n",
      "Step 0: loss = 0.02444477751851082, recon_loss = 0.010157531127333641, 0.01426592655479908, kl_loss = 0.0010659843683242798\n",
      "\n",
      "Epoch 887\n",
      "Step 0: loss = 0.027501922100782394, recon_loss = 0.010226855054497719, 0.017246628180146217, kl_loss = 0.001421898603439331\n",
      "\n",
      "Epoch 888\n",
      "Step 0: loss = 0.02267598733305931, recon_loss = 0.010584456846117973, 0.01201492641121149, kl_loss = 0.0038302019238471985\n",
      "\n",
      "Epoch 889\n",
      "Step 0: loss = 0.023810653015971184, recon_loss = 0.010798724368214607, 0.012953951954841614, kl_loss = 0.002898838371038437\n",
      "\n",
      "Epoch 890\n",
      "Step 0: loss = 0.024267125874757767, recon_loss = 0.011298727244138718, 0.012913811951875687, kl_loss = 0.0027293236926198006\n",
      "\n",
      "Epoch 891\n",
      "Step 0: loss = 0.033669669181108475, recon_loss = 0.012259336188435555, 0.021352874115109444, kl_loss = 0.002872898243367672\n",
      "\n",
      "Epoch 892\n",
      "Step 0: loss = 0.02664845809340477, recon_loss = 0.012726560235023499, 0.013836521655321121, kl_loss = 0.004268842749297619\n",
      "\n",
      "Epoch 893\n",
      "Step 0: loss = 0.019778722897171974, recon_loss = 0.010226352140307426, 0.009509498253464699, kl_loss = 0.002143627032637596\n",
      "\n",
      "Epoch 894\n",
      "Step 0: loss = 0.023662639781832695, recon_loss = 0.009843414649367332, 0.013785691931843758, kl_loss = 0.0016766926273703575\n",
      "\n",
      "Epoch 895\n",
      "Step 0: loss = 0.02582051232457161, recon_loss = 0.010197890922427177, 0.015600545331835747, kl_loss = 0.001103762537240982\n",
      "\n",
      "Epoch 896\n",
      "Step 0: loss = 0.02346079610288143, recon_loss = 0.009404662996530533, 0.013960842043161392, kl_loss = 0.004764598794281483\n",
      "\n",
      "Epoch 897\n",
      "Step 0: loss = 0.027033012360334396, recon_loss = 0.010294405743479729, 0.01669793389737606, kl_loss = 0.002033608965575695\n",
      "\n",
      "Epoch 898\n",
      "Step 0: loss = 0.02300385758280754, recon_loss = 0.009354108944535255, 0.013533350080251694, kl_loss = 0.005819960497319698\n",
      "\n",
      "Epoch 899\n",
      "Step 0: loss = 0.021828191354870796, recon_loss = 0.009984118863940239, 0.01179354079067707, kl_loss = 0.00252662505954504\n",
      "\n",
      "Epoch 900\n",
      "Step 0: loss = 0.0281081460416317, recon_loss = 0.011264117434620857, 0.016815371811389923, kl_loss = 0.0014328574761748314\n",
      "\n",
      "Epoch 901\n",
      "Step 0: loss = 0.02312520146369934, recon_loss = 0.00974893756210804, 0.013334572315216064, kl_loss = 0.0020846258848905563\n",
      "\n",
      "Epoch 902\n",
      "Step 0: loss = 0.02562539465725422, recon_loss = 0.009476341307163239, 0.01612015999853611, kl_loss = 0.0014446955174207687\n",
      "\n",
      "Epoch 903\n",
      "Step 0: loss = 0.024683380499482155, recon_loss = 0.00999920442700386, 0.014639325439929962, kl_loss = 0.002242565155029297\n",
      "\n",
      "Epoch 904\n",
      "Step 0: loss = 0.02307123690843582, recon_loss = 0.010354802012443542, 0.012690795585513115, kl_loss = 0.0012819468975067139\n",
      "\n",
      "Epoch 905\n",
      "Step 0: loss = 0.020677393302321434, recon_loss = 0.009670576080679893, 0.010970590636134148, kl_loss = 0.001811344176530838\n",
      "\n",
      "Epoch 906\n",
      "Step 0: loss = 0.028401531279087067, recon_loss = 0.00901024416089058, 0.019368669018149376, kl_loss = 0.001130896620452404\n",
      "\n",
      "Epoch 907\n",
      "Step 0: loss = 0.0236408319324255, recon_loss = 0.009649327024817467, 0.013907169923186302, kl_loss = 0.004216710105538368\n",
      "\n",
      "Epoch 908\n",
      "Step 0: loss = 0.02784707583487034, recon_loss = 0.009821856394410133, 0.017959825694561005, kl_loss = 0.0032696686685085297\n",
      "\n",
      "Epoch 909\n",
      "Step 0: loss = 0.021720169112086296, recon_loss = 0.009260926395654678, 0.012423155829310417, kl_loss = 0.0018043341115117073\n",
      "\n",
      "Epoch 910\n",
      "Step 0: loss = 0.024447834119200706, recon_loss = 0.00957571156322956, 0.014852970838546753, kl_loss = 0.0009576305747032166\n",
      "\n",
      "Epoch 911\n",
      "Step 0: loss = 0.018737461417913437, recon_loss = 0.009094148874282837, 0.009606214240193367, kl_loss = 0.0018548909574747086\n",
      "\n",
      "Epoch 912\n",
      "Step 0: loss = 0.01916510984301567, recon_loss = 0.00890137068927288, 0.01024148054420948, kl_loss = 0.0011129239574074745\n",
      "\n",
      "Epoch 913\n",
      "Step 0: loss = 0.02525615692138672, recon_loss = 0.009786456823348999, 0.015313832089304924, kl_loss = 0.00779335480183363\n",
      "\n",
      "Epoch 914\n",
      "Step 0: loss = 0.02522493340075016, recon_loss = 0.011395556852221489, 0.013754704967141151, kl_loss = 0.003733568824827671\n",
      "\n",
      "Epoch 915\n",
      "Step 0: loss = 0.024911243468523026, recon_loss = 0.010625004768371582, 0.01425158977508545, kl_loss = 0.00173247791826725\n",
      "\n",
      "Epoch 916\n",
      "Step 0: loss = 0.022562844678759575, recon_loss = 0.009990300983190536, 0.01253089401870966, kl_loss = 0.0020825183019042015\n",
      "\n",
      "Epoch 917\n",
      "Step 0: loss = 0.02200246974825859, recon_loss = 0.010280141606926918, 0.011684741824865341, kl_loss = 0.0018793325871229172\n",
      "\n",
      "Epoch 918\n",
      "Step 0: loss = 0.02145778201520443, recon_loss = 0.009447667747735977, 0.011996068060398102, kl_loss = 0.0007023010402917862\n",
      "\n",
      "Epoch 919\n",
      "Step 0: loss = 0.023425525054335594, recon_loss = 0.009940844029188156, 0.013468348421156406, kl_loss = 0.000816679559648037\n",
      "\n",
      "Epoch 920\n",
      "Step 0: loss = 0.027314625680446625, recon_loss = 0.01099286787211895, 0.01620604656636715, kl_loss = 0.0057855891063809395\n",
      "\n",
      "Epoch 921\n",
      "Step 0: loss = 0.02240491844713688, recon_loss = 0.010079123079776764, 0.012279348447918892, kl_loss = 0.0023223580792546272\n",
      "\n",
      "Epoch 922\n",
      "Step 0: loss = 0.0230545811355114, recon_loss = 0.011048521846532822, 0.011968597769737244, kl_loss = 0.0018730340525507927\n",
      "\n",
      "Epoch 923\n",
      "Step 0: loss = 0.02411310374736786, recon_loss = 0.00935349054634571, 0.01474087405949831, kl_loss = 0.0009368974715471268\n",
      "\n",
      "Epoch 924\n",
      "Step 0: loss = 0.02162676490843296, recon_loss = 0.009199485182762146, 0.01236972026526928, kl_loss = 0.0028779422864317894\n",
      "\n",
      "Epoch 925\n",
      "Step 0: loss = 0.03002307191491127, recon_loss = 0.009886335581541061, 0.020053276792168617, kl_loss = 0.00417301245033741\n",
      "\n",
      "Epoch 926\n",
      "Step 0: loss = 0.024212095886468887, recon_loss = 0.009859280660748482, 0.01429794728755951, kl_loss = 0.0027434062212705612\n",
      "\n",
      "Epoch 927\n",
      "Step 0: loss = 0.02136259712278843, recon_loss = 0.009204637259244919, 0.012105436995625496, kl_loss = 0.0026261406019330025\n",
      "\n",
      "Epoch 928\n",
      "Step 0: loss = 0.024071605876088142, recon_loss = 0.010554192587733269, 0.013466477394104004, kl_loss = 0.0025467630475759506\n",
      "\n",
      "Epoch 929\n",
      "Step 0: loss = 0.02000972256064415, recon_loss = 0.009807923808693886, 0.010166670195758343, kl_loss = 0.0017564324662089348\n",
      "\n",
      "Epoch 930\n",
      "Step 0: loss = 0.026979994028806686, recon_loss = 0.010035192593932152, 0.016905177384614944, kl_loss = 0.0019811606034636497\n",
      "\n",
      "Epoch 931\n",
      "Step 0: loss = 0.021524779498577118, recon_loss = 0.010021144524216652, 0.011483224108815193, kl_loss = 0.0010205740109086037\n",
      "\n",
      "Epoch 932\n",
      "Step 0: loss = 0.024708401411771774, recon_loss = 0.00900762714445591, 0.01564321294426918, kl_loss = 0.0028780419379472733\n",
      "\n",
      "Epoch 933\n",
      "Step 0: loss = 0.02144765481352806, recon_loss = 0.009327098727226257, 0.012065846472978592, kl_loss = 0.002735484391450882\n",
      "\n",
      "Epoch 934\n",
      "Step 0: loss = 0.025718631222844124, recon_loss = 0.009947791695594788, 0.01574503257870674, kl_loss = 0.0012903492897748947\n",
      "\n",
      "Epoch 935\n",
      "Step 0: loss = 0.022068992257118225, recon_loss = 0.009435778483748436, 0.012613575905561447, kl_loss = 0.000981857068836689\n",
      "\n",
      "Epoch 936\n",
      "Step 0: loss = 0.020907076075673103, recon_loss = 0.008969422429800034, 0.01191642601042986, kl_loss = 0.0010614506900310516\n",
      "\n",
      "Epoch 937\n",
      "Step 0: loss = 0.026053989306092262, recon_loss = 0.010167323052883148, 0.015804404392838478, kl_loss = 0.004113048315048218\n",
      "\n",
      "Epoch 938\n",
      "Step 0: loss = 0.030360911041498184, recon_loss = 0.011095775291323662, 0.019186360761523247, kl_loss = 0.003938710317015648\n",
      "\n",
      "Epoch 939\n",
      "Step 0: loss = 0.02585492469370365, recon_loss = 0.010438576340675354, 0.015337133780121803, kl_loss = 0.003960683010518551\n",
      "\n",
      "Epoch 940\n",
      "Step 0: loss = 0.022511126473546028, recon_loss = 0.010546451434493065, 0.011931242421269417, kl_loss = 0.0016716094687581062\n",
      "\n",
      "Epoch 941\n",
      "Step 0: loss = 0.02045048214495182, recon_loss = 0.00933496467769146, 0.011066031642258167, kl_loss = 0.002474200911819935\n",
      "\n",
      "Epoch 942\n",
      "Step 0: loss = 0.019268209114670753, recon_loss = 0.009544780477881432, 0.009685918688774109, kl_loss = 0.0018755057826638222\n",
      "\n",
      "Epoch 943\n",
      "Step 0: loss = 0.018852774053812027, recon_loss = 0.008490849286317825, 0.010340113192796707, kl_loss = 0.0010905861854553223\n",
      "\n",
      "Epoch 944\n",
      "Step 0: loss = 0.020047780126333237, recon_loss = 0.00814279355108738, 0.01189095713198185, kl_loss = 0.0007014535367488861\n",
      "\n",
      "Epoch 945\n",
      "Step 0: loss = 0.020130787044763565, recon_loss = 0.008022569119930267, 0.012066884897649288, kl_loss = 0.0020666252821683884\n",
      "\n",
      "Epoch 946\n",
      "Step 0: loss = 0.019697733223438263, recon_loss = 0.008462468162178993, 0.011215779930353165, kl_loss = 0.0009742900729179382\n",
      "\n",
      "Epoch 947\n",
      "Step 0: loss = 0.024787945672869682, recon_loss = 0.008371995761990547, 0.016385771334171295, kl_loss = 0.0015089139342308044\n",
      "\n",
      "Epoch 948\n",
      "Step 0: loss = 0.021540842950344086, recon_loss = 0.008981909602880478, 0.012541020289063454, kl_loss = 0.0008956911042332649\n",
      "\n",
      "Epoch 949\n",
      "Step 0: loss = 0.018844643607735634, recon_loss = 0.007742289453744888, 0.011080095544457436, kl_loss = 0.0011129220947623253\n",
      "\n",
      "Epoch 950\n",
      "Step 0: loss = 0.019690729677677155, recon_loss = 0.008872276172041893, 0.010785864666104317, kl_loss = 0.0016294270753860474\n",
      "\n",
      "Epoch 951\n",
      "Step 0: loss = 0.017457986250519753, recon_loss = 0.008586408570408821, 0.008829761296510696, kl_loss = 0.0020908061414957047\n",
      "\n",
      "Epoch 952\n",
      "Step 0: loss = 0.020479781553149223, recon_loss = 0.008050581440329552, 0.012388361617922783, kl_loss = 0.0020419154316186905\n",
      "\n",
      "Epoch 953\n",
      "Step 0: loss = 0.020922450348734856, recon_loss = 0.00860239751636982, 0.012269661761820316, kl_loss = 0.0025194883346557617\n",
      "\n",
      "Epoch 954\n",
      "Step 0: loss = 0.02387554571032524, recon_loss = 0.008427849039435387, 0.015429140999913216, kl_loss = 0.0009277528151869774\n",
      "\n",
      "Epoch 955\n",
      "Step 0: loss = 0.023357724770903587, recon_loss = 0.007813870906829834, 0.015521818771958351, kl_loss = 0.0011017480865120888\n",
      "\n",
      "Epoch 956\n",
      "Step 0: loss = 0.02223665826022625, recon_loss = 0.008624142035841942, 0.013593586161732674, kl_loss = 0.0009464826434850693\n",
      "\n",
      "Epoch 957\n",
      "Step 0: loss = 0.02271222695708275, recon_loss = 0.009356152266263962, 0.013312822207808495, kl_loss = 0.002162669785320759\n",
      "\n",
      "Epoch 958\n",
      "Step 0: loss = 0.023602362722158432, recon_loss = 0.009374774992465973, 0.014201809652149677, kl_loss = 0.0012889355421066284\n",
      "\n",
      "Epoch 959\n",
      "Step 0: loss = 0.0244193933904171, recon_loss = 0.009811695665121078, 0.014559105969965458, kl_loss = 0.002429647371172905\n",
      "\n",
      "Epoch 960\n",
      "Step 0: loss = 0.017468079924583435, recon_loss = 0.008890526369214058, 0.008556365966796875, kl_loss = 0.0010594073683023453\n",
      "\n",
      "Epoch 961\n",
      "Step 0: loss = 0.021061502397060394, recon_loss = 0.008703747764229774, 0.012343069538474083, kl_loss = 0.0007342575117945671\n",
      "\n",
      "Epoch 962\n",
      "Step 0: loss = 0.02115502767264843, recon_loss = 0.008706338703632355, 0.01242872979491949, kl_loss = 0.0009979009628295898\n",
      "\n",
      "Epoch 963\n",
      "Step 0: loss = 0.017094096168875694, recon_loss = 0.008433336392045021, 0.008615545928478241, kl_loss = 0.0022606588900089264\n",
      "\n",
      "Epoch 964\n",
      "Step 0: loss = 0.023504050448536873, recon_loss = 0.00940440408885479, 0.014058690518140793, kl_loss = 0.0020478079095482826\n",
      "\n",
      "Epoch 965\n",
      "Step 0: loss = 0.01892145536839962, recon_loss = 0.008297653868794441, 0.010596562176942825, kl_loss = 0.0013619549572467804\n",
      "\n",
      "Epoch 966\n",
      "Step 0: loss = 0.019014200195670128, recon_loss = 0.00865841843187809, 0.010338934138417244, kl_loss = 0.0008423859253525734\n",
      "\n",
      "Epoch 967\n",
      "Step 0: loss = 0.025766646489501, recon_loss = 0.009060399606823921, 0.016679367050528526, kl_loss = 0.0013439878821372986\n",
      "\n",
      "Epoch 968\n",
      "Step 0: loss = 0.0198808666318655, recon_loss = 0.008122790604829788, 0.011732574552297592, kl_loss = 0.0012750709429383278\n",
      "\n",
      "Epoch 969\n",
      "Step 0: loss = 0.021088112145662308, recon_loss = 0.007845597341656685, 0.013227288611233234, kl_loss = 0.000761290080845356\n",
      "\n",
      "Epoch 970\n",
      "Step 0: loss = 0.019935838878154755, recon_loss = 0.007809566333889961, 0.012014484964311123, kl_loss = 0.005589427426457405\n",
      "\n",
      "Epoch 971\n",
      "Step 0: loss = 0.018642231822013855, recon_loss = 0.007978647947311401, 0.01062389649450779, kl_loss = 0.0019843922927975655\n",
      "\n",
      "Epoch 972\n",
      "Step 0: loss = 0.016421284526586533, recon_loss = 0.0078797098249197, 0.008523499593138695, kl_loss = 0.0009038010612130165\n",
      "\n",
      "Epoch 973\n",
      "Step 0: loss = 0.02240254357457161, recon_loss = 0.008604282513260841, 0.013779206201434135, kl_loss = 0.0009526992216706276\n",
      "\n",
      "Epoch 974\n",
      "Step 0: loss = 0.020826835185289383, recon_loss = 0.008749213069677353, 0.012017033994197845, kl_loss = 0.0030294042080640793\n",
      "\n",
      "Epoch 975\n",
      "Step 0: loss = 0.020148485898971558, recon_loss = 0.008632240816950798, 0.011399809271097183, kl_loss = 0.005821744911372662\n",
      "\n",
      "Epoch 976\n",
      "Step 0: loss = 0.01936912350356579, recon_loss = 0.008325615897774696, 0.011009329929947853, kl_loss = 0.0017089201137423515\n",
      "\n",
      "Epoch 977\n",
      "Step 0: loss = 0.0237323809415102, recon_loss = 0.00853407196700573, 0.015145247802138329, kl_loss = 0.0026530390605330467\n",
      "\n",
      "Epoch 978\n",
      "Step 0: loss = 0.017952624708414078, recon_loss = 0.008066771551966667, 0.009854098781943321, kl_loss = 0.0015877606347203255\n",
      "\n",
      "Epoch 979\n",
      "Step 0: loss = 0.022567156702280045, recon_loss = 0.007867569103837013, 0.0146650904789567, kl_loss = 0.001724802888929844\n",
      "\n",
      "Epoch 980\n",
      "Step 0: loss = 0.020099841058254242, recon_loss = 0.008558893576264381, 0.01144087128341198, kl_loss = 0.005003791302442551\n",
      "\n",
      "Epoch 981\n",
      "Step 0: loss = 0.021734915673732758, recon_loss = 0.007695561274886131, 0.013987565413117409, kl_loss = 0.002589424140751362\n",
      "\n",
      "Epoch 982\n",
      "Step 0: loss = 0.01972789131104946, recon_loss = 0.008532736450433731, 0.011156055144965649, kl_loss = 0.0019549764692783356\n",
      "\n",
      "Epoch 983\n",
      "Step 0: loss = 0.021159138530492783, recon_loss = 0.007622484117746353, 0.01350413914769888, kl_loss = 0.0016257185488939285\n",
      "\n",
      "Epoch 984\n",
      "Step 0: loss = 0.018096741288900375, recon_loss = 0.007490759715437889, 0.01058102585375309, kl_loss = 0.0012477915734052658\n",
      "\n",
      "Epoch 985\n",
      "Step 0: loss = 0.01825156807899475, recon_loss = 0.007219396531581879, 0.011019711382687092, kl_loss = 0.0006230873987078667\n",
      "\n",
      "Epoch 986\n",
      "Step 0: loss = 0.02210751734673977, recon_loss = 0.007756490260362625, 0.014334393665194511, kl_loss = 0.0008316673338413239\n",
      "\n",
      "Epoch 987\n",
      "Step 0: loss = 0.020824983716011047, recon_loss = 0.007655942812561989, 0.013146539218723774, kl_loss = 0.0011250544339418411\n",
      "\n",
      "Epoch 988\n",
      "Step 0: loss = 0.017553670331835747, recon_loss = 0.007592987269163132, 0.009933194145560265, kl_loss = 0.0013744309544563293\n",
      "\n",
      "Epoch 989\n",
      "Step 0: loss = 0.01997770555317402, recon_loss = 0.007369494065642357, 0.012553507462143898, kl_loss = 0.0027351826429367065\n",
      "\n",
      "Epoch 990\n",
      "Step 0: loss = 0.019499316811561584, recon_loss = 0.007238907739520073, 0.012227021157741547, kl_loss = 0.001669381745159626\n",
      "\n",
      "Epoch 991\n",
      "Step 0: loss = 0.026456013321876526, recon_loss = 0.007902706041932106, 0.018534746021032333, kl_loss = 0.0009280648082494736\n",
      "\n",
      "Epoch 992\n",
      "Step 0: loss = 0.018946200609207153, recon_loss = 0.007855791598558426, 0.011072833091020584, kl_loss = 0.0008787615224719048\n",
      "\n",
      "Epoch 993\n",
      "Step 0: loss = 0.023664424195885658, recon_loss = 0.007136533036828041, 0.016505638137459755, kl_loss = 0.0011126846075057983\n",
      "\n",
      "Epoch 994\n",
      "Step 0: loss = 0.020245034247636795, recon_loss = 0.007327202707529068, 0.012775247916579247, kl_loss = 0.007129224948585033\n",
      "\n",
      "Epoch 995\n",
      "Step 0: loss = 0.020109685137867928, recon_loss = 0.007740067318081856, 0.012328819371759892, kl_loss = 0.0020398441702127457\n",
      "\n",
      "Epoch 996\n",
      "Step 0: loss = 0.022143632173538208, recon_loss = 0.008533034473657608, 0.013578766956925392, kl_loss = 0.0015915008261799812\n",
      "\n",
      "Epoch 997\n",
      "Step 0: loss = 0.023469630628824234, recon_loss = 0.008734909817576408, 0.014701342210173607, kl_loss = 0.0016689654439687729\n",
      "\n",
      "Epoch 998\n",
      "Step 0: loss = 0.020570527762174606, recon_loss = 0.00883033312857151, 0.011703471653163433, kl_loss = 0.0018361611291766167\n",
      "\n",
      "Epoch 999\n",
      "Step 0: loss = 0.020812148228287697, recon_loss = 0.008363695815205574, 0.012408707290887833, kl_loss = 0.001987299881875515\n",
      "\n",
      "Epoch 1000\n",
      "Step 0: loss = 0.02099805697798729, recon_loss = 0.00854698196053505, 0.012379670515656471, kl_loss = 0.0035702064633369446\n",
      "\n",
      "Epoch 1001\n",
      "Step 0: loss = 0.01874060370028019, recon_loss = 0.008730398491024971, 0.009956622496247292, kl_loss = 0.0026791179552674294\n",
      "\n",
      "Epoch 1002\n",
      "Step 0: loss = 0.020316261798143387, recon_loss = 0.008024781942367554, 0.01214587315917015, kl_loss = 0.007280288264155388\n",
      "\n",
      "Epoch 1003\n",
      "Step 0: loss = 0.022449061274528503, recon_loss = 0.009405728429555893, 0.012981592677533627, kl_loss = 0.003086998127400875\n",
      "\n",
      "Epoch 1004\n",
      "Step 0: loss = 0.02184048667550087, recon_loss = 0.008614782243967056, 0.013199239037930965, kl_loss = 0.0013232585042715073\n",
      "\n",
      "Epoch 1005\n",
      "Step 0: loss = 0.019537266343832016, recon_loss = 0.007967248558998108, 0.011534269899129868, kl_loss = 0.0017874157056212425\n",
      "\n",
      "Epoch 1006\n",
      "Step 0: loss = 0.01963134855031967, recon_loss = 0.007851850241422653, 0.011754602193832397, kl_loss = 0.0012447992339730263\n",
      "\n",
      "Epoch 1007\n",
      "Step 0: loss = 0.019605711102485657, recon_loss = 0.007315637543797493, 0.012242771685123444, kl_loss = 0.002365139313042164\n",
      "\n",
      "Epoch 1008\n",
      "Step 0: loss = 0.018692197278141975, recon_loss = 0.007871290668845177, 0.010740416124463081, kl_loss = 0.004024548456072807\n",
      "\n",
      "Epoch 1009\n",
      "Step 0: loss = 0.020581495016813278, recon_loss = 0.00836426392197609, 0.01216639019548893, kl_loss = 0.00254201702773571\n",
      "\n",
      "Epoch 1010\n",
      "Step 0: loss = 0.020414412021636963, recon_loss = 0.007825765758752823, 0.012558292597532272, kl_loss = 0.0015176516026258469\n",
      "\n",
      "Epoch 1011\n",
      "Step 0: loss = 0.023929452523589134, recon_loss = 0.008192483335733414, 0.015620378777384758, kl_loss = 0.005829513072967529\n",
      "\n",
      "Epoch 1012\n",
      "Step 0: loss = 0.022110164165496826, recon_loss = 0.008646698668599129, 0.013415815308690071, kl_loss = 0.0023825541138648987\n",
      "\n",
      "Epoch 1013\n",
      "Step 0: loss = 0.01696748659014702, recon_loss = 0.008485117927193642, 0.008442694321274757, kl_loss = 0.0019837636500597\n",
      "\n",
      "Epoch 1014\n",
      "Step 0: loss = 0.01709412783384323, recon_loss = 0.008047644048929214, 0.009010999463498592, kl_loss = 0.0017741648480296135\n",
      "\n",
      "Epoch 1015\n",
      "Step 0: loss = 0.021929936483502388, recon_loss = 0.009040392935276031, 0.012858306989073753, kl_loss = 0.001561800017952919\n",
      "\n",
      "Epoch 1016\n",
      "Step 0: loss = 0.022198369726538658, recon_loss = 0.008673001080751419, 0.01348079927265644, kl_loss = 0.002228507772088051\n",
      "\n",
      "Epoch 1017\n",
      "Step 0: loss = 0.020857948809862137, recon_loss = 0.008244996890425682, 0.012567722238600254, kl_loss = 0.002261417917907238\n",
      "\n",
      "Epoch 1018\n",
      "Step 0: loss = 0.018047552555799484, recon_loss = 0.007823426276445389, 0.01017956342548132, kl_loss = 0.0022281119599938393\n",
      "\n",
      "Epoch 1019\n",
      "Step 0: loss = 0.02537315897643566, recon_loss = 0.007382316514849663, 0.017965082079172134, kl_loss = 0.0012880275025963783\n",
      "\n",
      "Epoch 1020\n",
      "Step 0: loss = 0.01710311882197857, recon_loss = 0.007322277873754501, 0.009747294709086418, kl_loss = 0.0016772942617535591\n",
      "\n",
      "Epoch 1021\n",
      "Step 0: loss = 0.018817352131009102, recon_loss = 0.0071720704436302185, 0.011614935472607613, kl_loss = 0.0015172958374023438\n",
      "\n",
      "Epoch 1022\n",
      "Step 0: loss = 0.01704779453575611, recon_loss = 0.007350413128733635, 0.00966539978981018, kl_loss = 0.0015990789979696274\n",
      "\n",
      "Epoch 1023\n",
      "Step 0: loss = 0.018721578642725945, recon_loss = 0.006475802510976791, 0.012198781594634056, kl_loss = 0.0023497361689805984\n",
      "\n",
      "Epoch 1024\n",
      "Step 0: loss = 0.02179737389087677, recon_loss = 0.007118208333849907, 0.01463063433766365, kl_loss = 0.0024265358224511147\n",
      "\n",
      "Epoch 1025\n",
      "Step 0: loss = 0.019977714866399765, recon_loss = 0.00780472531914711, 0.012076755054295063, kl_loss = 0.00481181126087904\n",
      "\n",
      "Epoch 1026\n",
      "Step 0: loss = 0.020673612132668495, recon_loss = 0.007329870015382767, 0.013303402811288834, kl_loss = 0.002016942948102951\n",
      "\n",
      "Epoch 1027\n",
      "Step 0: loss = 0.020556194707751274, recon_loss = 0.007691655308008194, 0.012841296382248402, kl_loss = 0.0011621732264757156\n",
      "\n",
      "Epoch 1028\n",
      "Step 0: loss = 0.0213386919349432, recon_loss = 0.0071037933230400085, 0.014205563813447952, kl_loss = 0.0014667576178908348\n",
      "\n",
      "Epoch 1029\n",
      "Step 0: loss = 0.023134885355830193, recon_loss = 0.007542738690972328, 0.015574639663100243, kl_loss = 0.0008753202855587006\n",
      "\n",
      "Epoch 1030\n",
      "Step 0: loss = 0.026104722172021866, recon_loss = 0.0079983901232481, 0.018068889155983925, kl_loss = 0.0018721213564276695\n",
      "\n",
      "Epoch 1031\n",
      "Step 0: loss = 0.022508826106786728, recon_loss = 0.007831240072846413, 0.01461390033364296, kl_loss = 0.003184281289577484\n",
      "\n",
      "Epoch 1032\n",
      "Step 0: loss = 0.019785387441515923, recon_loss = 0.00700804777443409, 0.012717045843601227, kl_loss = 0.0030147219076752663\n",
      "\n",
      "Epoch 1033\n",
      "Step 0: loss = 0.02216031774878502, recon_loss = 0.0071121696382761, 0.01498374342918396, kl_loss = 0.0032202433794736862\n",
      "\n",
      "Epoch 1034\n",
      "Step 0: loss = 0.019974738359451294, recon_loss = 0.006780026480555534, 0.013160684145987034, kl_loss = 0.0017013056203722954\n",
      "\n",
      "Epoch 1035\n",
      "Step 0: loss = 0.018783483654260635, recon_loss = 0.006656359881162643, 0.01211100909858942, kl_loss = 0.0008057765662670135\n",
      "\n",
      "Epoch 1036\n",
      "Step 0: loss = 0.019245317205786705, recon_loss = 0.007141422480344772, 0.012034878134727478, kl_loss = 0.003450818359851837\n",
      "\n",
      "Epoch 1037\n",
      "Step 0: loss = 0.0231756791472435, recon_loss = 0.0074604954570531845, 0.01567395217716694, kl_loss = 0.0020615411922335625\n",
      "\n",
      "Epoch 1038\n",
      "Step 0: loss = 0.022778593003749847, recon_loss = 0.008033404126763344, 0.014709966257214546, kl_loss = 0.0017610937356948853\n",
      "\n",
      "Epoch 1039\n",
      "Step 0: loss = 0.023044822737574577, recon_loss = 0.008536243811249733, 0.014473390765488148, kl_loss = 0.0017593475058674812\n",
      "\n",
      "Epoch 1040\n",
      "Step 0: loss = 0.017176618799567223, recon_loss = 0.0075906626880168915, 0.009524606168270111, kl_loss = 0.0030675362795591354\n",
      "\n",
      "Epoch 1041\n",
      "Step 0: loss = 0.022598188370466232, recon_loss = 0.007907509803771973, 0.01465983409434557, kl_loss = 0.0015422962605953217\n",
      "\n",
      "Epoch 1042\n",
      "Step 0: loss = 0.017952734604477882, recon_loss = 0.006771788001060486, 0.011149914003908634, kl_loss = 0.0015516895800828934\n",
      "\n",
      "Epoch 1043\n",
      "Step 0: loss = 0.018962563946843147, recon_loss = 0.007948940619826317, 0.01098639890551567, kl_loss = 0.001361234113574028\n",
      "\n",
      "Epoch 1044\n",
      "Step 0: loss = 0.024179313331842422, recon_loss = 0.007469102740287781, 0.01668734848499298, kl_loss = 0.001143103465437889\n",
      "\n",
      "Epoch 1045\n",
      "Step 0: loss = 0.019531559199094772, recon_loss = 0.007570814341306686, 0.011922674253582954, kl_loss = 0.0019035758450627327\n",
      "\n",
      "Epoch 1046\n",
      "Step 0: loss = 0.01890825852751732, recon_loss = 0.00707617960870266, 0.01175497006624937, kl_loss = 0.0038555003702640533\n",
      "\n",
      "Epoch 1047\n",
      "Step 0: loss = 0.020524762570858, recon_loss = 0.0079641193151474, 0.012520244345068932, kl_loss = 0.0020199771970510483\n",
      "\n",
      "Epoch 1048\n",
      "Step 0: loss = 0.018292566761374474, recon_loss = 0.007061893120408058, 0.011185156181454659, kl_loss = 0.0022758357226848602\n",
      "\n",
      "Epoch 1049\n",
      "Step 0: loss = 0.02203952521085739, recon_loss = 0.007754703983664513, 0.014263004064559937, kl_loss = 0.0010908180847764015\n",
      "\n",
      "Epoch 1050\n",
      "Step 0: loss = 0.02038148231804371, recon_loss = 0.007346533238887787, 0.01301927212625742, kl_loss = 0.0007839268073439598\n",
      "\n",
      "Epoch 1051\n",
      "Step 0: loss = 0.020775698125362396, recon_loss = 0.007422024384140968, 0.013328572735190392, kl_loss = 0.0012550922110676765\n",
      "\n",
      "Epoch 1052\n",
      "Step 0: loss = 0.0235051978379488, recon_loss = 0.007983870804309845, 0.015489807352423668, kl_loss = 0.0015760036185383797\n",
      "\n",
      "Epoch 1053\n",
      "Step 0: loss = 0.020320221781730652, recon_loss = 0.007256137207150459, 0.013004754669964314, kl_loss = 0.0029664160683751106\n",
      "\n",
      "Epoch 1054\n",
      "Step 0: loss = 0.014217931777238846, recon_loss = 0.006347520276904106, 0.007849005982279778, kl_loss = 0.0010702833533287048\n",
      "\n",
      "Epoch 1055\n",
      "Step 0: loss = 0.01878601312637329, recon_loss = 0.006753847002983093, 0.012018343433737755, kl_loss = 0.0006911130622029305\n",
      "\n",
      "Epoch 1056\n",
      "Step 0: loss = 0.016118889674544334, recon_loss = 0.005941128358244896, 0.010162806138396263, kl_loss = 0.0007477840408682823\n",
      "\n",
      "Epoch 1057\n",
      "Step 0: loss = 0.018614942207932472, recon_loss = 0.006686052307486534, 0.011907927691936493, kl_loss = 0.0010481495410203934\n",
      "\n",
      "Epoch 1058\n",
      "Step 0: loss = 0.02023748680949211, recon_loss = 0.007414354011416435, 0.01281027216464281, kl_loss = 0.0006429441273212433\n",
      "\n",
      "Epoch 1059\n",
      "Step 0: loss = 0.020067717880010605, recon_loss = 0.0063743628561496735, 0.013642721809446812, kl_loss = 0.0025317342951893806\n",
      "\n",
      "Epoch 1060\n",
      "Step 0: loss = 0.020668089389801025, recon_loss = 0.006773022934794426, 0.013874100521206856, kl_loss = 0.0010482566431164742\n",
      "\n",
      "Epoch 1061\n",
      "Step 0: loss = 0.016940826550126076, recon_loss = 0.0068418364971876144, 0.010018748231232166, kl_loss = 0.0040120286867022514\n",
      "\n",
      "Epoch 1062\n",
      "Step 0: loss = 0.018873026594519615, recon_loss = 0.007359540089964867, 0.011460695415735245, kl_loss = 0.002639596350491047\n",
      "\n",
      "Epoch 1063\n",
      "Step 0: loss = 0.01924029365181923, recon_loss = 0.0066991932690143585, 0.0125181395560503, kl_loss = 0.0011480813845992088\n",
      "\n",
      "Epoch 1064\n",
      "Step 0: loss = 0.021923210471868515, recon_loss = 0.00694073922932148, 0.014947385527193546, kl_loss = 0.0017541926354169846\n",
      "\n",
      "Epoch 1065\n",
      "Step 0: loss = 0.018691740930080414, recon_loss = 0.007022557780146599, 0.011627105996012688, kl_loss = 0.002103854902088642\n",
      "\n",
      "Epoch 1066\n",
      "Step 0: loss = 0.02513911947607994, recon_loss = 0.006989331915974617, 0.01809859834611416, kl_loss = 0.0025594672188162804\n",
      "\n",
      "Epoch 1067\n",
      "Step 0: loss = 0.02375970594584942, recon_loss = 0.008298134431242943, 0.015435650013387203, kl_loss = 0.0012961607426404953\n",
      "\n",
      "Epoch 1068\n",
      "Step 0: loss = 0.02077668160200119, recon_loss = 0.007788997143507004, 0.012933941558003426, kl_loss = 0.002687164582312107\n",
      "\n",
      "Epoch 1069\n",
      "Step 0: loss = 0.019988393411040306, recon_loss = 0.007056590169668198, 0.012900298461318016, kl_loss = 0.0015752147883176804\n",
      "\n",
      "Epoch 1070\n",
      "Step 0: loss = 0.019313756376504898, recon_loss = 0.00729365274310112, 0.012002874165773392, kl_loss = 0.0008615050464868546\n",
      "\n",
      "Epoch 1071\n",
      "Step 0: loss = 0.026887280866503716, recon_loss = 0.008258374407887459, 0.018608707934617996, kl_loss = 0.0010098842903971672\n",
      "\n",
      "Epoch 1072\n",
      "Step 0: loss = 0.01681736670434475, recon_loss = 0.007348464801907539, 0.009439462795853615, kl_loss = 0.0014719795435667038\n",
      "\n",
      "Epoch 1073\n",
      "Step 0: loss = 0.020303864032030106, recon_loss = 0.007898703217506409, 0.012383181601762772, kl_loss = 0.00109894759953022\n",
      "\n",
      "Epoch 1074\n",
      "Step 0: loss = 0.01928829960525036, recon_loss = 0.007632177323102951, 0.011611835099756718, kl_loss = 0.0022143730893731117\n",
      "\n",
      "Epoch 1075\n",
      "Step 0: loss = 0.02116696536540985, recon_loss = 0.007439516484737396, 0.013696039095520973, kl_loss = 0.0015704436227679253\n",
      "\n",
      "Epoch 1076\n",
      "Step 0: loss = 0.023792898282408714, recon_loss = 0.007951227948069572, 0.01580926775932312, kl_loss = 0.0016201138496398926\n",
      "\n",
      "Epoch 1077\n",
      "Step 0: loss = 0.0181129053235054, recon_loss = 0.007571486756205559, 0.0105185741558671, kl_loss = 0.0011421935632824898\n",
      "\n",
      "Epoch 1078\n",
      "Step 0: loss = 0.026039090007543564, recon_loss = 0.008065527305006981, 0.017906159162521362, kl_loss = 0.003370131365954876\n",
      "\n",
      "Epoch 1079\n",
      "Step 0: loss = 0.01950954459607601, recon_loss = 0.0077895913273096085, 0.011674721725285053, kl_loss = 0.002261551097035408\n",
      "\n",
      "Epoch 1080\n",
      "Step 0: loss = 0.02048339881002903, recon_loss = 0.007031869143247604, 0.013416828587651253, kl_loss = 0.001735062338411808\n",
      "\n",
      "Epoch 1081\n",
      "Step 0: loss = 0.01830875128507614, recon_loss = 0.007086077705025673, 0.01118551567196846, kl_loss = 0.0018578972667455673\n",
      "\n",
      "Epoch 1082\n",
      "Step 0: loss = 0.019576173275709152, recon_loss = 0.00775885209441185, 0.01178634911775589, kl_loss = 0.0015486450865864754\n",
      "\n",
      "Epoch 1083\n",
      "Step 0: loss = 0.017292750999331474, recon_loss = 0.006440725177526474, 0.01082693412899971, kl_loss = 0.0012545827776193619\n",
      "\n",
      "Epoch 1084\n",
      "Step 0: loss = 0.013569372706115246, recon_loss = 0.006274024024605751, 0.007257559336721897, kl_loss = 0.0018894746899604797\n",
      "\n",
      "Epoch 1085\n",
      "Step 0: loss = 0.018349947407841682, recon_loss = 0.0068935733288526535, 0.011440806090831757, kl_loss = 0.0007783975452184677\n",
      "\n",
      "Epoch 1086\n",
      "Step 0: loss = 0.025222407653927803, recon_loss = 0.007041860371828079, 0.018153000622987747, kl_loss = 0.0013773227110505104\n",
      "\n",
      "Epoch 1087\n",
      "Step 0: loss = 0.020352093502879143, recon_loss = 0.007021639496088028, 0.013284625485539436, kl_loss = 0.0022914335131645203\n",
      "\n",
      "Epoch 1088\n",
      "Step 0: loss = 0.016253381967544556, recon_loss = 0.005879109725356102, 0.010345280170440674, kl_loss = 0.0014495635405182838\n",
      "\n",
      "Epoch 1089\n",
      "Step 0: loss = 0.01955818198621273, recon_loss = 0.006985999643802643, 0.012550057843327522, kl_loss = 0.0011062202975153923\n",
      "\n",
      "Epoch 1090\n",
      "Step 0: loss = 0.022744452580809593, recon_loss = 0.00705215148627758, 0.01566455140709877, kl_loss = 0.0013875039294362068\n",
      "\n",
      "Epoch 1091\n",
      "Step 0: loss = 0.016500744968652725, recon_loss = 0.0070600807666778564, 0.009416038170456886, kl_loss = 0.0012313192710280418\n",
      "\n",
      "Epoch 1092\n",
      "Step 0: loss = 0.016458651050925255, recon_loss = 0.006598351523280144, 0.009842297062277794, kl_loss = 0.0009001661092042923\n",
      "\n",
      "Epoch 1093\n",
      "Step 0: loss = 0.015704026445746422, recon_loss = 0.006553869694471359, 0.009132560342550278, kl_loss = 0.000879860483109951\n",
      "\n",
      "Epoch 1094\n",
      "Step 0: loss = 0.022019315510988235, recon_loss = 0.006956767290830612, 0.015019407495856285, kl_loss = 0.00215701200067997\n",
      "\n",
      "Epoch 1095\n",
      "Step 0: loss = 0.018556347116827965, recon_loss = 0.006364710628986359, 0.012163950130343437, kl_loss = 0.0013843383640050888\n",
      "\n",
      "Epoch 1096\n",
      "Step 0: loss = 0.01723438873887062, recon_loss = 0.0068382378667593, 0.010355864651501179, kl_loss = 0.002014297991991043\n",
      "\n",
      "Epoch 1097\n",
      "Step 0: loss = 0.0194979440420866, recon_loss = 0.006589651107788086, 0.012846128083765507, kl_loss = 0.0031082434579730034\n",
      "\n",
      "Epoch 1098\n",
      "Step 0: loss = 0.018229378387331963, recon_loss = 0.0075051914900541306, 0.010634670034050941, kl_loss = 0.00447580311447382\n",
      "\n",
      "Epoch 1099\n",
      "Step 0: loss = 0.019967466592788696, recon_loss = 0.00806082971394062, 0.01180319394916296, kl_loss = 0.005172231234610081\n",
      "\n",
      "Epoch 1100\n",
      "Step 0: loss = 0.019004611298441887, recon_loss = 0.007662201300263405, 0.011289743706583977, kl_loss = 0.0026332810521125793\n",
      "\n",
      "Epoch 1101\n",
      "Step 0: loss = 0.018718793988227844, recon_loss = 0.00711141899228096, 0.011579625308513641, kl_loss = 0.0013875039294362068\n",
      "\n",
      "Epoch 1102\n",
      "Step 0: loss = 0.01771964132785797, recon_loss = 0.006516352295875549, 0.01114104874432087, kl_loss = 0.00311196967959404\n",
      "\n",
      "Epoch 1103\n",
      "Step 0: loss = 0.019161706790328026, recon_loss = 0.006682705134153366, 0.01244379673153162, kl_loss = 0.0017602620646357536\n",
      "\n",
      "Epoch 1104\n",
      "Step 0: loss = 0.016023840755224228, recon_loss = 0.005992742255330086, 0.009969412349164486, kl_loss = 0.0030843177810311317\n",
      "\n",
      "Epoch 1105\n",
      "Step 0: loss = 0.019889995455741882, recon_loss = 0.006034841760993004, 0.013836145401000977, kl_loss = 0.0009503895416855812\n",
      "\n",
      "Epoch 1106\n",
      "Step 0: loss = 0.015515521168708801, recon_loss = 0.005204539746046066, 0.010285388678312302, kl_loss = 0.0012796605005860329\n",
      "\n",
      "Epoch 1107\n",
      "Step 0: loss = 0.018147587776184082, recon_loss = 0.005957631394267082, 0.01217278279364109, kl_loss = 0.0008586952462792397\n",
      "\n",
      "Epoch 1108\n",
      "Step 0: loss = 0.018367256969213486, recon_loss = 0.00616813637316227, 0.012165757827460766, kl_loss = 0.0016681607812643051\n",
      "\n",
      "Epoch 1109\n",
      "Step 0: loss = 0.019114304333925247, recon_loss = 0.00702318549156189, 0.012065865099430084, kl_loss = 0.0012627225369215012\n",
      "\n",
      "Epoch 1110\n",
      "Step 0: loss = 0.021731141954660416, recon_loss = 0.007116558030247688, 0.014576317742466927, kl_loss = 0.0019133128225803375\n",
      "\n",
      "Epoch 1111\n",
      "Step 0: loss = 0.019792532548308372, recon_loss = 0.007113456726074219, 0.012605576775968075, kl_loss = 0.003674939274787903\n",
      "\n",
      "Epoch 1112\n",
      "Step 0: loss = 0.026541201397776604, recon_loss = 0.008064255118370056, 0.018373724073171616, kl_loss = 0.0051611317321658134\n",
      "\n",
      "Epoch 1113\n",
      "Step 0: loss = 0.023844005540013313, recon_loss = 0.007730606943368912, 0.0160604789853096, kl_loss = 0.0026459544897079468\n",
      "\n",
      "Epoch 1114\n",
      "Step 0: loss = 0.01901051215827465, recon_loss = 0.007425116375088692, 0.011549670249223709, kl_loss = 0.0017862506210803986\n",
      "\n",
      "Epoch 1115\n",
      "Step 0: loss = 0.02296927385032177, recon_loss = 0.007462047040462494, 0.015485843643546104, kl_loss = 0.0010692039504647255\n",
      "\n",
      "Epoch 1116\n",
      "Step 0: loss = 0.02023376151919365, recon_loss = 0.006963297724723816, 0.013253660872578621, kl_loss = 0.0008401703089475632\n",
      "\n",
      "Epoch 1117\n",
      "Step 0: loss = 0.022844962775707245, recon_loss = 0.00763188861310482, 0.015192210674285889, kl_loss = 0.0010431939736008644\n",
      "\n",
      "Epoch 1118\n",
      "Step 0: loss = 0.01734323985874653, recon_loss = 0.007193427532911301, 0.010081484913825989, kl_loss = 0.0034163976088166237\n",
      "\n",
      "Epoch 1119\n",
      "Step 0: loss = 0.02412412315607071, recon_loss = 0.0075449757277965546, 0.016550134867429733, kl_loss = 0.0014505991712212563\n",
      "\n",
      "Epoch 1120\n",
      "Step 0: loss = 0.019285760819911957, recon_loss = 0.007019201293587685, 0.01220177672803402, kl_loss = 0.003239157609641552\n",
      "\n",
      "Epoch 1121\n",
      "Step 0: loss = 0.017454521730542183, recon_loss = 0.006464621052145958, 0.010936415754258633, kl_loss = 0.0026743076741695404\n",
      "\n",
      "Epoch 1122\n",
      "Step 0: loss = 0.020215749740600586, recon_loss = 0.00717056542634964, 0.013008106499910355, kl_loss = 0.0018538488075137138\n",
      "\n",
      "Epoch 1123\n",
      "Step 0: loss = 0.01755053736269474, recon_loss = 0.007360309362411499, 0.010173186659812927, kl_loss = 0.0008520418778061867\n",
      "\n",
      "Epoch 1124\n",
      "Step 0: loss = 0.02146545611321926, recon_loss = 0.006681779399514198, 0.014762439765036106, kl_loss = 0.0010617878288030624\n",
      "\n",
      "Epoch 1125\n",
      "Step 0: loss = 0.016500325873494148, recon_loss = 0.006990101188421249, 0.009484712034463882, kl_loss = 0.001275661401450634\n",
      "\n",
      "Epoch 1126\n",
      "Step 0: loss = 0.017446603626012802, recon_loss = 0.007346242666244507, 0.010071431286633015, kl_loss = 0.001446547918021679\n",
      "\n",
      "Epoch 1127\n",
      "Step 0: loss = 0.02034027874469757, recon_loss = 0.006344487890601158, 0.013977431692183018, kl_loss = 0.0009178798645734787\n",
      "\n",
      "Epoch 1128\n",
      "Step 0: loss = 0.016053123399615288, recon_loss = 0.006700059399008751, 0.009328899905085564, kl_loss = 0.0012082243338227272\n",
      "\n",
      "Epoch 1129\n",
      "Step 0: loss = 0.018950747326016426, recon_loss = 0.007200649008154869, 0.011733775027096272, kl_loss = 0.0008161189034581184\n",
      "\n",
      "Epoch 1130\n",
      "Step 0: loss = 0.02003915049135685, recon_loss = 0.006554482504725456, 0.013392826542258263, kl_loss = 0.004592113196849823\n",
      "\n",
      "Epoch 1131\n",
      "Step 0: loss = 0.016922922804951668, recon_loss = 0.00625043548643589, 0.010623471811413765, kl_loss = 0.002450767904520035\n",
      "\n",
      "Epoch 1132\n",
      "Step 0: loss = 0.020273610949516296, recon_loss = 0.006190979853272438, 0.01406488474458456, kl_loss = 0.0008873185142874718\n",
      "\n",
      "Epoch 1133\n",
      "Step 0: loss = 0.017962921410799026, recon_loss = 0.007197035476565361, 0.010743862949311733, kl_loss = 0.0011011874303221703\n",
      "\n",
      "Epoch 1134\n",
      "Step 0: loss = 0.015867818146944046, recon_loss = 0.005895432084798813, 0.009948153048753738, kl_loss = 0.0012116758152842522\n",
      "\n",
      "Epoch 1135\n",
      "Step 0: loss = 0.016844268888235092, recon_loss = 0.006089393049478531, 0.010742793790996075, kl_loss = 0.0006040716543793678\n",
      "\n",
      "Epoch 1136\n",
      "Step 0: loss = 0.021764518693089485, recon_loss = 0.006279647350311279, 0.015470941551029682, kl_loss = 0.0006965426728129387\n",
      "\n",
      "Epoch 1137\n",
      "Step 0: loss = 0.022673744708299637, recon_loss = 0.007227346301078796, 0.015415940433740616, kl_loss = 0.0015229452401399612\n",
      "\n",
      "Epoch 1138\n",
      "Step 0: loss = 0.018432704731822014, recon_loss = 0.0069555193185806274, 0.01142401434481144, kl_loss = 0.002658524550497532\n",
      "\n",
      "Epoch 1139\n",
      "Step 0: loss = 0.022419342771172523, recon_loss = 0.007604906335473061, 0.014788133092224598, kl_loss = 0.001315135508775711\n",
      "\n",
      "Epoch 1140\n",
      "Step 0: loss = 0.015478177927434444, recon_loss = 0.006693560630083084, 0.00874890387058258, kl_loss = 0.0017856806516647339\n",
      "\n",
      "Epoch 1141\n",
      "Step 0: loss = 0.01779676228761673, recon_loss = 0.007151002064347267, 0.01062840223312378, kl_loss = 0.0008679330348968506\n",
      "\n",
      "Epoch 1142\n",
      "Step 0: loss = 0.021564677357673645, recon_loss = 0.007160939276218414, 0.014362983405590057, kl_loss = 0.0020377449691295624\n",
      "\n",
      "Epoch 1143\n",
      "Step 0: loss = 0.02006695419549942, recon_loss = 0.00691998191177845, 0.013100074604153633, kl_loss = 0.002344883047044277\n",
      "\n",
      "Epoch 1144\n",
      "Step 0: loss = 0.0218952726572752, recon_loss = 0.006899569183588028, 0.014947338029742241, kl_loss = 0.0024183131754398346\n",
      "\n",
      "Epoch 1145\n",
      "Step 0: loss = 0.019794093444943428, recon_loss = 0.007307944819331169, 0.01246347464621067, kl_loss = 0.0011337203904986382\n",
      "\n",
      "Epoch 1146\n",
      "Step 0: loss = 0.020035870373249054, recon_loss = 0.0077506136149168015, 0.012271486222743988, kl_loss = 0.0006884979084134102\n",
      "\n",
      "Epoch 1147\n",
      "Step 0: loss = 0.024750718846917152, recon_loss = 0.0069469064474105835, 0.01777416095137596, kl_loss = 0.001482550986111164\n",
      "\n",
      "Epoch 1148\n",
      "Step 0: loss = 0.013759599067270756, recon_loss = 0.0064339227974414825, 0.007265100255608559, kl_loss = 0.0030287792906165123\n",
      "\n",
      "Epoch 1149\n",
      "Step 0: loss = 0.019019750878214836, recon_loss = 0.0072889383882284164, 0.011701750569045544, kl_loss = 0.0014531472697854042\n",
      "\n",
      "Epoch 1150\n",
      "Step 0: loss = 0.020232465118169785, recon_loss = 0.007167752832174301, 0.013024719431996346, kl_loss = 0.0019996892660856247\n",
      "\n",
      "Epoch 1151\n",
      "Step 0: loss = 0.01734153926372528, recon_loss = 0.00717599131166935, 0.010126810520887375, kl_loss = 0.0019369004294276237\n",
      "\n",
      "Epoch 1152\n",
      "Step 0: loss = 0.018538806587457657, recon_loss = 0.0069285798817873, 0.0115842679515481, kl_loss = 0.0012979228049516678\n",
      "\n",
      "Epoch 1153\n",
      "Step 0: loss = 0.0205115657299757, recon_loss = 0.00701509602367878, 0.013469692319631577, kl_loss = 0.0013388646766543388\n",
      "\n",
      "Epoch 1154\n",
      "Step 0: loss = 0.01867910847067833, recon_loss = 0.006367795169353485, 0.012230420485138893, kl_loss = 0.00404462032020092\n",
      "\n",
      "Epoch 1155\n",
      "Step 0: loss = 0.01877419836819172, recon_loss = 0.006528481841087341, 0.012217963114380836, kl_loss = 0.001387670636177063\n",
      "\n",
      "Epoch 1156\n",
      "Step 0: loss = 0.01765567436814308, recon_loss = 0.006534431129693985, 0.011101162061095238, kl_loss = 0.0010040318593382835\n",
      "\n",
      "Epoch 1157\n",
      "Step 0: loss = 0.014731020666658878, recon_loss = 0.00575115904211998, 0.008961090818047523, kl_loss = 0.0009385431185364723\n",
      "\n",
      "Epoch 1158\n",
      "Step 0: loss = 0.01806863397359848, recon_loss = 0.0063303615897893906, 0.011724820360541344, kl_loss = 0.000672576017677784\n",
      "\n",
      "Epoch 1159\n",
      "Step 0: loss = 0.017620867118239403, recon_loss = 0.006283123046159744, 0.011317605152726173, kl_loss = 0.00100693479180336\n",
      "\n",
      "Epoch 1160\n",
      "Step 0: loss = 0.01776004210114479, recon_loss = 0.0062146056443452835, 0.011529705487191677, kl_loss = 0.0007866322994232178\n",
      "\n",
      "Epoch 1161\n",
      "Step 0: loss = 0.020323211327195168, recon_loss = 0.006675561890006065, 0.013570345938205719, kl_loss = 0.003865220583975315\n",
      "\n",
      "Epoch 1162\n",
      "Step 0: loss = 0.019952237606048584, recon_loss = 0.007020078599452972, 0.012863699346780777, kl_loss = 0.0034229401499032974\n",
      "\n",
      "Epoch 1163\n",
      "Step 0: loss = 0.017774613574147224, recon_loss = 0.0068207066506147385, 0.0107866320759058, kl_loss = 0.008363773114979267\n",
      "\n",
      "Epoch 1164\n",
      "Step 0: loss = 0.017982549965381622, recon_loss = 0.006586110219359398, 0.011365117505192757, kl_loss = 0.001566150225698948\n",
      "\n",
      "Epoch 1165\n",
      "Step 0: loss = 0.02066667377948761, recon_loss = 0.006710270419716835, 0.01394190825521946, kl_loss = 0.0007247123867273331\n",
      "\n",
      "Epoch 1166\n",
      "Step 0: loss = 0.01751798205077648, recon_loss = 0.006915347650647163, 0.010577135719358921, kl_loss = 0.0012749191373586655\n",
      "\n",
      "Epoch 1167\n",
      "Step 0: loss = 0.017726298421621323, recon_loss = 0.0066006723791360855, 0.011047715321183205, kl_loss = 0.0038955071941018105\n",
      "\n",
      "Epoch 1168\n",
      "Step 0: loss = 0.016323938965797424, recon_loss = 0.006207667291164398, 0.010091226547956467, kl_loss = 0.0012522796168923378\n",
      "\n",
      "Epoch 1169\n",
      "Step 0: loss = 0.019638219848275185, recon_loss = 0.005964532494544983, 0.013650733977556229, kl_loss = 0.001147639937698841\n",
      "\n",
      "Epoch 1170\n",
      "Step 0: loss = 0.016916537657380104, recon_loss = 0.006134353578090668, 0.010756582021713257, kl_loss = 0.0012801103293895721\n",
      "\n",
      "Epoch 1171\n",
      "Step 0: loss = 0.015388351865112782, recon_loss = 0.00610039196908474, 0.009258399717509747, kl_loss = 0.001478026621043682\n",
      "\n",
      "Epoch 1172\n",
      "Step 0: loss = 0.02068858966231346, recon_loss = 0.006306147202849388, 0.01434311643242836, kl_loss = 0.0019662976264953613\n",
      "\n",
      "Epoch 1173\n",
      "Step 0: loss = 0.018068812787532806, recon_loss = 0.006352189928293228, 0.011693863198161125, kl_loss = 0.001138024963438511\n",
      "\n",
      "Epoch 1174\n",
      "Step 0: loss = 0.019225358963012695, recon_loss = 0.006882593035697937, 0.012316452339291573, kl_loss = 0.0013157138600945473\n",
      "\n",
      "Epoch 1175\n",
      "Step 0: loss = 0.02006174437701702, recon_loss = 0.006216598674654961, 0.013815440237522125, kl_loss = 0.0014852778986096382\n",
      "\n",
      "Epoch 1176\n",
      "Step 0: loss = 0.019445201382040977, recon_loss = 0.005534538999199867, 0.013895748183131218, kl_loss = 0.0007457295432686806\n",
      "\n",
      "Epoch 1177\n",
      "Step 0: loss = 0.01994837448000908, recon_loss = 0.0061609577387571335, 0.013773240149021149, kl_loss = 0.0007087830454111099\n",
      "\n",
      "Epoch 1178\n",
      "Step 0: loss = 0.020160317420959473, recon_loss = 0.006358534097671509, 0.013744447380304337, kl_loss = 0.0028667543083429337\n",
      "\n",
      "Epoch 1179\n",
      "Step 0: loss = 0.022866753861308098, recon_loss = 0.0076907481998205185, 0.015122437849640846, kl_loss = 0.002678362652659416\n",
      "\n",
      "Epoch 1180\n",
      "Step 0: loss = 0.019646262750029564, recon_loss = 0.007810434326529503, 0.01180002000182867, kl_loss = 0.0017904620617628098\n",
      "\n",
      "Epoch 1181\n",
      "Step 0: loss = 0.017063889652490616, recon_loss = 0.006512107327580452, 0.010526150465011597, kl_loss = 0.0012815482914447784\n",
      "\n",
      "Epoch 1182\n",
      "Step 0: loss = 0.016740167513489723, recon_loss = 0.006436288356781006, 0.010287347249686718, kl_loss = 0.0008265245705842972\n",
      "\n",
      "Epoch 1183\n",
      "Step 0: loss = 0.017057450488209724, recon_loss = 0.006126390770077705, 0.010898971930146217, kl_loss = 0.0016043540090322495\n",
      "\n",
      "Epoch 1184\n",
      "Step 0: loss = 0.019136948511004448, recon_loss = 0.0065989866852760315, 0.012512985616922379, kl_loss = 0.0012487685307860374\n",
      "\n",
      "Epoch 1185\n",
      "Step 0: loss = 0.01952010579407215, recon_loss = 0.0059801507741212845, 0.01346762664616108, kl_loss = 0.003616386093199253\n",
      "\n",
      "Epoch 1186\n",
      "Step 0: loss = 0.013520145788788795, recon_loss = 0.005733942613005638, 0.007737083360552788, kl_loss = 0.0024559814482927322\n",
      "\n",
      "Epoch 1187\n",
      "Step 0: loss = 0.021992024034261703, recon_loss = 0.006539715453982353, 0.015432382933795452, kl_loss = 0.0009963102638721466\n",
      "\n",
      "Epoch 1188\n",
      "Step 0: loss = 0.018822869285941124, recon_loss = 0.006147490814328194, 0.012660321779549122, kl_loss = 0.0007528224959969521\n",
      "\n",
      "Epoch 1189\n",
      "Step 0: loss = 0.016781533136963844, recon_loss = 0.005335807800292969, 0.01142056006938219, kl_loss = 0.00125835370272398\n",
      "\n",
      "Epoch 1190\n",
      "Step 0: loss = 0.019692426547408104, recon_loss = 0.005980623885989189, 0.013651181943714619, kl_loss = 0.0030310116708278656\n",
      "\n",
      "Epoch 1191\n",
      "Step 0: loss = 0.018893122673034668, recon_loss = 0.006145728752017021, 0.012719247490167618, kl_loss = 0.001407288946211338\n",
      "\n",
      "Epoch 1192\n",
      "Step 0: loss = 0.022371819242835045, recon_loss = 0.006858699023723602, 0.015454155392944813, kl_loss = 0.002948329783976078\n",
      "\n",
      "Epoch 1193\n",
      "Step 0: loss = 0.018240727484226227, recon_loss = 0.006533026695251465, 0.011605593375861645, kl_loss = 0.0051053231582045555\n",
      "\n",
      "Epoch 1194\n",
      "Step 0: loss = 0.01782364584505558, recon_loss = 0.006196625530719757, 0.011594798415899277, kl_loss = 0.0016111116856336594\n",
      "\n",
      "Epoch 1195\n",
      "Step 0: loss = 0.01982438750565052, recon_loss = 0.006219591945409775, 0.013587932102382183, kl_loss = 0.0008430825546383858\n",
      "\n",
      "Epoch 1196\n",
      "Step 0: loss = 0.020114321261644363, recon_loss = 0.0060248058289289474, 0.01404455117881298, kl_loss = 0.0022482117637991905\n",
      "\n",
      "Epoch 1197\n",
      "Step 0: loss = 0.021083518862724304, recon_loss = 0.007176661863923073, 0.013821354135870934, kl_loss = 0.004275163635611534\n",
      "\n",
      "Epoch 1198\n",
      "Step 0: loss = 0.02065608650445938, recon_loss = 0.00738215446472168, 0.013242023065686226, kl_loss = 0.0015954114496707916\n",
      "\n",
      "Epoch 1199\n",
      "Step 0: loss = 0.020754385739564896, recon_loss = 0.007116155698895454, 0.013542044907808304, kl_loss = 0.004809249192476273\n",
      "\n",
      "Epoch 1200\n",
      "Step 0: loss = 0.019803013652563095, recon_loss = 0.0062308721244335175, 0.013511124067008495, kl_loss = 0.0030507799237966537\n",
      "\n",
      "Epoch 1201\n",
      "Step 0: loss = 0.02207743190228939, recon_loss = 0.007048036903142929, 0.014992084354162216, kl_loss = 0.0018655015155673027\n",
      "\n",
      "Epoch 1202\n",
      "Step 0: loss = 0.017031768336892128, recon_loss = 0.006535643711686134, 0.010455947369337082, kl_loss = 0.002008904702961445\n",
      "\n",
      "Epoch 1203\n",
      "Step 0: loss = 0.017394090071320534, recon_loss = 0.006795797497034073, 0.010563832707703114, kl_loss = 0.0017230082303285599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_betas, batch_dirs) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch(betas, random_dirs, batch_size)):\n\u001b[0;32m----> 7\u001b[0m   loss_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_betas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m   losses\u001b[38;5;241m.\u001b[39mappend(loss_vals)\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;66;03m# tmp\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 9\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, inputs, dir_inputs, epoch)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(model, inputs, dir_inputs, epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m----> 9\u001b[0m     z_mean, z_log_var, z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_inputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# outputs = model.decoder([z, dir_inputs])\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder([z, dir_inputs])\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/training.py:561\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    559\u001b[0m     layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[0;32m--> 561\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/functional.py:511\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    494\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[1;32m    496\u001b[0m \u001b[38;5;124;03m    In this case `call` just reapplies\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m        a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/functional.py:668\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[1;32m    667\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[0;32m--> 668\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    672\u001b[0m     node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)\n\u001b[1;32m    673\u001b[0m ):\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/base_layer.py:1132\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[1;32m   1131\u001b[0m ):\n\u001b[0;32m-> 1132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/layers/core/lambda_layer.py:209\u001b[0m, in \u001b[0;36mLambda.call\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m var\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape(\n\u001b[1;32m    207\u001b[0m     watch_accessed_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    208\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m tape, tf\u001b[38;5;241m.\u001b[39mvariable_creator_scope(_variable_creator):\n\u001b[0;32m--> 209\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_variables(created_variables, tape\u001b[38;5;241m.\u001b[39mwatched_variables())\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[51], line 23\u001b[0m, in \u001b[0;36msampling\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampling\u001b[39m(args):\n\u001b[1;32m     22\u001b[0m   z_mean, z_log_var \u001b[38;5;241m=\u001b[39m args\n\u001b[0;32m---> 23\u001b[0m   eps \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_mean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m z_mean \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m z_log_var) \u001b[38;5;241m*\u001b[39m eps\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/random_ops.py:91\u001b[0m, in \u001b[0;36mrandom_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m     89\u001b[0m stddev_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(stddev, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstddev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m seed1, seed2 \u001b[38;5;241m=\u001b[39m random_seed\u001b[38;5;241m.\u001b[39mget_seed(seed)\n\u001b[0;32m---> 91\u001b[0m rnd \u001b[38;5;241m=\u001b[39m \u001b[43mgen_random_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_standard_normal\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m mul \u001b[38;5;241m=\u001b[39m rnd \u001b[38;5;241m*\u001b[39m stddev_tensor\n\u001b[1;32m     94\u001b[0m value \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(mul, mean_tensor, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/gen_random_ops.py:632\u001b[0m, in \u001b[0;36mrandom_standard_normal\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRandomStandardNormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m      \u001b[49m\u001b[43mseed2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m    636\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs, i)\n",
    "    losses.append(loss_vals)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon_loss = {loss_vals[1].numpy()}, {loss_vals[2].numpy()}, kl_loss = {loss_vals[3].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"loss_vae_64_0.02\", np.array(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHF0P4eISAH5"
   },
   "outputs": [],
   "source": [
    "vae.save_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JkrFRN7uOlf",
    "outputId": "90f21636-cdec-4d1e-eb4f-405a557a43b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7dfda7a63c70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS8sKCRfgUoH",
    "outputId": "b7f487ba-8083-4757-b9ea-250fd343a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  chem-checkpoint.zip\n",
      "   creating: my_checkpoint/\n",
      "  inflating: my_checkpoint/chekpont.index  \n",
      "  inflating: my_checkpoint/.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/checkpoint  \n",
      "  inflating: my_checkpoint/chekpont.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/.index    \n"
     ]
    }
   ],
   "source": [
    "# !zip -r my_checkpoint.zip my_checkpoint/\n",
    "!unzip chem-checkpoint.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EbHImoN2QaKX"
   },
   "outputs": [],
   "source": [
    "drawn_random_dirs = np.random.randn(50_000, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "\n",
    "def posterior_sampling(model, betas, random_dirs, num_samples=1):\n",
    "  zm, zlv, z = model.encoder((betas, random_dirs))\n",
    "  if num_samples == 1:\n",
    "      return model.decoder((z, random_dirs))\n",
    "  else:\n",
    "      samples = [sampling((zm, zlv)) for _ in range(num_samples)]\n",
    "      return tf.concat([model.decoder((sm, random_dirs))[:, None, :] for sm in samples], axis=1)\n",
    "\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs1 = np.random.randn(num_samples, d)\n",
    "  random_dirs2 = np.random.randn(num_samples, d)\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))    \n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [],
   "source": [
    "drawn_betas, dir1, drawn_betas2 = generate_new_betas(vae, 50_000)\n",
    "# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\n",
    "# generate_new_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 200\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd1 = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "# prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample][0])\n",
    "prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([-0.00631047,  0.12106761,  0.46212596, ...,  0.47387064,\n",
       "        -0.281489  , -0.05550657], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check agreement between vae and training samples\n",
    "from sklearn.metrics import jaccard_score\n",
    "def agreement(y_pred1, y_pred2, y_true):\n",
    "    tp1 = np.float32(y_pred1==1) * np.float32(y_true==1)\n",
    "    fp1 = np.float32(y_pred1==1) * np.float32(y_true==0)\n",
    "    tn1 = np.float32(y_pred1==0) * np.float32(y_true==0)\n",
    "    fn1 = np.float32(y_pred1==0) * np.float32(y_true==1)\n",
    "\n",
    "    tp2 = np.float32(y_pred2==1) * np.float32(y_true==1)\n",
    "    fp2 = np.float32(y_pred2==1) * np.float32(y_true==0)\n",
    "    tn2 = np.float32(y_pred2==0) * np.float32(y_true==0)\n",
    "    fn2 = np.float32(y_pred2==0) * np.float32(y_true==1)\n",
    "    print(np.sum(tp1)/len(tp1), np.sum(fp1)/len(tp1), np.sum(tn1)/len(tp1), np.sum(fn1)/len(tp1))\n",
    "    print(np.sum(tp2)/len(tp1), np.sum(fp2)/len(tp1), np.sum(tn2)/len(tp1), np.sum(fn2)/len(tp1))\n",
    "    print(np.sum(fp1==fp2)/len(tp1), np.sum(fn1==fn2)/len(tp1))\n",
    "    return jaccard_score(tp1, tp2), jaccard_score(fp1, fp2), jaccard_score(tn1, tn2), jaccard_score(fn1, fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5735723218007504 0.0029178824510212586 0.4210087536473531 0.0025010421008753647\n",
      "0.5731554814506045 0.0025010421008753647 0.42142559399749896 0.0029178824510212586\n",
      "0.9979157982492706 0.9979157982492706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9963715529753265,\n",
       " 0.4444444444444444,\n",
       " 0.9950641658440277,\n",
       " 0.4444444444444444)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement(prd1, prd2, Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 10, 4081])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0054231044>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps= betas\n",
    "oups = drawn_betas\n",
    "tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(tf.expand_dims(inps, axis=1)[:, :, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(oups[:, :, 1:], axis=-1)[0], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.012491584>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.abs(tf.cast(tf.expand_dims(inps, axis=1)[:, :, :1], dtype=tf.float32)-oups[:, :, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([ 0.01558006,  0.08848727,  0.02163708, ...,  0.32644653,\n",
       "        -0.05755342,  0.21104604], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IHicI0RhbDe",
    "outputId": "32426975-705a-4668-f061-08acd65bc48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.9977264>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas, drawn_betas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOFI9xq2b7_3",
    "outputId": "a11e5724-0e2a-428d-a65a-28064209e6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4081), dtype=float32, numpy=\n",
       "array([[ 3.5679474e-02, -8.5116491e-02, -2.7868379e-02, ...,\n",
       "         3.8362685e-01,  6.8785306e-03,  5.8818167e-01],\n",
       "       [ 2.0887703e-02,  5.4998733e-02,  2.2995186e-01, ...,\n",
       "         1.0724969e-01,  3.8565136e-04,  3.8573799e-01],\n",
       "       [-6.5227631e-03,  2.0269346e-01,  4.0428180e-01, ...,\n",
       "        -4.8758507e-01, -8.2871839e-02,  3.4979448e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFCgGG6jgkLL",
    "outputId": "5e799c27-fff1-41fc-d71f-3fc7ea88538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13627878, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "var = tf.math.reduce_variance(drawn_betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxnzX5lJQtjX"
   },
   "outputs": [],
   "source": [
    "np.mean(drawn_betas1 @ tf.transpose(drawn_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "TfAej5fqsfHh",
    "outputId": "fd888e0d-c076-4577-be0d-4de29be77dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ood_val_features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_data_to_features\u001b[49m(ood_val_data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#ood_test_features = convert_data_to_features(ood_test_data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ood_val_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m ood_val_data])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "#ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U07oDyUdsl1u"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ood_val_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m external_X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mood_val_features\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m external_Y \u001b[38;5;241m=\u001b[39m ood_val_labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ood_val_features' is not defined"
     ]
    }
   ],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "external_Y = ood_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "external_X = (external_X-mu_x)/sigma_x\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "1a114f68-6f8b-4ede-b395-df1c2dfa2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      " -0.03134901]\n",
      "tf.Tensor(\n",
      "[-0.13125554  0.30866534  0.63319725 -0.5780234  -0.21648076 -0.3846287\n",
      "  0.99320394 -0.1799166   0.678208    0.62888056], shape=(10,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "5ec19308-4552-4326-ec47-b86470a2f7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 1024)\n",
      "(937, 2040)\n",
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "ae104348-bf03-48d8-a6c3-d3112b1dcb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " ...\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W32O3S_gKnEp",
    "outputId": "8797bdae-deb8-43ed-ef0c-cbf43fb4aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 512) (937, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred > 0.5), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 100, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  # wghts = np.ones_like(projs > thresh[:, None])\n",
    "  wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return *aggregate_preds(preds), preds\n",
    "\n",
    "\n",
    "# drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs, betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, dir1, drawn_betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas1) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, betas) # 0.634\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, drawn_betas) # 0.85\n",
    "# ext_preds, mp_rand, sp_rand, pred = get_preds_and_aggregate(external_randfeats_X, betas) # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOyhZXJBrhaj",
    "outputId": "98cc9eb0-2e98-4b38-e823-337411f61e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0], tf.transpose(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "a9702828-7bea-436e-de46-89ca03c30b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.767455  ,  0.1001012 , -0.54182774, ..., -6.8707843 ,\n",
       "         7.257238  , -0.5050444 ], dtype=float32),\n",
       " array([-0.0186294 ,  0.06830448, -0.27256313, ..., -0.72351612,\n",
       "         0.81710885,  0.05805234]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "2fac824c-8637-4a35-8dfb-7b60045b10b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mext_preds\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ext_preds' is not defined"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "06eb05c6-f5f0-4d58-a8a2-ad76ee40b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "Total Positive Preds:  435.0\n",
      "Total Preds:  937\n",
      "% Positive Preds:  0.464247598719317\n",
      "\n",
      "First 10 Ground Truth:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Total Positive Ground Truth:  421.0\n",
      "Total Ground Truth:  937\n",
      "% Positive Ground Truth:  0.44930629669156885\n",
      "\n",
      "Accuracy:  0.6328708644610459\n"
     ]
    }
   ],
   "source": [
    "# testing_Y = Y\n",
    "# ext_preds = ext_probs > 0.5\n",
    "testing_Y = external_Y\n",
    "\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", testing_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(testing_Y))\n",
    "print(\"Total Ground Truth: \", len(testing_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(testing_Y) / len(testing_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == testing_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "L7kl6J3wLped",
    "outputId": "a5d20ac1-f339-47e4-cf12-18073bb4b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9a014d0220>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.2, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8571428571428571, 0.8536585365853658, 0.7857142857142857, 0.81, 0.7898550724637681, 0.7909604519774012, 0.7813953488372093, 0.7649402390438247, 0.738831615120275, 0.7284345047923323, 0.7308781869688386, 0.7222222222222222, 0.7126696832579186, 0.7034764826175869, 0.6950998185117967, 0.6829268292682927, 0.6651917404129793, 0.6557591623036649, 0.6211312700106724]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fbc16418e80>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABG40lEQVR4nO3deVxU5f4H8M/MwMywIyCrKIIrLqgohJpLYVwtW29ZdtWsLEvvLbm/FnNrt9tiVtfyZppli7bYqmlGmhtuIO4bgmwy7DCss57fH+DoyIAMMhxm+Lxfr3m9Zs55zsx3zu06H57nOc+RCIIggIiIiEgkUrELICIios6NYYSIiIhExTBCREREomIYISIiIlExjBAREZGoGEaIiIhIVAwjREREJCqGESIiIhKVk9gFtITRaMTFixfh4eEBiUQidjlERETUAoIgoLKyEsHBwZBKm+7/sIswcvHiRYSGhopdBhEREbVCTk4OunXr1uR+uwgjHh4eAOq/jKenp8jVEBERUUuo1WqEhoaafsebYhdh5NLQjKenJ8MIERGRnbnWFAtOYCUiIiJRMYwQERGRqBhGiIiISFQMI0RERCQqhhEiIiISFcMIERERiYphhIiIiETFMEJERESiYhghIiIiUVkdRnbu3InJkycjODgYEokEP/744zWP2bFjB4YNGwaFQoFevXph7dq1rSiViIiIHJHVYaS6uhpRUVFYsWJFi9pnZmbi1ltvxfjx45GWloann34ajz76KLZu3Wp1sUREROR4rL43zcSJEzFx4sQWt1+5ciV69uyJd955BwDQv39/7N69G++++y4SEhKs/XgiIiJyMDa/UV5ycjLi4+PNtiUkJODpp59u8hiNRgONRmN6rVarbVLb6t2ZyC2rMb32UDhhxsgw+LorbPJ5RERE1JjNw4hKpUJAQIDZtoCAAKjVatTW1sLFxaXRMUuXLsVLL71k69Kw6ehFpGaXm21TOMswZ3wvm382ERER1euQV9PMnz8fFRUVpkdOTo5NPuee6G6YMz4Cc8ZHYHA3LwBAtUZvk88iIiIiy2zeMxIYGIiCggKzbQUFBfD09LTYKwIACoUCCoXth0oejO1hel6jPYGjuRU2/0wiIiIyZ/Oekbi4OCQlJZlt27ZtG+Li4mz90URERGQHrA4jVVVVSEtLQ1paGoD6S3fT0tKQnZ0NoH6IZfr06ab2s2fPRkZGBp599lmcPn0aH374Ib755hvMmzevbb4BERER2TWrw8ihQ4cwdOhQDB06FACQmJiIoUOHYvHixQCA/Px8UzABgJ49e2LTpk3Ytm0boqKi8M477+CTTz7hZb1EREQEoBVzRsaNGwdBEJrcb2l11XHjxuHw4cPWfhQRERF1Ah3yahoiIiLqPBhGiIiISFQMI0RERCQqhhEiIiISFcMIERERiYphhIiIiETFMEJERESiYhghIiIiUTGMEBERkagYRoiIiEhUDCNEREQkKoYRIiIiEhXDCBEREYmKYYSIiIhExTBCREREomIYISIiIlExjBAREZGoGEaIiIhIVAwjREREJCqGESIiIhIVwwgRERGJimGEiIiIRMUwQkRERKJiGCEiIiJRMYwQERGRqBhGiIiISFQMI0RERCQqhhEiIiISFcMIERERiYphhIiIiETFMEJERESiYhghIiIiUTGMEBERkagYRoiIiEhUDCNEREQkKoYRIiIiEhXDCBEREYmKYYSIiIhExTBCREREomIYsdJPaXlY9vsZCIIgdilEREQOwUnsAuxJnc6Ap9anAQAmRwWjd4CHuAURERE5APaMWOFAZqnYJRARETkchhEr7DpXZHoukYhYCBERkQNhGLHCzrPFYpdARETkcBhGWqikSoMzBZVil0FERORwGEZaKCWrTOwSiIiIHBLDSAsxjBAREdkGw0gLHbzAK2mIiIhsgWGkBep0BhzPU4tdBhERkUNiGGmBswWV0BqM8HGTw9vVWexyiIiIHEqrwsiKFSsQFhYGpVKJ2NhYHDhwoMm2Op0OL7/8MiIiIqBUKhEVFYUtW7a0umAxnLhY3ysyINgTXF6EiIiobVkdRjZs2IDExEQsWbIEqampiIqKQkJCAgoLCy22X7hwIf73v//hgw8+wMmTJzF79mzcddddOHz48HUX316O51UAAAYEe4lcCRERkeOxOowsW7YMs2bNwsyZMxEZGYmVK1fC1dUVa9assdh+3bp1eOGFFzBp0iSEh4fjiSeewKRJk/DOO+9cd/Ht5cqeESIiImpbVoURrVaLlJQUxMfHX34DqRTx8fFITk62eIxGo4FSqTTb5uLigt27dzf5ORqNBmq12uwhFr3BiFP59Z8/MIQ9I0RERG3NqjBSXFwMg8GAgIAAs+0BAQFQqVQWj0lISMCyZctw7tw5GI1GbNu2DRs3bkR+fn6Tn7N06VJ4eXmZHqGhodaU2aYulFRDozfCVS5DDx9X0eogIiJyVDa/mua9995D79690a9fP8jlcsydOxczZ86EVNr0R8+fPx8VFRWmR05Ojq3LbFJ6YRUAoJe/O6RSTl8lIiJqa1aFET8/P8hkMhQUFJhtLygoQGBgoMVjunbtih9//BHV1dXIysrC6dOn4e7ujvDw8CY/R6FQwNPT0+whlvNF1QCAXl3dRauBiIjIkVkVRuRyOaKjo5GUlGTaZjQakZSUhLi4uGaPVSqVCAkJgV6vx/fff4877rijdRW3s0s9IxH+DCNERES24GTtAYmJiZgxYwaGDx+OmJgYLF++HNXV1Zg5cyYAYPr06QgJCcHSpUsBAPv370deXh6GDBmCvLw8vPjiizAajXj22Wfb9pvYyPmihjDCnhEiIiKbsDqMTJkyBUVFRVi8eDFUKhWGDBmCLVu2mCa1Zmdnm80Hqaurw8KFC5GRkQF3d3dMmjQJ69atg7e3d5t9CVsRBAHnr5gzQkRERG3P6jACAHPnzsXcuXMt7tuxY4fZ67Fjx+LkyZOt+RjRqdR1qNYa4CSVoIcvr6QhIiKyBd6bphmZxfWTV0N9XOEs46kiIiKyBf7CNiO3tBZAfRghIiIi22AYaUZuWQ0AoFsXF5ErISIiclwMI83ILavvGWEYISIish2GkWbkNPSMhHbhMA0REZGtMIw0gz0jREREtscw0gSt3giVug4AJ7ASERHZEsNIEy6W10IQAKWzFL5ucquP35tejMLKOhtURkRE5FgYRppwaYgmxNsFEol1d+vde74YUz/Zj39/c8QWpRERETkUhpEmFDQM0QR5WT9f5Nej+QCAshptm9ZERETkiBhGmlBYqQEA+HsqrDrOaBTwx8kCW5RERETkkBhGmnBpvoe/h9Kq447mVZiCDBEREV0bw0gTCtUNPSMelntG0hvu5nu1bSdVNquJiIjIETGMNMHUM9LEMM3sL1KRUdQ4kPx+gkM0RERE1mAYaYJpzshVwzQavdH0/PhFtdm+C8XVONdEjwkRERFZxjBigSAIpmGagKt6Rmq0BtPzQE/zoJJ0uhAAIJfxtBIREbUUfzUtqNLoUaurDx3NTWC9evmR5PPFAIDhYV1sVhsREZGjYRixoKChV8RD4QQXuaxFx+gNRuzPKAUAjIzwtVltREREjoZhxIJLk1e7WrHGyPGLalRq9PBUOiEy2NNs319ni7AnvbhNayQiInIUDCMWFFU2f1mvJXsbhmhuCPeF9Irxm4vltXh47UHM+vwQjEahbQslIiJyAAwjFpRV1y/j7mPhBnlrZ46weEzy+RIAjYdotp5QwWAUUKM1wCgwjBAREV2NYcSCshodAMDbtXEYGdfXHz393My2afQGHLzQMF+kl5/Zvq0nuAgaERFRcxhGLChvuMFdF1fnFrVPyy5Hnc4IP3c5evu7m7aXVmlxILPUJjUSERE5CiexC+iIymvre0a6WOgZsWRvwxBNXIQfJFfOF6moa/viiIiIHAx7RixobpjGkpSsMgBATE8fm9VERETkqBhGLLBmmMZgFJCWUw4AGNbd24ZVEREROSaGEQvKGsKIdwvCSHphFao0erjKZegb4NFov597y3pXiIiIOiuGEQvKq1s+TJOaXT9EE9XNG04W7kkzto9/2xZHRETkYBhGrqI3CqjU6AG0bALr4YYwMqyHt2nblauJjOvbtS3LIyIicjgMI1e5NF9EIgG8XK49THM0twJAfc/IJYXqy1fRxHJSKxERUbN4ae9VLl1J46l0hkwqabZtnc6Ac4VVAIDBV4SRsX384eeuwO1RwZA7Me8RERE1h2HkKtZcSXMqXw2DUYCfuwIBV9xUL9BLiYMLboZEIjG9HxEREVnGP9uvcqlnxKsF80UuDdEMCvE0W+wMQKPXREREZBnDyFWs6Rk5nncpjHi16L3v/V8y79xLRER0FYaRq6hr66+kacnk1QslNQCAAS0MI4ezy1FSzWEbIiKiKzGMXEVrMAIA3BUtn04TGeTZ4rYcvSEiIjLHMNKEloYRN7kMId4uNq6GiIjIcTGMNKGlYaRvoAekzVwCbGlVViIiIrqMv5RNcGthGOl3jSEad4UT3rxncFuURERE5JAYRprgrmxZGOkf2PjmeFe7b0To9ZZDRETksBhGmtDSYZo+Fu7US0RERC3HMNKE5sJIrdZget7L3709ymkxncGI95POYfuZQrFLISIiahGGkSY0N2dEdcWN8HzdFU22s2Rjai6e//4odA2XEFui0Rvw8i8n8evRi82+12mVGvM2pCGntMa07fPkLCzbdhbzvz9mVV1ERERiYRhpgkcL54xY6z9bzmD9wRycuKgGAAiCgDOqShiuWJn1x8N5WLMnE8t+P9vk+wiCgH9/cwQ/HM7DD4fzAAAVNTq8n3QOQH1gKuMCa0REZAcYRprQkqtpIrq6Wf2+l0KHwVjfMzJ/4zEkLN9pChQA8OX+bACAzth078m2kwWmQKNv6GX57/ZzqKjVmdqcLai0uj4iIqL2xjDShJZMYL3e+SIGo4D1B3MAAJ/tvQAAOJZbYboBX1MEQcDyP86ZbcsuqcFne7MAAF096oeOdp4rQkmV5rpqJCIisjWGkSa4yWXXbDO0e5fr+oz9mSWm5yMjfAEAXx3IuuZxW08U4GS+2mzbf7aehtZgxI29/XDPsG4AgBXbz+OOFXsgCPW9Mb+fUOGFH45Bq2+6x4WIiKi92WZihJ1zcZY1u3LqG3cPwu70YswcFXZdn/PLkcsTVOVOUqjrdPgprflJq0ajgPca5oU4yyTQGQSkZpdjd3oxJBJg/sT+ZsMzuWW1AICskmr88+vD0OiNmDgwEDf27npdtRMREbUV9oxYcK35IvfHdMd/pw6DwunavSdNMRiB308UmG37Oe0iarQGyBuCkNFYP6fkra2nTW1+P6nCqXw13BVOmDgwCACwO70YAHBvdDdEBnsiMth8VVhBABb/dAKahh4RvUEAERFRR8EwYoGtrqS5UkpWGUquutplY2ouAGBCZAAAIK+8Fl8fyMaK7edhNAoQBAHvJ6UDAGaOCoO3q7PpWBdnGRIn9AVQvxBb4oQ+pn2/HVfhr7NFZp91aeiGiIhIbK0KIytWrEBYWBiUSiViY2Nx4MCBZtsvX74cffv2hYuLC0JDQzFv3jzU1dU1e4yY3BSt7/FoiofCCTKpxDQXZesJldn+zOJqpGaXQyoBbh8SbPE9dqcX42S+Gi7OMjw8qqfZ5cCP3tgTgV5K0+t/3NDD9PylX06Yvc8nuzMw+KXfcTS3/Hq/FhER0XWzOoxs2LABiYmJWLJkCVJTUxEVFYWEhAQUFlpe8fOrr77C888/jyVLluDUqVNYvXo1NmzYgBdeeOG6i7eVli4Fb41VM4bj04dGmK50ScspBwAENQSIX4/mAwDG9OkKfw/LC6l9vDMDADBlRCi6uMlxOLvctO/RG8Ob/OzCSg16+LqiT0D91T970ktQWadHSlbZdX0nIiKitmB1GFm2bBlmzZqFmTNnIjIyEitXroSrqyvWrFljsf3evXsxatQoTJ06FWFhYbjlllvwwAMPXLM3RUy2CCM3hPtiTB/zSaMKJylu7O1ntu3uhithrnYyX41d54ohk0rwyOieAID7hte3/edNveDl4mzxuEteuWPgdc1xISIishWrwohWq0VKSgri4+Mvv4FUivj4eCQnJ1s8ZuTIkUhJSTGFj4yMDGzevBmTJk1q8nM0Gg3UarXZoz3ZIoxYMqZPV7jKL3+Wh8IJtzTMF7naql31vSKTBgUh1McVADA9Lgy7nh2Pf9/St9nPSRgQ0CgIAcDF8lq88utJZJfUWDiKiIiofVj1q1tcXAyDwYCAAPMfzICAAJw+fdriMVOnTkVxcTFGjx4NQRCg1+sxe/bsZodpli5dipdeesma0tqUazuFkYQBgTied3mBs0mDgqB0lsHPXQGJBOjiKkdpwyTXTQ3DOI9dMRwjlUpMweRqLs4yyGVSaA1GLLw1EgDg6y4HUB+2qjR6fLI7E4IAOEklmD+pv02+IxER0bXY/GqaHTt24PXXX8eHH36I1NRUbNy4EZs2bcIrr7zS5DHz589HRUWF6ZGTk2PrMs24ONt+OEMmlSC+vz+0V9ww746GiauhPq5YP+sGfP5wjGmf3ihgRFgXDOrm1aL3d5HL8P0TI7H16TGmwPLG3YPxxSOxGNe3vpfk0gU1Gi6CRkREIrKqC8DPzw8ymQwFBebrYxQUFCAwMNDiMYsWLcK0adPw6KOPAgAGDRqE6upqPPbYY1iwYAGk0sZ5SKFQQKGw7m64bUnpbPsrnmN7+sDbVY49DWuEAEBsuK/Z86tvdDc9Lsyqz7g6uAR6KRHopcSGQ+0b7oiIiJpj1a+uXC5HdHQ0kpKSTNuMRiOSkpIQFxdn8ZiamppGgUMmq+956KhrXSjbYaJnwoD68Bbdo35J+YEhnpBJJU229/dQmI65Xk7NfE5LnFapsfDHY8ivqG2TeoiIqHOzenJEYmIiZsyYgeHDhyMmJgbLly9HdXU1Zs6cCQCYPn06QkJCsHTpUgDA5MmTsWzZMgwdOhSxsbFIT0/HokWLMHnyZFMo6WiUNhymGdfXHzvOFOK2wfWrpz4/sR/6Bnjg/hHdmz1uamx3yJ3apsdmWlz9GiQSABsP52Ht3gsY1cvPtNhac0qqNHhozUGo1HUI9nbBk+N6tUlNRETUeVkdRqZMmYKioiIsXrwYKpUKQ4YMwZYtW0yTWrOzs816QhYuXAiJRIKFCxciLy8PXbt2xeTJk/Haa6+13bdoY7Ycpnnx9gEABphe+3so8fjYCIttFc5SKJykEARgakzzYcUaw7p3wbDuXfDappOmbR/8ee6aYcRoFDDvmyNQqesXrOMN94iIqC206rKRuXPnYu7cuRb37dixw/wDnJywZMkSLFmypDUfJQpFO0xgbQlXuRM+nTkCCicp/D2V1z7ASrU6g+m5n/u15+h8uCMdO69aVp6IiOh68d40FthymMZaIyP8EN3DxybvPe2GMNPza92PZ+/5YizbdhbA5VVjiYiI2gLDiAXKNpqb0dH1DfTAwlsbry9So9XjwU/2IfGbNABAWbUWT69Pg1GovzPwzf3927lSIiJyZJ3jV9dKHalnRAxvbjmDPekl2JiaB0EQsODHYyis1CCiqxtevmOg2OUREZGDYRixoDOHkX0ZJVi794Lp9cbUPGw+poKTVIL37h8KF3nnPTdERGQbDCMWtMeiZx1RjVaPZ787arZtyc8nAADzJvTBwJCWrf5KRERkjc75q3sNnbVn5M0tZ5BdWmN2B+AqjR7De3TB7CYuPyYiIrpeDCMWtMcKrB3NsdwK0/DMi7dHmra7yWV4d8qQZleHJSIiuh4MIw2uXJm+Mw7TZBRXAwDuG94N4/tevlrmxdsHNHlnYCIiorbQqkXPHJHeeHk10Y6y6Fl783WT44VJ/eHtKse/bu4NuUyCv0d3a7K9IACHs8vQP8iz0w5tERHR9WMYaVCnuxxGOmPPCAAsvK0+iABA4oQ+12z/efIFvJd0Dk+Oi8Czf+tn6/KIiMhBdc5fXQvqrlgaXS7rPKelq0f9MvCje/nhziEhVh1bVqMDAORX1LV5XURE1HmwZ6TBlT0jEknnmaw5aVAQvF3liAnz6VTfm4iIOg6GkQYaveHajRyQs0yKsX26WnWMXMb5IURE1HYYRhpcOUxDzZsW1wMSCWAUBHy654LY5RARkZ3rPJMjruHKYRpqXk8/Nyy6LRIh3i5il0JERA6AYaQBe0aIiIjEwTDSoK6TzhkhIiISG8NIAw7TEBERiYNhpAGHaYiIiMTBMNJAw54RIiIiUTCMNNAaGEaIiIjEwDBC1+2Hw3lIzS4TuwwiIrJTDCPUakZBMD1/47fTIlZCRET2jGHkKs4y3p+lpWq1l4e28itqRayEiIjsGcNIg28ej0NUNy98O3uk2KXYjduigkzPe3V1F7ESIiKyZwwjDWJ6+uCnuaMxJNRb7FLsRkRXd7z198Fil0FERHaOYYSIiIhExTBCREREomIYISIiIlExjBAREZGoGEaIiIhIVAwjREREJCqGESIiIhIVwwgRERGJimGEiIiIRMUwQkRERKJiGKE2l1FUhbxy3jiPiIhahmGE2sT2M0X45chFHMutQMLynbj3o71il0RERHbCSewCyL4JVzz/v2+PINTHFTqDgHx1nWg1ERGRfWHPCF0XncFoeq7RG5FeWAUAEATghR+OiVUWERHZEYYRui4TIgOa3PfV/myo63TtWA0REdkjhhG6Lv4eSqyaPtz0etKgQLP9gnD1EUREROYYRui6+bnLAQA9fF3x1t+j8M+beolcERER2ROGEbpuQ0K98fnDMdj4xEi4KZzwr5t7i10SERHZEV5NQ9dNIpFgTJ+uYpdBRER2ij0jREREJCqGESIiIhIVwwgRERGJimGEiIiIRMUwQkRERKJqVRhZsWIFwsLCoFQqERsbiwMHDjTZdty4cZBIJI0et956a6uLJvtXyZVZiYiogdVhZMOGDUhMTMSSJUuQmpqKqKgoJCQkoLCw0GL7jRs3Ij8/3/Q4fvw4ZDIZ7r333usunuzT//46j8Ev/Y5vD+VYfezBC6WYseYAjudV2KAyIiISg9VhZNmyZZg1axZmzpyJyMhIrFy5Eq6urlizZo3F9j4+PggMDDQ9tm3bBldXV4aRTiolqxRvbj0DQQB2niuGYMV68RlFVbh3ZTL+OluENbszbVglERG1J6vCiFarRUpKCuLj4y+/gVSK+Ph4JCcnt+g9Vq9ejfvvvx9ubm5NttFoNFCr1WYPsn/qOh2eWp8Gg7E+gPxy5CI++ut8i46tqNHhkc8OmV5vPJyHtXsYSIiIHIFVYaS4uBgGgwEBAeZ3ag0ICIBKpbrm8QcOHMDx48fx6KOPNttu6dKl8PLyMj1CQ0OtKZM6EL3BiMo6HQRBwMIfjiO3rNZs/3t/nEOdztDse+gMRjz5VQoyi6vNtm87VdDm9RIRUftr16tpVq9ejUGDBiEmJqbZdvPnz0dFRYXpkZNj/dwC6hju/3gfYl5Lwurdmfj5yEXIpBLcPSzEtF+jN+K1TaeaPF4QBLz48wnsSS+Bq1yGR0f3NO0L8FTatHYiImofVoURPz8/yGQyFBSY/0VaUFCAwMDAJo6qV11djfXr1+ORRx655ucoFAp4enqaPcg+nSusQq3OgFcbAsfTN/fGf+4ZjBt7+5narNuXhZzSGovHr9uXhS/3Z0MiAd67fygW3haJRxoCSVZJDe5duRcbDmbb/osQEZHNWBVG5HI5oqOjkZSUZNpmNBqRlJSEuLi4Zo/99ttvodFo8I9//KN1lZLdiwnzwZPje8FZJsWMuDCzfS/+fAJAfU/Iyr/OY9XODOzLKMFLv5wEADz3t36YEFk/POjvoQAApGSV4eCFMnx7KLf9vgQREbU5q+/am5iYiBkzZmD48OGIiYnB8uXLUV1djZkzZwIApk+fjpCQECxdutTsuNWrV+POO++Er69v21ROdufd+4dAJpUAAOIjAzBnfARWbK+fwJp0uhDbzxTifGEV3vjtNADAVS6DwSjgjiHBeHxMeJPv2/LrcYiIqCOyOoxMmTIFRUVFWLx4MVQqFYYMGYItW7aYJrVmZ2dDKjXvcDlz5gx2796N33//vW2qpg7NSSpBXLgvymt10OgNyCiqxsJb+yPE28Ws3R1DQkxhBABmfnrQbH+N1oD+QZ544+7BkEgkpu1yJy4cTETkSCSCNQs9iEStVsPLywsVFRWcP2JHjEYBF0qqkVtWixt7+5kFiku2HFdh9hcpFo+XSSXY8X/jEOrjara9sLIO//0zHZ5KZ/x3ezqie3TB90+MtMl3ICKi1mvp7zf/xCSbkUolCO/qjjF9uloMIgAwqpcvPBSXO+h6+rmZhnJWTY9uFEQAwN9DiZfvGIiBIV62KZyIiNqV1cM0RG3JQ+mM9Y/fgFvf3w0A+N+0aGh0RuiNRgzt3sWq96rVGvB/3x2BwkmKG3v74VxBFZ5J6NtkECIioo6BYYREFxnkidfuGogBwV7oE+Bh9fEpWWX4ZFcG9qQXY/uZIgDAxtQ8APXzUvoGWv+eRETUfhhGSHQSiQQPxvZoxZGXpzu92sTCaTqDsZVVERFRe+GcEbJbXVzlYpdARERtgGGE7FZMTx88POry8vDP/a0f/nlTLzw0Mgx+7vULo/3r68PXvPcNERGJi8M0ZLckEgme/VtfnCusRHSPLpg9Ntw0WXXt3gsAgIziaqRml2FkhF8z70RERGJiGCG7pnSWYd0jsY229w3wwJmCSgCAkdNGiIg6NA7TkEN6d8oQsUsgIqIWYhghhxQZ7Il+vKSXiMguMIyQw9t2UoXH1x1CgbpO7FKIiMgChhFyeJ8lZ2HriQLcuWIPymu0rX6fWq0Bvx69iIoaXRtWR0REDCPUaeRX1OH7hpVZrVWl0WP6mv2Y+9VhrNiRDkEQsOtcES6W17ZxlUREnQ+vpiGHJXdqnLXLqlveM6Ku0yG9sAq9/N3x0JoDSM0uBwCUVGnxr/Vp+OXIRUglwPb/G4cevm5tVTYRUafDMEIO69+39MWe9GKM7+uPB1btAwAonS13BqrrdJDLpDAYBWj0RtTpDBj31g5oLSwnv+nYRdTp6rcbBeDxdSnY8vQY230RIiIHxzBCDmtsn64Y26crAOD+EaFYfzDHYruDF0oxffUB6I1GOMukqNEa4O+hMAsi3q7OGN3LD78ezTcFkUtOqyqxP6MEUqkEI8J8bPeFiIgcFOeMUKd2WqXGvSuTUaszQGcQUKOtXzq+sFJj1u7rWTdgcDcvAICTVIL37h+CuHBf0/4pH+/DvSuTseloPg5kluLTPZkwGutv5HfyohqLfjyO43kV7fStiIjsC3tGqNPKKqnGtNUHLO6TSSVInNAHe9KLsWTyAPQN9IC7wglHcytw3/BQjOnTFb39PTDp/V1mx835KtX0/PcTBXhkdE88tf4wqrUGfJuSgy8fvQHRPbo0WZPOYIS6VgffhnvrEBF1Bgwj1CkVqOvwj9X7UVSpQS9/d5TX6BDgqYCn0hllNVp8PG04uvu6Ys74XqZjQn1c8d+pw0yvFU3MP7kkOaMEyRkl9W2dpKjTGXHPR3vx/gNDcXtUcKP2mcXVeGTtQeSU1WDHM+Mhl0mx93wxEgYEQuksa6NvTkTU8TCMUKfyxb5sTBoUhMfXpSCntBZhvq74alYsfN0UkErqb74nCILphnvNCfdzw7N/64sePm6QSoAnvky12O62wUEI93PD+3+mA6i/k3BGURWeju9janMgsxSPrTuE8oY1TH45chFv/HYaAODiLMOKB4fi57SLePCGHpyXQkQORyIIgiB2EdeiVqvh5eWFiooKeHp6il0O2aE5X6Zi07F8s22Bnkp8OzsOoT6ubfIZKVllKK3WYkCwJ277YDdKq7X4e3Q3vHnPYOw8V4SHPj1oatvTzw3b/28cAGBjai6e+/4odIaW/V/xiXERmBffx+Kly0REHUlLf7/ZM0KdgovcfJiji6sz1j0S02ZBBIDZXJA9z92EzOJq9A/ygEQiwbi+/vhl7mhM/u9uAICvmxyCIODdbWdNPSYTBwbixEU1sktrmv2cj3acR1ZJNT58MNpsuyAI+Dw5C4eyyvDqHQPh5eqMsmoturjJ2+w7EhHZAntGqFM4llthCgIA8NOcUYgK9W73OjYdzcecr1IRFeqNHj6u+PnIRQD1vR3P3NIXMz49gF3nivHEuAg8Hd8b7247hzW7MxEb7oNd54pN7yOXSTEtrgd83eXIKa1BSZUWEgmw9USBqU23Li7ILavFu1OiML6vP77Yl4Wb+gUgMpj/HyKi9tHS32+GEeo0TuWr8eaW03hsTATiInyvfYANXAojlzhJJXjtroGYMqI7AKCyTgdVRR16B1y+47DeYISTTAp1nQ4Tl+9CnpVL0I/t0xV/nS0yvX5kdE/8+5Y+cJWzY5SIbKulv98cdKZOo3+QJz6dGSNaELmap9IJnz8cYwoiAOChdDYLIgDgJJM2tHfGf+4Z3OT7ucpleGhkWKPtVwYRAFi9OxMzr5i/QkQkNv5pRNSOAjzr1w9ROkux8cmR6OXvcY0jzPUJcIePmxz9Aj1QVqNDDx9XBHopUaPV4/mJ/eHjJsdDI8Pwx6kCnC2oxDeHcgEAQ7t743DDvXUAYH9mKbR6IyfBElGHwGEaonYkCAKO56kR5ucKD6Vzq97DYBQgk1770uP9GSV46ZeTePCG7pga0x16o4DZ61KQdLrQ1GbtzBEY19e/VXUQEV0L54wQUSOn8tWY+N7lVWOVzlIcfzHBNBRERNSWOGeEiBrpG+CBe4Z1M72u0xlh6Ph/jxCRg2MYIepEpFIJ3rkvCt88Hid2KUREJgwjRJ1QvyDrJs4SEdkSwwgRERGJimGEqJM7eVEtdglE1MkxjBB1cnd9uBfVGr3YZRBRJ8YwQtQJyWVSyK+4nJdhhIjExDBC1AkpnWX4ePrlu/4u+fkE/v4Re0iISBwMI0Sd1Li+/qaVXH87rsKhrDKc4PwRIhIBwwhRJ3b1svJnCirxefIF1OkMIlVERJ0Rb5RH1In9e0If5FfU4fcTKlysqMOiH48DAHzdFLh1cFCL3uNAZikulFTj3uhu0OiNSC+swoBgT0gk175/DhERwDBC1Kk9PjYCALDzXJHZ9jlfpcLfMw7De3RpMlTUaPVYuvk01u3LAgBo9Eas3pWBCyU1+OCBoZgcFWzb4onIYXCYhogwKMQLSmcp3OQy07Z7Vybj20O5AIA6nQFf7s/CaZUagiDgcHYZbn1/tymIAMCiH4/jQkkNAKBAXde+X4CI7Bp7RogI7943BFqDESu2p+ODP9NN2zOKq3EqX437/peMyrrLV9rIpBIYjAICPZWo0eqhbtjnoXBCJa/IISIrsWeEiCCVSqB0liFxQh/8NGeUafu65Au47YPdZkEEAAxGAbdHBWPr02MwNbYHwv3csGr6cMRHBgAA0nLKkVdei/eTzuFYbgUE3hmYiJohEezgXwm1Wg0vLy9UVFTA09NT7HKIHN7Kv87jjd9Om16PjPDFxfJa0zBMU3NC5n6Vil+P5jfaPuvGnlhwa6TtCiaiDqmlv98cpiGiRh4aGYaIru5wVzghzM8VQV4uAIDskhp4uTjDy9XZ4nHeTWxPySqzWa1EZP8YRoioEaWzDBMahlyu1N3Xtdnj/nlTbxRVanAgsxT3jQjFd4dyUVKthauc/9QQUdP4LwQRtZkATyX+N2246XVkkCeeWp+GlKwylFVr0cVNLmJ1RNRRcQIrEdlcrc6Aed+kiV0GEXVQDCNEZDOeystzSNILqwAAgiDAaOzw8+aJqB0xjBCRzdzY2w/3DOsGoH5ya9KpAoz+z3bc+79kXu5LRCatCiMrVqxAWFgYlEolYmNjceDAgWbbl5eXY86cOQgKCoJCoUCfPn2wefPmVhVMRPbDSSbF5Kj6e9yczq/EI58dQl55LVKyymCw0DuSVVKNNbszkVlc3d6lEpGIrA4jGzZsQGJiIpYsWYLU1FRERUUhISEBhYWFFttrtVpMmDABFy5cwHfffYczZ85g1apVCAkJue7iich+6K8KHz8czru8z2DExzvPY+xbO/Dyrycx/u0dWJd8oZ0rJCKxWH01zbJlyzBr1izMnDkTALBy5Ups2rQJa9aswfPPP9+o/Zo1a1BaWoq9e/fC2bl+/DgsLOz6qiYiu9HTzw1yJylCu7jgjXsG48kvU1FUqcEz3x3F+H7+KFRr8Nz3R3Esr8LsuNOqSgBAemEl3ktKh05vxNybeqFvoAecZRxhJnIkVq3AqtVq4erqiu+++w533nmnafuMGTNQXl6On376qdExkyZNgo+PD1xdXfHTTz+ha9eumDp1Kp577jnIZLJG7QFAo9FAo9GYXqvVaoSGhnIFViI7VVmng5vcCVKpBCVVGkS/+kejNp5KJzw8uieW/3EOAHDf8G4I9HLB+0nnzNoNCvHCz3NHNXk3YSLqOFq6AqtVf14UFxfDYDAgIMB8MaSAgACoVCqLx2RkZOC7776DwWDA5s2bsWjRIrzzzjt49dVXm/ycpUuXwsvLy/QIDQ21pkwi6mA8lM6QSuvDg6+7AvKrejYmDgzEH4lj8XR8Hzx1c28AwDeHchsFEQA4lleByf/djT9PF5i2GY0CSqu1NvwGRGRLNu/rNBqN8Pf3x8cff4zo6GhMmTIFCxYswMqVK5s8Zv78+aioqDA9cnJybF0mEbWjl+4YAADwc5dj5T+G4aN/RMPfU9moXVcPBVZMHYatT4/B3cMuzzM7nqfGw2sP4VxBJY7mlmPS+7sw7JVtCHt+E45fNdxDRB2fVXNG/Pz8IJPJUFBQYLa9oKAAgYGBFo8JCgqCs7Oz2ZBM//79oVKpoNVqIZc3XpFRoVBAoVBYUxoR2ZEHYrrj/hGhFodaYsN94LNPjr8NDMRzf+sHL5f6uWbv3BuFAnUd9qSXmNpOeHdno+N/P1mAgSFetiueiNqcVT0jcrkc0dHRSEpKMm0zGo1ISkpCXFycxWNGjRqF9PR0GI1G07azZ88iKCjIYhAhos6hqTkfIyP8kLpoAl6/a5ApiFxq/+WjN+DACzc3OsZTefnvKo3OgO1nClGl0bd90URkE1YP0yQmJmLVqlX47LPPcOrUKTzxxBOorq42XV0zffp0zJ8/39T+iSeeQGlpKZ566imcPXsWmzZtwuuvv445c+a03bcgok7D31OJzx+OQYi3C4aEeuPrWTfg6IsJmB7XAwDw8a4MzPz0IN7eekbkSomopay+tHfKlCkoKirC4sWLoVKpMGTIEGzZssU0qTU7OxtS6eWMExoaiq1bt2LevHkYPHgwQkJC8NRTT+G5555ru29BRJ3KmD5dsef5myzuu3R9YPL5Etzz0V7cOTQE027o0Y7VEZG1rLq0VywtvTSIiDqvrSdUeGHjMXT1UJjWKAGAqG5e+GnuaBErI+q8bHJpLxFRR5UwIBApiybg4dE9zbZ3+L+2iMj6YRoioo7s9qhgGI0CqjR6vLrplNk+g1GA3miEwsnygotEJA72jBCRQ1E6y3B/THeEd3UzbTMaBXxzKAexr/+BW97dCb3BiJzSGsz5MhWj//Mnb8xHJDL2jBCRQyuu1ODuj/YiLae8/nWVFq9uOoWvDmRDq69fciAtpww9/dwaHbvrXBFe+fUkzhZUYendg/BATHcIgoAdZ4qQUVyNGXE94MT75BBdN05gJSKH9OfpAjy89pDptZtchmqtodlj7hnWDc/9rS/UdTq8tukUtp8pMu1zlknw/RMj8dqmU9ifWQoAuG1wEN65L6rZYZ9arQESSX2PDVFn09Lfb/aMEJFDcpNf/uftrqEheO5v/TDmre3Q6o0I8XbBwlv749uUXPx5utDU7vvUXHyfmguZVAKD0fzvNJ1BwO3/3WO27dej+UgYEIiL5bXYfFyFxAl9cCSnHMu2nQUAeLk4o6JWBwC4qZ8/Vs8Yzhv8EVnAnhEickgGo4DvU3PRy98dw7p3AQD8cuQiSqo0uD+mO5TOMhy8UIrZ61JQYuEmexMiAzB/Yv1y9FfeZfjuoSFQqeuw93z9svTOMgl0hpb9M3pk8S3wcnVutF1nMOJITjkGd/OG3InDPuQ4Wvr7zTBCRARgze5MvPzrSfTyd8fLtw/AyF5+AABBEDDnq1RodEbMm9AHA0O8YDAKuPvDPTiS2/imfJ5KJ6jr6peiD/F2gbvCCWcK6tc92fXseDjJJAj0VEIiqe99+flIHpb/cQ5ZJTV4fGw45k/s36r6a7R6bDtZgOgeXdCti2srzwJR22IYISKyUk5pDYK9XSCTXnso5bO9F/DxzgxMi+uBuHBfLNt2Fjf398f9I7pD7iSFwShAJpVAZzCi94LfGh2/6LZIfH0gG+mFVWbbf5k7GoO6texGf0ajgGqtHp8nZ+GTXRkoq9FhfN+u+HRmTMu+MJGNMYwQEXUATYWRS7xcnBHR1Q2p2eWmbWseGo6b+gU0eUxqdhne++Mc/jpb1GhfdI8u+P6JkddVM1Fb4QqsREQdgLNMipv7+aN/kCfuHxFqtu+pm3tj13Pjsei2SLPtD689hHX7shq9V2p2GWasOYC7P9xrFkR6+btjyvD6907JKsP6A9lQVdTZ4NsQ2QZ7RoiI2tHxvArsyyjB3cO6wcdNbtqeVVKNsW/tMGv78KieWDw5EilZZXgv6Rx2NgQQmVSCvgEecJZJ8NiYCEwcGIhtpwrw+LoU07H9Aj2w5ekxUNfp8NX+bFTW6fDvCX1hEAT8cuQi9meU4qn43gj2dmmX702dE4dpiIjsTHmNFje/85fp6h5nmQRxEX5mIeSeYSGYM74XeviaL9J2vqgKN7/zl+m1RAI8OS4Cn+/NQqVGb9p+5eXGAPDt7DiMCPOx5deiToxhhIjIDmn1RsxcewB70ktM2y6FkLnje6O7b9NXytTpDNhxphCzv0ht8efNurEnFtwaee2GRK3AOSNERHZI7iTFe/cPhUwqgZNUginDQ7H93+Pw5t+jmg0iQP0qr4O6eZteDwrxwsp/ROO1uwYCAKQS4JU7BuCHJy9PcO34f45SZ8AVWImIOhg/dwW2zRsDF7kMQV7WzekI8XbBJ9OHw1UhQ1y4r2nF1wdje5i1mz02Aiv/Ot9mNRNdD4YRIqIOKLyre6uPjY9s+rJgoo6IwzREREQkKoYRIqJO7JPdmfg8+QKMRk4eIfEwjBARdUJy2eUl7xf/dAL/2XJaxGqos2MYISLqhO4bEQoXZ5np9Z7zxdAbjCJWRJ0ZwwgRUSfUrYsrjr54C8b37QoAOJ6nxvyNx7D9TCEmvrcLYc9vwgdJ50SukjoLLnpGRNSJbTme3+wiaTf29oPBKGDNQyOgvKInhaglWvr7zUt7iYg6sQmRgXggpju+PpANAHCVy1CjNZj27zpXDAD4LiUXpdVaRAZ54qZ+/pBKJRbfj6g1GEaIiDoxmVSCFyb1g85gRE8/N/wjtgc8lE4Y89Z2FKo1MAgCDEYBC388bnbcPcO64ZmEvgj0Upq2lVVrsfFwHrr7uGIC1zohK3CYhoiIGqlquLnere/vQlZJjcU2fu5yqOv00OqN6NbFBblltaZ9k6OC8cEDQ9ulVuq4eG8aIiJqNXeFE9wVTnj1zoGYP7Ef9s2/GYtvM7+hXnGVFlp9/RU4VwYRAPjlyEXUaC/fLbi0WosV29PxyNqDyCyutv0XILvCnhEiIrLKun1ZWHTVsM3N/fwxa0w49mWUYPkf9Vfh3BIZgIEhXrhYXosfDudB0xBcnp/YD7PHRrR73dT+Wvr7zTBCRERWEQQB54uqEObrBieZeQd7rdaA/ou3WDzOWSaBzlD/k/P+A0Ph4yrH0O7eqNUZ4Kl0htyJnfWOhlfTEBGRTUgkEvTy97C4T+ksxQ3hPtiXUWradktkAB4bE45fj+Zj7d4LAIB/fX3Y7LhJgwLx4YPRAICzBZUAgD4Blj+DHA97RoiIqM0ZjAK0eiO0eiO8XJ0BAIWVdRj/1g5UX3Hp8CWeSifoDAJqdfX75DIpUhdPgLuCfzPbM/aMEBGRaGRSCVzkMrjILy+U5u+hxG9PjcHKnecR7ueGpFOFcJHL8OfpQqjr9GbHaw1GVNbpGEY6Cf6vTERE7aa7rytev2sQAODRG8NRqK7DpPd3w89djjqdAbcNDsbKv85DbxRgFIDtZwqRW1qD+2O6w1nGOSWOisM0REQkKkEQIJFcXtG1z4LfoDUYEeiphEpdZ9r+R+JY9PJ3N72urNPBxVkGvVFA0qlC5JXXwN9DiYEhnk3OaaH2xWEaIiKyC1cGkStdGUQA4OCFUvT0c8OfpwvxefIF01L1lvz6z9EYGOLVpnWS7bBnhIiIOpTnvjuKMwWVuG94KMK7uuH+j/e16n2+eCQWEf5uCPJyaeMKqaW4zggRETmEJ75IwW/HVabXXVyd8ffobvBycUZ+RR1u7u+P2J6+qNMZcMu7O1FSrTU7/td/jkZFrQ4HMkvx4A3d4e+hvPojyEY4TENERA7hsTHhyC2rhUpdh/kT+2HSoCAonWWN2rkpnPDnv8dhwrt/obBSY9r+wMf7UNlwrx2JBHg6vg8AQKs3Ys/5YoT7uaGHr1v7fBmyiD0jRETkUCpqdMgorsL9H+8zLUF/pTUPDce+jFJ8n5KLkmot+gV6YMvTY0So1PFxmIaIiDq1n9LysDE1D5MGBeJIbgW+2p/dbPvbo4LxPu803KZ4114iIurU7hgSgs8ejsGUEd1x55AQs3039/PHoqvuQvzzkYvQ6BuvDku2xzkjRETk8GJ6+uDcaxPx5+lCDO7mhSAvFwiCAKWzFJ/tvYCzBVVil9ipsWeEiIg6BWeZFAkDAk2X+kokEjwY2wPfPzFS5MqIYYSIiKjBWVUVVBV1ONdw52BqHxymISIiajD5v7sB1F8C/Oe/x6GnHy/5bQ/sGSEiok7NVe6EEG/zVVoFARj/9g6oKuqaOIraEi/tJSKiTq9ao8fGw3kortRg1a4M1GjNr6oZ3csP704Zgq4eCpEqtE9cZ4SIiKgVDl0oxd9XJlvcN39iP+SU1WDy4GDEhvu2c2X2h8vBExERtcLwMB/sfGY8fj6Sh60nCnAsr8K0b+lvpwEAOaW1DCNtiHNGiIiIrtLd1xVzb+qNX/45Gj/PHdVov87QeJl5ar1WhZEVK1YgLCwMSqUSsbGxOHDgQJNt165dC4lEYvZQKnnHRCIisg+Du3kjc+kkbJs3Bm/9fbDY5Tgkq8PIhg0bkJiYiCVLliA1NRVRUVFISEhAYWFhk8d4enoiPz/f9MjKyrquoomIiNqTRCJB7wAPyJ3qfzYziqpxz0d78d4f52AHUy87PKvDyLJlyzBr1izMnDkTkZGRWLlyJVxdXbFmzZomj5FIJAgMDDQ9AgICrqtoIiIiManUdUjJKsO7f5zFzLUHxS7H7lkVRrRaLVJSUhAfH3/5DaRSxMfHIznZ8sxjAKiqqkKPHj0QGhqKO+64AydOnGj2czQaDdRqtdmDiIhIbAOCveDt6gwPxeXrP3adKxaxIsdgVRgpLi6GwWBo1LMREBAAlUpl8Zi+fftizZo1+Omnn/DFF1/AaDRi5MiRyM3NbfJzli5dCi8vL9MjNDTUmjKJiIhsope/O9IW34KjL96CxQ13/TUYBYQ9vwkjlyZh9e5MfJB0DpV1OpxRVeJ8URV0BiNOq9QwGjmc0xSr1hm5ePEiQkJCsHfvXsTFxZm2P/vss/jrr7+wf//+a76HTqdD//798cADD+CVV16x2Eaj0UCj0Zheq9VqhIaGcp0RIiLqMIqrNBj+6h8tbv/s3/riyXG9bFhRx9PSdUas6hnx8/ODTCZDQUGB2faCggIEBga26D2cnZ0xdOhQpKenN9lGoVDA09PT7EFERNSR+Lkr8Mn04Zge16NF7d/ccgZHcsptW5SdsmrRM7lcjujoaCQlJeHOO+8EABiNRiQlJWHu3Lkteg+DwYBjx45h0qRJVhdLRETUkcRHBiA+MgBzb+qFao0B6YVVSD5fgvj+/iiu1uLQhVKoKurw+8n6P+LvWLEH9wzrhulxPRAV6i1u8R2I1SuwJiYmYsaMGRg+fDhiYmKwfPlyVFdXY+bMmQCA6dOnIyQkBEuXLgUAvPzyy7jhhhvQq1cvlJeX46233kJWVhYeffTRtv0mREREIvH3UAIeQE8/N0yIvDyv8vaoYKQXVprCCAB8n5qL71NzcXM/fwzu5o2n4nuLUXKHYnUYmTJlCoqKirB48WKoVCoMGTIEW7ZsMU1qzc7OhlR6efSnrKwMs2bNgkqlQpcuXRAdHY29e/ciMjKy7b4FERFRB9XL3wNbnr4Rt72/G/orJrEmnS7EgQulDCPgjfKIiIjaTV55LSYu3wkXuQwF6voLNVZMHYZbBweJXJlt8EZ5REREHUyItwuOLLkF2aU1GPvWDgDAvA1p0BuN+D41DzvPFuGZhL54bEw4nGWd5/Zx7BkhIiJqZ3qDEfd8tBdHciuabJM8/yYEebm0Y1VtzyaX9hIREdH1c5JJseHxOCid63+GQ30ah464pX9i/Ns7cFrl+KuQc5iGiIhIBEpnGX6cMwo1WgOGhnpDIpEg+XwJHli1z9Qms7gam4+p0C/QsUcF2DNCREQkkn6BnhjWvQskEgkAIC7CFxseuwGRQZfDx19ni6DVG8UqsV1wzggREVEH9OLPJ7B27wXT636BHlg8ORIjI/zEK8pKnDNCRERkx/oHeZi9Pq2qxNRV+7ExNRc5pTUiVWUbDCNEREQd0JQR3fHd7DhEdHUz2574zRE8+WUqDEYBdToD6nQGkSpsOxymISIi6uCO51Xgtg92W9wX4u2CHc+M65DrknCYhoiIyEEMDPHCiZcS8NtTNzbal1deC3WtToSq2g7DCBERkR1wUzihf5AnPn84Bgsm9cd/7hlk2vfPrw/jm4M5sIPBDos4TENERGSnwp7f1GjbUzf3Rmm1Fk/F94afu0KEqi7jMA0REZGDi+/v32jbe0nnsG5fFn47li9CRa3DFViJiIjs1CczRgAADl4oxb0rkwEAzjIJdAYBWkOHH/gwYc8IERGRnRsR5oP01yYiZWE8Jg0KErscqzGMEBEROQAnmRS+Is8RaS0O0xARETmgg5mlSD5fgn0ZJZge1wMGQUBJlRZTRoRiRJiP2OWZYRghIiJyQFtOqEzPP9xx3vT8u5Rc3De8G8L83PDkuF5ilNYIwwgREZEDCfdzBwCE+bpCIpEgs7i6UZtvDuVCKgGeGBthumOwmBhGiIiIHMi/bu6F+0Z0Q6CnEgBw8EIZevq5QSoBbl72F6QSCUqrtTAKgCAAHSCLcNEzIiKizkJvMKK8Vofhr/5h2vbulCjcNbSbTT6Pi54RERGRGSeZFG5yJzhJL3eHzNtwBKqKOhGrYhghIiLqVFzkMvzw5Ch09bh8GXB+Ra2IFTGMEBERdTqDunnh4IJ4KJzqY8BdH+5FSZVGtHoYRoiIiDopb1dn0/P9maWi1cGraYiIiDqp5VOGIr2wEjf3D0Cwt4todTCMEBERdVJxEb6Ii/AVuwwO0xAREZG4GEaIiIhIVAwjREREJCqGESIiIhIVwwgRERGJimGEiIiIRMUwQkRERKJiGCEiIiJRMYwQERGRqBhGiIiISFQMI0RERCQqhhEiIiISFcMIERERicou7torCAIAQK1Wi1wJERERtdSl3+1Lv+NNsYswUllZCQAIDQ0VuRIiIiKyVmVlJby8vJrcLxGuFVc6AKPRiIsXL8LDwwMSiaTN3letViM0NBQ5OTnw9PRss/clczzP7Yfnun3wPLcPnuf2YcvzLAgCKisrERwcDKm06ZkhdtEzIpVK0a1bN5u9v6enJ/9Dbwc8z+2H57p98Dy3D57n9mGr89xcj8glnMBKREREomIYISIiIlF16jCiUCiwZMkSKBQKsUtxaDzP7Yfnun3wPLcPnuf20RHOs11MYCUiIiLH1al7RoiIiEh8DCNEREQkKoYRIiIiEhXDCBEREYnK4cPIihUrEBYWBqVSidjYWBw4cKDZ9t9++y369esHpVKJQYMGYfPmze1UqX2z5jyvWrUKN954I7p06YIuXbogPj7+mv+70GXW/jd9yfr16yGRSHDnnXfatkAHYe15Li8vx5w5cxAUFASFQoE+ffrw348WsPY8L1++HH379oWLiwtCQ0Mxb9481NXVtVO19mnnzp2YPHkygoODIZFI8OOPP17zmB07dmDYsGFQKBTo1asX1q5da9siBQe2fv16QS6XC2vWrBFOnDghzJo1S/D29hYKCgostt+zZ48gk8mEN998Uzh58qSwcOFCwdnZWTh27Fg7V25frD3PU6dOFVasWCEcPnxYOHXqlPDQQw8JXl5eQm5ubjtXbn+sPdeXZGZmCiEhIcKNN94o3HHHHe1TrB2z9jxrNBph+PDhwqRJk4Tdu3cLmZmZwo4dO4S0tLR2rty+WHuev/zyS0GhUAhffvmlkJmZKWzdulUICgoS5s2b186V25fNmzcLCxYsEDZu3CgAEH744Ydm22dkZAiurq5CYmKicPLkSeGDDz4QZDKZsGXLFpvV6NBhJCYmRpgzZ47ptcFgEIKDg4WlS5dabH/fffcJt956q9m22NhY4fHHH7dpnfbO2vN8Nb1eL3h4eAifffaZrUp0GK0513q9Xhg5cqTwySefCDNmzGAYaQFrz/NHH30khIeHC1qttr1KdAjWnuc5c+YIN910k9m2xMREYdSoUTat05G0JIw8++yzwoABA8y2TZkyRUhISLBZXQ47TKPVapGSkoL4+HjTNqlUivj4eCQnJ1s8Jjk52aw9ACQkJDTZnlp3nq9WU1MDnU4HHx8fW5XpEFp7rl9++WX4+/vjkUceaY8y7V5rzvPPP/+MuLg4zJkzBwEBARg4cCBef/11GAyG9irb7rTmPI8cORIpKSmmoZyMjAxs3rwZkyZNapeaOwsxfgvt4kZ5rVFcXAyDwYCAgACz7QEBATh9+rTFY1QqlcX2KpXKZnXau9ac56s999xzCA4ObvQfP5lrzbnevXs3Vq9ejbS0tHao0DG05jxnZGTgzz//xIMPPojNmzcjPT0dTz75JHQ6HZYsWdIeZdud1pznqVOnori4GKNHj4YgCNDr9Zg9ezZeeOGF9ii502jqt1CtVqO2thYuLi5t/pkO2zNC9uGNN97A+vXr8cMPP0CpVIpdjkOprKzEtGnTsGrVKvj5+YldjkMzGo3w9/fHxx9/jOjoaEyZMgULFizAypUrxS7NoezYsQOvv/46PvzwQ6SmpmLjxo3YtGkTXnnlFbFLo+vksD0jfn5+kMlkKCgoMNteUFCAwMBAi8cEBgZa1Z5ad54vefvtt/HGG2/gjz/+wODBg21ZpkOw9lyfP38eFy5cwOTJk03bjEYjAMDJyQlnzpxBRESEbYu2Q635bzooKAjOzs6QyWSmbf3794dKpYJWq4VcLrdpzfaoNed50aJFmDZtGh599FEAwKBBg1BdXY3HHnsMCxYsgFTKv6/bQlO/hZ6enjbpFQEcuGdELpcjOjoaSUlJpm1GoxFJSUmIi4uzeExcXJxZewDYtm1bk+2pdecZAN5880288sor2LJlC4YPH94epdo9a891v379cOzYMaSlpZket99+O8aPH4+0tDSEhoa2Z/l2ozX/TY8aNQrp6emmsAcAZ8+eRVBQEINIE1pznmtqahoFjksBUOBt1tqMKL+FNpsa2wGsX79eUCgUwtq1a4WTJ08Kjz32mODt7S2oVCpBEARh2rRpwvPPP29qv2fPHsHJyUl4++23hVOnTglLlizhpb0tYO15fuONNwS5XC589913Qn5+vulRWVkp1lewG9ae66vxapqWsfY8Z2dnCx4eHsLcuXOFM2fOCL/++qvg7+8vvPrqq2J9Bbtg7XlesmSJ4OHhIXz99ddCRkaG8PvvvwsRERHCfffdJ9ZXsAuVlZXC4cOHhcOHDwsAhGXLlgmHDx8WsrKyBEEQhOeff16YNm2aqf2lS3ufeeYZ4dSpU8KKFSt4ae/1+uCDD4Tu3bsLcrlciImJEfbt22faN3bsWGHGjBlm7b/55huhT58+glwuFwYMGCBs2rSpnSu2T9ac5x49eggAGj2WLFnS/oXbIWv/m74Sw0jLWXue9+7dK8TGxgoKhUIIDw8XXnvtNUGv17dz1fbHmvOs0+mEF198UYiIiBCUSqUQGhoqPPnkk0JZWVn7F25Htm/fbvHf3EvndsaMGcLYsWMbHTNkyBBBLpcL4eHhwqeffmrTGiWCwL4tIiIiEo/DzhkhIiIi+8AwQkRERKJiGCEiIiJRMYwQERGRqBhGiIiISFQMI0RERCQqhhEiIiISFcMIERERiYphhIiIiETFMEJERESiYhghIiIiUTGMEBERkaj+H2WtcPxc3RKqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_probs)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
