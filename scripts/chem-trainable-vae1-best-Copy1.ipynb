{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-21 15:56:29.948937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-21 15:56:31.277198: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-21 15:56:31.277347: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-21 15:56:31.277365: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "20637ffb-f819-4f0a-b926-d24784746a9e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jkF1N4olXTtZ",
    "outputId": "de7467ba-69c5-42b4-a918-755fe87765d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "weKR7jCuMvQl"
   },
   "outputs": [],
   "source": [
    "with open('../datasets-ood/chem/train.csv', 'r') as f:\n",
    "  dataX = np.float32(np.array([line.strip().split(',')[2:] for line in f])[1:])\n",
    "\n",
    "with open('../datasets-ood/chem/train.csv', 'r') as f:\n",
    "  dataY = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "X = dataX\n",
    "Y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets-ood/chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('../datasets-ood/chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "2948628d-0085-48ba-8e9b-4bbe93f27e66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5997, 1024)\n",
      "(5997,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, d, D=128):\n",
    "\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, X, d, D=128):\n",
    "\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot/2, sigma_rot/4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "    self.Ws = self.sample_features(X)\n",
    "    \n",
    "  def sample_features(self, X, ):\n",
    "    L = int(0.3 * len(X))\n",
    "    M = self.Ws.shape[0] * self.Ws.shape[2]\n",
    "    # N = np.random.choice(M, int(M/100), replace=False)\n",
    "    N = int(M/100)+10\n",
    "    phi_Xt = tf.transpose(self.get_features(X[np.random.choice(len(X), L)])) / np.sqrt(L)\n",
    "    phi_phi_T = phi_Xt @ tf.transpose(phi_Xt)\n",
    "    mu = np.power(10, 0)\n",
    "    diag = np.diag(phi_phi_T @ np.linalg.inv(phi_phi_T + mu))\n",
    "    diag = diag / np.sum(diag)\n",
    "    print(M, len(diag)//2, phi_Xt.shape, self.Ws.shape)\n",
    "    print(\"Diag\", M, diag, np.argsort(diag)[-N])\n",
    "    k, N, D = self.Ws.shape\n",
    "    diag = diag[:len(diag)//2] + diag[len(diag)//2:]\n",
    "    w_indices = np.unique(np.argsort(diag)[-N:])\n",
    "    _Ws = np.reshape(np.transpose(self.Ws, [1, 2, 0]), (N, -1))[:, w_indices]\n",
    "    return np.transpose(np.reshape(_Ws, (N, -1, 1)), [2, 0, 1])\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    # phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.transpose(phis, [1, 2, 0])[:, None, :, :]  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3S8unT73bEtM"
   },
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, X_ids, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  # clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  print(np.mean(X), np.mean(Y), \"ASDASDADASD\")\n",
    "  clf = SVC(random_state=0, tol=1e-5, kernel='linear').fit(X, Y)\n",
    "  # clf = LinearSVC(random_state=0, tol=1e-5).fit(X, Y)\n",
    "  support = (clf.support_, clf.n_support_)\n",
    "\n",
    "  def get_supp(support):\n",
    "      supps_, n_supps_ = support\n",
    "      supps_0 = supps_[:n_supps_[0]]\n",
    "      supps_1 = supps_[n_supps_[0]:]\n",
    "      return X_ids[supps_0], X_ids[supps_1]\n",
    "\n",
    "  support = get_supp(support)\n",
    "    \n",
    "  # clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  print(wgts.shape)\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print(np.mean(prd[:, 0]==Y))\n",
    "  return wgts, None\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  supps = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    print(X_sub.shape, X_ids.shape)\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    print(\"CHECKKKKK \", np.mean(get_rand_feats(X_sub@pca_projs, model)))\n",
    "    print(np.sum(X), np.sum(X@pca_projs))\n",
    "    beta, supp = linear_coefs(get_rand_feats(X_sub@pca_projs, model), np.argwhere(X_ids), Y_sub)\n",
    "    print(\"Dir\", dir)\n",
    "    print(\"Beta\", beta)\n",
    "    # beta = linear_coefs(X_sub, Y_sub)\n",
    "\n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    supps.append(supp)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas, supps\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997, 1024)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0052576065"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('gpu:1'):\n",
    "    s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(232.08453, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims = [0.05, 0.25, 0.3, 0.4, 0.8]\n",
    "dims = [0.05, 0.2, 0.3, 0.4, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('pca.npy', pca_projs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONENNN -24.82146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08707595, None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(pca_projs), print(\"ONENNN\", np.sum(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, pca_projs.shape\n",
    "random_dirs = np.random.randn(N, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "f3d09ad5-4e02-44a5-e001-fe83e3d99938",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19985221\n",
      "-0.0052576065 -0.00623703\n",
      "0.19985221 0.545 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [ 0.04905563  0.03169993 -0.01668578 ... -0.02326956  0.02807696\n",
      "  0.05754044]\n",
      "Beta [[ 0.29058316  0.03562289 -0.00618984 ...  0.03539478 -0.0066337\n",
      "   0.00280665]]\n",
      "(1201, 1024) (5997,)\n",
      "CHECKKKKK  0.18812044\n",
      "-0.0052576065 -0.00623703\n",
      "0.18812044 0.6053289 ASDASDADASD\n",
      "0.9941715237302248\n",
      "(1, 4081)\n",
      "Dir [-0.05284877 -0.01113672  0.01344122 ... -0.01854731  0.02493274\n",
      " -0.03622702]\n",
      "Beta [[ 0.04925948 -0.00100127  0.0172976  ... -0.01054612  0.00822153\n",
      "   0.00358063]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19500722\n",
      "-0.0052576065 -0.00623703\n",
      "0.19500722 0.5775 ASDASDADASD\n",
      "0.995\n",
      "(1, 4081)\n",
      "Dir [-0.00040978  0.04100408 -0.0269265  ... -0.0292302  -0.01387339\n",
      "  0.0406016 ]\n",
      "Beta [[ 0.44189246 -0.00574511 -0.04819742 ... -0.00822744  0.00855847\n",
      "   0.00052637]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19672775\n",
      "-0.0052576065 -0.00623703\n",
      "0.19672775 0.46916667 ASDASDADASD\n",
      "0.995\n",
      "(1, 4081)\n",
      "Dir [ 0.02473948 -0.01989506 -0.02670362 ... -0.0214983  -0.04580616\n",
      "  0.0076278 ]\n",
      "Beta [[ 2.71267106e-01  1.17258847e-02 -3.26267266e-02 ... -9.34990606e-03\n",
      "   1.77147908e-02 -2.57165840e-04]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.2016437\n",
      "-0.0052576065 -0.00623703\n",
      "0.2016437 0.55833334 ASDASDADASD\n",
      "0.9958333333333333\n",
      "(1, 4081)\n",
      "Dir [-0.0249224   0.01626729 -0.01536909 ... -0.01208532 -0.01406717\n",
      "  0.01407208]\n",
      "Beta [[-0.02087686  0.02410965 -0.00586908 ...  0.00902178  0.0010695\n",
      "   0.00741659]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19509709\n",
      "-0.0052576065 -0.00623703\n",
      "0.19509709 0.5025 ASDASDADASD\n",
      "0.9958333333333333\n",
      "(1, 4081)\n",
      "Dir [-0.01864907 -0.03720603  0.05605809 ...  0.02410797  0.0335988\n",
      "  0.01960076]\n",
      "Beta [[ 0.22501573 -0.02803904 -0.06101294 ...  0.01490491  0.01253886\n",
      "   0.00619937]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19849451\n",
      "-0.0052576065 -0.00623703\n",
      "0.19849451 0.5566667 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [ 0.02646618 -0.04022221  0.01357067 ... -0.02566839 -0.01645584\n",
      " -0.01219936]\n",
      "Beta [[ 0.52979223 -0.01265296  0.02973078 ...  0.01345125  0.00133469\n",
      "   0.01385535]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20362836\n",
      "-0.0052576065 -0.00623703\n",
      "0.20362836 0.48916668 ASDASDADASD\n",
      "0.9933333333333333\n",
      "(1, 4081)\n",
      "Dir [ 0.01856821  0.0023587  -0.00064745 ...  0.00581698  0.06765707\n",
      "  0.02807538]\n",
      "Beta [[ 0.12235992 -0.00847422 -0.01238014 ... -0.01306574  0.01262169\n",
      "   0.00343372]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20510614\n",
      "-0.0052576065 -0.00623703\n",
      "0.20510614 0.58416665 ASDASDADASD\n",
      "0.9933333333333333\n",
      "(1, 4081)\n",
      "Dir [ 0.01724229  0.02803767  0.02921948 ... -0.00069007 -0.06495243\n",
      "  0.03521613]\n",
      "Beta [[-0.32094528  0.05372081 -0.02618266 ...  0.02926008  0.01982413\n",
      "   0.00365832]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19092399\n",
      "-0.0052576065 -0.00623703\n",
      "0.19092399 0.59 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [ 0.03303573 -0.01577196  0.00706746 ... -0.02927634 -0.00509305\n",
      "  0.00611053]\n",
      "Beta [[ 0.10755376 -0.00892447  0.00013843 ...  0.0175174  -0.01194291\n",
      "   0.00359371]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19750738\n",
      "-0.0052576065 -0.00623703\n",
      "0.19750738 0.5075 ASDASDADASD\n",
      "0.995\n",
      "(1, 4081)\n",
      "Dir [ 0.03168633  0.06495862  0.04103932 ...  0.00095118 -0.00735116\n",
      " -0.01611738]\n",
      "Beta [[ 1.19742750e-01 -2.73717677e-02 -4.83500962e-03 ... -2.76753106e-02\n",
      "  -3.78011051e-05  3.81076905e-03]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19782507\n",
      "-0.0052576065 -0.00623703\n",
      "0.19782507 0.5625 ASDASDADASD\n",
      "0.995\n",
      "(1, 4081)\n",
      "Dir [ 0.00277891 -0.03651961  0.01369712 ... -0.03154832 -0.01678524\n",
      "  0.04267827]\n",
      "Beta [[-0.07784422 -0.01631388 -0.00436065 ... -0.01353326 -0.00746534\n",
      "  -0.00097958]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20123374\n",
      "-0.0052576065 -0.00623703\n",
      "0.20123374 0.45416668 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [ 0.04800621  0.01082783 -0.02037785 ...  0.03341993  0.01415094\n",
      " -0.03749007]\n",
      "Beta [[ 0.09156004 -0.01550183  0.01278264 ... -0.01319262 -0.01457541\n",
      "   0.00300322]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19937482\n",
      "-0.0052576065 -0.00623703\n",
      "0.19937482 0.5316667 ASDASDADASD\n",
      "0.9933333333333333\n",
      "(1, 4081)\n",
      "Dir [0.02691227 0.00787205 0.02408651 ... 0.06152159 0.01881851 0.0252725 ]\n",
      "Beta [[ 0.17263075 -0.02686615 -0.0301869  ...  0.00762774 -0.00359561\n",
      "   0.006026  ]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20046435\n",
      "-0.0052576065 -0.00623703\n",
      "0.20046435 0.47583333 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [0.02253449 0.01077164 0.02421499 ... 0.04154573 0.0611235  0.03692525]\n",
      "Beta [[-0.09171491 -0.02377048  0.03397559 ... -0.00462669  0.01082214\n",
      "   0.00167106]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19585408\n",
      "-0.0052576065 -0.00623703\n",
      "0.19585408 0.5708333 ASDASDADASD\n",
      "0.9941666666666666\n",
      "(1, 4081)\n",
      "Dir [ 0.01723905  0.00932982  0.02862695 ...  0.00473735  0.0417287\n",
      " -0.00043299]\n",
      "Beta [[ 4.34289569e-01  3.16561914e-02 -3.34190580e-03 ...  1.93999842e-02\n",
      "  -5.82446787e-03  1.48088304e-04]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19518363\n",
      "-0.0052576065 -0.00623703\n",
      "0.19518363 0.50083333 ASDASDADASD\n",
      "0.9925\n",
      "(1, 4081)\n",
      "Dir [ 0.00818381  0.05524835 -0.01712511 ...  0.02450519  0.01362116\n",
      "  0.02711455]\n",
      "Beta [[ 4.37762881e-01 -1.43685366e-02 -2.13051405e-02 ...  1.37165559e-04\n",
      "  -9.31245353e-03  6.02629631e-04]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19450207\n",
      "-0.0052576065 -0.00623703\n",
      "0.19450207 0.5316667 ASDASDADASD\n",
      "0.9958333333333333\n",
      "(1, 4081)\n",
      "Dir [ 0.00060991  0.01967341 -0.00978845 ... -0.03734414 -0.04589148\n",
      " -0.03576715]\n",
      "Beta [[ 0.41252257  0.01148889  0.02932781 ...  0.03258901 -0.00089913\n",
      "  -0.00080381]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19845985\n",
      "-0.0052576065 -0.00623703\n",
      "0.19845985 0.49416667 ASDASDADASD\n",
      "0.9908333333333333\n",
      "(1, 4081)\n",
      "Dir [-0.03870521  0.04986369  0.02366639 ...  0.0055116  -0.00092787\n",
      " -0.01240668]\n",
      "Beta [[-0.28734433 -0.02562378 -0.00585154 ...  0.01090687  0.01332554\n",
      "   0.00056884]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19934012\n",
      "-0.0052576065 -0.00623703\n",
      "0.19934012 0.49666667 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [-0.01427566  0.08973892 -0.00928323 ...  0.003695    0.0188399\n",
      " -0.08265545]\n",
      "Beta [[ 0.33126367  0.02942609 -0.03237829 ...  0.00242114  0.00225396\n",
      "   0.00848012]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19853115\n",
      "-0.0052576065 -0.00623703\n",
      "0.19853115 0.5175 ASDASDADASD\n",
      "0.9941666666666666\n",
      "(1, 4081)\n",
      "Dir [ 0.00408598 -0.01200283  0.0584     ... -0.02660609  0.00062351\n",
      " -0.01699404]\n",
      "Beta [[-0.11376778  0.02672064  0.02516242 ... -0.01835691  0.00978474\n",
      "   0.00831344]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20173645\n",
      "-0.0052576065 -0.00623703\n",
      "0.20173645 0.535 ASDASDADASD\n",
      "0.9983333333333333\n",
      "(1, 4081)\n",
      "Dir [0.03055826 0.01532836 0.00438357 ... 0.04857082 0.00185673 0.00526939]\n",
      "Beta [[ 0.52129538 -0.00568791 -0.02081707 ...  0.01618903 -0.00322451\n",
      "   0.00446763]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19852637\n",
      "-0.0052576065 -0.00623703\n",
      "0.19852637 0.41916665 ASDASDADASD\n",
      "0.9908333333333333\n",
      "(1, 4081)\n",
      "Dir [ 0.05139564 -0.01552068 -0.01545289 ... -0.01340144  0.02881572\n",
      "  0.04791447]\n",
      "Beta [[-0.44791342  0.04033693 -0.0079302  ... -0.00797998  0.02043867\n",
      "   0.00142537]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19786227\n",
      "-0.0052576065 -0.00623703\n",
      "0.19786227 0.61333334 ASDASDADASD\n",
      "0.9933333333333333\n",
      "(1, 4081)\n",
      "Dir [-0.02897424 -0.0220174  -0.04966025 ...  0.01914678 -0.03873034\n",
      " -0.00213593]\n",
      "Beta [[ 0.1370959  -0.00487517 -0.01215066 ... -0.02686213  0.01664864\n",
      "  -0.00427096]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20143713\n",
      "-0.0052576065 -0.00623703\n",
      "0.20143713 0.625 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [ 0.01484983 -0.02794425  0.09615445 ... -0.01269538  0.02036687\n",
      "  0.02159965]\n",
      "Beta [[-0.04511551  0.02722621  0.00580853 ... -0.00510483  0.01490847\n",
      "   0.00902028]]\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19359674\n",
      "-0.0052576065 -0.00623703\n",
      "0.19359674 0.5475 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [ 0.03062971  0.00729675  0.02662    ... -0.01826016  0.03652041\n",
      " -0.02010462]\n",
      "Beta [[ 0.0448096  -0.03409597 -0.01752248 ... -0.00646482 -0.00049106\n",
      "   0.00350205]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.197001\n",
      "-0.0052576065 -0.00623703\n",
      "0.197001 0.6075 ASDASDADASD\n",
      "0.9941666666666666\n",
      "(1, 4081)\n",
      "Dir [-0.00880713  0.02878523  0.00481571 ... -0.0019655  -0.04885442\n",
      " -0.03262503]\n",
      "Beta [[ 0.46759252 -0.00676028 -0.03149822 ...  0.00588738  0.00637101\n",
      "   0.00615338]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20095055\n",
      "-0.0052576065 -0.00623703\n",
      "0.20095055 0.57666665 ASDASDADASD\n",
      "0.9925\n",
      "(1, 4081)\n",
      "Dir [-0.01670731  0.01235236  0.03696373 ...  0.01358034 -0.00010722\n",
      "  0.00289552]\n",
      "Beta [[ 0.09884624 -0.02359238 -0.000368   ...  0.02177835  0.01017605\n",
      "   0.00222358]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19596587\n",
      "-0.0052576065 -0.00623703\n",
      "0.19596587 0.6016667 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [ 0.0360369   0.02274474 -0.05182789 ...  0.04126596 -0.06215097\n",
      " -0.01369314]\n",
      "Beta [[ 0.03975566  0.01244685 -0.02086509 ... -0.03427629 -0.01259591\n",
      "   0.00366753]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.18973969\n",
      "-0.0052576065 -0.00623703\n",
      "0.18973969 0.5183333 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [-0.00805216 -0.02645824  0.00730783 ... -0.04042667  0.0149599\n",
      " -0.02170477]\n",
      "Beta [[-0.33894509  0.00377694  0.01178628 ... -0.02951412  0.01414912\n",
      "   0.0045863 ]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20214275\n",
      "-0.0052576065 -0.00623703\n",
      "0.20214275 0.48416665 ASDASDADASD\n",
      "0.995\n",
      "(1, 4081)\n",
      "Dir [-0.00081147 -0.05391486 -0.03064158 ... -0.0459241   0.00935409\n",
      "  0.03047794]\n",
      "Beta [[-0.06308828  0.0258972  -0.01308646 ... -0.01633004  0.00568437\n",
      "   0.00527397]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19817367\n",
      "-0.0052576065 -0.00623703\n",
      "0.19817367 0.56416667 ASDASDADASD\n",
      "0.9958333333333333\n",
      "(1, 4081)\n",
      "Dir [ 0.02757937 -0.01715149  0.03477351 ...  0.00739955 -0.0188983\n",
      "  0.0065274 ]\n",
      "Beta [[ 2.41129706e-01  2.58641941e-02 -2.34840715e-02 ... -1.14881356e-03\n",
      "   3.63224266e-03 -2.23397540e-04]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19616279\n",
      "-0.0052576065 -0.00623703\n",
      "0.19616279 0.5933333 ASDASDADASD\n",
      "0.995\n",
      "(1, 4081)\n",
      "Dir [-0.02731269 -0.0449276   0.00519761 ... -0.00221666  0.03832545\n",
      "  0.010679  ]\n",
      "Beta [[-0.30630315  0.01933386 -0.01966547 ... -0.0101271   0.01411215\n",
      "   0.0033118 ]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19909434\n",
      "-0.0052576065 -0.00623703\n",
      "0.19909434 0.46583334 ASDASDADASD\n",
      "0.99\n",
      "(1, 4081)\n",
      "Dir [-0.01133762  0.01109737  0.02932701 ...  0.02740344  0.02703836\n",
      "  0.02165434]\n",
      "Beta [[-0.31777351  0.02641443  0.01097169 ... -0.02023591  0.01402608\n",
      "   0.00389905]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19962175\n",
      "-0.0052576065 -0.00623703\n",
      "0.19962175 0.46833333 ASDASDADASD\n",
      "0.9966666666666667\n",
      "(1, 4081)\n",
      "Dir [-0.01670854 -0.02516782  0.00111923 ...  0.01664929  0.01695187\n",
      "  0.04930396]\n",
      "Beta [[-0.15452742 -0.02387635 -0.03508687 ... -0.00220003  0.0063511\n",
      "   0.00578504]]\n",
      "(1201, 1024) (5997,)\n",
      "CHECKKKKK  0.19620302\n",
      "-0.0052576065 -0.00623703\n",
      "0.19620302 0.49875104 ASDASDADASD\n",
      "0.9975020815986678\n",
      "(1, 4081)\n",
      "Dir [ 0.02835336 -0.00352622 -0.00281817 ... -0.03798012 -0.05117366\n",
      "  0.02139806]\n",
      "Beta [[-0.62563903 -0.03677458 -0.05312481 ...  0.02574476  0.02131805\n",
      "   0.00743353]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19786273\n",
      "-0.0052576065 -0.00623703\n",
      "0.19786273 0.5275 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [-0.03313058 -0.04394817  0.07625089 ... -0.00813994 -0.06616709\n",
      " -0.00631788]\n",
      "Beta [[ 0.26731936  0.01329462 -0.00407638 ...  0.00339419  0.01212475\n",
      "   0.00282141]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19705743\n",
      "-0.0052576065 -0.00623703\n",
      "0.19705743 0.56416667 ASDASDADASD\n",
      "0.9933333333333333\n",
      "(1, 4081)\n",
      "Dir [-0.00755533  0.04385818 -0.01777965 ...  0.0159064   0.01014189\n",
      "  0.04044614]\n",
      "Beta [[0.22939822 0.01594152 0.01635699 ... 0.0099436  0.00503329 0.00043653]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.2031487\n",
      "-0.0052576065 -0.00623703\n",
      "0.2031487 0.465 ASDASDADASD\n",
      "0.9958333333333333\n",
      "(1, 4081)\n",
      "Dir [ 0.02221212  0.00021779  0.01579742 ... -0.00147898 -0.04087674\n",
      "  0.00766586]\n",
      "Beta [[ 0.26667012 -0.00834492 -0.0081463  ... -0.01164679  0.00398021\n",
      "   0.0072923 ]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19499257\n",
      "-0.0052576065 -0.00623703\n",
      "0.19499257 0.60083336 ASDASDADASD\n",
      "0.9941666666666666\n",
      "(1, 4081)\n",
      "Dir [ 0.01422252  0.0145564   0.01125891 ... -0.00995903  0.0146182\n",
      "  0.02518615]\n",
      "Beta [[-1.03943338e-01 -3.99159102e-02 -3.16575475e-02 ...  1.33479122e-02\n",
      "  -4.30443908e-03 -7.36545411e-05]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19975345\n",
      "-0.0052576065 -0.00623703\n",
      "0.19975345 0.56333333 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [ 0.02763073 -0.0138209  -0.01898838 ...  0.04554876  0.04404834\n",
      "  0.00482867]\n",
      "Beta [[ 0.09653178 -0.02211042  0.01688055 ...  0.00537951  0.01011429\n",
      "   0.00744174]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19926873\n",
      "-0.0052576065 -0.00623703\n",
      "0.19926873 0.56 ASDASDADASD\n",
      "0.9991666666666666\n",
      "(1, 4081)\n",
      "Dir [ 0.01887661 -0.00855011  0.00010025 ...  0.0003071  -0.00779199\n",
      "  0.03332362]\n",
      "Beta [[-0.50708197 -0.0145317  -0.01086364 ... -0.00302583  0.00873062\n",
      "   0.00393756]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.19939035\n",
      "-0.0052576065 -0.00623703\n",
      "0.19939035 0.51166666 ASDASDADASD\n",
      "0.9958333333333333\n",
      "(1, 4081)\n",
      "Dir [0.02325606 0.00351524 0.00050783 ... 0.06383455 0.03053234 0.04322279]\n",
      "Beta [[-0.01798853 -0.00817929 -0.00583333 ...  0.01146615  0.00586393\n",
      "   0.00171657]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20137054\n",
      "-0.0052576065 -0.00623703\n",
      "0.20137054 0.575 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [-0.0294684  -0.05950361  0.00925005 ...  0.02062023  0.04575946\n",
      " -0.02460216]\n",
      "Beta [[-0.28207162 -0.02782198 -0.00400253 ... -0.00501652 -0.01109525\n",
      "   0.0038967 ]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20163551\n",
      "-0.0052576065 -0.00623703\n",
      "0.20163551 0.58166665 ASDASDADASD\n",
      "0.9975\n",
      "(1, 4081)\n",
      "Dir [-0.02831673  0.00296754  0.02709131 ...  0.01871651  0.03450499\n",
      "  0.0431927 ]\n",
      "Beta [[ 0.23736721  0.00991127  0.01817237 ... -0.01038922 -0.00126302\n",
      "   0.00563924]]\n",
      "(1200, 1024) (5997,)\n",
      "CHECKKKKK  0.20100173\n",
      "-0.0052576065 -0.00623703\n",
      "0.20100173 0.5083333 ASDASDADASD\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m random_dirs \u001b[38;5;241m=\u001b[39m random_dirs \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(random_dirs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m betas, supps \u001b[38;5;241m=\u001b[39m \u001b[43mget_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpca_projs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercentile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[247], line 16\u001b[0m, in \u001b[0;36mget_models\u001b[0;34m(X, Y, pca_projs, dirs, model, percentile)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCHECKKKKK \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(get_rand_feats(X_sub\u001b[38;5;129m@pca_projs\u001b[39m, model)))\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(X), np\u001b[38;5;241m.\u001b[39msum(X\u001b[38;5;129m@pca_projs\u001b[39m))\n\u001b[0;32m---> 16\u001b[0m beta, supp \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_coefs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_rand_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sub\u001b[49m\u001b[38;5;129;43m@pca_projs\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_sub\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDir\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mdir\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, beta)\n",
      "Cell \u001b[0;32mIn[192], line 25\u001b[0m, in \u001b[0;36mlinear_coefs\u001b[0;34m(X, X_ids, Y)\u001b[0m\n\u001b[1;32m     22\u001b[0m support \u001b[38;5;241m=\u001b[39m get_supp(support)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# clf = LogisticRegression(random_state=0).fit(X, Y)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     26\u001b[0m wgts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((clf\u001b[38;5;241m.\u001b[39mintercept_[:,\u001b[38;5;28;01mNone\u001b[39;00m], clf\u001b[38;5;241m.\u001b[39mcoef_))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(wgts\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/base.py:706\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    683\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    816\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    431\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[1;32m    432\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/sklearn/svm/_base.py:452\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m should be equal to \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe number of samples at training time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    448\u001b[0m         )\n\u001b[1;32m    450\u001b[0m svm_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n\u001b[0;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msvm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "\n",
    "N = 512    # ~ 8k\n",
    "# N = 16    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas, supps = get_models(X, Y, pca_projs, random_dirs, model, percentile=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directional sampling\n",
    "# pick direction and find beta\n",
    "# find poor performing points' clusters means say at x+10, x+30 ... % data (x is what was trained on)\n",
    "# add all those cluster mean directions to the random_dir set\n",
    "# repeat \n",
    "# should improve accuracy since poor performing points get classified and also reduce number of betas required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dirs:  1\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  1\n",
      "New dirs:  10\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  11\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9916666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.997504159733777\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Total dirs:  61\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.9941763727121464\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "Total dirs:  111\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9916666666666667\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Total dirs:  161\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9950041631973355\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "Total dirs:  211\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9958368026644463\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  261\n",
      "New dirs:  50\n",
      "Step 0\n",
      "(1201, 1024) (5997,)\n",
      "0.9966694421315571\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9925\n",
      "(1, 2449)\n",
      "Step 25\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9975\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1201, 1024) (5997,)\n",
      "0.9925062447960034\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1202, 1024) (5997,)\n",
      "0.9966722129783694\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9983333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9933333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.995\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9958333333333333\n",
      "(1, 2449)\n",
      "(1200, 1024) (5997,)\n",
      "0.9941666666666666\n",
      "(1, 2449)\n",
      "Done\n",
      "(1200, 1024) (5997,)\n",
      "0.9966666666666667\n",
      "(1, 2449)\n",
      "Total dirs:  311\n"
     ]
    }
   ],
   "source": [
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "nclusters = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "random_dirs = []\n",
    "betas, supps = [], []\n",
    "iters = 100\n",
    "max_rand_dirs = 300\n",
    "new_rand_dirs = np.random.randn(1, d)\n",
    "num_rand_dirs = 0\n",
    "for i in range(iters):\n",
    "    if num_rand_dirs > max_rand_dirs:\n",
    "        break\n",
    "    new_rand_dirs = new_rand_dirs / np.linalg.norm(new_rand_dirs, axis=1, keepdims=True)\n",
    "    new_rand_dirs = new_rand_dirs[np.random.choice(len(new_rand_dirs), size=min(len(new_rand_dirs), 50), replace=False, p=None)]\n",
    "    random_dirs.append(new_rand_dirs)\n",
    "    num_rand_dirs += len(new_rand_dirs)\n",
    "    #print(new_rand_dirs.shape)\n",
    "    print(\"New dirs: \", len(new_rand_dirs))\n",
    "    \n",
    "    _betas, _supps = get_models(X, Y, pca_projs, new_rand_dirs, model, percentile=20)\n",
    "    betas.append(_betas)\n",
    "    supps += _supps\n",
    "    _new_rand_dirs = []\n",
    "    for _dir, _beta in zip(new_rand_dirs, _betas):\n",
    "        for ix, prcnt in enumerate(range(25, 100, 20)):\n",
    "            X_sub, X_ids = project_and_filter(X, _dir, prcnt)\n",
    "            Y_sub = Y[X_ids]\n",
    "            _beta = tf.squeeze(_beta)\n",
    "            _dir = tf.constant(_dir)\n",
    "            prd = softmax(get_rand_feats(X_sub@pca_projs, model), _beta)\n",
    "\n",
    "            incrct = np.where(prd != Y_sub)\n",
    "            # print(len(prd), len(Y_sub), get_rand_feats(X_sub@pca_projs, model).shape, _beta.shape, prd)\n",
    "            incrct_X = get_rand_feats(X_sub[incrct]@pca_projs, model)\n",
    "            if len(incrct_X) < 20:\n",
    "                continue\n",
    "            kmeans = KMeans(n_clusters=nclusters[ix], random_state=0, n_init=\"auto\").fit(incrct_X)\n",
    "            mp = kmeans.cluster_centers_\n",
    "            neigh = NearestNeighbors(n_neighbors=5)\n",
    "            neigh.fit(incrct_X)\n",
    "\n",
    "            n_ind = neigh.kneighbors(mp, 3, return_distance=False)\n",
    "            nngr_id = [_n_ind[np.random.choice(3, 1)[0]] for _n_ind in n_ind]\n",
    "\n",
    "            _new_rand_dirs.append(X_sub[incrct][nngr_id] + np.random.randn(*X_sub[incrct][nngr_id].shape)*0.1)\n",
    "    new_rand_dirs = np.concatenate(_new_rand_dirs, axis=0)\n",
    "    print(\"Total dirs: \", num_rand_dirs)\n",
    "\n",
    "betas = np.concatenate(betas, axis=0)\n",
    "random_dirs = np.concatenate(random_dirs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5697, 1024)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=-0.1706338411346348>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_simi(a, b):\n",
    "    return tf.losses.CosineSimilarity()(a, b)    \n",
    "\n",
    "check_simi(betas[2], betas[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "# np.save('random_dirs-chem2.npy', random_dirs)\n",
    "# np.save('betas-chem2.npy', betas)\n",
    "# np.save('Ws-chem2.npy', model.Ws)\n",
    "\n",
    "np.save('chem-random_dirs-svm-best1.npy', random_dirs)\n",
    "np.save('chem-betas-svm-best1.npy', betas)\n",
    "np.save('chem-Ws-svm-best1.npy', model.Ws)\n",
    "\n",
    "np.save('chem-pca_projs_data-svm-best1.npy', pca_projs)\n",
    "np.save('chem-pca_projs_dir-svm-best1.npy', np.identity(X.shape[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"chem-supps-svm.npy\", \"wb\") as fp:\n",
    "#     pickle.dump(supps, fp)\n",
    "\n",
    "with open(\"chem-supps-svm-best.npy\", \"rb\") as fp:\n",
    "    supps_b = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-chem2.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./betas-chem2.npy')))\n",
    "model = define_rand_feats(X_prjs, 2)\n",
    "model.Ws = tf.constant(np.load('./Ws-chem2.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "CnxEkcWwWlwn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "random_dirs = tf.constant(np.load('./chem-random_dirs-svm.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('./chem-betas-svm.npy')))\n",
    "# model = define_rand_feats(X, 1.5)\n",
    "model = define_rand_feats(X, 2)\n",
    "model.Ws = tf.constant(np.load('./chem-Ws-svm.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "BCHvO4qeupNQ"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[43mrandom_dirs1\u001b[49m, random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.allclose(random_dirs1, random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_dirs1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mrandom_dirs1\u001b[49m \u001b[38;5;241m-\u001b[39m random_dirs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_dirs1' is not defined"
     ]
    }
   ],
   "source": [
    "np.sum(random_dirs1 - random_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "264b9e19-74f8-4d13-ee50-bb4796f0d4ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 4081)\n",
      "(512, 1024)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "15119544-cde9-4462-b565-7deb6dd61daf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min var:  2.082857549308727e-06 Max var:  0.09755260247485288 Mean var:  tf.Tensor(0.00026783628375975856, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# var = tf.math.reduce_variance(betas, axis=0)\n",
    "# mean_var = tf.reduce_mean(var)\n",
    "# print(var)\n",
    "\n",
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(\"Min var: \", np.min(var), \"Max var: \", np.max(var), \"Mean var: \", mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "b44e65f3-b44b-40fd-e564-63e35d43997c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.932"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 1\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "def softmax_prob(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) ) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 25)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd1 = prd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0NZNorQW0vg",
    "outputId": "afcb4822-0cda-4c13-ea68-8454473bb529"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5446808510638298"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[10], 25)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "perds = np.zeros((ex_X.shape[0],))\n",
    "cnt = np.zeros((ex_X.shape[0],))\n",
    "print(perds.shape)\n",
    "for sm in range(512):\n",
    "    X_sub, X_ids = project_and_filter(ex_X, random_dirs[sm], 15)\n",
    "    Y_sub = ex_Y[X_ids]\n",
    "    prd = softmax_prob(get_rand_feats(X_sub@pca_projs, model), betas[sm])\n",
    "    # print(prd.shape, X_ids, ex_X.shape)\n",
    "    perds[X_ids] += prd\n",
    "    cnt[X_ids] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.501992552056132"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zer_ids = np.argwhere(cnt!=0)\n",
    "veX = perds[zer_ids]\n",
    "veX /= cnt[zer_ids]\n",
    "((veX>0.5)==Y_sub).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print((np.mean(tf.stack(perds, axis=0), axis=0)==Y_sub).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m supps_0, supps_1 \u001b[38;5;241m=\u001b[39m supps_b[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m supps_0\u001b[38;5;241m.\u001b[39mshape, supps_1\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "supps_0, supps_1 = supps_b[sample]\n",
    "supps_0.shape, supps_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026,  0.7023512 , -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       ...,\n",
       "       [-0.04052026, -0.29764882, -0.08170752, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882,  0.91829246, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901],\n",
       "       [-0.04052026, -0.29764882,  0.91829246, ..., -0.04052026,\n",
       "        -0.06053026, -0.03134901]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppvs_0 = X[supps_0.reshape((-1,))]\n",
    "suppvs_1 = X[supps_1.reshape((-1,))]\n",
    "y_supp_0 = Y[supps_0.reshape((-1,))]\n",
    "y_supp_1 = Y[supps_1.reshape((-1,))]\n",
    "suppvs_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 1024)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suppvs_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.99999557 -0.99999864 -1.00000015 -1.00000274 -0.99999844 -1.00000071\n",
      " -1.00000048 -0.99999712 -0.99999474 -1.00000096 -0.99999895 -1.0000031\n",
      " -1.00000271 -0.99999792 -0.99999714 -0.99999876 -0.99999873 -0.99999996\n",
      " -1.00000344 -0.99999897 -0.9999988  -1.00000246 -0.99999731 -0.99999866\n",
      " -0.99999845 -0.99999905 -1.00000161 -0.9999993  -0.99999992 -1.00000107\n",
      " -1.0000001  -0.99999536 -0.99999485 -0.99999945 -1.00000243 -0.99999581\n",
      " -0.99999901 -1.00000531 -1.         -0.9999974  -0.99999841 -0.99999726\n",
      " -1.0000001  -1.00000406 -1.00000235 -1.00000085 -1.00000044 -1.00000429\n",
      " -1.00000434 -1.00000024 -1.00000118 -0.999996   -0.99999972 -0.9999999\n",
      " -1.00000058 -0.99999397 -0.99999636 -1.00000286 -1.00000797 -1.00000088\n",
      " -1.00000028 -1.00000142 -1.0000043  -0.99999972 -0.99999621 -1.00000086\n",
      " -1.00000302 -1.00000322 -1.00000138 -1.00000425 -0.99999989 -1.0000025\n",
      " -1.00000054 -0.99999991 -0.9999996  -0.99999743 -0.99999847 -1.00000061\n",
      " -1.00000114 -1.00000165 -1.00000441 -0.99999933 -0.99999731 -0.99999625\n",
      " -1.0000037  -1.00000287 -0.99999944 -0.99999899 -1.0000044  -1.0000004\n",
      " -0.99999581 -1.00000248 -1.0000064  -0.99999798 -0.99999999 -0.99999881\n",
      " -0.99999808 -1.00000072 -1.00000203 -0.99999953 -0.99999995 -0.99999593\n",
      " -1.00000357 -1.00000529 -0.99999768 -1.00000481 -1.00000473 -0.99999755\n",
      " -0.99999789  0.89253063 -0.99999876 -1.00000252 -0.99999586 -1.00000442\n",
      " -0.99999629 -1.00000115 -0.9999997  -0.99999353 -1.00000014 -0.99999954\n",
      " -1.00000217 -0.9999988  -0.99999832 -0.99999735 -1.00000248 -0.99999779\n",
      " -1.00000045 -0.99999884 -1.00000063 -0.99999683 -0.9999978  -0.99999795\n",
      " -0.99999801 -1.00000365 -0.99999959 -0.99999692 -0.99999894 -1.00000214\n",
      " -0.99999843 -0.99999089 -0.99999655 -0.9999992  -1.00000184 -0.99999908\n",
      " -1.00000461 -0.99999957 -1.00000653 -1.00000206 -1.00000463 -0.99999801\n",
      " -0.99999928 -1.00000223 -0.9999973  -0.99999701 -0.9999971  -0.99999629\n",
      " -1.00000057 -1.00000184 -1.00000487 -0.99999784 -1.00000173 -1.00000592\n",
      " -1.00000484 -0.99999972 -0.9999973  -0.99999867 -0.99999415 -0.99999746\n",
      " -0.99999313 -1.0000024  -0.99999341 -0.99999853 -0.99999876 -0.99999852\n",
      " -1.00000494 -0.99999825 -1.00000011 -0.99999596 -0.9999956  -1.00000199\n",
      " -0.99999946 -0.99999383 -1.0000001  -0.99999952 -0.9999991  -1.00000252\n",
      " -0.999998   -1.00000384 -1.00000118 -0.99999517 -0.99999922 -1.0000009\n",
      " -0.99999826 -0.99999685 -0.99999451 -1.00000002 -0.99999946 -0.99999747\n",
      " -0.99999298 -0.99999739 -1.00000293 -1.00000114 -1.00000154 -0.99999784\n",
      " -0.99999468 -0.99999999 -0.99999775 -1.00000061 -1.00000053 -1.00000053\n",
      " -0.99999498 -0.99999889 -1.00000104 -0.99999952 -0.99999912 -1.0000007\n",
      " -1.00000199 -0.99999876 -1.00000152 -1.0000017  -0.99999833 -1.0000039\n",
      " -0.99999812 -1.00000013 -1.00000347 -0.99999859 -0.99999663 -0.99999868\n",
      " -1.0000014  -1.00000149 -1.00000252 -0.99999527 -0.99999682 -1.00000403\n",
      " -1.00000005 -0.9999948  -0.99999384 -0.99999698 -1.00000233 -0.99999743\n",
      " -0.99999908 -1.00000696 -0.99999999 -0.99999722 -0.9999993  -1.00000078\n",
      " -1.00000247 -0.99999992 -0.99999872  0.36856059 -0.99999907 -0.9999998\n",
      " -0.99999839 -0.99999575 -0.99999872 -1.00000171 -0.99999992 -0.99999876\n",
      " -1.00000106 -0.99999893 -0.99999619 -1.00000254 -0.17124262 -0.99999992\n",
      " -0.99999897 -1.00000023 -1.00000021 -0.99999993 -1.00000322 -0.99999314\n",
      " -1.00000368 -1.00000043 -0.99999718 -1.00000063 -0.99999859 -0.99999798\n",
      " -0.99999768 -0.9999984  -1.00000031 -0.99999497 -1.00000271 -1.\n",
      " -0.9999989  -1.00000092 -1.00000034 -0.99999778 -0.99999774 -0.99999749\n",
      " -1.00000044 -0.99999659 -0.99999723 -1.00000046 -1.00000401 -1.00000262\n",
      " -0.99999467 -0.99999473 -1.00000239 -0.99999617 -0.99999812 -1.00000387\n",
      " -1.00000201 -1.00000031 -0.99999892 -0.99999793 -1.00000155 -0.99999784\n",
      " -1.00000221 -0.99999727 -1.00000276 -1.00000142 -0.9999993  -0.9707472\n",
      " -1.00000239 -1.00000272 -0.9999995  -1.00000308 -0.99999798 -1.00000052\n",
      " -1.00000039 -0.99999548 -0.9999959  -1.00000357 -1.00000262 -0.9999942\n",
      " -0.99999939 -0.99999865 -0.99999932 -1.00000143 -0.99999898 -0.99999929\n",
      " -1.00000118 -1.00000508 -0.99999844 -0.99999526 -0.9999998  -0.9999979\n",
      " -0.99999888 -1.00000198 -1.00000279 -1.00000012 -1.00000345 -1.00000023\n",
      " -0.9999979  -1.00000409 -1.00000257 -0.99999924 -0.99999843 -0.99999737\n",
      " -0.99999582 -0.99999584 -0.99999923 -0.99999941 -1.00000319 -0.99999846\n",
      " -1.00000199 -0.99999927 -0.99999965 -1.00000136 -1.00000332 -0.99999471\n",
      " -1.00000336 -0.99999931 -1.00000327 -1.00000506 -0.15631103 -1.00000319\n",
      " -1.00000222 -0.99999952 -0.99999703 -0.99999816 -0.99999448 -0.99999711\n",
      " -1.0000037  -0.99999945  0.47949874 -1.000002   -1.00000096 -1.00000417\n",
      " -1.00000333 -0.99999964 -1.0000007 ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "tf.Tensor(0.013705432773427331, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def get_margin(X, wgts):\n",
    "  sd = (np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T) * 1.0\n",
    "  return sd[:]\n",
    "\n",
    "def loss_2(labels, margins):\n",
    "    l = 2*labels - 1\n",
    "    return tf.reduce_sum(tf.reduce_mean(tf.nn.relu(-margins * l), axis=0))\n",
    "\n",
    "get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model).shape, betas[sample].shape\n",
    "print(get_margin(get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model), betas[sample]), y_supp_0)\n",
    "# loss_2 = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "print(loss_2(y_supp_0[:len(suppvs_0)], get_margin(get_rand_feats(tf.cast(suppvs_0@pca_projs, dtype=tf.float32), model), betas[sample]*3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 64\n",
    "\n",
    "# Encoder\n",
    "# beta_input = layers.Input(shape=(beta_dim,))\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([dir_input])\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(latent_dim, activation=tf.nn.elu)(x)\n",
    "# z_mean = layers.Dense(latent_dim)(x)\n",
    "# z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "# z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "z = layers.Dense(latent_dim)(x)\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "# decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs])\n",
    "x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([dir_input], z, name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([dir_input])])\n",
    "vae = models.Model([dir_input], outputs, name=\"autoenc\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin_batched(supp_vs, wgts):\n",
    "  sd = (tf.concat([tf.ones((*supp_vs.shape[:-1], 1)), supp_vs], axis=-1) @ wgts[:, :, None]) * 1.0\n",
    "  return sd[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ots = vae((betas[:1], random_dirs[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "#def vae_loss(inputs, outputs, z_mean, z_log_var, reg=0.002):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "#  intercp_loss = tf.reduce_mean(tf.abs(tf.cast(inputs[:, :1], dtype=tf.float32) - outputs[:, :1]))\n",
    "#  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs[:, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs[:, 1:], axis=-1)[0], axis=-1))\n",
    "#  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "#  total_loss = recon_loss + intercp_loss + reg * kl_loss\n",
    "#  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(inputs, outputs, supp_vs, supp_ys, reg=1.0, regr=1.0):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs, dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs, axis=-1)[0], axis=-1))\n",
    "  # print(supp_vs.shape, outputs.shape)\n",
    "  supp_margins = get_margin_batched(supp_vs, outputs)\n",
    "  supp_loss = loss_2(supp_ys, supp_margins)\n",
    "  # print(supp_ys[0][30:], supp_margins[0][30:])\n",
    "  # print(supp_margins.shape, supp_ys.shape)\n",
    "  # kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = reg * recon_loss + regr * supp_loss\n",
    "  return total_loss, reg*recon_loss, regr*supp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.1964941>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=1.0429045>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.09860455>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=27.492508>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_loss(betas[:1], ots[:1], tf.ones((1, 32)), tf.ones((1, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def train_step(model, inputs, dir_inputs, supp_vs, supp_ys, alpha, ralpha=1.0):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z = model.encoder([dir_inputs])\n",
    "    outputs = model.decoder([z])\n",
    "    total_loss, recon_loss, supp_loss = vae_loss(inputs, outputs, supp_vs, supp_ys, alpha, ralpha)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, supp_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 19:41:59.480502: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1576b9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-29 19:41:59.480542: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-04-29 19:41:59.486897: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-29 19:41:59.594610: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-29 19:41:59.674717: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fb840184ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fb840184ca0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0526553>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.999847>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.051767856>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.52023363>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(vae, betas[:32], random_dirs[:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(381, 204), dtype=float32, numpy=\n",
       "array([[-9.73291   , -0.08359981,  4.8285213 , ..., -0.418212  ,\n",
       "         0.04979858, -0.38527712],\n",
       "       [-1.4835713 ,  1.0568206 ,  2.5018466 , ..., -0.06100272,\n",
       "         0.1693849 , -0.38147366],\n",
       "       [-7.0123835 , -9.252638  , -1.9807861 , ...,  0.136215  ,\n",
       "         0.12370304,  0.3105353 ],\n",
       "       ...,\n",
       "       [-4.699366  ,  2.6286366 , -1.7926993 , ..., -0.3052967 ,\n",
       "         0.18089612, -0.36616707],\n",
       "       [-1.7197244 ,  1.9622226 ,  2.9822083 , ..., -0.33606544,\n",
       "        -0.01879999,  0.30114657],\n",
       "       [-1.7413275 ,  1.9585967 ,  3.080233  , ..., -0.47907415,\n",
       "         0.07559836,  0.45271814]], dtype=float32)>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[supps_0.reshape((-1,))] @ pca_projs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(X, betas, dirs, supps, batch_size, support_batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  supps_ = [supps[i] for i in indices]\n",
    "  supps_ = np.array([np.stack([i[np.random.choice(len(i), support_batch_size)],\n",
    "                     j[np.random.choice(len(j), support_batch_size)]], axis=0) for i, j in supps_])\n",
    "  def supp_get_vs(_supps):\n",
    "      supps_0, supps_1 = np.transpose(_supps, (1, 0, 2, 3))\n",
    "      suppvs_0 = X[supps_0.reshape((-1,))] @ pca_projs\n",
    "      suppvs_1 = X[supps_1.reshape((-1,))] @ pca_projs\n",
    "      suppvs_0 = tf.reshape(get_rand_feats(tf.cast(suppvs_0, dtype=tf.float32), model), (batch_size, support_batch_size, -1))\n",
    "      suppvs_1 = tf.reshape(get_rand_feats(tf.cast(suppvs_1, dtype=tf.float32), model), (batch_size, support_batch_size, -1))\n",
    "      y_supp_0 = Y[supps_0.reshape((-1,))].reshape((batch_size, support_batch_size, -1))\n",
    "      y_supp_1 = Y[supps_1.reshape((-1,))].reshape((batch_size, support_batch_size, -1))\n",
    "      return np.concatenate([suppvs_0, suppvs_1], axis=1), np.concatenate([y_supp_0, y_supp_1], axis=1)\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield betas[i:i+batch_size], dirs[i:i+batch_size], *supp_get_vs(supps_[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object batch at 0x7fb77c7be890>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch(betas, random_dirs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "a6e1ccc1-2cd0-4549-e61a-6dae8aa1062a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Step 0: loss = 1.0042555332183838, recon loss = 1.0042555332183838, supp loss = 0.0\n",
      "Step 0: loss = 0.6852933168411255, recon loss = 0.6852933168411255, supp loss = 0.0\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.5890307426452637, recon loss = 0.5890307426452637, supp loss = 0.0\n",
      "Step 0: loss = 0.5855118036270142, recon loss = 0.5855118036270142, supp loss = 0.0\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.5907216668128967, recon loss = 0.5907216668128967, supp loss = 0.0\n",
      "Step 0: loss = 0.5872777700424194, recon loss = 0.5872777700424194, supp loss = 0.0\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.5776413083076477, recon loss = 0.5776413083076477, supp loss = 0.0\n",
      "Step 0: loss = 0.5796323418617249, recon loss = 0.5796323418617249, supp loss = 0.0\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.5833013653755188, recon loss = 0.5833013653755188, supp loss = 0.0\n",
      "Step 0: loss = 0.5857218503952026, recon loss = 0.5857218503952026, supp loss = 0.0\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.578827440738678, recon loss = 0.578827440738678, supp loss = 0.0\n",
      "Step 0: loss = 0.5807657241821289, recon loss = 0.5807657241821289, supp loss = 0.0\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.5597935318946838, recon loss = 0.5597935318946838, supp loss = 0.0\n",
      "Step 0: loss = 0.5642254948616028, recon loss = 0.5642254948616028, supp loss = 0.0\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.5692940950393677, recon loss = 0.5692940950393677, supp loss = 0.0\n",
      "Step 0: loss = 0.5721639394760132, recon loss = 0.5721639394760132, supp loss = 0.0\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.5468140244483948, recon loss = 0.5468140244483948, supp loss = 0.0\n",
      "Step 0: loss = 0.5705466270446777, recon loss = 0.5705466270446777, supp loss = 0.0\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.5546731352806091, recon loss = 0.5546731352806091, supp loss = 0.0\n",
      "Step 0: loss = 0.5715173482894897, recon loss = 0.5715173482894897, supp loss = 0.0\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.5451689958572388, recon loss = 0.5451689958572388, supp loss = 0.0\n",
      "Step 0: loss = 0.5563899278640747, recon loss = 0.5563899278640747, supp loss = 0.0\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.5485641956329346, recon loss = 0.5485641956329346, supp loss = 0.0\n",
      "Step 0: loss = 0.5772020816802979, recon loss = 0.5772020816802979, supp loss = 0.0\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.5570698380470276, recon loss = 0.5570698380470276, supp loss = 0.0\n",
      "Step 0: loss = 0.5731750726699829, recon loss = 0.5731750726699829, supp loss = 0.0\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.5487866401672363, recon loss = 0.5487866401672363, supp loss = 0.0\n",
      "Step 0: loss = 0.5671803951263428, recon loss = 0.5671803951263428, supp loss = 0.0\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.5453540086746216, recon loss = 0.5453540086746216, supp loss = 0.0\n",
      "Step 0: loss = 0.5705669522285461, recon loss = 0.5705669522285461, supp loss = 0.0\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.5264188051223755, recon loss = 0.5264188051223755, supp loss = 0.0\n",
      "Step 0: loss = 0.557187557220459, recon loss = 0.557187557220459, supp loss = 0.0\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.5639516711235046, recon loss = 0.5639516711235046, supp loss = 0.0\n",
      "Step 0: loss = 0.5667285919189453, recon loss = 0.5667285919189453, supp loss = 0.0\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.5454390048980713, recon loss = 0.5454390048980713, supp loss = 0.0\n",
      "Step 0: loss = 0.5714215636253357, recon loss = 0.5714215636253357, supp loss = 0.0\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.5128263235092163, recon loss = 0.5128263235092163, supp loss = 0.0\n",
      "Step 0: loss = 0.5413665771484375, recon loss = 0.5413665771484375, supp loss = 0.0\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.517399787902832, recon loss = 0.517399787902832, supp loss = 0.0\n",
      "Step 0: loss = 0.5611250400543213, recon loss = 0.5611250400543213, supp loss = 0.0\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.521978497505188, recon loss = 0.521978497505188, supp loss = 0.0\n",
      "Step 0: loss = 0.5696579217910767, recon loss = 0.5696579217910767, supp loss = 0.0\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.5147208571434021, recon loss = 0.5147208571434021, supp loss = 0.0\n",
      "Step 0: loss = 0.5676119327545166, recon loss = 0.5676119327545166, supp loss = 0.0\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.5065177083015442, recon loss = 0.5065177083015442, supp loss = 0.0\n",
      "Step 0: loss = 0.5380655527114868, recon loss = 0.5380655527114868, supp loss = 0.0\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.4950907230377197, recon loss = 0.4950907230377197, supp loss = 0.0\n",
      "Step 0: loss = 0.5465813279151917, recon loss = 0.5465813279151917, supp loss = 0.0\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.5096533298492432, recon loss = 0.5096533298492432, supp loss = 0.0\n",
      "Step 0: loss = 0.5566259622573853, recon loss = 0.5566259622573853, supp loss = 0.0\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.5155072212219238, recon loss = 0.5155072212219238, supp loss = 0.0\n",
      "Step 0: loss = 0.5622957944869995, recon loss = 0.5622957944869995, supp loss = 0.0\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.48227232694625854, recon loss = 0.48227232694625854, supp loss = 0.0\n",
      "Step 0: loss = 0.5682346820831299, recon loss = 0.5682346820831299, supp loss = 0.0\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.48908868432044983, recon loss = 0.48908868432044983, supp loss = 0.0\n",
      "Step 0: loss = 0.5502667427062988, recon loss = 0.5502667427062988, supp loss = 0.0\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.5043078064918518, recon loss = 0.5043078064918518, supp loss = 0.0\n",
      "Step 0: loss = 0.5549970269203186, recon loss = 0.5549970269203186, supp loss = 0.0\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.521740198135376, recon loss = 0.521740198135376, supp loss = 0.0\n",
      "Step 0: loss = 0.5657455325126648, recon loss = 0.5657455325126648, supp loss = 0.0\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.4818391799926758, recon loss = 0.4818391799926758, supp loss = 0.0\n",
      "Step 0: loss = 0.5580413341522217, recon loss = 0.5580413341522217, supp loss = 0.0\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.48943209648132324, recon loss = 0.48943209648132324, supp loss = 0.0\n",
      "Step 0: loss = 0.5493830442428589, recon loss = 0.5493830442428589, supp loss = 0.0\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.4818977117538452, recon loss = 0.4818977117538452, supp loss = 0.0\n",
      "Step 0: loss = 0.5493513345718384, recon loss = 0.5493513345718384, supp loss = 0.0\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.49236005544662476, recon loss = 0.49236005544662476, supp loss = 0.0\n",
      "Step 0: loss = 0.5498393774032593, recon loss = 0.5498393774032593, supp loss = 0.0\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.4638493061065674, recon loss = 0.4638493061065674, supp loss = 0.0\n",
      "Step 0: loss = 0.567531943321228, recon loss = 0.567531943321228, supp loss = 0.0\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.48951321840286255, recon loss = 0.48951321840286255, supp loss = 0.0\n",
      "Step 0: loss = 0.5650511980056763, recon loss = 0.5650511980056763, supp loss = 0.0\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.46689867973327637, recon loss = 0.46689867973327637, supp loss = 0.0\n",
      "Step 0: loss = 0.5684894323348999, recon loss = 0.5684894323348999, supp loss = 0.0\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.47684648633003235, recon loss = 0.47684648633003235, supp loss = 0.0\n",
      "Step 0: loss = 0.5434831976890564, recon loss = 0.5434831976890564, supp loss = 0.0\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.45619937777519226, recon loss = 0.45619937777519226, supp loss = 0.0\n",
      "Step 0: loss = 0.5503052473068237, recon loss = 0.5503052473068237, supp loss = 0.0\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.47440552711486816, recon loss = 0.47440552711486816, supp loss = 0.0\n",
      "Step 0: loss = 0.5584657192230225, recon loss = 0.5584657192230225, supp loss = 0.0\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.4625561237335205, recon loss = 0.4625561237335205, supp loss = 0.0\n",
      "Step 0: loss = 0.5683376789093018, recon loss = 0.5683376789093018, supp loss = 0.0\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.4897136688232422, recon loss = 0.4897136688232422, supp loss = 0.0\n",
      "Step 0: loss = 0.5670008063316345, recon loss = 0.5670008063316345, supp loss = 0.0\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.47720199823379517, recon loss = 0.47720199823379517, supp loss = 0.0\n",
      "Step 0: loss = 0.5529204607009888, recon loss = 0.5529204607009888, supp loss = 0.0\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.4630039930343628, recon loss = 0.4630039930343628, supp loss = 0.0\n",
      "Step 0: loss = 0.5483227968215942, recon loss = 0.5483227968215942, supp loss = 0.0\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.47808122634887695, recon loss = 0.47808122634887695, supp loss = 0.0\n",
      "Step 0: loss = 0.5663935542106628, recon loss = 0.5663935542106628, supp loss = 0.0\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.45988166332244873, recon loss = 0.45988166332244873, supp loss = 0.0\n",
      "Step 0: loss = 0.566714882850647, recon loss = 0.566714882850647, supp loss = 0.0\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.4706369638442993, recon loss = 0.4706369638442993, supp loss = 0.0\n",
      "Step 0: loss = 0.5406227111816406, recon loss = 0.5406227111816406, supp loss = 0.0\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.45513948798179626, recon loss = 0.45513948798179626, supp loss = 0.0\n",
      "Step 0: loss = 0.5411928296089172, recon loss = 0.5411928296089172, supp loss = 0.0\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.46712544560432434, recon loss = 0.46712544560432434, supp loss = 0.0\n",
      "Step 0: loss = 0.5493159294128418, recon loss = 0.5493159294128418, supp loss = 0.0\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.46600422263145447, recon loss = 0.46600422263145447, supp loss = 0.0\n",
      "Step 0: loss = 0.5580888986587524, recon loss = 0.5580888986587524, supp loss = 0.0\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.4446013271808624, recon loss = 0.4446013271808624, supp loss = 0.0\n",
      "Step 0: loss = 0.5464833974838257, recon loss = 0.5464833974838257, supp loss = 0.0\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.44230860471725464, recon loss = 0.44230860471725464, supp loss = 0.0\n",
      "Step 0: loss = 0.5373497605323792, recon loss = 0.5373497605323792, supp loss = 0.0\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.4466401934623718, recon loss = 0.4466401934623718, supp loss = 0.0\n",
      "Step 0: loss = 0.5521135330200195, recon loss = 0.5521135330200195, supp loss = 0.0\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.47584784030914307, recon loss = 0.47584784030914307, supp loss = 0.0\n",
      "Step 0: loss = 0.5655933618545532, recon loss = 0.5655933618545532, supp loss = 0.0\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.46929383277893066, recon loss = 0.46929383277893066, supp loss = 0.0\n",
      "Step 0: loss = 0.5444082021713257, recon loss = 0.5444082021713257, supp loss = 0.0\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.4622483551502228, recon loss = 0.4622483551502228, supp loss = 0.0\n",
      "Step 0: loss = 0.5622776746749878, recon loss = 0.5622776746749878, supp loss = 0.0\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.45162200927734375, recon loss = 0.45162200927734375, supp loss = 0.0\n",
      "Step 0: loss = 0.5461131930351257, recon loss = 0.5461131930351257, supp loss = 0.0\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.46431678533554077, recon loss = 0.46431678533554077, supp loss = 0.0\n",
      "Step 0: loss = 0.5579123497009277, recon loss = 0.5579123497009277, supp loss = 0.0\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.4446917772293091, recon loss = 0.4446917772293091, supp loss = 0.0\n",
      "Step 0: loss = 0.5347268581390381, recon loss = 0.5347268581390381, supp loss = 0.0\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.4457235634326935, recon loss = 0.4457235634326935, supp loss = 0.0\n",
      "Step 0: loss = 0.5408508777618408, recon loss = 0.5408508777618408, supp loss = 0.0\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.46020305156707764, recon loss = 0.46020305156707764, supp loss = 0.0\n",
      "Step 0: loss = 0.548920750617981, recon loss = 0.548920750617981, supp loss = 0.0\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.4542720317840576, recon loss = 0.4542720317840576, supp loss = 0.0\n",
      "Step 0: loss = 0.560981810092926, recon loss = 0.560981810092926, supp loss = 0.0\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.4310118556022644, recon loss = 0.4310118556022644, supp loss = 0.0\n",
      "Step 0: loss = 0.5432827472686768, recon loss = 0.5432827472686768, supp loss = 0.0\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.43623900413513184, recon loss = 0.43623900413513184, supp loss = 0.0\n",
      "Step 0: loss = 0.5384852886199951, recon loss = 0.5384852886199951, supp loss = 0.0\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.4457632601261139, recon loss = 0.4457632601261139, supp loss = 0.0\n",
      "Step 0: loss = 0.5330519080162048, recon loss = 0.5330519080162048, supp loss = 0.0\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.4425084888935089, recon loss = 0.4425084888935089, supp loss = 0.0\n",
      "Step 0: loss = 0.5482263565063477, recon loss = 0.5482263565063477, supp loss = 0.0\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.4707346558570862, recon loss = 0.4707346558570862, supp loss = 0.0\n",
      "Step 0: loss = 0.5734330415725708, recon loss = 0.5734330415725708, supp loss = 0.0\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.47442054748535156, recon loss = 0.47442054748535156, supp loss = 0.0\n",
      "Step 0: loss = 0.558100700378418, recon loss = 0.558100700378418, supp loss = 0.0\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.4619362950325012, recon loss = 0.4619362950325012, supp loss = 0.0\n",
      "Step 0: loss = 0.5564182996749878, recon loss = 0.5564182996749878, supp loss = 0.0\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.44840800762176514, recon loss = 0.44840800762176514, supp loss = 0.0\n",
      "Step 0: loss = 0.5610336065292358, recon loss = 0.5610336065292358, supp loss = 0.0\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.42382922768592834, recon loss = 0.42382922768592834, supp loss = 0.0\n",
      "Step 0: loss = 0.5214217901229858, recon loss = 0.5214217901229858, supp loss = 0.0\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.4437015652656555, recon loss = 0.4437015652656555, supp loss = 0.0\n",
      "Step 0: loss = 0.5528862476348877, recon loss = 0.5528862476348877, supp loss = 0.0\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.4311370849609375, recon loss = 0.4311370849609375, supp loss = 0.0\n",
      "Step 0: loss = 0.5493203401565552, recon loss = 0.5493203401565552, supp loss = 0.0\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.44969213008880615, recon loss = 0.44969213008880615, supp loss = 0.0\n",
      "Step 0: loss = 0.5668150186538696, recon loss = 0.5668150186538696, supp loss = 0.0\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.4622461199760437, recon loss = 0.4622461199760437, supp loss = 0.0\n",
      "Step 0: loss = 0.5587430596351624, recon loss = 0.5587430596351624, supp loss = 0.0\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.4483047127723694, recon loss = 0.4483047127723694, supp loss = 0.0\n",
      "Step 0: loss = 0.5616792440414429, recon loss = 0.5616792440414429, supp loss = 0.0\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.42373940348625183, recon loss = 0.42373940348625183, supp loss = 0.0\n",
      "Step 0: loss = 0.559605598449707, recon loss = 0.559605598449707, supp loss = 0.0\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.437059223651886, recon loss = 0.437059223651886, supp loss = 0.0\n",
      "Step 0: loss = 0.5714961290359497, recon loss = 0.5714961290359497, supp loss = 0.0\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.43524158000946045, recon loss = 0.43524158000946045, supp loss = 0.0\n",
      "Step 0: loss = 0.5425041913986206, recon loss = 0.5425041913986206, supp loss = 0.0\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.44529491662979126, recon loss = 0.44529491662979126, supp loss = 0.0\n",
      "Step 0: loss = 0.5788650512695312, recon loss = 0.5788650512695312, supp loss = 0.0\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.445456862449646, recon loss = 0.445456862449646, supp loss = 0.0\n",
      "Step 0: loss = 0.5644669532775879, recon loss = 0.5644669532775879, supp loss = 0.0\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.4512329697608948, recon loss = 0.4512329697608948, supp loss = 0.0\n",
      "Step 0: loss = 0.5237018465995789, recon loss = 0.5237018465995789, supp loss = 0.0\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.3954017758369446, recon loss = 0.3954017758369446, supp loss = 0.0\n",
      "Step 0: loss = 0.5588990449905396, recon loss = 0.5588990449905396, supp loss = 0.0\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.42990320920944214, recon loss = 0.42990320920944214, supp loss = 0.0\n",
      "Step 0: loss = 0.5325595736503601, recon loss = 0.5325595736503601, supp loss = 0.0\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.4180629849433899, recon loss = 0.4180629849433899, supp loss = 0.0\n",
      "Step 0: loss = 0.5420719385147095, recon loss = 0.5420719385147095, supp loss = 0.0\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.4368266761302948, recon loss = 0.4368266761302948, supp loss = 0.0\n",
      "Step 0: loss = 0.5459205508232117, recon loss = 0.5459205508232117, supp loss = 0.0\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.42999595403671265, recon loss = 0.42999595403671265, supp loss = 0.0\n",
      "Step 0: loss = 0.5386070609092712, recon loss = 0.5386070609092712, supp loss = 0.0\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.4276208281517029, recon loss = 0.4276208281517029, supp loss = 0.0\n",
      "Step 0: loss = 0.5147741436958313, recon loss = 0.5147741436958313, supp loss = 0.0\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.4473734498023987, recon loss = 0.4473734498023987, supp loss = 0.0\n",
      "Step 0: loss = 0.5451198816299438, recon loss = 0.5451198816299438, supp loss = 0.0\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.4064348340034485, recon loss = 0.4064348340034485, supp loss = 0.0\n",
      "Step 0: loss = 0.5701874494552612, recon loss = 0.5701874494552612, supp loss = 0.0\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.4211263656616211, recon loss = 0.4211263656616211, supp loss = 0.0\n",
      "Step 0: loss = 0.5361911654472351, recon loss = 0.5361911654472351, supp loss = 0.0\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.43628165125846863, recon loss = 0.43628165125846863, supp loss = 0.0\n",
      "Step 0: loss = 0.5357764959335327, recon loss = 0.5357764959335327, supp loss = 0.0\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.40461087226867676, recon loss = 0.40461087226867676, supp loss = 0.0\n",
      "Step 0: loss = 0.5469390153884888, recon loss = 0.5469390153884888, supp loss = 0.0\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.42312514781951904, recon loss = 0.42312514781951904, supp loss = 0.0\n",
      "Step 0: loss = 0.5571380853652954, recon loss = 0.5571380853652954, supp loss = 0.0\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.39402514696121216, recon loss = 0.39402514696121216, supp loss = 0.0\n",
      "Step 0: loss = 0.5221155285835266, recon loss = 0.5221155285835266, supp loss = 0.0\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.42049360275268555, recon loss = 0.42049360275268555, supp loss = 0.0\n",
      "Step 0: loss = 0.5354539155960083, recon loss = 0.5354539155960083, supp loss = 0.0\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.42465558648109436, recon loss = 0.42465558648109436, supp loss = 0.0\n",
      "Step 0: loss = 0.5335597395896912, recon loss = 0.5335597395896912, supp loss = 0.0\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.41361671686172485, recon loss = 0.41361671686172485, supp loss = 0.0\n",
      "Step 0: loss = 0.5320206880569458, recon loss = 0.5320206880569458, supp loss = 0.0\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.39464491605758667, recon loss = 0.39464491605758667, supp loss = 0.0\n",
      "Step 0: loss = 0.5679604411125183, recon loss = 0.5679604411125183, supp loss = 0.0\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.3843154013156891, recon loss = 0.3843154013156891, supp loss = 0.0\n",
      "Step 0: loss = 0.54737389087677, recon loss = 0.54737389087677, supp loss = 0.0\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.4179891347885132, recon loss = 0.4179891347885132, supp loss = 0.0\n",
      "Step 0: loss = 0.5484305024147034, recon loss = 0.5484305024147034, supp loss = 0.0\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.408954918384552, recon loss = 0.408954918384552, supp loss = 0.0\n",
      "Step 0: loss = 0.5350269675254822, recon loss = 0.5350269675254822, supp loss = 0.0\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.3750723898410797, recon loss = 0.3750723898410797, supp loss = 0.0\n",
      "Step 0: loss = 0.5464074611663818, recon loss = 0.5464074611663818, supp loss = 0.0\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.4284130334854126, recon loss = 0.4284130334854126, supp loss = 0.0\n",
      "Step 0: loss = 0.5413212776184082, recon loss = 0.5413212776184082, supp loss = 0.0\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.41088318824768066, recon loss = 0.41088318824768066, supp loss = 0.0\n",
      "Step 0: loss = 0.5534234046936035, recon loss = 0.5534234046936035, supp loss = 0.0\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.39110225439071655, recon loss = 0.39110225439071655, supp loss = 0.0\n",
      "Step 0: loss = 0.5382544994354248, recon loss = 0.5382544994354248, supp loss = 0.0\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.4005512595176697, recon loss = 0.4005512595176697, supp loss = 0.0\n",
      "Step 0: loss = 0.5495636463165283, recon loss = 0.5495636463165283, supp loss = 0.0\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.37299755215644836, recon loss = 0.37299755215644836, supp loss = 0.0\n",
      "Step 0: loss = 0.5208603143692017, recon loss = 0.5208603143692017, supp loss = 0.0\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.3801218271255493, recon loss = 0.3801218271255493, supp loss = 0.0\n",
      "Step 0: loss = 0.5440890192985535, recon loss = 0.5440890192985535, supp loss = 0.0\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.38982900977134705, recon loss = 0.38982900977134705, supp loss = 0.0\n",
      "Step 0: loss = 0.5370023250579834, recon loss = 0.5370023250579834, supp loss = 0.0\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.3891181945800781, recon loss = 0.3891181945800781, supp loss = 0.0\n",
      "Step 0: loss = 0.5494783520698547, recon loss = 0.5494783520698547, supp loss = 0.0\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.39468374848365784, recon loss = 0.39468374848365784, supp loss = 0.0\n",
      "Step 0: loss = 0.547770619392395, recon loss = 0.547770619392395, supp loss = 0.0\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.3805350065231323, recon loss = 0.3805350065231323, supp loss = 0.0\n",
      "Step 0: loss = 0.5260554552078247, recon loss = 0.5260554552078247, supp loss = 0.0\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.3726755976676941, recon loss = 0.3726755976676941, supp loss = 0.0\n",
      "Step 0: loss = 0.5291498899459839, recon loss = 0.5291498899459839, supp loss = 0.0\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.3616214692592621, recon loss = 0.3616214692592621, supp loss = 0.0\n",
      "Step 0: loss = 0.5178523063659668, recon loss = 0.5178523063659668, supp loss = 0.0\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.3836846649646759, recon loss = 0.3836846649646759, supp loss = 0.0\n",
      "Step 0: loss = 0.5520641803741455, recon loss = 0.5520641803741455, supp loss = 0.0\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.40408098697662354, recon loss = 0.40408098697662354, supp loss = 0.0\n",
      "Step 0: loss = 0.5383238196372986, recon loss = 0.5383238196372986, supp loss = 0.0\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.38884401321411133, recon loss = 0.38884401321411133, supp loss = 0.0\n",
      "Step 0: loss = 0.5160590410232544, recon loss = 0.5160590410232544, supp loss = 0.0\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.3740857243537903, recon loss = 0.3740857243537903, supp loss = 0.0\n",
      "Step 0: loss = 0.5489023923873901, recon loss = 0.5489023923873901, supp loss = 0.0\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.37671852111816406, recon loss = 0.37671852111816406, supp loss = 0.0\n",
      "Step 0: loss = 0.5391459465026855, recon loss = 0.5391459465026855, supp loss = 0.0\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.40356937050819397, recon loss = 0.40356937050819397, supp loss = 0.0\n",
      "Step 0: loss = 0.5524640083312988, recon loss = 0.5524640083312988, supp loss = 0.0\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.4057019352912903, recon loss = 0.4057019352912903, supp loss = 0.0\n",
      "Step 0: loss = 0.5492218136787415, recon loss = 0.5492218136787415, supp loss = 0.0\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.3739245533943176, recon loss = 0.3739245533943176, supp loss = 0.0\n",
      "Step 0: loss = 0.5648807287216187, recon loss = 0.5648807287216187, supp loss = 0.0\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.3730887770652771, recon loss = 0.3730887770652771, supp loss = 0.0\n",
      "Step 0: loss = 0.52293860912323, recon loss = 0.52293860912323, supp loss = 0.0\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.3932236135005951, recon loss = 0.3932236135005951, supp loss = 0.0\n",
      "Step 0: loss = 0.5362192392349243, recon loss = 0.5362192392349243, supp loss = 0.0\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.3476845324039459, recon loss = 0.3476845324039459, supp loss = 0.0\n",
      "Step 0: loss = 0.5223602652549744, recon loss = 0.5223602652549744, supp loss = 0.0\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.36167609691619873, recon loss = 0.36167609691619873, supp loss = 0.0\n",
      "Step 0: loss = 0.5365090370178223, recon loss = 0.5365090370178223, supp loss = 0.0\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.37149667739868164, recon loss = 0.37149667739868164, supp loss = 0.0\n",
      "Step 0: loss = 0.5329421162605286, recon loss = 0.5329421162605286, supp loss = 0.0\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.3787074089050293, recon loss = 0.3787074089050293, supp loss = 0.0\n",
      "Step 0: loss = 0.5242352485656738, recon loss = 0.5242352485656738, supp loss = 0.0\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.36335036158561707, recon loss = 0.36335036158561707, supp loss = 0.0\n",
      "Step 0: loss = 0.5216166377067566, recon loss = 0.5216166377067566, supp loss = 0.0\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.3692753314971924, recon loss = 0.3692753314971924, supp loss = 0.0\n",
      "Step 0: loss = 0.5166130065917969, recon loss = 0.5166130065917969, supp loss = 0.0\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.37235385179519653, recon loss = 0.37235385179519653, supp loss = 0.0\n",
      "Step 0: loss = 0.5273212194442749, recon loss = 0.5273212194442749, supp loss = 0.0\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.3473671078681946, recon loss = 0.3473671078681946, supp loss = 0.0\n",
      "Step 0: loss = 0.5160890817642212, recon loss = 0.5160890817642212, supp loss = 0.0\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.37658512592315674, recon loss = 0.37658512592315674, supp loss = 0.0\n",
      "Step 0: loss = 0.5534236431121826, recon loss = 0.5534236431121826, supp loss = 0.0\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.3577668368816376, recon loss = 0.3577668368816376, supp loss = 0.0\n",
      "Step 0: loss = 0.5391830205917358, recon loss = 0.5391830205917358, supp loss = 0.0\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.3593124747276306, recon loss = 0.3593124747276306, supp loss = 0.0\n",
      "Step 0: loss = 0.5114463567733765, recon loss = 0.5114463567733765, supp loss = 0.0\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.3905816674232483, recon loss = 0.3905816674232483, supp loss = 0.0\n",
      "Step 0: loss = 0.5276695489883423, recon loss = 0.5276695489883423, supp loss = 0.0\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.34998565912246704, recon loss = 0.34998565912246704, supp loss = 0.0\n",
      "Step 0: loss = 0.536384105682373, recon loss = 0.536384105682373, supp loss = 0.0\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.35049012303352356, recon loss = 0.35049012303352356, supp loss = 0.0\n",
      "Step 0: loss = 0.5454021692276001, recon loss = 0.5454021692276001, supp loss = 0.0\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.35012874007225037, recon loss = 0.35012874007225037, supp loss = 0.0\n",
      "Step 0: loss = 0.5329641103744507, recon loss = 0.5329641103744507, supp loss = 0.0\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.3460300862789154, recon loss = 0.3460300862789154, supp loss = 0.0\n",
      "Step 0: loss = 0.5044606924057007, recon loss = 0.5044606924057007, supp loss = 0.0\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.3274235427379608, recon loss = 0.3274235427379608, supp loss = 0.0\n",
      "Step 0: loss = 0.5690267086029053, recon loss = 0.5690267086029053, supp loss = 0.0\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.34682145714759827, recon loss = 0.34682145714759827, supp loss = 0.0\n",
      "Step 0: loss = 0.5032373666763306, recon loss = 0.5032373666763306, supp loss = 0.0\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.36522388458251953, recon loss = 0.36522388458251953, supp loss = 0.0\n",
      "Step 0: loss = 0.5480072498321533, recon loss = 0.5480072498321533, supp loss = 0.0\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.3301016688346863, recon loss = 0.3301016688346863, supp loss = 0.0\n",
      "Step 0: loss = 0.5188256502151489, recon loss = 0.5188256502151489, supp loss = 0.0\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.32685789465904236, recon loss = 0.32685789465904236, supp loss = 0.0\n",
      "Step 0: loss = 0.5156031250953674, recon loss = 0.5156031250953674, supp loss = 0.0\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.3405306041240692, recon loss = 0.3405306041240692, supp loss = 0.0\n",
      "Step 0: loss = 0.5143288969993591, recon loss = 0.5143288969993591, supp loss = 0.0\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.3404829502105713, recon loss = 0.3404829502105713, supp loss = 0.0\n",
      "Step 0: loss = 0.5337139368057251, recon loss = 0.5337139368057251, supp loss = 0.0\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.3114236891269684, recon loss = 0.3114236891269684, supp loss = 0.0\n",
      "Step 0: loss = 0.507530689239502, recon loss = 0.507530689239502, supp loss = 0.0\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.3116111755371094, recon loss = 0.3116111755371094, supp loss = 0.0\n",
      "Step 0: loss = 0.527229905128479, recon loss = 0.527229905128479, supp loss = 0.0\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.30508682131767273, recon loss = 0.30508682131767273, supp loss = 0.0\n",
      "Step 0: loss = 0.512191891670227, recon loss = 0.512191891670227, supp loss = 0.0\n",
      "\n",
      "Epoch 151\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "fine_tune_epochs = 3\n",
    "batch_size = 32\n",
    "support_batch_size = 300\n",
    "alpha0 = 4\n",
    "nalpha = alpha0\n",
    "for i in range(epochs):\n",
    "  nalpha = min(nalpha * 1.02, 60)\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs, supp_vs, supp_ys) in enumerate(batch(X, betas, random_dirs, supps_b, batch_size, support_batch_size)):\n",
    "    # print(supp_vs.shape, supp_ys.shape)\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs, supp_vs, supp_ys, 1.0, 0.0)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon loss = {loss_vals[1].numpy()}, supp loss = {loss_vals[2].numpy()}\")\n",
    "    for j in range(fine_tune_epochs):\n",
    "        loss_vals = train_step(vae, batch_betas, tf.linalg.normalize(batch_dirs + np.random.randn(*batch_dirs.shape)*0.1, axis=-1)[0], supp_vs, supp_ys, 1.0, 0.0)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon loss = {loss_vals[1].numpy()}, supp loss = {loss_vals[2].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qHF0P4eISAH5"
   },
   "outputs": [],
   "source": [
    "vae.save_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JkrFRN7uOlf",
    "outputId": "90f21636-cdec-4d1e-eb4f-405a557a43b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7dfda7a63c70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('./my_checkpoint/chekpont')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bS8sKCRfgUoH",
    "outputId": "b7f487ba-8083-4757-b9ea-250fd343a5d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  chem-checkpoint.zip\n",
      "   creating: my_checkpoint/\n",
      "  inflating: my_checkpoint/chekpont.index  \n",
      "  inflating: my_checkpoint/.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/checkpoint  \n",
      "  inflating: my_checkpoint/chekpont.data-00000-of-00001  \n",
      "  inflating: my_checkpoint/.index    \n"
     ]
    }
   ],
   "source": [
    "# !zip -r my_checkpoint.zip my_checkpoint/\n",
    "!unzip chem-checkpoint.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "EbHImoN2QaKX"
   },
   "outputs": [],
   "source": [
    "drawn_random_dirs = np.random.randn(50_000, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "\n",
    "def posterior_sampling(model, betas, random_dirs, num_samples=1):\n",
    "  zm, zlv, z = model.encoder((betas, random_dirs))\n",
    "  if num_samples == 1:\n",
    "      return model.decoder((z, random_dirs))\n",
    "  else:\n",
    "      samples = [sampling((zm, zlv)) for _ in range(num_samples)]\n",
    "      return tf.concat([model.decoder((sm, random_dirs))[:, None, :] for sm in samples], axis=1)\n",
    "\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs1 = np.random.randn(num_samples, d)\n",
    "  random_dirs2 = np.random.randn(num_samples, d)\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))    \n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m drawn_betas, dir1, drawn_betas2 \u001b[38;5;241m=\u001b[39m generate_new_betas(\u001b[43mvae\u001b[49m, \u001b[38;5;241m50_000\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# generate_new_betas\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vae' is not defined"
     ]
    }
   ],
   "source": [
    "drawn_betas, dir1, drawn_betas2 = generate_new_betas(vae, 50_000)\n",
    "# drawn_betas = posterior_sampling(vae, betas, np.random.randn(512, d), 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 1)\n",
    "# drawn_betas = posterior_sampling(vae, betas, random_dirs, 100)\n",
    "# generate_new_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 200\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "prd1 = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 40)\n",
    "Y_sub = Y[X_ids]\n",
    "# prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample][0])\n",
    "prd2 = softmax(get_rand_feats(X_sub@pca_projs, model), drawn_betas[sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([-0.00631047,  0.12106761,  0.46212596, ...,  0.47387064,\n",
       "        -0.281489  , -0.05550657], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check agreement between vae and training samples\n",
    "from sklearn.metrics import jaccard_score\n",
    "def agreement(y_pred1, y_pred2, y_true):\n",
    "    tp1 = np.float32(y_pred1==1) * np.float32(y_true==1)\n",
    "    fp1 = np.float32(y_pred1==1) * np.float32(y_true==0)\n",
    "    tn1 = np.float32(y_pred1==0) * np.float32(y_true==0)\n",
    "    fn1 = np.float32(y_pred1==0) * np.float32(y_true==1)\n",
    "\n",
    "    tp2 = np.float32(y_pred2==1) * np.float32(y_true==1)\n",
    "    fp2 = np.float32(y_pred2==1) * np.float32(y_true==0)\n",
    "    tn2 = np.float32(y_pred2==0) * np.float32(y_true==0)\n",
    "    fn2 = np.float32(y_pred2==0) * np.float32(y_true==1)\n",
    "    print(np.sum(tp1)/len(tp1), np.sum(fp1)/len(tp1), np.sum(tn1)/len(tp1), np.sum(fn1)/len(tp1))\n",
    "    print(np.sum(tp2)/len(tp1), np.sum(fp2)/len(tp1), np.sum(tn2)/len(tp1), np.sum(fn2)/len(tp1))\n",
    "    print(np.sum(fp1==fp2)/len(tp1), np.sum(fn1==fn2)/len(tp1))\n",
    "    return jaccard_score(tp1, tp2), jaccard_score(fp1, fp2), jaccard_score(tn1, tn2), jaccard_score(fn1, fn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5735723218007504 0.0029178824510212586 0.4210087536473531 0.0025010421008753647\n",
      "0.5731554814506045 0.0025010421008753647 0.42142559399749896 0.0029178824510212586\n",
      "0.9979157982492706 0.9979157982492706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9963715529753265,\n",
       " 0.4444444444444444,\n",
       " 0.9950641658440277,\n",
       " 0.4444444444444444)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreement(prd1, prd2, Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([512, 10, 4081])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.0054231044>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps= betas\n",
    "oups = drawn_betas\n",
    "tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(tf.expand_dims(inps, axis=1)[:, :, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(oups[:, :, 1:], axis=-1)[0], axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.012491584>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(tf.abs(tf.cast(tf.expand_dims(inps, axis=1)[:, :, :1], dtype=tf.float32)-oups[:, :, :1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4081,), dtype=float32, numpy=\n",
       " array([ 0.01558006,  0.08848727,  0.02163708, ...,  0.32644653,\n",
       "        -0.05755342,  0.21104604], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4081,), dtype=float64, numpy=\n",
       " array([ 0.00832155,  0.00116945,  0.10095101, ...,  0.12449417,\n",
       "        -0.06915902, -0.01289894])>)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0], betas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IHicI0RhbDe",
    "outputId": "32426975-705a-4668-f061-08acd65bc48f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.99283946>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.losses.CosineSimilarity(axis=-1)(drawn_betas1, drawn_betas2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOFI9xq2b7_3",
    "outputId": "a11e5724-0e2a-428d-a65a-28064209e6bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 4081), dtype=float32, numpy=\n",
       "array([[ 3.5679474e-02, -8.5116491e-02, -2.7868379e-02, ...,\n",
       "         3.8362685e-01,  6.8785306e-03,  5.8818167e-01],\n",
       "       [ 2.0887703e-02,  5.4998733e-02,  2.2995186e-01, ...,\n",
       "         1.0724969e-01,  3.8565136e-04,  3.8573799e-01],\n",
       "       [-6.5227631e-03,  2.0269346e-01,  4.0428180e-01, ...,\n",
       "        -4.8758507e-01, -8.2871839e-02,  3.4979448e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas1[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFCgGG6jgkLL",
    "outputId": "5e799c27-fff1-41fc-d71f-3fc7ea88538b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.13627878, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "var = tf.math.reduce_variance(drawn_betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxnzX5lJQtjX"
   },
   "outputs": [],
   "source": [
    "np.mean(drawn_betas1 @ tf.transpose(drawn_betas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "TfAej5fqsfHh",
    "outputId": "fd888e0d-c076-4577-be0d-4de29be77dd2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'convert_data_to_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ood_val_features \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_data_to_features\u001b[49m(ood_val_data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#ood_test_features = convert_data_to_features(ood_test_data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ood_val_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m ood_val_data])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'convert_data_to_features' is not defined"
     ]
    }
   ],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "#ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "#ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "U07oDyUdsl1u"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ood_val_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m external_X \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mood_val_features\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      2\u001b[0m external_Y \u001b[38;5;241m=\u001b[39m ood_val_labels\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ood_val_features' is not defined"
     ]
    }
   ],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "external_Y = ood_val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets-ood/chem/val_ood.csv', 'r') as f:\n",
    "  external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "with open('../datasets-ood/chem/val_ood.csv', 'r') as f:\n",
    "  external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "external_X = (external_X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('external_X.npy', external_X)\n",
    "np.save('external_Y.npy', external_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)\n",
    "randfeats_X = get_rand_feats(X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "1a114f68-6f8b-4ede-b395-df1c2dfa2067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      " -0.03134901]\n",
      "tf.Tensor(\n",
      "[-0.13125554  0.30866534  0.63319725 -0.5780234  -0.21648076 -0.3846287\n",
      "  0.99320394 -0.1799166   0.678208    0.62888056], shape=(10,), dtype=float32)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "5ec19308-4552-4326-ec47-b86470a2f7b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 1024)\n",
      "(937, 2040)\n",
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "ae104348-bf03-48d8-a6c3-d3112b1dcb37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.9594797  -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " ...\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]\n",
      " [-0.04052026 -0.29764882 -0.08170752 ... -0.04052026 -0.06053026\n",
      "  -0.03134901]]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W32O3S_gKnEp",
    "outputId": "8797bdae-deb8-43ed-ef0c-cbf43fb4aad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937, 512) (937, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 5, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  # wghts = np.ones_like(projs > thresh[:, None])\n",
    "  wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  # wghts = (projs >= thresh[:, None]) * tf.nn.softmax(projs*(projs >= thresh[:, None]), axis=-1)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return aggregate_preds(preds)\n",
    "\n",
    "\n",
    "# drawn_betas = tf.reshape(drawn_betas, (-1, drawn_betas.shape[-1]))\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(randfeats_X, X, random_dirs[:512], betas[:512]) # 0.622\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs[:512], betas[:512]) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, betas[:512]) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, dir1, drawn_betas) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, drawn_betas1) # 0.622\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(external_randfeats_X, betas) # 0.634\n",
    "# ext_preds, mp_rand, sp_rand = get_preds_and_aggregate(randfeats_X, drawn_betas) # 0.85\n",
    "# ext_preds, mp_rand, sp_rand, pred = get_preds_and_aggregate(external_randfeats_X, betas) # 0.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DOyhZXJBrhaj",
    "outputId": "98cc9eb0-2e98-4b38-e823-337411f61e28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0], tf.transpose(tf.linalg.normalize(tf.ones((100, 200)), axis=-1)[0])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "a9702828-7bea-436e-de46-89ca03c30b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.767455  ,  0.1001012 , -0.54182774, ..., -6.8707843 ,\n",
       "         7.257238  , -0.5050444 ], dtype=float32),\n",
       " array([-0.0186294 ,  0.06830448, -0.27256313, ..., -0.72351612,\n",
       "         0.81710885,  0.05805234]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "2fac824c-8637-4a35-8dfb-7b60045b10b4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ext_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mext_preds\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ext_preds' is not defined"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "06eb05c6-f5f0-4d58-a8a2-ad76ee40b954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [False  True False False  True False False False False False]\n",
      "Total Positive Preds:  426\n",
      "Total Preds:  937\n",
      "% Positive Preds:  0.45464247598719315\n",
      "\n",
      "First 10 Ground Truth:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Total Positive Ground Truth:  421.0\n",
      "Total Ground Truth:  937\n",
      "% Positive Ground Truth:  0.44930629669156885\n",
      "\n",
      "Accuracy:  0.6595517609391676\n"
     ]
    }
   ],
   "source": [
    "# testing_Y = Y\n",
    "ext_preds = ext_preds > 0.5\n",
    "testing_Y = external_Y\n",
    "\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", testing_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(testing_Y))\n",
    "print(\"Total Ground Truth: \", len(testing_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(testing_Y) / len(testing_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == testing_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "id": "L7kl6J3wLped",
    "outputId": "a5d20ac1-f339-47e4-cf12-18073bb4b59f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f56b4751160>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.2, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.8571428571428571, 0.8536585365853658, 0.7857142857142857, 0.81, 0.7898550724637681, 0.7909604519774012, 0.7813953488372093, 0.7649402390438247, 0.738831615120275, 0.7284345047923323, 0.7308781869688386, 0.7222222222222222, 0.7126696832579186, 0.7034764826175869, 0.6950998185117967, 0.6829268292682927, 0.6651917404129793, 0.6557591623036649, 0.6211312700106724]\n"
     ]
    }
   ],
   "source": [
    "print(accus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd084658310>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_preds)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9670373318045292"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc(r[r<0.2], p[r<0.2]) / np.max(r[r<0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
