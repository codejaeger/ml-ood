{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKwffW42qtTW",
    "outputId": "3e50e565-4197-4084-eade-a2f848560b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rdkit\n",
      "  Downloading rdkit-2023.9.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.9 MB 3.9 MB/s eta 0:00:01    |█████████                       | 9.7 MB 3.9 MB/s eta 0:00:07     |███████████                     | 12.0 MB 3.9 MB/s eta 0:00:06\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /playpen/debman/ood/lib/python3.8/site-packages (from rdkit) (10.3.0)\n",
      "Requirement already satisfied: numpy in /playpen/debman/ood/lib/python3.8/site-packages (from rdkit) (1.22.4)\n",
      "Installing collected packages: rdkit\n",
      "Successfully installed rdkit-2023.9.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EXxipvmSWDIi"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FDwgr2mvrr43"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 23:28:12.611245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 23:28:13.589090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-11 23:28:13.589197: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-11 23:28:13.589211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "MiA1dcJqpTKA"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from scipy.linalg import null_space\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyVyoLZXEp70",
    "outputId": "d63bda04-9b34-4f59-c696-954296a31a7e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yKeTe4yOnToa",
    "outputId": "ecdaff3b-fa7e-4b7a-e107-eec949d43d9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cfg', 'split', 'statistics'])\n",
      "\n",
      "cfg 6\n",
      "Nested Keys in cfg: dict_keys(['path', 'uncertainty', 'classification_threshold', 'fractions', 'noise_filter', 'domain'])\n",
      "\n",
      "split 5\n",
      "Nested Keys in split: dict_keys(['train', 'ood_val', 'ood_test', 'iid_val', 'iid_test'])\n",
      "\n",
      "statistics 16\n",
      "Nested Keys in statistics: dict_keys(['thr_for_cls', 'positive_samples', 'negative_samples', 'positive_rate', 'train domain number', 'val domain number', 'test domain number', 'train datapoints', 'ood_val datapoints', 'ood_test datapoints', 'iid_val datapoints', 'iid_test datapoints', 'ood_val domain number', 'ood_test domain number', 'iid_val domain number', 'iid_test domain number'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drugood-sbap-core-ic50-protein\n",
    "\n",
    "# file_path = '/content/drive/MyDrive/sbap_core_ec50_protein.json'\n",
    "file_path = './sbap_core_potency_protein.json'\n",
    "with open(file_path, 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "# Data is a nested JSON structired\n",
    "print(data.keys())\n",
    "print()\n",
    "\n",
    "for key in data.keys():\n",
    "  print(key, len(data[key]))\n",
    "  nested_keys = data[key].keys()\n",
    "  print(f\"Nested Keys in {key}: {nested_keys}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANTG9zLyqr8K",
    "outputId": "565e32d4-3363-4e06-eea9-8ff24397c5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data First Row:\n",
      "{'smiles': 'CCN1C(=O)c2cccc3cccc1c23', 'reg_label': 4.6, 'assay_id': 688665, 'protein': 'MADKVLKEKRKLFIRSMGEGTINGLLDELLQTRVLNKEEMEKVKRENATVMDKTRALIDSVIPKGAQACQICITYICEEDSYLAGTLGLSADQTSGNYLNMQDSQGVLSSFPAPQAVQDNPAMPTSSGSEGNVKLCSLEEAQRIWKQKSAEIYPIMDKSSRTRLALIICNEEFDSIPRRTGAEVDITGMTMLLQNLGYSVDVKKNLTASDMTTELEAFAHRPEHKTSDSTFLVFMSHGIREGICGKKHSEQVPDILQLNAIFNMLNTKNCPSLKDKPKVIIIQACRGDSPGVVWFKDSVGVSGNLSLPTTEEFEDDAIKKAHIEKDFIAFCSSTPDNVSWRHPTMGSVFIGRLIEHMQEYACSCDVEEIFRKVRFSFEQPDGRAQMPTTERVTLTRCFYLFPGH', 'cls_label': 0, 'domain_id': 41}\n",
      "\n",
      "OOD Validation Data First Row:\n",
      "{'smiles': 'OC(c1ccncc1)(c1ccc(F)cc1)c1ccc(F)cc1', 'reg_label': 5.3, 'assay_id': 688400, 'protein': 'MSQLSSTLKRYTESARYTDAHYAKSGYGAYTPSSYGANLAASLLEKEKLGFKPVPTSSFLTRPRTYGPSSLLDYDRGRPLLRPDITGGGKRAESQTRGTERPLGSGLSGGSGFPYGVTNNCLSYLPINAYDQGVTLTQKLDSQSDLARDFSSLRTSDSYRIDPRNLGRSPMLARTRKELCTLQGLYQTASCPEYLVDYLENYGRKGSASQVPSQAPPSRVPEIISPTYRPIGRYTLWETGKGQAPGPSRSSSPGRDGMNSKSAQGLAGLRNLGNTCFMNSILQCLSNTRELRDYCLQRLYMRDLHHGSNAHTALVEEFAKLIQTIWTSSPNDVVSPSEFKTQIQRYAPRFVGYNQQDAQEFLRFLLDGLHNEVNRVTLRPKSNPENLDHLPDDEKGRQMWRKYLEREDSRIGDLFVGQLKSSLTCTDCGYCSTVFDPFWDLSLPIAKRGYPEVTLMDCMRLFTKEDVLDGDEKPTCCRCRGRKRCIKKFSIQRFPKILVLHLKRFSESRIRTSKLTTFVNFPLRDLDLREFASENTNHAVYNLYAVSNHSGTTMGGHYTAYCRSPGTGEWHTFNDSSVTPMSSSQVRTSDAYLLFYELASPPSRM', 'cls_label': 1, 'domain_id': 0}\n",
      "\n",
      "OOD Test Data First Row:\n",
      "{'smiles': 'Cn1ncc(Br)c1C(=O)N/N=C/c1cccc(OC2CSC2)c1', 'reg_label': 4.5, 'assay_id': 688319, 'protein': 'MPLCTLRQMLGEARKHKYGVGAFNVNNMEQIQGIMKAVVQLKSPVILQCSRGALKYSDMIYLKKLCEAALEKHPDIPICIHLDHGDTLESVKMAIDLGFSSVMIDASHHPFDENVRITKEVVAYAHARSVSVEAELGTLGGIEEDVQNTVQLTEPQDAKKFVELTGVDALAVAIGTSHGAYKFKSESDIRLAIDRVKTISDLTGIPLVMHGSSSVPKDVKDMINKYGGKMPDAVGVPIESIVHAIGEGVCKINVDSDSRMAMTGAIRKVFVEHPEKFDPRDYLGPGRDAITEMLIPKIKAFGSAGHAGDYKVVSLEEAKAWYK', 'cls_label': 0, 'domain_id': 34}\n",
      "\n",
      "IID Validation Data First Row:\n",
      "{'smiles': 'CN1CCN(c2ncc3nc(-c4cn(C)c5ccccc45)c(=O)n(CCc4ccccc4)c3n2)CC1', 'reg_label': 5.4, 'assay_id': 688665, 'protein': 'MADKVLKEKRKLFIRSMGEGTINGLLDELLQTRVLNKEEMEKVKRENATVMDKTRALIDSVIPKGAQACQICITYICEEDSYLAGTLGLSADQTSGNYLNMQDSQGVLSSFPAPQAVQDNPAMPTSSGSEGNVKLCSLEEAQRIWKQKSAEIYPIMDKSSRTRLALIICNEEFDSIPRRTGAEVDITGMTMLLQNLGYSVDVKKNLTASDMTTELEAFAHRPEHKTSDSTFLVFMSHGIREGICGKKHSEQVPDILQLNAIFNMLNTKNCPSLKDKPKVIIIQACRGDSPGVVWFKDSVGVSGNLSLPTTEEFEDDAIKKAHIEKDFIAFCSSTPDNVSWRHPTMGSVFIGRLIEHMQEYACSCDVEEIFRKVRFSFEQPDGRAQMPTTERVTLTRCFYLFPGH', 'cls_label': 1, 'domain_id': 41}\n",
      "\n",
      "IID Test Data First Row:\n",
      "{'smiles': 'Cc1nc2c(c(=O)n1Cc1ccco1)c1nc3ccccc3nc1n2Cc1ccccc1', 'reg_label': 4.6, 'assay_id': 688665, 'protein': 'MADKVLKEKRKLFIRSMGEGTINGLLDELLQTRVLNKEEMEKVKRENATVMDKTRALIDSVIPKGAQACQICITYICEEDSYLAGTLGLSADQTSGNYLNMQDSQGVLSSFPAPQAVQDNPAMPTSSGSEGNVKLCSLEEAQRIWKQKSAEIYPIMDKSSRTRLALIICNEEFDSIPRRTGAEVDITGMTMLLQNLGYSVDVKKNLTASDMTTELEAFAHRPEHKTSDSTFLVFMSHGIREGICGKKHSEQVPDILQLNAIFNMLNTKNCPSLKDKPKVIIIQACRGDSPGVVWFKDSVGVSGNLSLPTTEEFEDDAIKKAHIEKDFIAFCSSTPDNVSWRHPTMGSVFIGRLIEHMQEYACSCDVEEIFRKVRFSFEQPDGRAQMPTTERVTLTRCFYLFPGH', 'cls_label': 0, 'domain_id': 41}\n"
     ]
    }
   ],
   "source": [
    "# Access split section of data\n",
    "split_data = data['split']\n",
    "\n",
    "# Load Data Splits\n",
    "train_data = split_data['train']\n",
    "iid_val_data = split_data['iid_val']\n",
    "iid_test_data = split_data['iid_test']\n",
    "ood_val_data = split_data['ood_val']\n",
    "ood_test_data = split_data['ood_test']\n",
    "\n",
    "# You can now print or inspect the first row of each of these as DataFrames\n",
    "print(\"Train Data First Row:\")\n",
    "print(train_data[0])\n",
    "\n",
    "print(\"\\nOOD Validation Data First Row:\")\n",
    "print(ood_val_data[0])\n",
    "\n",
    "print(\"\\nOOD Test Data First Row:\")\n",
    "print(ood_test_data[0])\n",
    "\n",
    "print(\"\\nIID Validation Data First Row:\")\n",
    "print(iid_val_data[0])\n",
    "\n",
    "print(\"\\nIID Test Data First Row:\")\n",
    "print(iid_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3FToq5IgqvoT"
   },
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, DataStructs\n",
    "import numpy as np\n",
    "\n",
    "# RDKit for SMILES\n",
    "def smiles_to_ecfp(smiles, radius=2, nbits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    ecfp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nbits)\n",
    "    array = np.zeros((0,), dtype=int)\n",
    "    DataStructs.ConvertToNumpyArray(ecfp, array)\n",
    "    return array\n",
    "\n",
    "# Function to convert your data into a feature matrix\n",
    "def convert_data_to_features(data):\n",
    "    smiles_features = []\n",
    "    #protein_features = []\n",
    "    for entry in data:\n",
    "        smiles = entry['smiles']\n",
    "        #protein = entry['protein']\n",
    "\n",
    "        smiles_features.append(smiles_to_ecfp(smiles))\n",
    "\n",
    "        # Truncate or pad protein sequence if necessary\n",
    "        #protein_sequence = protein[:100]  # example with max length of 100\n",
    "        #protein_sequence = protein_sequence.ljust(100, 'A')  # padding with alanine (can choose any)\n",
    "        #protein_features.append(one_hot_protein(protein_sequence))\n",
    "\n",
    "    smiles_features = np.array(smiles_features)\n",
    "    #protein_features = np.array(protein_features)\n",
    "\n",
    "    # Combine and normalize features\n",
    "    #combined_features = np.hstack((smiles_features, protein_features))\n",
    "    #combined_features = (combined_features - np.mean(combined_features, axis=0)) / np.std(combined_features, axis=0)\n",
    "\n",
    "    return smiles_features\n",
    "    #return combined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2_0-PyyMqy80"
   },
   "outputs": [],
   "source": [
    "### ALL IID DATA\n",
    "train_features = convert_data_to_features(train_data)\n",
    "#val_features = convert_data_to_features(iid_val_data)\n",
    "#test_features = convert_data_to_features(iid_test_data)\n",
    "\n",
    "train_labels = np.array([entry['cls_label'] for entry in train_data])\n",
    "#val_labels = np.array([entry['cls_label'] for entry in iid_val_data])\n",
    "#test_labels = np.array([entry['cls_label'] for entry in iid_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gnD69yzs2rpL",
    "outputId": "8ae134b0-f17f-42a4-8b13-081da9f01c5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 0 1 1 1 0 0]\n",
      "6880\n",
      "12699\n",
      "0.5417749429088905\n"
     ]
    }
   ],
   "source": [
    "## Very imbalanced\n",
    "print(train_labels[:10])\n",
    "print(sum(train_labels))\n",
    "print(len(train_labels))\n",
    "print(sum(train_labels)/len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "sucs4AA_q09l"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 23:28:21.394766: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 23:28:21.542011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9803 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:09:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Now you can use train_features in place of dataX and train_labels in place of dataY\n",
    "X = tf.cast(train_features, tf.float32)\n",
    "Y = tf.cast(train_labels, tf.float32)\n",
    "\n",
    "pperm = np.random.permutation(len(X))\n",
    "\n",
    "X = tf.constant(np.array(X)[pperm])\n",
    "Y = tf.constant(np.array(Y)[pperm])\n",
    "\n",
    "X = X[:int(len(X)/1.3)]\n",
    "Y = Y[:len(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.537674, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(sum(Y)/len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.50000066, 0.017522836)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.std(X, 0, keepdims=True)), np.min(np.std(X, 0, keepdims=True)), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9U56G1VRx-As"
   },
   "outputs": [],
   "source": [
    "# standardize the data\n",
    "mu_x = np.mean(X, 0, keepdims=True)\n",
    "# sigma_x = np.std(X, 0, keepdims=True)\n",
    "sigma_x = np.ones_like(mu_x)\n",
    "X = (X-mu_x)/sigma_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhQ0HK11qm36",
    "outputId": "0dfcbb26-6d3d-4223-8f94-78c1bbdd5a52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9768, 2048)\n",
      "(9768,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TfAej5fqsfHh"
   },
   "outputs": [],
   "source": [
    "ood_val_features = convert_data_to_features(ood_val_data)\n",
    "# ood_test_features = convert_data_to_features(ood_test_data)\n",
    "\n",
    "ood_val_labels = np.array([entry['cls_label'] for entry in ood_val_data])\n",
    "# ood_test_labels = np.array([entry['cls_label'] for entry in ood_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "U07oDyUdsl1u"
   },
   "outputs": [],
   "source": [
    "external_X = tf.cast(ood_val_features, tf.float32)\n",
    "# external_X = tf.cast(ood_test_features, tf.float32)\n",
    "external_Y = ood_val_labels\n",
    "# external_Y = ood_test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "w4f7gcI3MOqu"
   },
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  def __init__(self, sigma_rot, d, D=160):\n",
    "\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot*2, sigma_rot*4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandFeats:\n",
    "  # def __init__(self, sigma_rot, d, D=196):\n",
    "  def __init__(self, sigma_rot, X, d, D=128):\n",
    "\n",
    "    # self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot]\n",
    "    self.sigmas = [sigma_rot/4, sigma_rot/2, sigma_rot, sigma_rot/2, sigma_rot/4]\n",
    "    self.D = D\n",
    "    self.Ws = []\n",
    "    for sigma in self.sigmas:\n",
    "      self.Ws.append(np.float32(np.random.randn(d, D)/sigma))\n",
    "    self.Ws = np.stack(self.Ws, 0)\n",
    "    self.Ws = self.sample_features(X)\n",
    "    \n",
    "  def sample_features(self, X, ):\n",
    "    L = int(0.3 * len(X))\n",
    "    M = self.Ws.shape[0] * self.Ws.shape[2]\n",
    "    # N = np.random.choice(M, int(M/100), replace=False)\n",
    "    N = int(M/100)+10\n",
    "    phi_Xt = tf.transpose(self.get_features(X[np.random.choice(len(X), L)])) / np.sqrt(L)\n",
    "    phi_phi_T = phi_Xt @ tf.transpose(phi_Xt)\n",
    "    mu = np.power(10, 0)\n",
    "    diag = np.diag(phi_phi_T @ np.linalg.inv(phi_phi_T + mu))\n",
    "    diag = diag / np.sum(diag)\n",
    "    print(M, len(diag)//2, phi_Xt.shape, self.Ws.shape)\n",
    "    print(\"Diag\", M, diag, np.argsort(diag)[-N])\n",
    "    k, N, D = self.Ws.shape\n",
    "    diag = diag[:len(diag)//2] + diag[len(diag)//2:]\n",
    "    w_indices = np.unique(np.argsort(diag)[-N:])\n",
    "    _Ws = np.reshape(np.transpose(self.Ws, [1, 2, 0]), (N, -1))[:, w_indices]\n",
    "    return np.transpose(np.reshape(_Ws, (N, -1, 1)), [2, 0, 1])\n",
    "\n",
    "  def get_features(self, x_in):\n",
    "    # phis = []\n",
    "    # TODO: vectorize\n",
    "    # for W in Ws:\n",
    "    #   XW = np.matmul(x_in, W)\n",
    "    #   phis.append(\n",
    "    #     np.concatenate([np.sin(XW), np.cos(XW)], -1))\n",
    "    # return np.concatenate(phis, -1)\n",
    "    phis = tf.matmul(x_in, self.Ws)  # k x N x D\n",
    "    # phis = tf.transpose(phis, [1, 2, 0])  # N x D x k\n",
    "    phis = tf.transpose(phis, [1, 2, 0])[:, None, :, :]  # N x D x k\n",
    "    phis = tf.concat((tf.sin(phis), tf.cos(phis)), 1)\n",
    "    return tf.reshape(phis, [x_in.shape[0], -1])\n",
    "\n",
    "  def __call__(self, x_in):\n",
    "    return self.get_features(x_in)\n",
    "\n",
    "# def define_rand_feats(ndata_feats, nrand_feats=1000, gamma=1.0):\n",
    "def define_rand_feats(X, xD):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    ndata_feats: scalar value of total number of data features\n",
    "    nrand_feats: scalar value of total number of desired random features\n",
    "    gamma: Float, scale of frequencies\n",
    "\n",
    "  Returns:\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "  \"\"\"\n",
    "  tf.random.set_seed(123129) # For reproducibility\n",
    "  from scipy.spatial import distance\n",
    "  rprm = np.random.permutation(X.shape[0])\n",
    "  ds = distance.cdist(X[rprm[:100], :], X[rprm[100:], :])\n",
    "  sigma_rot = np.mean(np.sort(ds)[:, 5])\n",
    "  model = RandFeats(sigma_rot, X, X.shape[1], int(X.shape[1]*xD))\n",
    "\n",
    "  # Ws = gamma*tf.random.normal((ndata_feats, nrand_feats))\n",
    "  # bs = 2.0*np.pi*tf.random.uniform((1,nrand_feats))\n",
    "  # return Ws, bs\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dx = [1.5, 2, 4, 8, 10, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "lUdTgThu3CDN"
   },
   "outputs": [],
   "source": [
    "def get_rand_feats(X, model):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Ws: ndata_feats x nrand_feats weight matrix\n",
    "    bs: 1 x nrand_feats bias vector\n",
    "\n",
    "  Returns:\n",
    "    Phis: N x D matrix of random features\n",
    "  \"\"\"\n",
    "  # XWs = tf.matmul(X, Ws)\n",
    "  # return tf.cos(XWs+bs)\n",
    "  return model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "OdWKikf20dfX"
   },
   "outputs": [],
   "source": [
    "def linear_coefs(X, X_ids, Y):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    X: N x d matrix of input features\n",
    "    Y: N x 1 matrix (column vector) of output response\n",
    "\n",
    "  Returns:\n",
    "    Beta: d x 1 matrix of linear coefficients\n",
    "  \"\"\"\n",
    "  # clf = LogisticRegression(random_state=0, solver='liblinear').fit(X, Y)\n",
    "  clf = SVC(random_state=0, tol=1e-5, kernel='linear').fit(X, Y)\n",
    "  # clf = LinearSVC(random_state=0, tol=1e-5).fit(X, Y)\n",
    "  support = (clf.support_, clf.n_support_)\n",
    "\n",
    "  def get_supp(support):\n",
    "      supps_, n_supps_ = support\n",
    "      supps_0 = supps_[:n_supps_[0]]\n",
    "      supps_1 = supps_[n_supps_[0]:]\n",
    "      return X_ids[supps_0], X_ids[supps_1]\n",
    "\n",
    "  support = get_supp(support)\n",
    "    \n",
    "  # clf = LogisticRegression(random_state=0).fit(X, Y)\n",
    "  print(clf.score(X, Y))\n",
    "  wgts = np.hstack((clf.intercept_[:,None], clf.coef_))\n",
    "  print(wgts.shape)\n",
    "  prd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.T)) > 0.5) *1.0\n",
    "  # print(np.mean(prd[:, 0]==Y))\n",
    "  return wgts, None\n",
    "  # beta = tf.linalg.solve(tf.matmul(tf.transpose(X),X), tf.matmul(tf.transpose(X), Y[:, None]))\n",
    "  # return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "sXCQKFR3zVf8"
   },
   "outputs": [],
   "source": [
    "def project_and_filter(X, dir, percentile=75):\n",
    "  projs = np.dot(X, dir)\n",
    "  thresh = np.percentile(projs, 100 - percentile)\n",
    "  filtered_idxs = projs >= thresh\n",
    "  return X[filtered_idxs], filtered_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "U6sPtWN-zvlP"
   },
   "outputs": [],
   "source": [
    "def get_models(X, Y, pca_projs, dirs, model, percentile=75):\n",
    "  #X_subsets = []\n",
    "  #data_ids = []\n",
    "  #Y_subsets = []\n",
    "  betas = []\n",
    "  supps = []\n",
    "  i = 0\n",
    "  for dir in dirs: # TODO: Vectorize\n",
    "    if i % 25 == 0: print(f\"Step {i}\")\n",
    "    X_sub, X_ids = project_and_filter(X, dir, percentile)\n",
    "    Y_sub = Y[X_ids]\n",
    "    print(X_sub.shape, X_ids.shape)\n",
    "    # print((X_sub@pca_projs).shape)\n",
    "    beta, supp = linear_coefs(get_rand_feats(X_sub@pca_projs, model), np.argwhere(X_ids), Y_sub)\n",
    "    # beta = linear_coefs(X_sub, Y_sub)\n",
    "\n",
    "    #X_subsets.append(X_sub)\n",
    "    #data_ids.append(X_ids)\n",
    "    #Y_subsets.append(Y_sub)\n",
    "    betas.append(beta)\n",
    "    supps.append(supp)\n",
    "    i += 1\n",
    "    if i == len(dirs) - 1: print(f\"Done\")\n",
    "\n",
    "  # cant do this because subsets of variable sizes\n",
    "  #X_subsets = np.array(X_subsets)\n",
    "  #data_ids = np.array(data_ids)\n",
    "  #Y_subsets = np.array(Y_subsets)\n",
    "  betas = np.array(betas)\n",
    "\n",
    "  return betas, supps\n",
    "  #return X_subsets, data_ids, Y_subsets, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, u, v = tf.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = [0.05, 0.1, 0.2, 0.5, 0.8]\n",
    "pca_projs = v[:, :int(X.shape[-1]*dims[1])]\n",
    "# pca_projs = v[:, :int(X.shape[-1]*dims[-2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([9768, 2048]), TensorShape([2048, 204]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, pca_projs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIvRCVks0XyQ",
    "outputId": "e9f47524-f11d-4cd9-9db1-de9eb10f0b29",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "(1956, 2048) (9768,)\n",
      "0.9667689161554193\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9508951406649616\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9687819856704196\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9452405322415558\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.970317297850563\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "Step 25\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.960613810741688\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9508700102354145\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9636828644501279\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9616368286445013\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9682702149437052\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9421699078812692\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "Step 50\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9493346980552713\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9687819856704196\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9549872122762149\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9631713554987212\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.951381780962129\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9477993858751279\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9672466734902764\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1957, 2048) (9768,)\n",
      "0.9565661727133368\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9667519181585678\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "Step 75\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9677584442169908\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9518935516888434\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9488229273285568\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.970317297850563\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9534526854219949\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9682702149437052\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9560102301790281\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.951381780962129\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "Step 100\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9534526854219949\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9468302658486708\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9493346980552713\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9698209718670077\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9672466734902764\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9503582395087001\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9472876151484135\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "Step 125\n",
      "(1955, 2048) (9768,)\n",
      "0.9667519181585678\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9493606138107417\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9503582395087001\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9550102249488752\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9657289002557545\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9483111566018424\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9365728900255754\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9595907928388747\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.959079283887468\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "Step 150\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9508951406649616\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9554987212276215\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9483111566018424\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9518935516888434\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9616368286445013\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "Step 175\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9718526100307062\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.951918158567775\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9544757033248081\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9518935516888434\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.946775844421699\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9483111566018424\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9560102301790281\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9698055271238485\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9611253196930947\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "Step 200\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9616368286445013\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9529170931422722\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "Step 225\n",
      "(1955, 2048) (9768,)\n",
      "0.9621483375959079\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9657289002557545\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9483111566018424\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9529411764705882\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9529170931422722\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9472876151484135\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9493346980552713\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "Step 250\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9708290685772774\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9672634271099744\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9483111566018424\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9687979539641943\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9483111566018424\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "Step 275\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9682702149437052\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.950383631713555\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9565217391304348\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9560102301790281\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9457523029682702\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9493346980552713\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9457800511508951\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9452405322415558\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "Step 300\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9447287615148413\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9493346980552713\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9488229273285568\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9749232343909928\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9488229273285568\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9539641943734015\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9508951406649616\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9560327198364008\n",
      "(1, 1837)\n",
      "Step 325\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9739263803680982\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9667689161554193\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9518935516888434\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9595907928388747\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9508700102354145\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9677584442169908\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.970317297850563\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.948849104859335\n",
      "(1, 1837)\n",
      "Step 350\n",
      "(1954, 2048) (9768,)\n",
      "0.9472876151484135\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9759467758444217\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9595907928388747\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.965711361310133\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9570332480818414\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9544524053224156\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.973899692937564\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9375639713408394\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9565439672801636\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "Step 375\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9385875127942682\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.970317297850563\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.970843989769821\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9432225063938618\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9682702149437052\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9590583418628454\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9559877175025588\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9468030690537085\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9529170931422722\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9692937563971341\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9677584442169908\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9570332480818414\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.960613810741688\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.946775844421699\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9472876151484135\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9554987212276215\n",
      "(1, 1837)\n",
      "Step 400\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9601023017902813\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9472876151484135\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9595907928388747\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9477993858751279\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9580562659846548\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9503582395087001\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9631713554987212\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9616171954964176\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9560102301790281\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "Step 425\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9677584442169908\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9503582395087001\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9524296675191816\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9508700102354145\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9554759467758445\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9508700102354145\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9713408393039918\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9529170931422722\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9518935516888434\n",
      "(1, 1837)\n",
      "Step 450\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9565217391304348\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9575230296827022\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9498721227621484\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9611054247697032\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9626407369498464\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9564994882292733\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9667349027635619\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9550102249488752\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9662231320368475\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9544757033248081\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9534288638689867\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9641760491299898\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.95496417604913\n",
      "(1, 1837)\n",
      "Step 475\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.962128966223132\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9687819856704196\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9539406345957011\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9498464687819856\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9626598465473146\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9692937563971341\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9646878198567042\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9595701125895599\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9672466734902764\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9600818833162743\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9493346980552713\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9580348004094166\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9651995905834186\n",
      "(1, 1837)\n",
      "(1956, 2048) (9768,)\n",
      "0.9514314928425358\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9570112589559877\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "Step 500\n",
      "(1954, 2048) (9768,)\n",
      "0.9631525076765609\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9524053224155579\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9677584442169908\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9585465711361311\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9508700102354145\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9636642784032753\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9477993858751279\n",
      "(1, 1837)\n",
      "(1954, 2048) (9768,)\n",
      "0.9672466734902764\n",
      "(1, 1837)\n",
      "(1955, 2048) (9768,)\n",
      "0.9611253196930947\n",
      "(1, 1837)\n",
      "Done\n",
      "(1954, 2048) (9768,)\n",
      "0.9605936540429887\n",
      "(1, 1837)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(74)\n",
    "X_prjs = np.array(X@pca_projs)\n",
    "# model = define_rand_feats(X_prjs, Dx[2])\n",
    "model = define_rand_feats(X_prjs, 1.5) # 1.5 best number of features, for any other lower or higher the id accuracy decreases\n",
    "\n",
    "N = 2**9    # ~ 8k\n",
    "# N = 2**2    # ~ 8k\n",
    "d = X.shape[-1]\n",
    "random_dirs = np.random.randn(N, d) # Maybe do the random directions in the random feature space??? Feel like that makes more sense\n",
    "\n",
    "random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "\n",
    "#X_subsets, data_ids, Y_subsets, betas = get_models(X, Y, random_dirs, Ws, bs, percentile=33)\n",
    "betas, supps = get_models(X, Y, pca_projs, random_dirs, model, percentile=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "7mIR1KmZaMyK"
   },
   "outputs": [],
   "source": [
    "# np.save('random_dirs-drug1.npy', random_dirs)\n",
    "# np.save('betas-drug1.npy', betas)\n",
    "# np.save('Ws-drug1.npy', model.Ws)\n",
    "\n",
    "np.save('random_dirs-drug_svm1.npy', random_dirs)\n",
    "np.save('betas-drug_svm1.npy', betas)\n",
    "np.save('Ws-drug_svm1.npy', model.Ws)\n",
    "\n",
    "# random_dirs = np.load('random_dirs.npy')\n",
    "# betas = np.load('betas.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"drug-supps1.npy\", \"wb\") as fp:\n",
    "    pickle.dump(supps, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "SYKgRDoVX-qO"
   },
   "outputs": [],
   "source": [
    "random_dirs = tf.constant(np.load('./random_dirs-drug1.npy'))\n",
    "betas = tf.squeeze(tf.constant(np.load('betas-drug1.npy')))\n",
    "model = define_rand_feats(X_prjs, 1.5)\n",
    "model.Ws = tf.constant(np.load('Ws-drug1.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QlEhsHL3VV3",
    "outputId": "6a74610c-2b52-4351-8e22-982a404874c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1837)\n",
      "(512, 2048)\n"
     ]
    }
   ],
   "source": [
    "betas = tf.squeeze(betas)\n",
    "print(betas.shape)\n",
    "random_dirs = tf.constant(random_dirs)\n",
    "print(random_dirs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_CpbBpp5jpF",
    "outputId": "4a7e7449-d9f9-40a7-a1b1-64f4c28f5d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.022034624074942756, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "var = tf.math.reduce_variance(betas, axis=0)\n",
    "mean_var = tf.reduce_mean(var)\n",
    "print(mean_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6jsrK0piF7WW",
    "outputId": "18b27514-bdd1-423f-b39d-48220eeec813"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517911975435005"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 0\n",
    "def softmax(X, wgts):\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((X.shape[0], 1)), X], axis=-1) @ wgts.numpy().T)) > 0.5) *1.0\n",
    "  return sd[:]\n",
    "\n",
    "X_sub, X_ids = project_and_filter(X, random_dirs[sample], 20)\n",
    "Y_sub = Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "# prd = softmax(X_sub, betas[sample])\n",
    "\n",
    "np.mean(prd == Y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5213089802130898"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_X = tf.cast(external_X, tf.float32)\n",
    "ex_Y = external_Y\n",
    "ex_X = (ex_X-mu_x)/sigma_x\n",
    "\n",
    "X_sub, X_ids = project_and_filter(ex_X, random_dirs[sample], 20)\n",
    "Y_sub = ex_Y[X_ids]\n",
    "prd = softmax(get_rand_feats(X_sub@pca_projs, model), betas[sample])\n",
    "# prd = softmax(X_sub, betas[sample])\n",
    "\n",
    "(prd == Y_sub).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARSClVlo7Jlq"
   },
   "source": [
    "## Should test Betas performance first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_bklenRt7L2Z"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers, models\n",
    "\n",
    "# beta_dim = betas.shape[-1]\n",
    "# input_dir_dim = random_dirs.shape[-1]\n",
    "# latent_dim = 64\n",
    "\n",
    "# # Encoder\n",
    "# beta_input = layers.Input(shape=(beta_dim,))\n",
    "# beta_x = layers.Dense(256, activation=tf.nn.elu)(beta_input)\n",
    "# dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "# encoder_inputs = layers.Concatenate()([beta_x, dir_input])\n",
    "# x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(x)\n",
    "# # x = layers.Dense(32, activation=tf.nn.elu)(x)\n",
    "# z_mean = layers.Dense(latent_dim)(x)\n",
    "# z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "# def sampling(args):\n",
    "#   z_mean, z_log_var = args\n",
    "#   eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "#   return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "# z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "# ### Using direction in Decoder is weird\n",
    "# ### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# # Decoder\n",
    "# latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "# decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "# decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# # x = layers.Dense(32, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(64, activation=tf.nn.elu)(decoder_inputs)\n",
    "# # x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "# # x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# # Instantiate model\n",
    "# encoder = models.Model([beta_input, dir_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "# decoder = models.Model([latent_inputs, decoder_dir_input], beta_output, name=\"decoder\")\n",
    "\n",
    "# # VAE\n",
    "# outputs = decoder([encoder([beta_input, dir_input])[2], dir_input])\n",
    "# vae = models.Model([beta_input, dir_input], outputs, name=\"vae\")\n",
    "# vae.encoder = encoder\n",
    "# vae.decoder = decoder\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "beta_dim = betas.shape[-1]\n",
    "input_dir_dim = random_dirs.shape[-1]\n",
    "latent_dim = 32\n",
    "\n",
    "# Encoder\n",
    "beta_input = layers.Input(shape=(beta_dim,))\n",
    "beta_x = layers.Dense(512, activation=tf.nn.elu)(beta_input)\n",
    "dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "encoder_inputs = layers.Concatenate()([beta_x, dir_input])\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(encoder_inputs)\n",
    "# x = layers.Dense(256, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(128, activation=tf.nn.elu)(encoder_inputs)\n",
    "x = layers.Dense(32, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(8, activation=tf.nn.elu)(x)\n",
    "z_mean = layers.Dense(latent_dim)(x)\n",
    "z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "def sampling(args):\n",
    "  z_mean, z_log_var = args\n",
    "  eps = tf.random.normal(shape=tf.shape(z_mean))\n",
    "  return z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "\n",
    "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
    "\n",
    "\n",
    "### Using direction in Decoder is weird\n",
    "### Likely just train VAE solely on betas with directions\n",
    "\n",
    "\n",
    "# Decoder\n",
    "latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "decoder_dir_input = layers.Input(shape=(input_dir_dim,))\n",
    "decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# decoder_inputs = layers.Concatenate()([latent_inputs, decoder_dir_input])\n",
    "# x = layers.Dense(8, activation=tf.nn.elu)(decoder_inputs)\n",
    "x = layers.Dense(32, activation=tf.nn.elu)(decoder_inputs)\n",
    "# x = layers.Dense(128, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(256, activation=tf.nn.elu)(x)\n",
    "x = layers.Dense(512, activation=tf.nn.elu)(x)\n",
    "# x = layers.Dense(1024, activation=tf.nn.elu)(x)\n",
    "beta_output = layers.Dense(beta_dim)(x)\n",
    "\n",
    "# Instantiate model\n",
    "encoder = models.Model([beta_input, dir_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
    "decoder = models.Model([latent_inputs, decoder_dir_input], beta_output, name=\"decoder\")\n",
    "\n",
    "# VAE\n",
    "outputs = decoder([encoder([beta_input, dir_input])[2], dir_input])\n",
    "vae = models.Model([beta_input, dir_input], outputs, name=\"vae\")\n",
    "vae.encoder = encoder\n",
    "vae.decoder = decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "GEVOITgr-mEL"
   },
   "outputs": [],
   "source": [
    "# def vae_loss(inputs, outputs, z_mean, z_log_var, reg=1.0):\n",
    "#   # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "#   recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs, dtype=tf.float32), axis=-1)[0] *\n",
    "#                                               tf.linalg.normalize(outputs, axis=-1)[0], axis=-1))\n",
    "#   kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "#   total_loss = recon_loss + 0.001 * kl_loss\n",
    "#   return total_loss, recon_loss, kl_loss\n",
    "\n",
    "\n",
    "def vae_loss(inputs, outputs, z_mean, z_log_var, reg=0.002):\n",
    "  # recon_loss = tf.reduce_mean(tf.reduce_sum(tf.square(inputs - outputs), axis=-1))\n",
    "  intercp_loss = tf.reduce_mean(tf.abs(tf.cast(inputs[:, :1], dtype=tf.float32) - outputs[:, :1]))\n",
    "  recon_loss = tf.reduce_mean(1-tf.reduce_sum(tf.linalg.normalize(tf.cast(inputs[:, 1:], dtype=tf.float32), axis=-1)[0] *\n",
    "                                              tf.linalg.normalize(outputs[:, 1:], axis=-1)[0], axis=-1))\n",
    "  kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1))\n",
    "  total_loss = recon_loss + intercp_loss + reg * kl_loss\n",
    "  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "bjyT0zzy_Q8E"
   },
   "outputs": [],
   "source": [
    "# opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# def train_step(model, inputs, dir_inputs):\n",
    "#   with tf.GradientTape() as tape:\n",
    "#     z_mean, z_log_var, z = model.encoder([inputs, dir_inputs])\n",
    "#     outputs = model.decoder([z, dir_inputs])\n",
    "#     total_loss, recon_loss, kl_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
    "#   grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "#   opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "#   return total_loss, recon_loss, kl_loss\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# def regul(epoch):\n",
    "#     if epoch%10<=5:\n",
    "#         return epoch*0.2\n",
    "\n",
    "def train_step(model, inputs, dir_inputs, epoch=None):\n",
    "  with tf.GradientTape() as tape:\n",
    "    z_mean, z_log_var, z = model.encoder([inputs, dir_inputs])\n",
    "    # outputs = model.decoder([z, dir_inputs])\n",
    "    outputs = model.decoder([z, dir_inputs])\n",
    "    \n",
    "    total_loss, recon_loss, intercp_loss, kl_loss = vae_loss(inputs, outputs, z_mean, z_log_var)\n",
    "  grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "  return total_loss, recon_loss, intercp_loss, kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "usu_v5FxBgmn"
   },
   "outputs": [],
   "source": [
    "def batch(betas, dirs, batch_size):\n",
    "  num_samples = betas.shape[0]\n",
    "  indices = np.arange(num_samples)\n",
    "  np.random.shuffle(indices)\n",
    "  betas = np.array(betas)[indices]\n",
    "  dirs = np.array(dirs)[indices]\n",
    "  for i in range(0, betas.shape[0], batch_size):\n",
    "    yield betas[i:i+batch_size], dirs[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rhn1yqRa_UBV",
    "outputId": "8dd8c722-dc02-4ed6-f74c-cd21085979b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-29 16:36:25.094225: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0xa957770 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-29 16:36:25.094293: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA TITAN RTX, Compute Capability 7.5\n",
      "2024-04-29 16:36:25.103621: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-29 16:36:25.230654: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-04-29 16:36:25.325716: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7f3d08279a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7f3d08279a60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Step 0: loss = 1.031902551651001, recon_loss = 1.0005407333374023, 0.030233047902584076, kl_loss = 0.5643701553344727\n",
      "\n",
      "Epoch 1\n",
      "Step 0: loss = 0.5398221611976624, recon_loss = 0.4598732590675354, 0.050131745636463165, kl_loss = 14.90858268737793\n",
      "\n",
      "Epoch 2\n",
      "Step 0: loss = 0.4789101183414459, recon_loss = 0.4323146343231201, 0.027332181110978127, kl_loss = 9.63165283203125\n",
      "\n",
      "Epoch 3\n",
      "Step 0: loss = 0.46514639258384705, recon_loss = 0.43041133880615234, 0.023049436509609222, kl_loss = 5.842798233032227\n",
      "\n",
      "Epoch 4\n",
      "Step 0: loss = 0.46472030878067017, recon_loss = 0.42605066299438477, 0.027711644768714905, kl_loss = 5.479010581970215\n",
      "\n",
      "Epoch 5\n",
      "Step 0: loss = 0.4423600137233734, recon_loss = 0.41747426986694336, 0.013072353787720203, kl_loss = 5.906695365905762\n",
      "\n",
      "Epoch 6\n",
      "Step 0: loss = 0.4413948059082031, recon_loss = 0.422066867351532, 0.010247493162751198, kl_loss = 4.540215969085693\n",
      "\n",
      "Epoch 7\n",
      "Step 0: loss = 0.45119747519493103, recon_loss = 0.420248806476593, 0.022686507552862167, kl_loss = 4.131078720092773\n",
      "\n",
      "Epoch 8\n",
      "Step 0: loss = 0.4432313144207001, recon_loss = 0.420820951461792, 0.01371864415705204, kl_loss = 4.345867156982422\n",
      "\n",
      "Epoch 9\n",
      "Step 0: loss = 0.44624412059783936, recon_loss = 0.42258065938949585, 0.018035590648651123, kl_loss = 2.813932418823242\n",
      "\n",
      "Epoch 10\n",
      "Step 0: loss = 0.4359058141708374, recon_loss = 0.4166889190673828, 0.011336619965732098, kl_loss = 3.940138339996338\n",
      "\n",
      "Epoch 11\n",
      "Step 0: loss = 0.43933019042015076, recon_loss = 0.42198652029037476, 0.010752849280834198, kl_loss = 3.2954020500183105\n",
      "\n",
      "Epoch 12\n",
      "Step 0: loss = 0.449293851852417, recon_loss = 0.4223220944404602, 0.010036643594503403, kl_loss = 8.467557907104492\n",
      "\n",
      "Epoch 13\n",
      "Step 0: loss = 0.43717697262763977, recon_loss = 0.41752326488494873, 0.008672531694173813, kl_loss = 5.490591049194336\n",
      "\n",
      "Epoch 14\n",
      "Step 0: loss = 0.4441981017589569, recon_loss = 0.41644713282585144, 0.01962055265903473, kl_loss = 4.065195083618164\n",
      "\n",
      "Epoch 15\n",
      "Step 0: loss = 0.4393223822116852, recon_loss = 0.4211030602455139, 0.01270503643900156, kl_loss = 2.7571470737457275\n",
      "\n",
      "Epoch 16\n",
      "Step 0: loss = 0.42458730936050415, recon_loss = 0.4117942154407501, 0.007169762626290321, kl_loss = 2.811664342880249\n",
      "\n",
      "Epoch 17\n",
      "Step 0: loss = 0.43204978108406067, recon_loss = 0.41564786434173584, 0.009448463097214699, kl_loss = 3.4767210483551025\n",
      "\n",
      "Epoch 18\n",
      "Step 0: loss = 0.4260396957397461, recon_loss = 0.412246972322464, 0.008471338078379631, kl_loss = 2.6606967449188232\n",
      "\n",
      "Epoch 19\n",
      "Step 0: loss = 0.4240041673183441, recon_loss = 0.41292399168014526, 0.00638830941170454, kl_loss = 2.345937490463257\n",
      "\n",
      "Epoch 20\n",
      "Step 0: loss = 0.42314276099205017, recon_loss = 0.41143563389778137, 0.00780761893838644, kl_loss = 1.9497590065002441\n",
      "\n",
      "Epoch 21\n",
      "Step 0: loss = 0.41435474157333374, recon_loss = 0.4054229259490967, 0.00521410396322608, kl_loss = 1.858856201171875\n",
      "\n",
      "Epoch 22\n",
      "Step 0: loss = 0.4189525246620178, recon_loss = 0.40916383266448975, 0.0062599582597613335, kl_loss = 1.764377236366272\n",
      "\n",
      "Epoch 23\n",
      "Step 0: loss = 0.4161168038845062, recon_loss = 0.40214329957962036, 0.01093306764960289, kl_loss = 1.5202102661132812\n",
      "\n",
      "Epoch 24\n",
      "Step 0: loss = 0.40424999594688416, recon_loss = 0.392336905002594, 0.008502975106239319, kl_loss = 1.7050589323043823\n",
      "\n",
      "Epoch 25\n",
      "Step 0: loss = 0.40286141633987427, recon_loss = 0.39287883043289185, 0.006568985991179943, kl_loss = 1.7067950963974\n",
      "\n",
      "Epoch 26\n",
      "Step 0: loss = 0.39633709192276, recon_loss = 0.38618505001068115, 0.0064706457778811455, kl_loss = 1.8407011032104492\n",
      "\n",
      "Epoch 27\n",
      "Step 0: loss = 0.3878847360610962, recon_loss = 0.37696003913879395, 0.007086005061864853, kl_loss = 1.9193496704101562\n",
      "\n",
      "Epoch 28\n",
      "Step 0: loss = 0.3809984028339386, recon_loss = 0.36955708265304565, 0.007315616123378277, kl_loss = 2.062856674194336\n",
      "\n",
      "Epoch 29\n",
      "Step 0: loss = 0.3727850615978241, recon_loss = 0.36275342106819153, 0.005951972678303719, kl_loss = 2.039829730987549\n",
      "\n",
      "Epoch 30\n",
      "Step 0: loss = 0.3699643611907959, recon_loss = 0.359622597694397, 0.006264641880989075, kl_loss = 2.038571357727051\n",
      "\n",
      "Epoch 31\n",
      "Step 0: loss = 0.363342821598053, recon_loss = 0.35640597343444824, 0.003650928381830454, kl_loss = 1.6429558992385864\n",
      "\n",
      "Epoch 32\n",
      "Step 0: loss = 0.35510551929473877, recon_loss = 0.34727558493614197, 0.004057493060827255, kl_loss = 1.8862221240997314\n",
      "\n",
      "Epoch 33\n",
      "Step 0: loss = 0.3586341440677643, recon_loss = 0.35017985105514526, 0.0050970627926290035, kl_loss = 1.6786086559295654\n",
      "\n",
      "Epoch 34\n",
      "Step 0: loss = 0.3472104072570801, recon_loss = 0.3399997353553772, 0.0036198147572577, kl_loss = 1.795426368713379\n",
      "\n",
      "Epoch 35\n",
      "Step 0: loss = 0.336438924074173, recon_loss = 0.3302798569202423, 0.0029989611357450485, kl_loss = 1.5800557136535645\n",
      "\n",
      "Epoch 36\n",
      "Step 0: loss = 0.3345234990119934, recon_loss = 0.32793962955474854, 0.003939525689929724, kl_loss = 1.3221662044525146\n",
      "\n",
      "Epoch 37\n",
      "Step 0: loss = 0.3371717929840088, recon_loss = 0.33080625534057617, 0.003726957133039832, kl_loss = 1.3192830085754395\n",
      "\n",
      "Epoch 38\n",
      "Step 0: loss = 0.336080938577652, recon_loss = 0.32747882604599, 0.006041949149221182, kl_loss = 1.2800872325897217\n",
      "\n",
      "Epoch 39\n",
      "Step 0: loss = 0.32244226336479187, recon_loss = 0.3158424198627472, 0.004462620243430138, kl_loss = 1.0686002969741821\n",
      "\n",
      "Epoch 40\n",
      "Step 0: loss = 0.3261575996875763, recon_loss = 0.3186674118041992, 0.005442306399345398, kl_loss = 1.0239354372024536\n",
      "\n",
      "Epoch 41\n",
      "Step 0: loss = 0.3254695236682892, recon_loss = 0.32018589973449707, 0.003437991254031658, kl_loss = 0.9228179454803467\n",
      "\n",
      "Epoch 42\n",
      "Step 0: loss = 0.3182103931903839, recon_loss = 0.31161364912986755, 0.00479639507830143, kl_loss = 0.9001845121383667\n",
      "\n",
      "Epoch 43\n",
      "Step 0: loss = 0.3129729628562927, recon_loss = 0.30852049589157104, 0.0029029827564954758, kl_loss = 0.7747427225112915\n",
      "\n",
      "Epoch 44\n",
      "Step 0: loss = 0.30782249569892883, recon_loss = 0.30384376645088196, 0.0028091061394661665, kl_loss = 0.5848079919815063\n",
      "\n",
      "Epoch 45\n",
      "Step 0: loss = 0.3093229830265045, recon_loss = 0.3045809864997864, 0.003573313821107149, kl_loss = 0.5843325257301331\n",
      "\n",
      "Epoch 46\n",
      "Step 0: loss = 0.3072328269481659, recon_loss = 0.3028125464916229, 0.0033219908364117146, kl_loss = 0.549135148525238\n",
      "\n",
      "Epoch 47\n",
      "Step 0: loss = 0.310040146112442, recon_loss = 0.3063783645629883, 0.0025833966210484505, kl_loss = 0.5391913652420044\n",
      "\n",
      "Epoch 48\n",
      "Step 0: loss = 0.301110178232193, recon_loss = 0.2960624098777771, 0.0040484205819666386, kl_loss = 0.49967488646507263\n",
      "\n",
      "Epoch 49\n",
      "Step 0: loss = 0.30379587411880493, recon_loss = 0.2998136281967163, 0.003078985493630171, kl_loss = 0.45162397623062134\n",
      "\n",
      "Epoch 50\n",
      "Step 0: loss = 0.29790645837783813, recon_loss = 0.2938547134399414, 0.003178883111104369, kl_loss = 0.436420738697052\n",
      "\n",
      "Epoch 51\n",
      "Step 0: loss = 0.3002007007598877, recon_loss = 0.2967871129512787, 0.002740801777690649, kl_loss = 0.33638668060302734\n",
      "\n",
      "Epoch 52\n",
      "Step 0: loss = 0.2997749447822571, recon_loss = 0.2951590418815613, 0.004067590925842524, kl_loss = 0.27415788173675537\n",
      "\n",
      "Epoch 53\n",
      "Step 0: loss = 0.2974802255630493, recon_loss = 0.2941425144672394, 0.002740301191806793, kl_loss = 0.29871100187301636\n",
      "\n",
      "Epoch 54\n",
      "Step 0: loss = 0.2946283519268036, recon_loss = 0.29114097356796265, 0.0028872976545244455, kl_loss = 0.3000352382659912\n",
      "\n",
      "Epoch 55\n",
      "Step 0: loss = 0.29762324690818787, recon_loss = 0.2934448719024658, 0.00360198225826025, kl_loss = 0.2882101833820343\n",
      "\n",
      "Epoch 56\n",
      "Step 0: loss = 0.29880157113075256, recon_loss = 0.29355674982070923, 0.004741188138723373, kl_loss = 0.25181883573532104\n",
      "\n",
      "Epoch 57\n",
      "Step 0: loss = 0.2978178858757019, recon_loss = 0.29346680641174316, 0.0039176903665065765, kl_loss = 0.2166903018951416\n",
      "\n",
      "Epoch 58\n",
      "Step 0: loss = 0.29627737402915955, recon_loss = 0.29118335247039795, 0.004705812782049179, kl_loss = 0.1941053867340088\n",
      "\n",
      "Epoch 59\n",
      "Step 0: loss = 0.2942018210887909, recon_loss = 0.2900691330432892, 0.00370550318621099, kl_loss = 0.21359077095985413\n",
      "\n",
      "Epoch 60\n",
      "Step 0: loss = 0.2933785617351532, recon_loss = 0.28927817940711975, 0.003757895901799202, kl_loss = 0.17124122381210327\n",
      "\n",
      "Epoch 61\n",
      "Step 0: loss = 0.29020076990127563, recon_loss = 0.2870832085609436, 0.0027692823205143213, kl_loss = 0.17413091659545898\n",
      "\n",
      "Epoch 62\n",
      "Step 0: loss = 0.28989332914352417, recon_loss = 0.28694775700569153, 0.0026436292100697756, kl_loss = 0.15098148584365845\n",
      "\n",
      "Epoch 63\n",
      "Step 0: loss = 0.29082468152046204, recon_loss = 0.2873803973197937, 0.0031880601309239864, kl_loss = 0.12810736894607544\n",
      "\n",
      "Epoch 64\n",
      "Step 0: loss = 0.2910471558570862, recon_loss = 0.28686875104904175, 0.003926989622414112, kl_loss = 0.12570618093013763\n",
      "\n",
      "Epoch 65\n",
      "Step 0: loss = 0.28339844942092896, recon_loss = 0.27962666749954224, 0.0035457643680274487, kl_loss = 0.11300447583198547\n",
      "\n",
      "Epoch 66\n",
      "Step 0: loss = 0.2874583601951599, recon_loss = 0.2834564745426178, 0.0038112723268568516, kl_loss = 0.09530596435070038\n",
      "\n",
      "Epoch 67\n",
      "Step 0: loss = 0.2871381640434265, recon_loss = 0.2840820550918579, 0.002858188468962908, kl_loss = 0.09896311908960342\n",
      "\n",
      "Epoch 68\n",
      "Step 0: loss = 0.28763261437416077, recon_loss = 0.2844218611717224, 0.0030215950682759285, kl_loss = 0.09457993507385254\n",
      "\n",
      "Epoch 69\n",
      "Step 0: loss = 0.28379786014556885, recon_loss = 0.2799587845802307, 0.0036729376297444105, kl_loss = 0.08306949585676193\n",
      "\n",
      "Epoch 70\n",
      "Step 0: loss = 0.28247204422950745, recon_loss = 0.2787138521671295, 0.003624153323471546, kl_loss = 0.06702889502048492\n",
      "\n",
      "Epoch 71\n",
      "Step 0: loss = 0.29215312004089355, recon_loss = 0.2889993190765381, 0.003034917637705803, kl_loss = 0.05943965166807175\n",
      "\n",
      "Epoch 72\n",
      "Step 0: loss = 0.28715306520462036, recon_loss = 0.28412848711013794, 0.0029409416019916534, kl_loss = 0.041819605976343155\n",
      "\n",
      "Epoch 73\n",
      "Step 0: loss = 0.2846163213253021, recon_loss = 0.28041940927505493, 0.004102475941181183, kl_loss = 0.04721599817276001\n",
      "\n",
      "Epoch 74\n",
      "Step 0: loss = 0.286916047334671, recon_loss = 0.28386566042900085, 0.00297206686809659, kl_loss = 0.03915296494960785\n",
      "\n",
      "Epoch 75\n",
      "Step 0: loss = 0.2818768620491028, recon_loss = 0.2788775861263275, 0.002922199433669448, kl_loss = 0.03854070603847504\n",
      "\n",
      "Epoch 76\n",
      "Step 0: loss = 0.28646916151046753, recon_loss = 0.28286096453666687, 0.0035243150778114796, kl_loss = 0.04194745048880577\n",
      "\n",
      "Epoch 77\n",
      "Step 0: loss = 0.28568774461746216, recon_loss = 0.2830883860588074, 0.002533760154619813, kl_loss = 0.03280311077833176\n",
      "\n",
      "Epoch 78\n",
      "Step 0: loss = 0.2835838496685028, recon_loss = 0.28087204694747925, 0.0026569520123302937, kl_loss = 0.027420487254858017\n",
      "\n",
      "Epoch 79\n",
      "Step 0: loss = 0.28397929668426514, recon_loss = 0.2808009386062622, 0.003110532183200121, kl_loss = 0.03391087055206299\n",
      "\n",
      "Epoch 80\n",
      "Step 0: loss = 0.2869667410850525, recon_loss = 0.2838958501815796, 0.003012982662767172, kl_loss = 0.0289519801735878\n",
      "\n",
      "Epoch 81\n",
      "Step 0: loss = 0.28123506903648376, recon_loss = 0.2785390615463257, 0.0026530995965003967, kl_loss = 0.021460721269249916\n",
      "\n",
      "Epoch 82\n",
      "Step 0: loss = 0.28487682342529297, recon_loss = 0.28190484642982483, 0.00293187377974391, kl_loss = 0.020059965550899506\n",
      "\n",
      "Epoch 83\n",
      "Step 0: loss = 0.28030797839164734, recon_loss = 0.27642256021499634, 0.00384435523301363, kl_loss = 0.02053263783454895\n",
      "\n",
      "Epoch 84\n",
      "Step 0: loss = 0.2838767170906067, recon_loss = 0.28165069222450256, 0.002187675330787897, kl_loss = 0.019181624054908752\n",
      "\n",
      "Epoch 85\n",
      "Step 0: loss = 0.2825450003147125, recon_loss = 0.28059983253479004, 0.0019024813082069159, kl_loss = 0.021335970610380173\n",
      "\n",
      "Epoch 86\n",
      "Step 0: loss = 0.2810233533382416, recon_loss = 0.2784910798072815, 0.002497295616194606, kl_loss = 0.017499705776572227\n",
      "\n",
      "Epoch 87\n",
      "Step 0: loss = 0.2836461663246155, recon_loss = 0.28084003925323486, 0.0027642869390547276, kl_loss = 0.020922545343637466\n",
      "\n",
      "Epoch 88\n",
      "Step 0: loss = 0.2830196022987366, recon_loss = 0.28102248907089233, 0.0019584845285862684, kl_loss = 0.01930532231926918\n",
      "\n",
      "Epoch 89\n",
      "Step 0: loss = 0.28328844904899597, recon_loss = 0.2806268632411957, 0.0026312980335205793, kl_loss = 0.015146899968385696\n",
      "\n",
      "Epoch 90\n",
      "Step 0: loss = 0.28282201290130615, recon_loss = 0.2802658975124359, 0.00252506323158741, kl_loss = 0.015527153387665749\n",
      "\n",
      "Epoch 91\n",
      "Step 0: loss = 0.2827986776828766, recon_loss = 0.279819130897522, 0.002949686488136649, kl_loss = 0.014927037991583347\n",
      "\n",
      "Epoch 92\n",
      "Step 0: loss = 0.2820904850959778, recon_loss = 0.279555082321167, 0.002504467498511076, kl_loss = 0.015471664257347584\n",
      "\n",
      "Epoch 93\n",
      "Step 0: loss = 0.27915892004966736, recon_loss = 0.2763819992542267, 0.002746385522186756, kl_loss = 0.015270235016942024\n",
      "\n",
      "Epoch 94\n",
      "Step 0: loss = 0.2857299745082855, recon_loss = 0.2829965353012085, 0.002702533034607768, kl_loss = 0.015451500192284584\n",
      "\n",
      "Epoch 95\n",
      "Step 0: loss = 0.28319624066352844, recon_loss = 0.2814635634422302, 0.0017099452670663595, kl_loss = 0.011365100741386414\n",
      "\n",
      "Epoch 96\n",
      "Step 0: loss = 0.28186866641044617, recon_loss = 0.27991539239883423, 0.0019258372485637665, kl_loss = 0.013729747384786606\n",
      "\n",
      "Epoch 97\n",
      "Step 0: loss = 0.28117868304252625, recon_loss = 0.27880698442459106, 0.0023457519710063934, kl_loss = 0.012981862761080265\n",
      "\n",
      "Epoch 98\n",
      "Step 0: loss = 0.2865156829357147, recon_loss = 0.28315722942352295, 0.0033313403837382793, kl_loss = 0.013565479777753353\n",
      "\n",
      "Epoch 99\n",
      "Step 0: loss = 0.28001993894577026, recon_loss = 0.2773281931877136, 0.002659181598573923, kl_loss = 0.016292348504066467\n",
      "\n",
      "Epoch 100\n",
      "Step 0: loss = 0.2793605923652649, recon_loss = 0.27659815549850464, 0.0027339491061866283, kl_loss = 0.014240480959415436\n",
      "\n",
      "Epoch 101\n",
      "Step 0: loss = 0.2832081913948059, recon_loss = 0.2799695134162903, 0.0032130584586411715, kl_loss = 0.012819933705031872\n",
      "\n",
      "Epoch 102\n",
      "Step 0: loss = 0.2777950167655945, recon_loss = 0.2753865718841553, 0.0023835194297134876, kl_loss = 0.012454942800104618\n",
      "\n",
      "Epoch 103\n",
      "Step 0: loss = 0.2779080867767334, recon_loss = 0.2756623327732086, 0.0022217538207769394, kl_loss = 0.011988404206931591\n",
      "\n",
      "Epoch 104\n",
      "Step 0: loss = 0.27685290575027466, recon_loss = 0.27455389499664307, 0.0022791631054133177, kl_loss = 0.009918926283717155\n",
      "\n",
      "Epoch 105\n",
      "Step 0: loss = 0.27982398867607117, recon_loss = 0.27732062339782715, 0.0024818778038024902, kl_loss = 0.010738624259829521\n",
      "\n",
      "Epoch 106\n",
      "Step 0: loss = 0.2801285982131958, recon_loss = 0.2778782248497009, 0.002230845158919692, kl_loss = 0.009762151166796684\n",
      "\n",
      "Epoch 107\n",
      "Step 0: loss = 0.2852727174758911, recon_loss = 0.2818903625011444, 0.003352037165313959, kl_loss = 0.015158016234636307\n",
      "\n",
      "Epoch 108\n",
      "Step 0: loss = 0.2860413193702698, recon_loss = 0.28327205777168274, 0.0027503850869834423, kl_loss = 0.009438639506697655\n",
      "\n",
      "Epoch 109\n",
      "Step 0: loss = 0.2793697416782379, recon_loss = 0.27694663405418396, 0.002405688166618347, kl_loss = 0.008714860305190086\n",
      "\n",
      "Epoch 110\n",
      "Step 0: loss = 0.2763769030570984, recon_loss = 0.2737429738044739, 0.002608603099361062, kl_loss = 0.012672776356339455\n",
      "\n",
      "Epoch 111\n",
      "Step 0: loss = 0.2845342457294464, recon_loss = 0.28197595477104187, 0.0025401406455785036, kl_loss = 0.009071581065654755\n",
      "\n",
      "Epoch 112\n",
      "Step 0: loss = 0.28328901529312134, recon_loss = 0.2803857922554016, 0.002886043395847082, kl_loss = 0.008583705872297287\n",
      "\n",
      "Epoch 113\n",
      "Step 0: loss = 0.2796260416507721, recon_loss = 0.277396559715271, 0.0022092843428254128, kl_loss = 0.01010502316057682\n",
      "\n",
      "Epoch 114\n",
      "Step 0: loss = 0.2808452546596527, recon_loss = 0.27824199199676514, 0.0025852001272141933, kl_loss = 0.009034289047122002\n",
      "\n",
      "Epoch 115\n",
      "Step 0: loss = 0.2760652005672455, recon_loss = 0.27332690358161926, 0.0027165154460817575, kl_loss = 0.010889234021306038\n",
      "\n",
      "Epoch 116\n",
      "Step 0: loss = 0.27795618772506714, recon_loss = 0.275554358959198, 0.0023769731633365154, kl_loss = 0.012434011325240135\n",
      "\n",
      "Epoch 117\n",
      "Step 0: loss = 0.282722532749176, recon_loss = 0.2800138592720032, 0.0026913813780993223, kl_loss = 0.008637567982077599\n",
      "\n",
      "Epoch 118\n",
      "Step 0: loss = 0.2767249345779419, recon_loss = 0.2739606499671936, 0.002749806037172675, kl_loss = 0.007241492159664631\n",
      "\n",
      "Epoch 119\n",
      "Step 0: loss = 0.2812994122505188, recon_loss = 0.279461145401001, 0.0018221022328361869, kl_loss = 0.008080287836492062\n",
      "\n",
      "Epoch 120\n",
      "Step 0: loss = 0.2768198549747467, recon_loss = 0.27456337213516235, 0.002240820089355111, kl_loss = 0.007837790064513683\n",
      "\n",
      "Epoch 121\n",
      "Step 0: loss = 0.2808569371700287, recon_loss = 0.2781441807746887, 0.002694710623472929, kl_loss = 0.009033219888806343\n",
      "\n",
      "Epoch 122\n",
      "Step 0: loss = 0.27809667587280273, recon_loss = 0.2754356861114502, 0.0026381800416857004, kl_loss = 0.011406445875763893\n",
      "\n",
      "Epoch 123\n",
      "Step 0: loss = 0.28093263506889343, recon_loss = 0.2781181335449219, 0.0027977239806205034, kl_loss = 0.008383925072848797\n",
      "\n",
      "Epoch 124\n",
      "Step 0: loss = 0.28292545676231384, recon_loss = 0.2800012230873108, 0.002907820977270603, kl_loss = 0.008211220614612103\n",
      "\n",
      "Epoch 125\n",
      "Step 0: loss = 0.27943116426467896, recon_loss = 0.2761317789554596, 0.0032797143794596195, kl_loss = 0.009836878627538681\n",
      "\n",
      "Epoch 126\n",
      "Step 0: loss = 0.2808780074119568, recon_loss = 0.277825266122818, 0.003017175244167447, kl_loss = 0.017783448100090027\n",
      "\n",
      "Epoch 127\n",
      "Step 0: loss = 0.2810029685497284, recon_loss = 0.27887117862701416, 0.0021040006540715694, kl_loss = 0.013893963769078255\n",
      "\n",
      "Epoch 128\n",
      "Step 0: loss = 0.27929678559303284, recon_loss = 0.2772420048713684, 0.002035353798419237, kl_loss = 0.009711206890642643\n",
      "\n",
      "Epoch 129\n",
      "Step 0: loss = 0.27827826142311096, recon_loss = 0.2762987017631531, 0.0019668489694595337, kl_loss = 0.0063680438324809074\n",
      "\n",
      "Epoch 130\n",
      "Step 0: loss = 0.2787723243236542, recon_loss = 0.2771090865135193, 0.0016436739824712276, kl_loss = 0.009770266711711884\n",
      "\n",
      "Epoch 131\n",
      "Step 0: loss = 0.27633193135261536, recon_loss = 0.2743905782699585, 0.0019265939481556416, kl_loss = 0.007381180301308632\n",
      "\n",
      "Epoch 132\n",
      "Step 0: loss = 0.28205111622810364, recon_loss = 0.2789101004600525, 0.0031252847984433174, kl_loss = 0.007868682034313679\n",
      "\n",
      "Epoch 133\n",
      "Step 0: loss = 0.28023475408554077, recon_loss = 0.27821487188339233, 0.0019935357850044966, kl_loss = 0.013174734078347683\n",
      "\n",
      "Epoch 134\n",
      "Step 0: loss = 0.2772083878517151, recon_loss = 0.27498960494995117, 0.0022056451998651028, kl_loss = 0.00657634437084198\n",
      "\n",
      "Epoch 135\n",
      "Step 0: loss = 0.2782403528690338, recon_loss = 0.27599823474884033, 0.0022236064542084932, kl_loss = 0.009250273928046227\n",
      "\n",
      "Epoch 136\n",
      "Step 0: loss = 0.2819501757621765, recon_loss = 0.27908530831336975, 0.0028482074849307537, kl_loss = 0.008328929543495178\n",
      "\n",
      "Epoch 137\n",
      "Step 0: loss = 0.2820034623146057, recon_loss = 0.2800310254096985, 0.0019598742946982384, kl_loss = 0.006295622326433659\n",
      "\n",
      "Epoch 138\n",
      "Step 0: loss = 0.27865612506866455, recon_loss = 0.27667132019996643, 0.0019725069869309664, kl_loss = 0.00615295022726059\n",
      "\n",
      "Epoch 139\n",
      "Step 0: loss = 0.2768470048904419, recon_loss = 0.2743299901485443, 0.0025033780839294195, kl_loss = 0.006821179762482643\n",
      "\n",
      "Epoch 140\n",
      "Step 0: loss = 0.2770049273967743, recon_loss = 0.2745542824268341, 0.0024376732762902975, kl_loss = 0.006484110839664936\n",
      "\n",
      "Epoch 141\n",
      "Step 0: loss = 0.28111758828163147, recon_loss = 0.27845731377601624, 0.0026337397284805775, kl_loss = 0.013269271701574326\n",
      "\n",
      "Epoch 142\n",
      "Step 0: loss = 0.27827510237693787, recon_loss = 0.27611416578292847, 0.0021498489659279585, kl_loss = 0.005539636127650738\n",
      "\n",
      "Epoch 143\n",
      "Step 0: loss = 0.27749210596084595, recon_loss = 0.27560678124427795, 0.0018675215542316437, kl_loss = 0.008900722488760948\n",
      "\n",
      "Epoch 144\n",
      "Step 0: loss = 0.2798093259334564, recon_loss = 0.2781293988227844, 0.0016706412425264716, kl_loss = 0.004648352973163128\n",
      "\n",
      "Epoch 145\n",
      "Step 0: loss = 0.27722489833831787, recon_loss = 0.2755827307701111, 0.0016322295414283872, kl_loss = 0.004955640062689781\n",
      "\n",
      "Epoch 146\n",
      "Step 0: loss = 0.280388742685318, recon_loss = 0.2784842848777771, 0.0018961592577397823, kl_loss = 0.0041364263743162155\n",
      "\n",
      "Epoch 147\n",
      "Step 0: loss = 0.27747905254364014, recon_loss = 0.27532607316970825, 0.0021387136075645685, kl_loss = 0.007131889462471008\n",
      "\n",
      "Epoch 148\n",
      "Step 0: loss = 0.2773193120956421, recon_loss = 0.2751658260822296, 0.0021344609558582306, kl_loss = 0.009503641165792942\n",
      "\n",
      "Epoch 149\n",
      "Step 0: loss = 0.2794778048992157, recon_loss = 0.2775845527648926, 0.0018844024743884802, kl_loss = 0.004429233260452747\n",
      "\n",
      "Epoch 150\n",
      "Step 0: loss = 0.280962198972702, recon_loss = 0.27943533658981323, 0.0015173748834058642, kl_loss = 0.00474172830581665\n",
      "\n",
      "Epoch 151\n",
      "Step 0: loss = 0.27503442764282227, recon_loss = 0.2733946442604065, 0.0016321954317390919, kl_loss = 0.0037991367280483246\n",
      "\n",
      "Epoch 152\n",
      "Step 0: loss = 0.27691537141799927, recon_loss = 0.27485737204551697, 0.002048310823738575, kl_loss = 0.004850199446082115\n",
      "\n",
      "Epoch 153\n",
      "Step 0: loss = 0.2754260301589966, recon_loss = 0.27396196126937866, 0.0014558121329173446, kl_loss = 0.00412941537797451\n",
      "\n",
      "Epoch 154\n",
      "Step 0: loss = 0.2780853509902954, recon_loss = 0.2765061855316162, 0.001569921150803566, kl_loss = 0.004626474343240261\n",
      "\n",
      "Epoch 155\n",
      "Step 0: loss = 0.2759978771209717, recon_loss = 0.2735884189605713, 0.002400753553956747, kl_loss = 0.004344691522419453\n",
      "\n",
      "Epoch 156\n",
      "Step 0: loss = 0.2803344428539276, recon_loss = 0.2779325544834137, 0.0023921532556414604, kl_loss = 0.004877926781773567\n",
      "\n",
      "Epoch 157\n",
      "Step 0: loss = 0.27594321966171265, recon_loss = 0.2737976908683777, 0.00213180435821414, kl_loss = 0.006864794529974461\n",
      "\n",
      "Epoch 158\n",
      "Step 0: loss = 0.2751877009868622, recon_loss = 0.2737084925174713, 0.0014622975140810013, kl_loss = 0.008450186811387539\n",
      "\n",
      "Epoch 159\n",
      "Step 0: loss = 0.28188881278038025, recon_loss = 0.2799583077430725, 0.0019228020682930946, kl_loss = 0.003843526355922222\n",
      "\n",
      "Epoch 160\n",
      "Step 0: loss = 0.27560511231422424, recon_loss = 0.2739165425300598, 0.001676514744758606, kl_loss = 0.006040777079761028\n",
      "\n",
      "Epoch 161\n",
      "Step 0: loss = 0.2775198221206665, recon_loss = 0.27542078495025635, 0.002087207743898034, kl_loss = 0.0059158410876989365\n",
      "\n",
      "Epoch 162\n",
      "Step 0: loss = 0.2813863158226013, recon_loss = 0.27978697419166565, 0.0015904088504612446, kl_loss = 0.004474216140806675\n",
      "\n",
      "Epoch 163\n",
      "Step 0: loss = 0.27666354179382324, recon_loss = 0.27468401193618774, 0.001970568671822548, kl_loss = 0.004490597173571587\n",
      "\n",
      "Epoch 164\n",
      "Step 0: loss = 0.2785308361053467, recon_loss = 0.2760947644710541, 0.0024218368344008923, kl_loss = 0.007125357165932655\n",
      "\n",
      "Epoch 165\n",
      "Step 0: loss = 0.276935875415802, recon_loss = 0.2749401926994324, 0.0019332756055518985, kl_loss = 0.03119734674692154\n",
      "\n",
      "Epoch 166\n",
      "Step 0: loss = 0.27691853046417236, recon_loss = 0.2755541205406189, 0.001331457868218422, kl_loss = 0.016476359218358994\n",
      "\n",
      "Epoch 167\n",
      "Step 0: loss = 0.2806570827960968, recon_loss = 0.27894869446754456, 0.0016831331886351109, kl_loss = 0.012616426683962345\n",
      "\n",
      "Epoch 168\n",
      "Step 0: loss = 0.2786378264427185, recon_loss = 0.2770126461982727, 0.0016090420540422201, kl_loss = 0.008082570508122444\n",
      "\n",
      "Epoch 169\n",
      "Step 0: loss = 0.27731791138648987, recon_loss = 0.2754359245300293, 0.0018689832650125027, kl_loss = 0.006502204574644566\n",
      "\n",
      "Epoch 170\n",
      "Step 0: loss = 0.27525970339775085, recon_loss = 0.2731906473636627, 0.0020537362433969975, kl_loss = 0.007654028944671154\n",
      "\n",
      "Epoch 171\n",
      "Step 0: loss = 0.2774207293987274, recon_loss = 0.2757093906402588, 0.0017003029352054, kl_loss = 0.0055114468559622765\n",
      "\n",
      "Epoch 172\n",
      "Step 0: loss = 0.2764357626438141, recon_loss = 0.27490752935409546, 0.001512515009380877, kl_loss = 0.007857621647417545\n",
      "\n",
      "Epoch 173\n",
      "Step 0: loss = 0.276234894990921, recon_loss = 0.27389106154441833, 0.002336503006517887, kl_loss = 0.0036692917346954346\n",
      "\n",
      "Epoch 174\n",
      "Step 0: loss = 0.2772555947303772, recon_loss = 0.2756946086883545, 0.0015532061224803329, kl_loss = 0.0038923993706703186\n",
      "\n",
      "Epoch 175\n",
      "Step 0: loss = 0.2765100300312042, recon_loss = 0.2748785614967346, 0.0016251024790108204, kl_loss = 0.0031929686665534973\n",
      "\n",
      "Epoch 176\n",
      "Step 0: loss = 0.27759599685668945, recon_loss = 0.2763686776161194, 0.0012190189445391297, kl_loss = 0.004162642173469067\n",
      "\n",
      "Epoch 177\n",
      "Step 0: loss = 0.28196704387664795, recon_loss = 0.2805056571960449, 0.0014536412199959159, kl_loss = 0.0038808388635516167\n",
      "\n",
      "Epoch 178\n",
      "Step 0: loss = 0.2792757451534271, recon_loss = 0.2779887318611145, 0.001279665157198906, kl_loss = 0.003686307929456234\n",
      "\n",
      "Epoch 179\n",
      "Step 0: loss = 0.27476048469543457, recon_loss = 0.272949755191803, 0.0018046488985419273, kl_loss = 0.0030333176255226135\n",
      "\n",
      "Epoch 180\n",
      "Step 0: loss = 0.2732192575931549, recon_loss = 0.2712506055831909, 0.001961872912943363, kl_loss = 0.0033882558345794678\n",
      "\n",
      "Epoch 181\n",
      "Step 0: loss = 0.2762082517147064, recon_loss = 0.27436310052871704, 0.001837008516304195, kl_loss = 0.00407512579113245\n",
      "\n",
      "Epoch 182\n",
      "Step 0: loss = 0.2794528305530548, recon_loss = 0.2778782844543457, 0.0015639435732737184, kl_loss = 0.005300203338265419\n",
      "\n",
      "Epoch 183\n",
      "Step 0: loss = 0.2779136598110199, recon_loss = 0.2759923040866852, 0.0019133592722937465, kl_loss = 0.00398846622556448\n",
      "\n",
      "Epoch 184\n",
      "Step 0: loss = 0.2808557450771332, recon_loss = 0.2793201804161072, 0.0015247694682329893, kl_loss = 0.005398910492658615\n",
      "\n",
      "Epoch 185\n",
      "Step 0: loss = 0.2787003219127655, recon_loss = 0.2766571044921875, 0.002035896759480238, kl_loss = 0.003666701726615429\n",
      "\n",
      "Epoch 186\n",
      "Step 0: loss = 0.27782338857650757, recon_loss = 0.27552562952041626, 0.0022832578979432583, kl_loss = 0.00726406741887331\n",
      "\n",
      "Epoch 187\n",
      "Step 0: loss = 0.28271856904029846, recon_loss = 0.2802760601043701, 0.0024172007106244564, kl_loss = 0.012656951323151588\n",
      "\n",
      "Epoch 188\n",
      "Step 0: loss = 0.27873021364212036, recon_loss = 0.2766069173812866, 0.0020967908203601837, kl_loss = 0.0132517134770751\n",
      "\n",
      "Epoch 189\n",
      "Step 0: loss = 0.27822422981262207, recon_loss = 0.27585071325302124, 0.0023390455171465874, kl_loss = 0.017242778092622757\n",
      "\n",
      "Epoch 190\n",
      "Step 0: loss = 0.28222134709358215, recon_loss = 0.28013738989830017, 0.002043408341705799, kl_loss = 0.020286185666918755\n",
      "\n",
      "Epoch 191\n",
      "Step 0: loss = 0.2768768072128296, recon_loss = 0.2753628194332123, 0.0014862196985632181, kl_loss = 0.013882467523217201\n",
      "\n",
      "Epoch 192\n",
      "Step 0: loss = 0.27718839049339294, recon_loss = 0.2759726047515869, 0.0012083490146324039, kl_loss = 0.003722810186445713\n",
      "\n",
      "Epoch 193\n",
      "Step 0: loss = 0.27521541714668274, recon_loss = 0.27318787574768066, 0.0020151680801063776, kl_loss = 0.006189405918121338\n",
      "\n",
      "Epoch 194\n",
      "Step 0: loss = 0.2763756811618805, recon_loss = 0.27472367882728577, 0.001643480616621673, kl_loss = 0.004258973523974419\n",
      "\n",
      "Epoch 195\n",
      "Step 0: loss = 0.2738403379917145, recon_loss = 0.2726778984069824, 0.0011562330182641745, kl_loss = 0.003100787289440632\n",
      "\n",
      "Epoch 196\n",
      "Step 0: loss = 0.27550044655799866, recon_loss = 0.2737196385860443, 0.001764319371432066, kl_loss = 0.008242705836892128\n",
      "\n",
      "Epoch 197\n",
      "Step 0: loss = 0.2772965431213379, recon_loss = 0.2749861180782318, 0.002298640552908182, kl_loss = 0.005891311913728714\n",
      "\n",
      "Epoch 198\n",
      "Step 0: loss = 0.27372753620147705, recon_loss = 0.271975040435791, 0.0017437360947951674, kl_loss = 0.004377770237624645\n",
      "\n",
      "Epoch 199\n",
      "Step 0: loss = 0.27979305386543274, recon_loss = 0.27852505445480347, 0.0012580761685967445, kl_loss = 0.004964212886989117\n",
      "\n",
      "Epoch 200\n",
      "Step 0: loss = 0.2785865366458893, recon_loss = 0.27698880434036255, 0.0015901888255029917, kl_loss = 0.0037667062133550644\n",
      "\n",
      "Epoch 201\n",
      "Step 0: loss = 0.27640315890312195, recon_loss = 0.2746613621711731, 0.0017133967485278845, kl_loss = 0.014194655232131481\n",
      "\n",
      "Epoch 202\n",
      "Step 0: loss = 0.2740679681301117, recon_loss = 0.27275264263153076, 0.001301951939240098, kl_loss = 0.0066838497295975685\n",
      "\n",
      "Epoch 203\n",
      "Step 0: loss = 0.2786845862865448, recon_loss = 0.2770689129829407, 0.0016034141881391406, kl_loss = 0.006125227548182011\n",
      "\n",
      "Epoch 204\n",
      "Step 0: loss = 0.27444154024124146, recon_loss = 0.27267932891845703, 0.0017421008087694645, kl_loss = 0.010059146210551262\n",
      "\n",
      "Epoch 205\n",
      "Step 0: loss = 0.27595990896224976, recon_loss = 0.27475231885910034, 0.0011968356557190418, kl_loss = 0.0053779687732458115\n",
      "\n",
      "Epoch 206\n",
      "Step 0: loss = 0.27668827772140503, recon_loss = 0.2749839425086975, 0.0016903555952012539, kl_loss = 0.006994826719164848\n",
      "\n",
      "Epoch 207\n",
      "Step 0: loss = 0.27836543321609497, recon_loss = 0.2769799530506134, 0.0013764873147010803, kl_loss = 0.0045067258179187775\n",
      "\n",
      "Epoch 208\n",
      "Step 0: loss = 0.2795448303222656, recon_loss = 0.27804291248321533, 0.0014956850791350007, kl_loss = 0.003108271397650242\n",
      "\n",
      "Epoch 209\n",
      "Step 0: loss = 0.2749025225639343, recon_loss = 0.2737042307853699, 0.0011834613978862762, kl_loss = 0.007414133287966251\n",
      "\n",
      "Epoch 210\n",
      "Step 0: loss = 0.27480408549308777, recon_loss = 0.27334800362586975, 0.0014460881939157844, kl_loss = 0.004998206160962582\n",
      "\n",
      "Epoch 211\n",
      "Step 0: loss = 0.2782086133956909, recon_loss = 0.27646419405937195, 0.0017357906326651573, kl_loss = 0.004315145313739777\n",
      "\n",
      "Epoch 212\n",
      "Step 0: loss = 0.2810291051864624, recon_loss = 0.27946892380714417, 0.0015542168403044343, kl_loss = 0.0029760655015707016\n",
      "\n",
      "Epoch 213\n",
      "Step 0: loss = 0.27371764183044434, recon_loss = 0.27218255400657654, 0.0015264912508428097, kl_loss = 0.0042867474257946014\n",
      "\n",
      "Epoch 214\n",
      "Step 0: loss = 0.2780875563621521, recon_loss = 0.2764959931373596, 0.0015832050703465939, kl_loss = 0.004177681170403957\n",
      "\n",
      "Epoch 215\n",
      "Step 0: loss = 0.27627742290496826, recon_loss = 0.27455952763557434, 0.0016993440221995115, kl_loss = 0.009270522743463516\n",
      "\n",
      "Epoch 216\n",
      "Step 0: loss = 0.2770341634750366, recon_loss = 0.2752250134944916, 0.0018026370089501143, kl_loss = 0.0032687196508049965\n",
      "\n",
      "Epoch 217\n",
      "Step 0: loss = 0.2772005796432495, recon_loss = 0.2757253646850586, 0.0014696884900331497, kl_loss = 0.0027626510709524155\n",
      "\n",
      "Epoch 218\n",
      "Step 0: loss = 0.2768600285053253, recon_loss = 0.2753582000732422, 0.0014936196384951472, kl_loss = 0.004095434211194515\n",
      "\n",
      "Epoch 219\n",
      "Step 0: loss = 0.27922919392585754, recon_loss = 0.2776477038860321, 0.0015769924502819777, kl_loss = 0.002247750759124756\n",
      "\n",
      "Epoch 220\n",
      "Step 0: loss = 0.2810821831226349, recon_loss = 0.27915361523628235, 0.001920995651744306, kl_loss = 0.0037839291617274284\n",
      "\n",
      "Epoch 221\n",
      "Step 0: loss = 0.27938908338546753, recon_loss = 0.27778884768486023, 0.0015886173350736499, kl_loss = 0.005810781382024288\n",
      "\n",
      "Epoch 222\n",
      "Step 0: loss = 0.27597132325172424, recon_loss = 0.2739638090133667, 0.0019998857751488686, kl_loss = 0.003818676806986332\n",
      "\n",
      "Epoch 223\n",
      "Step 0: loss = 0.2762061655521393, recon_loss = 0.27472686767578125, 0.001470625982619822, kl_loss = 0.0043380651623010635\n",
      "\n",
      "Epoch 224\n",
      "Step 0: loss = 0.27876555919647217, recon_loss = 0.27733203768730164, 0.001426886417903006, kl_loss = 0.0033206939697265625\n",
      "\n",
      "Epoch 225\n",
      "Step 0: loss = 0.2750745117664337, recon_loss = 0.27323174476623535, 0.0018362117698416114, kl_loss = 0.003276280127465725\n",
      "\n",
      "Epoch 226\n",
      "Step 0: loss = 0.27492353320121765, recon_loss = 0.2736733853816986, 0.0012441659346222878, kl_loss = 0.002992604859173298\n",
      "\n",
      "Epoch 227\n",
      "Step 0: loss = 0.2763185203075409, recon_loss = 0.27469465136528015, 0.0016041493508964777, kl_loss = 0.009863758459687233\n",
      "\n",
      "Epoch 228\n",
      "Step 0: loss = 0.2766111493110657, recon_loss = 0.27527546882629395, 0.0013032038696110249, kl_loss = 0.01624695211648941\n",
      "\n",
      "Epoch 229\n",
      "Step 0: loss = 0.27797770500183105, recon_loss = 0.2760319411754608, 0.00193964468780905, kl_loss = 0.0030556032434105873\n",
      "\n",
      "Epoch 230\n",
      "Step 0: loss = 0.27352964878082275, recon_loss = 0.27137523889541626, 0.002149556763470173, kl_loss = 0.0024285446852445602\n",
      "\n",
      "Epoch 231\n",
      "Step 0: loss = 0.2785736322402954, recon_loss = 0.2770628035068512, 0.0014926049625501037, kl_loss = 0.009111626073718071\n",
      "\n",
      "Epoch 232\n",
      "Step 0: loss = 0.27784788608551025, recon_loss = 0.27661556005477905, 0.0012262726668268442, kl_loss = 0.0030259201303124428\n",
      "\n",
      "Epoch 233\n",
      "Step 0: loss = 0.2784672975540161, recon_loss = 0.27709388732910156, 0.0013646227307617664, kl_loss = 0.00439775176346302\n",
      "\n",
      "Epoch 234\n",
      "Step 0: loss = 0.2761246860027313, recon_loss = 0.2747694253921509, 0.0013486390234902501, kl_loss = 0.003310900181531906\n",
      "\n",
      "Epoch 235\n",
      "Step 0: loss = 0.27524104714393616, recon_loss = 0.2739315927028656, 0.0013028699904680252, kl_loss = 0.0032930243760347366\n",
      "\n",
      "Epoch 236\n",
      "Step 0: loss = 0.27414101362228394, recon_loss = 0.2728084325790405, 0.0013272766955196857, kl_loss = 0.0026528658345341682\n",
      "\n",
      "Epoch 237\n",
      "Step 0: loss = 0.27524781227111816, recon_loss = 0.27399998903274536, 0.0012426452012732625, kl_loss = 0.002587059512734413\n",
      "\n",
      "Epoch 238\n",
      "Step 0: loss = 0.2770264446735382, recon_loss = 0.2757209241390228, 0.0012989480746909976, kl_loss = 0.003298083320260048\n",
      "\n",
      "Epoch 239\n",
      "Step 0: loss = 0.27864328026771545, recon_loss = 0.276439905166626, 0.0021944805048406124, kl_loss = 0.004443555139005184\n",
      "\n",
      "Epoch 240\n",
      "Step 0: loss = 0.2739178538322449, recon_loss = 0.271915078163147, 0.0019931052811443806, kl_loss = 0.004825481213629246\n",
      "\n",
      "Epoch 241\n",
      "Step 0: loss = 0.27305662631988525, recon_loss = 0.2713525891304016, 0.0016890456900000572, kl_loss = 0.007498938590288162\n",
      "\n",
      "Epoch 242\n",
      "Step 0: loss = 0.28021010756492615, recon_loss = 0.2783188819885254, 0.0018802026752382517, kl_loss = 0.005519326776266098\n",
      "\n",
      "Epoch 243\n",
      "Step 0: loss = 0.276529997587204, recon_loss = 0.2751604914665222, 0.0013394607231020927, kl_loss = 0.01501838956028223\n",
      "\n",
      "Epoch 244\n",
      "Step 0: loss = 0.27656516432762146, recon_loss = 0.27501770853996277, 0.0015388736501336098, kl_loss = 0.004289096221327782\n",
      "\n",
      "Epoch 245\n",
      "Step 0: loss = 0.2751837968826294, recon_loss = 0.2735911011695862, 0.0015863787848502398, kl_loss = 0.0031582769006490707\n",
      "\n",
      "Epoch 246\n",
      "Step 0: loss = 0.275411993265152, recon_loss = 0.27427923679351807, 0.0011287685483694077, kl_loss = 0.0019903508946299553\n",
      "\n",
      "Epoch 247\n",
      "Step 0: loss = 0.2739714980125427, recon_loss = 0.27204763889312744, 0.0019197554793208838, kl_loss = 0.002051856368780136\n",
      "\n",
      "Epoch 248\n",
      "Step 0: loss = 0.27041202783584595, recon_loss = 0.26867955923080444, 0.0017155418172478676, kl_loss = 0.008463647216558456\n",
      "\n",
      "Epoch 249\n",
      "Step 0: loss = 0.2758335471153259, recon_loss = 0.2742460370063782, 0.0015566382789984345, kl_loss = 0.015438978560268879\n",
      "\n",
      "Epoch 250\n",
      "Step 0: loss = 0.27339494228363037, recon_loss = 0.27184993028640747, 0.0015310945454984903, kl_loss = 0.006962216459214687\n",
      "\n",
      "Epoch 251\n",
      "Step 0: loss = 0.272603839635849, recon_loss = 0.27086353302001953, 0.0017328308895230293, kl_loss = 0.003742886707186699\n",
      "\n",
      "Epoch 252\n",
      "Step 0: loss = 0.2763548493385315, recon_loss = 0.27493658661842346, 0.0014110131887719035, kl_loss = 0.003618716262280941\n",
      "\n",
      "Epoch 253\n",
      "Step 0: loss = 0.2703845500946045, recon_loss = 0.2687888741493225, 0.001584844198077917, kl_loss = 0.00540520716458559\n",
      "\n",
      "Epoch 254\n",
      "Step 0: loss = 0.2779051661491394, recon_loss = 0.27632594108581543, 0.0015720439841970801, kl_loss = 0.0035956483334302902\n",
      "\n",
      "Epoch 255\n",
      "Step 0: loss = 0.26724183559417725, recon_loss = 0.2660108208656311, 0.0012172353453934193, kl_loss = 0.006886610761284828\n",
      "\n",
      "Epoch 256\n",
      "Step 0: loss = 0.2751285433769226, recon_loss = 0.2725365161895752, 0.0025836587883532047, kl_loss = 0.004193710163235664\n",
      "\n",
      "Epoch 257\n",
      "Step 0: loss = 0.2739163935184479, recon_loss = 0.27223843336105347, 0.0016681842971593142, kl_loss = 0.00488491915166378\n",
      "\n",
      "Epoch 258\n",
      "Step 0: loss = 0.27358660101890564, recon_loss = 0.27179843187332153, 0.0017798192566260695, kl_loss = 0.004178370349109173\n",
      "\n",
      "Epoch 259\n",
      "Step 0: loss = 0.27242350578308105, recon_loss = 0.27054405212402344, 0.0018611929845064878, kl_loss = 0.009131312370300293\n",
      "\n",
      "Epoch 260\n",
      "Step 0: loss = 0.27348095178604126, recon_loss = 0.2719230353832245, 0.0015427571488544345, kl_loss = 0.0075850412249565125\n",
      "\n",
      "Epoch 261\n",
      "Step 0: loss = 0.27531519532203674, recon_loss = 0.2734284996986389, 0.0018618236063048244, kl_loss = 0.012440930120646954\n",
      "\n",
      "Epoch 262\n",
      "Step 0: loss = 0.2706534266471863, recon_loss = 0.26933524012565613, 0.0013019945472478867, kl_loss = 0.008088305592536926\n",
      "\n",
      "Epoch 263\n",
      "Step 0: loss = 0.2734568417072296, recon_loss = 0.27214428782463074, 0.0013005854561924934, kl_loss = 0.005983424372971058\n",
      "\n",
      "Epoch 264\n",
      "Step 0: loss = 0.27011358737945557, recon_loss = 0.2678107023239136, 0.0022939941845834255, kl_loss = 0.0044386135414242744\n",
      "\n",
      "Epoch 265\n",
      "Step 0: loss = 0.2728946805000305, recon_loss = 0.27132999897003174, 0.0015511527890339494, kl_loss = 0.0067674387246370316\n",
      "\n",
      "Epoch 266\n",
      "Step 0: loss = 0.2729603052139282, recon_loss = 0.27135586738586426, 0.001588225713931024, kl_loss = 0.008103283122181892\n",
      "\n",
      "Epoch 267\n",
      "Step 0: loss = 0.27014338970184326, recon_loss = 0.2689589858055115, 0.0011712308041751385, kl_loss = 0.0065900953486561775\n",
      "\n",
      "Epoch 268\n",
      "Step 0: loss = 0.2683538496494293, recon_loss = 0.2666630446910858, 0.0016833286499604583, kl_loss = 0.0037339376285672188\n",
      "\n",
      "Epoch 269\n",
      "Step 0: loss = 0.26910969614982605, recon_loss = 0.2677760720252991, 0.001325586112216115, kl_loss = 0.004022493958473206\n",
      "\n",
      "Epoch 270\n",
      "Step 0: loss = 0.2747246325016022, recon_loss = 0.273165762424469, 0.0015469194622710347, kl_loss = 0.005976524204015732\n",
      "\n",
      "Epoch 271\n",
      "Step 0: loss = 0.27042222023010254, recon_loss = 0.26916664838790894, 0.0012476436095312238, kl_loss = 0.0039667971432209015\n",
      "\n",
      "Epoch 272\n",
      "Step 0: loss = 0.27178955078125, recon_loss = 0.27049678564071655, 0.0012869115453213453, kl_loss = 0.002918270416557789\n",
      "\n",
      "Epoch 273\n",
      "Step 0: loss = 0.2694093585014343, recon_loss = 0.26778513193130493, 0.0016161573585122824, kl_loss = 0.00404499564319849\n",
      "\n",
      "Epoch 274\n",
      "Step 0: loss = 0.2780970335006714, recon_loss = 0.276860773563385, 0.001226250547915697, kl_loss = 0.005009702406823635\n",
      "\n",
      "Epoch 275\n",
      "Step 0: loss = 0.2714323401451111, recon_loss = 0.26985710859298706, 0.0015670860884711146, kl_loss = 0.004063603468239307\n",
      "\n",
      "Epoch 276\n",
      "Step 0: loss = 0.2676577866077423, recon_loss = 0.26632851362228394, 0.0013051793212071061, kl_loss = 0.012041457928717136\n",
      "\n",
      "Epoch 277\n",
      "Step 0: loss = 0.2672673165798187, recon_loss = 0.2656202018260956, 0.0016314845997840166, kl_loss = 0.007811661809682846\n",
      "\n",
      "Epoch 278\n",
      "Step 0: loss = 0.2643069624900818, recon_loss = 0.2624204456806183, 0.0018779791425913572, kl_loss = 0.00426595751196146\n",
      "\n",
      "Epoch 279\n",
      "Step 0: loss = 0.27004194259643555, recon_loss = 0.2678985893726349, 0.00213500764220953, kl_loss = 0.004167436622083187\n",
      "\n",
      "Epoch 280\n",
      "Step 0: loss = 0.27065300941467285, recon_loss = 0.26917457580566406, 0.0014679571613669395, kl_loss = 0.0052382079884409904\n",
      "\n",
      "Epoch 281\n",
      "Step 0: loss = 0.27383455634117126, recon_loss = 0.2719939947128296, 0.0018313683103770018, kl_loss = 0.004592437297105789\n",
      "\n",
      "Epoch 282\n",
      "Step 0: loss = 0.2676396667957306, recon_loss = 0.26638609170913696, 0.0012436599936336279, kl_loss = 0.004958915524184704\n",
      "\n",
      "Epoch 283\n",
      "Step 0: loss = 0.26837044954299927, recon_loss = 0.2668737769126892, 0.0014782786602154374, kl_loss = 0.009188948199152946\n",
      "\n",
      "Epoch 284\n",
      "Step 0: loss = 0.2711716294288635, recon_loss = 0.2699706256389618, 0.0011906858999282122, kl_loss = 0.005154074169695377\n",
      "\n",
      "Epoch 285\n",
      "Step 0: loss = 0.2625052332878113, recon_loss = 0.2612455487251282, 0.0012499219737946987, kl_loss = 0.004894752986729145\n",
      "\n",
      "Epoch 286\n",
      "Step 0: loss = 0.26494544744491577, recon_loss = 0.26345592737197876, 0.0014837421476840973, kl_loss = 0.0028925174847245216\n",
      "\n",
      "Epoch 287\n",
      "Step 0: loss = 0.26115673780441284, recon_loss = 0.25969982147216797, 0.0014475949574261904, kl_loss = 0.004660280421376228\n",
      "\n",
      "Epoch 288\n",
      "Step 0: loss = 0.25682514905929565, recon_loss = 0.2553304433822632, 0.0014903765404596925, kl_loss = 0.0021552778780460358\n",
      "\n",
      "Epoch 289\n",
      "Step 0: loss = 0.26194968819618225, recon_loss = 0.26024290919303894, 0.0017022276297211647, kl_loss = 0.002275623381137848\n",
      "\n",
      "Epoch 290\n",
      "Step 0: loss = 0.2645378112792969, recon_loss = 0.26306474208831787, 0.001465498935431242, kl_loss = 0.0037887440994381905\n",
      "\n",
      "Epoch 291\n",
      "Step 0: loss = 0.2624722719192505, recon_loss = 0.2611989378929138, 0.0012682105880230665, kl_loss = 0.002570214681327343\n",
      "\n",
      "Epoch 292\n",
      "Step 0: loss = 0.2599906325340271, recon_loss = 0.2572840452194214, 0.0026992405764758587, kl_loss = 0.0036822743713855743\n",
      "\n",
      "Epoch 293\n",
      "Step 0: loss = 0.25599512457847595, recon_loss = 0.25389325618743896, 0.0020851953886449337, kl_loss = 0.00832315906882286\n",
      "\n",
      "Epoch 294\n",
      "Step 0: loss = 0.2584753930568695, recon_loss = 0.25635063648223877, 0.002108474727720022, kl_loss = 0.008128862828016281\n",
      "\n",
      "Epoch 295\n",
      "Step 0: loss = 0.25666409730911255, recon_loss = 0.25479334592819214, 0.001861804979853332, kl_loss = 0.004472242668271065\n",
      "\n",
      "Epoch 296\n",
      "Step 0: loss = 0.2637775242328644, recon_loss = 0.26229894161224365, 0.0014710640534758568, kl_loss = 0.0037625273689627647\n",
      "\n",
      "Epoch 297\n",
      "Step 0: loss = 0.2616387903690338, recon_loss = 0.26008808612823486, 0.0015425322344526649, kl_loss = 0.0040891170501708984\n",
      "\n",
      "Epoch 298\n",
      "Step 0: loss = 0.2554665803909302, recon_loss = 0.2538168728351593, 0.0016113313613459468, kl_loss = 0.019197281450033188\n",
      "\n",
      "Epoch 299\n",
      "Step 0: loss = 0.25992172956466675, recon_loss = 0.2583506107330322, 0.0015577294398099184, kl_loss = 0.0066843898966908455\n",
      "\n",
      "Epoch 300\n",
      "Step 0: loss = 0.2597278356552124, recon_loss = 0.2581055760383606, 0.0016108643030747771, kl_loss = 0.005698075518012047\n",
      "\n",
      "Epoch 301\n",
      "Step 0: loss = 0.25827357172966003, recon_loss = 0.25664040446281433, 0.001622960902750492, kl_loss = 0.005094518885016441\n",
      "\n",
      "Epoch 302\n",
      "Step 0: loss = 0.2665310800075531, recon_loss = 0.26462265849113464, 0.0018996244762092829, kl_loss = 0.004395931027829647\n",
      "\n",
      "Epoch 303\n",
      "Step 0: loss = 0.257950097322464, recon_loss = 0.25604766607284546, 0.0018961522728204727, kl_loss = 0.0031511513516306877\n",
      "\n",
      "Epoch 304\n",
      "Step 0: loss = 0.25209474563598633, recon_loss = 0.25072067975997925, 0.0013686155434697866, kl_loss = 0.0027311863377690315\n",
      "\n",
      "Epoch 305\n",
      "Step 0: loss = 0.24757136404514313, recon_loss = 0.24612480401992798, 0.0014352977741509676, kl_loss = 0.005634401924908161\n",
      "\n",
      "Epoch 306\n",
      "Step 0: loss = 0.24976195394992828, recon_loss = 0.2483185976743698, 0.0014339019544422626, kl_loss = 0.004724297672510147\n",
      "\n",
      "Epoch 307\n",
      "Step 0: loss = 0.24605509638786316, recon_loss = 0.24444052577018738, 0.0016029790276661515, kl_loss = 0.005798754282295704\n",
      "\n",
      "Epoch 308\n",
      "Step 0: loss = 0.2444494515657425, recon_loss = 0.2425370067358017, 0.0018903531599789858, kl_loss = 0.01105071883648634\n",
      "\n",
      "Epoch 309\n",
      "Step 0: loss = 0.2461048662662506, recon_loss = 0.24403361976146698, 0.0020572668872773647, kl_loss = 0.006990499794483185\n",
      "\n",
      "Epoch 310\n",
      "Step 0: loss = 0.2431732416152954, recon_loss = 0.24202507734298706, 0.0011256844736635685, kl_loss = 0.01123949233442545\n",
      "\n",
      "Epoch 311\n",
      "Step 0: loss = 0.25362586975097656, recon_loss = 0.2513613700866699, 0.0022539219353348017, kl_loss = 0.005292441695928574\n",
      "\n",
      "Epoch 312\n",
      "Step 0: loss = 0.24400678277015686, recon_loss = 0.2422831505537033, 0.001714130281470716, kl_loss = 0.004752880893647671\n",
      "\n",
      "Epoch 313\n",
      "Step 0: loss = 0.25028228759765625, recon_loss = 0.24862316250801086, 0.0016504774102941155, kl_loss = 0.004314280115067959\n",
      "\n",
      "Epoch 314\n",
      "Step 0: loss = 0.2512352764606476, recon_loss = 0.2499052733182907, 0.0013213572092354298, kl_loss = 0.004314017482101917\n",
      "\n",
      "Epoch 315\n",
      "Step 0: loss = 0.2514881193637848, recon_loss = 0.2494584619998932, 0.0020233355462551117, kl_loss = 0.0031664641574025154\n",
      "\n",
      "Epoch 316\n",
      "Step 0: loss = 0.2429780215024948, recon_loss = 0.2410738170146942, 0.0018989068921655416, kl_loss = 0.0026493221521377563\n",
      "\n",
      "Epoch 317\n",
      "Step 0: loss = 0.24304062128067017, recon_loss = 0.2410338968038559, 0.001998349092900753, kl_loss = 0.004183953627943993\n",
      "\n",
      "Epoch 318\n",
      "Step 0: loss = 0.23067842423915863, recon_loss = 0.22865761816501617, 0.0020096066873520613, kl_loss = 0.005605839192867279\n",
      "\n",
      "Epoch 319\n",
      "Step 0: loss = 0.24291877448558807, recon_loss = 0.24094566702842712, 0.0019620638340711594, kl_loss = 0.005524523556232452\n",
      "\n",
      "Epoch 320\n",
      "Step 0: loss = 0.23718078434467316, recon_loss = 0.2355368733406067, 0.0016365793999284506, kl_loss = 0.0036644069477915764\n",
      "\n",
      "Epoch 321\n",
      "Step 0: loss = 0.23297426104545593, recon_loss = 0.23148420453071594, 0.0014818659983575344, kl_loss = 0.0040952423587441444\n",
      "\n",
      "Epoch 322\n",
      "Step 0: loss = 0.21408121287822723, recon_loss = 0.2123108059167862, 0.0017634797841310501, kl_loss = 0.0034627998247742653\n",
      "\n",
      "Epoch 323\n",
      "Step 0: loss = 0.24660265445709229, recon_loss = 0.24474355578422546, 0.001850747736170888, kl_loss = 0.004170558415353298\n",
      "\n",
      "Epoch 324\n",
      "Step 0: loss = 0.21851035952568054, recon_loss = 0.21618318557739258, 0.0023187007755041122, kl_loss = 0.004241308197379112\n",
      "\n",
      "Epoch 325\n",
      "Step 0: loss = 0.2248290330171585, recon_loss = 0.22257989645004272, 0.0022430941462516785, kl_loss = 0.003019227646291256\n",
      "\n",
      "Epoch 326\n",
      "Step 0: loss = 0.24158494174480438, recon_loss = 0.24008239805698395, 0.001491494127549231, kl_loss = 0.005528195761144161\n",
      "\n",
      "Epoch 327\n",
      "Step 0: loss = 0.2314324527978897, recon_loss = 0.22911012172698975, 0.0023050683084875345, kl_loss = 0.00863092951476574\n",
      "\n",
      "Epoch 328\n",
      "Step 0: loss = 0.2210654467344284, recon_loss = 0.21810166537761688, 0.0029382759239524603, kl_loss = 0.01275408174842596\n",
      "\n",
      "Epoch 329\n",
      "Step 0: loss = 0.22479400038719177, recon_loss = 0.2224539965391159, 0.0023161007557064295, kl_loss = 0.011947017163038254\n",
      "\n",
      "Epoch 330\n",
      "Step 0: loss = 0.2206147015094757, recon_loss = 0.21847748756408691, 0.002121188212186098, kl_loss = 0.008011587895452976\n",
      "\n",
      "Epoch 331\n",
      "Step 0: loss = 0.19912710785865784, recon_loss = 0.19717323780059814, 0.0019359586294740438, kl_loss = 0.008953453041613102\n",
      "\n",
      "Epoch 332\n",
      "Step 0: loss = 0.2202061265707016, recon_loss = 0.21832376718521118, 0.001865793950855732, kl_loss = 0.008284015581011772\n",
      "\n",
      "Epoch 333\n",
      "Step 0: loss = 0.2192806452512741, recon_loss = 0.21697747707366943, 0.0022942950017750263, kl_loss = 0.004433603957295418\n",
      "\n",
      "Epoch 334\n",
      "Step 0: loss = 0.19974260032176971, recon_loss = 0.19757279753684998, 0.0021619461476802826, kl_loss = 0.00392353069037199\n",
      "\n",
      "Epoch 335\n",
      "Step 0: loss = 0.20473554730415344, recon_loss = 0.20267938077449799, 0.00204561953432858, kl_loss = 0.005273141898214817\n",
      "\n",
      "Epoch 336\n",
      "Step 0: loss = 0.20346978306770325, recon_loss = 0.200587198138237, 0.0028636357747018337, kl_loss = 0.009473573416471481\n",
      "\n",
      "Epoch 337\n",
      "Step 0: loss = 0.22041955590248108, recon_loss = 0.21725957095623016, 0.0031454009003937244, kl_loss = 0.007291136309504509\n",
      "\n",
      "Epoch 338\n",
      "Step 0: loss = 0.2133028656244278, recon_loss = 0.21101486682891846, 0.002258282620459795, kl_loss = 0.014856432564556599\n",
      "\n",
      "Epoch 339\n",
      "Step 0: loss = 0.20156556367874146, recon_loss = 0.19897028803825378, 0.00257662171497941, kl_loss = 0.00932582188397646\n",
      "\n",
      "Epoch 340\n",
      "Step 0: loss = 0.21279829740524292, recon_loss = 0.2104087471961975, 0.0023629851639270782, kl_loss = 0.013284214772284031\n",
      "\n",
      "Epoch 341\n",
      "Step 0: loss = 0.19911977648735046, recon_loss = 0.1969180703163147, 0.002177671529352665, kl_loss = 0.012017156928777695\n",
      "\n",
      "Epoch 342\n",
      "Step 0: loss = 0.20115165412425995, recon_loss = 0.19852189719676971, 0.002611525123938918, kl_loss = 0.009116600267589092\n",
      "\n",
      "Epoch 343\n",
      "Step 0: loss = 0.19814735651016235, recon_loss = 0.1959453523159027, 0.0021833532955497503, kl_loss = 0.009330305270850658\n",
      "\n",
      "Epoch 344\n",
      "Step 0: loss = 0.1909913271665573, recon_loss = 0.18877285718917847, 0.002205025404691696, kl_loss = 0.006722205318510532\n",
      "\n",
      "Epoch 345\n",
      "Step 0: loss = 0.19863183796405792, recon_loss = 0.19553834199905396, 0.0030843899585306644, kl_loss = 0.00455254502594471\n",
      "\n",
      "Epoch 346\n",
      "Step 0: loss = 0.1896311640739441, recon_loss = 0.18642041087150574, 0.003194026416167617, kl_loss = 0.008364017121493816\n",
      "\n",
      "Epoch 347\n",
      "Step 0: loss = 0.19183672964572906, recon_loss = 0.18979179859161377, 0.002034620614722371, kl_loss = 0.005159344524145126\n",
      "\n",
      "Epoch 348\n",
      "Step 0: loss = 0.19471438229084015, recon_loss = 0.19268140196800232, 0.0020245970226824284, kl_loss = 0.004193948581814766\n",
      "\n",
      "Epoch 349\n",
      "Step 0: loss = 0.18630526959896088, recon_loss = 0.18383637070655823, 0.0024606063961982727, kl_loss = 0.004149738699197769\n",
      "\n",
      "Epoch 350\n",
      "Step 0: loss = 0.1902046948671341, recon_loss = 0.18818938732147217, 0.0020076809450984, kl_loss = 0.0038170991465449333\n",
      "\n",
      "Epoch 351\n",
      "Step 0: loss = 0.1937812715768814, recon_loss = 0.19166266918182373, 0.0021127606742084026, kl_loss = 0.002919449470937252\n",
      "\n",
      "Epoch 352\n",
      "Step 0: loss = 0.1766705960035324, recon_loss = 0.1749931275844574, 0.0016710972413420677, kl_loss = 0.0031859949231147766\n",
      "\n",
      "Epoch 353\n",
      "Step 0: loss = 0.1810411661863327, recon_loss = 0.17897352576255798, 0.0020602745935320854, kl_loss = 0.003678753972053528\n",
      "\n",
      "Epoch 354\n",
      "Step 0: loss = 0.17797605693340302, recon_loss = 0.17544305324554443, 0.002526060910895467, kl_loss = 0.0034721307456493378\n",
      "\n",
      "Epoch 355\n",
      "Step 0: loss = 0.17996972799301147, recon_loss = 0.17731323838233948, 0.002650773385539651, kl_loss = 0.002861443907022476\n",
      "\n",
      "Epoch 356\n",
      "Step 0: loss = 0.1765073835849762, recon_loss = 0.1731509268283844, 0.0033500236459076405, kl_loss = 0.003217129036784172\n",
      "\n",
      "Epoch 357\n",
      "Step 0: loss = 0.17759840190410614, recon_loss = 0.175496444106102, 0.002097175922244787, kl_loss = 0.0023911697790026665\n",
      "\n",
      "Epoch 358\n",
      "Step 0: loss = 0.1598430722951889, recon_loss = 0.1557581126689911, 0.0040755015797913074, kl_loss = 0.004733133129775524\n",
      "\n",
      "Epoch 359\n",
      "Step 0: loss = 0.16908548772335052, recon_loss = 0.16607508063316345, 0.0029964318964630365, kl_loss = 0.006988401524722576\n",
      "\n",
      "Epoch 360\n",
      "Step 0: loss = 0.16472022235393524, recon_loss = 0.16217941045761108, 0.002529796212911606, kl_loss = 0.005507710389792919\n",
      "\n",
      "Epoch 361\n",
      "Step 0: loss = 0.1707732230424881, recon_loss = 0.16846582293510437, 0.0022972896695137024, kl_loss = 0.005055662244558334\n",
      "\n",
      "Epoch 362\n",
      "Step 0: loss = 0.16381071507930756, recon_loss = 0.16128715872764587, 0.0025153865572065115, kl_loss = 0.004079598933458328\n",
      "\n",
      "Epoch 363\n",
      "Step 0: loss = 0.1655234396457672, recon_loss = 0.16220185160636902, 0.003312450833618641, kl_loss = 0.004564343951642513\n",
      "\n",
      "Epoch 364\n",
      "Step 0: loss = 0.1587676852941513, recon_loss = 0.15551751852035522, 0.00323929893784225, kl_loss = 0.005428698845207691\n",
      "\n",
      "Epoch 365\n",
      "Step 0: loss = 0.1670653522014618, recon_loss = 0.16367951035499573, 0.003359894035384059, kl_loss = 0.012970897369086742\n",
      "\n",
      "Epoch 366\n",
      "Step 0: loss = 0.14781175553798676, recon_loss = 0.14497865736484528, 0.002819409826770425, kl_loss = 0.006847213953733444\n",
      "\n",
      "Epoch 367\n",
      "Step 0: loss = 0.16318918764591217, recon_loss = 0.16074198484420776, 0.0024361307732760906, kl_loss = 0.005535867065191269\n",
      "\n",
      "Epoch 368\n",
      "Step 0: loss = 0.16343331336975098, recon_loss = 0.1603841632604599, 0.003041408956050873, kl_loss = 0.003874286077916622\n",
      "\n",
      "Epoch 369\n",
      "Step 0: loss = 0.1574275642633438, recon_loss = 0.15503767132759094, 0.0023850996512919664, kl_loss = 0.002395438961684704\n",
      "\n",
      "Epoch 370\n",
      "Step 0: loss = 0.1479308307170868, recon_loss = 0.14487139880657196, 0.0030528034549206495, kl_loss = 0.0033157970756292343\n",
      "\n",
      "Epoch 371\n",
      "Step 0: loss = 0.15455767512321472, recon_loss = 0.15153929591178894, 0.003012879053130746, kl_loss = 0.0027501098811626434\n",
      "\n",
      "Epoch 372\n",
      "Step 0: loss = 0.14332884550094604, recon_loss = 0.14100854098796844, 0.002313766395673156, kl_loss = 0.003273521549999714\n",
      "\n",
      "Epoch 373\n",
      "Step 0: loss = 0.13566602766513824, recon_loss = 0.13261544704437256, 0.0030444497242569923, kl_loss = 0.0030632540583610535\n",
      "\n",
      "Epoch 374\n",
      "Step 0: loss = 0.14547039568424225, recon_loss = 0.14300325512886047, 0.0024590366519987583, kl_loss = 0.004052785225212574\n",
      "\n",
      "Epoch 375\n",
      "Step 0: loss = 0.14341874420642853, recon_loss = 0.14027491211891174, 0.003135442268103361, kl_loss = 0.004191011190414429\n",
      "\n",
      "Epoch 376\n",
      "Step 0: loss = 0.13959786295890808, recon_loss = 0.1373659074306488, 0.0022262712009251118, kl_loss = 0.0028361976146698\n",
      "\n",
      "Epoch 377\n",
      "Step 0: loss = 0.1429046243429184, recon_loss = 0.14005142450332642, 0.0028479837346822023, kl_loss = 0.0026087723672389984\n",
      "\n",
      "Epoch 378\n",
      "Step 0: loss = 0.13586416840553284, recon_loss = 0.13228623569011688, 0.003569364547729492, kl_loss = 0.004281223751604557\n",
      "\n",
      "Epoch 379\n",
      "Step 0: loss = 0.13539128005504608, recon_loss = 0.1316106915473938, 0.003771746065467596, kl_loss = 0.004415774717926979\n",
      "\n",
      "Epoch 380\n",
      "Step 0: loss = 0.14229954779148102, recon_loss = 0.13831083476543427, 0.003970154095441103, kl_loss = 0.009276553057134151\n",
      "\n",
      "Epoch 381\n",
      "Step 0: loss = 0.14077606797218323, recon_loss = 0.13882222771644592, 0.0019478859612718225, kl_loss = 0.0029828958213329315\n",
      "\n",
      "Epoch 382\n",
      "Step 0: loss = 0.14153583347797394, recon_loss = 0.13791434466838837, 0.0036040388513356447, kl_loss = 0.008725058287382126\n",
      "\n",
      "Epoch 383\n",
      "Step 0: loss = 0.12549327313899994, recon_loss = 0.12239985913038254, 0.0030731039587408304, kl_loss = 0.010153346695005894\n",
      "\n",
      "Epoch 384\n",
      "Step 0: loss = 0.13743720948696136, recon_loss = 0.13483931124210358, 0.0025866087526082993, kl_loss = 0.005650603212416172\n",
      "\n",
      "Epoch 385\n",
      "Step 0: loss = 0.13033217191696167, recon_loss = 0.12707224488258362, 0.00325512932613492, kl_loss = 0.0024005742743611336\n",
      "\n",
      "Epoch 386\n",
      "Step 0: loss = 0.1487594097852707, recon_loss = 0.14578469097614288, 0.0029672076925635338, kl_loss = 0.0037567131221294403\n",
      "\n",
      "Epoch 387\n",
      "Step 0: loss = 0.12761181592941284, recon_loss = 0.12343482673168182, 0.0041704243049025536, kl_loss = 0.0032844841480255127\n",
      "\n",
      "Epoch 388\n",
      "Step 0: loss = 0.12697210907936096, recon_loss = 0.12366947531700134, 0.003297661431133747, kl_loss = 0.0024917209520936012\n",
      "\n",
      "Epoch 389\n",
      "Step 0: loss = 0.12193354219198227, recon_loss = 0.1177835464477539, 0.004141408484429121, kl_loss = 0.004293572157621384\n",
      "\n",
      "Epoch 390\n",
      "Step 0: loss = 0.12509694695472717, recon_loss = 0.12113653868436813, 0.003953162580728531, kl_loss = 0.003617876209318638\n",
      "\n",
      "Epoch 391\n",
      "Step 0: loss = 0.12266915291547775, recon_loss = 0.11844954639673233, 0.004212177358567715, kl_loss = 0.003713020123541355\n",
      "\n",
      "Epoch 392\n",
      "Step 0: loss = 0.11238005757331848, recon_loss = 0.10996571183204651, 0.002405051840469241, kl_loss = 0.004645104520022869\n",
      "\n",
      "Epoch 393\n",
      "Step 0: loss = 0.1401296705007553, recon_loss = 0.13720755279064178, 0.0029143551364541054, kl_loss = 0.003882315009832382\n",
      "\n",
      "Epoch 394\n",
      "Step 0: loss = 0.11165579408407211, recon_loss = 0.10886095464229584, 0.0027867467142641544, kl_loss = 0.004043803550302982\n",
      "\n",
      "Epoch 395\n",
      "Step 0: loss = 0.11159685999155045, recon_loss = 0.10848910361528397, 0.003097672015428543, kl_loss = 0.005040420219302177\n",
      "\n",
      "Epoch 396\n",
      "Step 0: loss = 0.12653781473636627, recon_loss = 0.12248006463050842, 0.0040477728471159935, kl_loss = 0.004994490183889866\n",
      "\n",
      "Epoch 397\n",
      "Step 0: loss = 0.11710246652364731, recon_loss = 0.11445535719394684, 0.0026427172124385834, kl_loss = 0.0021926425397396088\n",
      "\n",
      "Epoch 398\n",
      "Step 0: loss = 0.11818498373031616, recon_loss = 0.11513923108577728, 0.0030389721505343914, kl_loss = 0.003389280289411545\n",
      "\n",
      "Epoch 399\n",
      "Step 0: loss = 0.11329477280378342, recon_loss = 0.11025567352771759, 0.0030273415613919497, kl_loss = 0.005879366770386696\n",
      "\n",
      "Epoch 400\n",
      "Step 0: loss = 0.10421589016914368, recon_loss = 0.10012061893939972, 0.004080675542354584, kl_loss = 0.007297842763364315\n",
      "\n",
      "Epoch 401\n",
      "Step 0: loss = 0.11594271659851074, recon_loss = 0.11292605102062225, 0.002997609321027994, kl_loss = 0.00953004788607359\n",
      "\n",
      "Epoch 402\n",
      "Step 0: loss = 0.10567464679479599, recon_loss = 0.10230910778045654, 0.0033490003552287817, kl_loss = 0.008271599188446999\n",
      "\n",
      "Epoch 403\n",
      "Step 0: loss = 0.11355189979076385, recon_loss = 0.11071673035621643, 0.002827378688380122, kl_loss = 0.0038949232548475266\n",
      "\n",
      "Epoch 404\n",
      "Step 0: loss = 0.11613106727600098, recon_loss = 0.11279290914535522, 0.003328625811263919, kl_loss = 0.004766212776303291\n",
      "\n",
      "Epoch 405\n",
      "Step 0: loss = 0.10638830810785294, recon_loss = 0.10323292016983032, 0.003133559599518776, kl_loss = 0.010915317572653294\n",
      "\n",
      "Epoch 406\n",
      "Step 0: loss = 0.102434903383255, recon_loss = 0.09898564964532852, 0.00343680358491838, kl_loss = 0.006224538199603558\n",
      "\n",
      "Epoch 407\n",
      "Step 0: loss = 0.10338538140058517, recon_loss = 0.10030033439397812, 0.0030779759399592876, kl_loss = 0.003535921685397625\n",
      "\n",
      "Epoch 408\n",
      "Step 0: loss = 0.09932240843772888, recon_loss = 0.09614364057779312, 0.003164428286254406, kl_loss = 0.007169454358518124\n",
      "\n",
      "Epoch 409\n",
      "Step 0: loss = 0.10272932797670364, recon_loss = 0.09970758110284805, 0.003012686502188444, kl_loss = 0.0045281704515218735\n",
      "\n",
      "Epoch 410\n",
      "Step 0: loss = 0.10151169449090958, recon_loss = 0.09812949597835541, 0.003374300431460142, kl_loss = 0.003947164863348007\n",
      "\n",
      "Epoch 411\n",
      "Step 0: loss = 0.10208481550216675, recon_loss = 0.09907904267311096, 0.0029980035033077, kl_loss = 0.0038861045613884926\n",
      "\n",
      "Epoch 412\n",
      "Step 0: loss = 0.10138145834207535, recon_loss = 0.09813766926527023, 0.003237317781895399, kl_loss = 0.0032357219606637955\n",
      "\n",
      "Epoch 413\n",
      "Step 0: loss = 0.09313394129276276, recon_loss = 0.08936293423175812, 0.0037677111104130745, kl_loss = 0.001646147109568119\n",
      "\n",
      "Epoch 414\n",
      "Step 0: loss = 0.09629763662815094, recon_loss = 0.09367392957210541, 0.00261902529746294, kl_loss = 0.0023397449404001236\n",
      "\n",
      "Epoch 415\n",
      "Step 0: loss = 0.08819007873535156, recon_loss = 0.08490651845932007, 0.0032764479983597994, kl_loss = 0.0035582147538661957\n",
      "\n",
      "Epoch 416\n",
      "Step 0: loss = 0.09160836786031723, recon_loss = 0.08784724771976471, 0.0037565412931144238, kl_loss = 0.002292495220899582\n",
      "\n",
      "Epoch 417\n",
      "Step 0: loss = 0.09400832653045654, recon_loss = 0.09027419984340668, 0.003729230258613825, kl_loss = 0.002445739693939686\n",
      "\n",
      "Epoch 418\n",
      "Step 0: loss = 0.08575130999088287, recon_loss = 0.08225017786026001, 0.003495029639452696, kl_loss = 0.003049907274544239\n",
      "\n",
      "Epoch 419\n",
      "Step 0: loss = 0.0884832888841629, recon_loss = 0.08536507934331894, 0.003110694233328104, kl_loss = 0.0037571797147393227\n",
      "\n",
      "Epoch 420\n",
      "Step 0: loss = 0.08884533494710922, recon_loss = 0.08484186232089996, 0.003997699823230505, kl_loss = 0.0028865672647953033\n",
      "\n",
      "Epoch 421\n",
      "Step 0: loss = 0.0876762643456459, recon_loss = 0.08496366441249847, 0.00270005501806736, kl_loss = 0.00627391692250967\n",
      "\n",
      "Epoch 422\n",
      "Step 0: loss = 0.07876833528280258, recon_loss = 0.07492691278457642, 0.0038256547413766384, kl_loss = 0.007883593440055847\n",
      "\n",
      "Epoch 423\n",
      "Step 0: loss = 0.09282573312520981, recon_loss = 0.08912043273448944, 0.0036807172000408173, kl_loss = 0.012291394174098969\n",
      "\n",
      "Epoch 424\n",
      "Step 0: loss = 0.08862864971160889, recon_loss = 0.08577948808670044, 0.0028245029971003532, kl_loss = 0.012330508790910244\n",
      "\n",
      "Epoch 425\n",
      "Step 0: loss = 0.07856777310371399, recon_loss = 0.07563649117946625, 0.0029083266854286194, kl_loss = 0.011479279957711697\n",
      "\n",
      "Epoch 426\n",
      "Step 0: loss = 0.07990191131830215, recon_loss = 0.07777120172977448, 0.002118275035172701, kl_loss = 0.006215657107532024\n",
      "\n",
      "Epoch 427\n",
      "Step 0: loss = 0.08228413015604019, recon_loss = 0.07779526710510254, 0.004479230381548405, kl_loss = 0.004816044121980667\n",
      "\n",
      "Epoch 428\n",
      "Step 0: loss = 0.07865872234106064, recon_loss = 0.07500990480184555, 0.0036381850950419903, kl_loss = 0.00531502440571785\n",
      "\n",
      "Epoch 429\n",
      "Step 0: loss = 0.08093976229429245, recon_loss = 0.07723244279623032, 0.0036996277049183846, kl_loss = 0.003843878395855427\n",
      "\n",
      "Epoch 430\n",
      "Step 0: loss = 0.07396534830331802, recon_loss = 0.06990489363670349, 0.004054204095155001, kl_loss = 0.003125237300992012\n",
      "\n",
      "Epoch 431\n",
      "Step 0: loss = 0.07898890972137451, recon_loss = 0.07463584840297699, 0.004345611669123173, kl_loss = 0.0037263575941324234\n",
      "\n",
      "Epoch 432\n",
      "Step 0: loss = 0.07362087070941925, recon_loss = 0.07062870264053345, 0.0029814057052135468, kl_loss = 0.005380859598517418\n",
      "\n",
      "Epoch 433\n",
      "Step 0: loss = 0.0727987140417099, recon_loss = 0.06999661028385162, 0.002797221764922142, kl_loss = 0.0024404870346188545\n",
      "\n",
      "Epoch 434\n",
      "Step 0: loss = 0.07699905335903168, recon_loss = 0.07379814982414246, 0.003197088837623596, kl_loss = 0.0019059954211115837\n",
      "\n",
      "Epoch 435\n",
      "Step 0: loss = 0.07677090167999268, recon_loss = 0.07362688332796097, 0.003136238781735301, kl_loss = 0.003890465945005417\n",
      "\n",
      "Epoch 436\n",
      "Step 0: loss = 0.0782470628619194, recon_loss = 0.074840247631073, 0.0033981334418058395, kl_loss = 0.004339820705354214\n",
      "\n",
      "Epoch 437\n",
      "Step 0: loss = 0.08094765245914459, recon_loss = 0.07749845087528229, 0.0034426148049533367, kl_loss = 0.0032931286841630936\n",
      "\n",
      "Epoch 438\n",
      "Step 0: loss = 0.07530093938112259, recon_loss = 0.0703180655837059, 0.004972459748387337, kl_loss = 0.005207723006606102\n",
      "\n",
      "Epoch 439\n",
      "Step 0: loss = 0.07198944687843323, recon_loss = 0.06684526801109314, 0.005137124098837376, kl_loss = 0.0035281451418995857\n",
      "\n",
      "Epoch 440\n",
      "Step 0: loss = 0.07632407546043396, recon_loss = 0.07355871796607971, 0.002755281049758196, kl_loss = 0.005035919137299061\n",
      "\n",
      "Epoch 441\n",
      "Step 0: loss = 0.06637924909591675, recon_loss = 0.06197650358080864, 0.004395473748445511, kl_loss = 0.0036369655281305313\n",
      "\n",
      "Epoch 442\n",
      "Step 0: loss = 0.07252790778875351, recon_loss = 0.06887467205524445, 0.0036303563974797726, kl_loss = 0.011441471055150032\n",
      "\n",
      "Epoch 443\n",
      "Step 0: loss = 0.0707768127322197, recon_loss = 0.06667346507310867, 0.0040846881456673145, kl_loss = 0.009329755790531635\n",
      "\n",
      "Epoch 444\n",
      "Step 0: loss = 0.06269711256027222, recon_loss = 0.05896909907460213, 0.0037203088868409395, kl_loss = 0.0038527576252818108\n",
      "\n",
      "Epoch 445\n",
      "Step 0: loss = 0.0673908218741417, recon_loss = 0.06336671113967896, 0.004018312785774469, kl_loss = 0.002897723577916622\n",
      "\n",
      "Epoch 446\n",
      "Step 0: loss = 0.07294707745313644, recon_loss = 0.06861783564090729, 0.004325237590819597, kl_loss = 0.002001146785914898\n",
      "\n",
      "Epoch 447\n",
      "Step 0: loss = 0.06894999742507935, recon_loss = 0.06555894762277603, 0.0033861021511256695, kl_loss = 0.0024734139442443848\n",
      "\n",
      "Epoch 448\n",
      "Step 0: loss = 0.06941413879394531, recon_loss = 0.06536690890789032, 0.004040298983454704, kl_loss = 0.0034632589668035507\n",
      "\n",
      "Epoch 449\n",
      "Step 0: loss = 0.06860613077878952, recon_loss = 0.0642714872956276, 0.004328686743974686, kl_loss = 0.0029759928584098816\n",
      "\n",
      "Epoch 450\n",
      "Step 0: loss = 0.06737768650054932, recon_loss = 0.06336794793605804, 0.004006312228739262, kl_loss = 0.0017135785892605782\n",
      "\n",
      "Epoch 451\n",
      "Step 0: loss = 0.0673309862613678, recon_loss = 0.064336396753788, 0.0029900497756898403, kl_loss = 0.002270044758915901\n",
      "\n",
      "Epoch 452\n",
      "Step 0: loss = 0.061995524913072586, recon_loss = 0.05851031094789505, 0.003478950820863247, kl_loss = 0.0031303763389587402\n",
      "\n",
      "Epoch 453\n",
      "Step 0: loss = 0.06381454318761826, recon_loss = 0.06057139113545418, 0.003234751056879759, kl_loss = 0.004200766794383526\n",
      "\n",
      "Epoch 454\n",
      "Step 0: loss = 0.06539355963468552, recon_loss = 0.061254557222127914, 0.00412868894636631, kl_loss = 0.005155924707651138\n",
      "\n",
      "Epoch 455\n",
      "Step 0: loss = 0.06547605246305466, recon_loss = 0.061423372477293015, 0.004045061767101288, kl_loss = 0.003810165449976921\n",
      "\n",
      "Epoch 456\n",
      "Step 0: loss = 0.06113461032509804, recon_loss = 0.05746464431285858, 0.00366427144035697, kl_loss = 0.002847690135240555\n",
      "\n",
      "Epoch 457\n",
      "Step 0: loss = 0.05923312157392502, recon_loss = 0.055333442986011505, 0.0038916361518204212, kl_loss = 0.004021679051220417\n",
      "\n",
      "Epoch 458\n",
      "Step 0: loss = 0.058607764542102814, recon_loss = 0.05579244717955589, 0.0028054427821189165, kl_loss = 0.004937704652547836\n",
      "\n",
      "Epoch 459\n",
      "Step 0: loss = 0.0627707690000534, recon_loss = 0.05892478674650192, 0.003838595934212208, kl_loss = 0.0036911023780703545\n",
      "\n",
      "Epoch 460\n",
      "Step 0: loss = 0.06034077703952789, recon_loss = 0.057098694145679474, 0.0032313114497810602, kl_loss = 0.0053854817524552345\n",
      "\n",
      "Epoch 461\n",
      "Step 0: loss = 0.061395518481731415, recon_loss = 0.05685994029045105, 0.0045195333659648895, kl_loss = 0.008021770976483822\n",
      "\n",
      "Epoch 462\n",
      "Step 0: loss = 0.06038548797369003, recon_loss = 0.05640271306037903, 0.003974460996687412, kl_loss = 0.0041567254811525345\n",
      "\n",
      "Epoch 463\n",
      "Step 0: loss = 0.06213471293449402, recon_loss = 0.05858887359499931, 0.0035385091323405504, kl_loss = 0.003665624186396599\n",
      "\n",
      "Epoch 464\n",
      "Step 0: loss = 0.057128261774778366, recon_loss = 0.053411006927490234, 0.0037121176719665527, kl_loss = 0.002568085677921772\n",
      "\n",
      "Epoch 465\n",
      "Step 0: loss = 0.05669476464390755, recon_loss = 0.05356793478131294, 0.0031204777769744396, kl_loss = 0.0031761256977915764\n",
      "\n",
      "Epoch 466\n",
      "Step 0: loss = 0.055909767746925354, recon_loss = 0.05049425736069679, 0.005402959883213043, kl_loss = 0.006274865940213203\n",
      "\n",
      "Epoch 467\n",
      "Step 0: loss = 0.05603812634944916, recon_loss = 0.05228808522224426, 0.003739302046597004, kl_loss = 0.005370627157390118\n",
      "\n",
      "Epoch 468\n",
      "Step 0: loss = 0.05961890146136284, recon_loss = 0.05623113736510277, 0.003378488589078188, kl_loss = 0.0046379584819078445\n",
      "\n",
      "Epoch 469\n",
      "Step 0: loss = 0.06093713268637657, recon_loss = 0.055902257561683655, 0.005022181198000908, kl_loss = 0.006346775218844414\n",
      "\n",
      "Epoch 470\n",
      "Step 0: loss = 0.053621258586645126, recon_loss = 0.04948626831173897, 0.004128898493945599, kl_loss = 0.0030447766184806824\n",
      "\n",
      "Epoch 471\n",
      "Step 0: loss = 0.05598611384630203, recon_loss = 0.05216167867183685, 0.003817058401182294, kl_loss = 0.0036877738311886787\n",
      "\n",
      "Epoch 472\n",
      "Step 0: loss = 0.05602908134460449, recon_loss = 0.05195624381303787, 0.0040659294463694096, kl_loss = 0.003452426753938198\n",
      "\n",
      "Epoch 473\n",
      "Step 0: loss = 0.05358526110649109, recon_loss = 0.04785596579313278, 0.00571901910007, kl_loss = 0.005136455409228802\n",
      "\n",
      "Epoch 474\n",
      "Step 0: loss = 0.05773790180683136, recon_loss = 0.05396655946969986, 0.0037631604354828596, kl_loss = 0.004089778289198875\n",
      "\n",
      "Epoch 475\n",
      "Step 0: loss = 0.05249952897429466, recon_loss = 0.04840841889381409, 0.004086427390575409, kl_loss = 0.0023413831368088722\n",
      "\n",
      "Epoch 476\n",
      "Step 0: loss = 0.05622249096632004, recon_loss = 0.050628602504730225, 0.005587900523096323, kl_loss = 0.00299264770001173\n",
      "\n",
      "Epoch 477\n",
      "Step 0: loss = 0.05512812361121178, recon_loss = 0.050094813108444214, 0.005026700906455517, kl_loss = 0.003303760662674904\n",
      "\n",
      "Epoch 478\n",
      "Step 0: loss = 0.05541561543941498, recon_loss = 0.04928291589021683, 0.006124165840446949, kl_loss = 0.0042673395946621895\n",
      "\n",
      "Epoch 479\n",
      "Step 0: loss = 0.050450753420591354, recon_loss = 0.04599394649267197, 0.0044477335177361965, kl_loss = 0.00453697144985199\n",
      "\n",
      "Epoch 480\n",
      "Step 0: loss = 0.05360986292362213, recon_loss = 0.04821532964706421, 0.005384739488363266, kl_loss = 0.004895997233688831\n",
      "\n",
      "Epoch 481\n",
      "Step 0: loss = 0.04812732711434364, recon_loss = 0.04475616663694382, 0.0033578965812921524, kl_loss = 0.006633575074374676\n",
      "\n",
      "Epoch 482\n",
      "Step 0: loss = 0.050413601100444794, recon_loss = 0.04563958942890167, 0.00476754130795598, kl_loss = 0.0032362164929509163\n",
      "\n",
      "Epoch 483\n",
      "Step 0: loss = 0.04844411090016365, recon_loss = 0.04519805312156677, 0.003232213668525219, kl_loss = 0.006921995431184769\n",
      "\n",
      "Epoch 484\n",
      "Step 0: loss = 0.04916832223534584, recon_loss = 0.04554098844528198, 0.003616655245423317, kl_loss = 0.005340396426618099\n",
      "\n",
      "Epoch 485\n",
      "Step 0: loss = 0.048637520521879196, recon_loss = 0.043869711458683014, 0.0047586592845618725, kl_loss = 0.004574882797896862\n",
      "\n",
      "Epoch 486\n",
      "Step 0: loss = 0.04945817589759827, recon_loss = 0.04464459419250488, 0.004770405124872923, kl_loss = 0.02158856764435768\n",
      "\n",
      "Epoch 487\n",
      "Step 0: loss = 0.04833291098475456, recon_loss = 0.04411838948726654, 0.0042063770815730095, kl_loss = 0.004072372801601887\n",
      "\n",
      "Epoch 488\n",
      "Step 0: loss = 0.04826603829860687, recon_loss = 0.04354051500558853, 0.004698737524449825, kl_loss = 0.013392702676355839\n",
      "\n",
      "Epoch 489\n",
      "Step 0: loss = 0.04788024351000786, recon_loss = 0.04237198084592819, 0.005501122213900089, kl_loss = 0.0035698693245649338\n",
      "\n",
      "Epoch 490\n",
      "Step 0: loss = 0.0479227714240551, recon_loss = 0.04319857805967331, 0.004712053574621677, kl_loss = 0.006070445291697979\n",
      "\n",
      "Epoch 491\n",
      "Step 0: loss = 0.04717637225985527, recon_loss = 0.04272649437189102, 0.004439195152372122, kl_loss = 0.005342491902410984\n",
      "\n",
      "Epoch 492\n",
      "Step 0: loss = 0.04594112187623978, recon_loss = 0.04179341346025467, 0.004140344448387623, kl_loss = 0.0036831004545092583\n",
      "\n",
      "Epoch 493\n",
      "Step 0: loss = 0.04391572251915932, recon_loss = 0.03988363593816757, 0.004025542177259922, kl_loss = 0.0032719727605581284\n",
      "\n",
      "Epoch 494\n",
      "Step 0: loss = 0.04561375454068184, recon_loss = 0.04062293469905853, 0.004986198153346777, kl_loss = 0.0023110192269086838\n",
      "\n",
      "Epoch 495\n",
      "Step 0: loss = 0.0454779788851738, recon_loss = 0.040365226566791534, 0.005107277072966099, kl_loss = 0.0027371617034077644\n",
      "\n",
      "Epoch 496\n",
      "Step 0: loss = 0.04570460692048073, recon_loss = 0.04079754650592804, 0.004901879467070103, kl_loss = 0.002590755932033062\n",
      "\n",
      "Epoch 497\n",
      "Step 0: loss = 0.04067668691277504, recon_loss = 0.03717159852385521, 0.0035000182688236237, kl_loss = 0.0025359736755490303\n",
      "\n",
      "Epoch 498\n",
      "Step 0: loss = 0.04420878365635872, recon_loss = 0.040222182869911194, 0.003978597931563854, kl_loss = 0.004000148735940456\n",
      "\n",
      "Epoch 499\n",
      "Step 0: loss = 0.043515849858522415, recon_loss = 0.038401030004024506, 0.005101687274873257, kl_loss = 0.006566214375197887\n",
      "\n",
      "Epoch 500\n",
      "Step 0: loss = 0.04181087762117386, recon_loss = 0.03741674870252609, 0.004387637600302696, kl_loss = 0.0032439623028039932\n",
      "\n",
      "Epoch 501\n",
      "Step 0: loss = 0.04245151951909065, recon_loss = 0.038246363401412964, 0.0041974205523729324, kl_loss = 0.0038683945313096046\n",
      "\n",
      "Epoch 502\n",
      "Step 0: loss = 0.042482148855924606, recon_loss = 0.03687647730112076, 0.0055982815101742744, kl_loss = 0.0036961501464247704\n",
      "\n",
      "Epoch 503\n",
      "Step 0: loss = 0.04445471614599228, recon_loss = 0.04094560816884041, 0.0035008748527616262, kl_loss = 0.004116671159863472\n",
      "\n",
      "Epoch 504\n",
      "Step 0: loss = 0.04003890976309776, recon_loss = 0.03616013377904892, 0.0038741235621273518, kl_loss = 0.0023265043273568153\n",
      "\n",
      "Epoch 505\n",
      "Step 0: loss = 0.044257406145334244, recon_loss = 0.037920333445072174, 0.0063323406502604485, kl_loss = 0.0023650089278817177\n",
      "\n",
      "Epoch 506\n",
      "Step 0: loss = 0.0404779314994812, recon_loss = 0.03585796430706978, 0.004610875621438026, kl_loss = 0.004545114003121853\n",
      "\n",
      "Epoch 507\n",
      "Step 0: loss = 0.038561463356018066, recon_loss = 0.03312761336565018, 0.005428534001111984, kl_loss = 0.0026580234989523888\n",
      "\n",
      "Epoch 508\n",
      "Step 0: loss = 0.03744795173406601, recon_loss = 0.03323475643992424, 0.0042074741795659065, kl_loss = 0.0028612306341528893\n",
      "\n",
      "Epoch 509\n",
      "Step 0: loss = 0.040264446288347244, recon_loss = 0.03754626214504242, 0.002713289577513933, kl_loss = 0.0024470947682857513\n",
      "\n",
      "Epoch 510\n",
      "Step 0: loss = 0.03978545218706131, recon_loss = 0.03496163338422775, 0.004812417086213827, kl_loss = 0.005702476017177105\n",
      "\n",
      "Epoch 511\n",
      "Step 0: loss = 0.04021682217717171, recon_loss = 0.03549052029848099, 0.004719108808785677, kl_loss = 0.0035972818732261658\n",
      "\n",
      "Epoch 512\n",
      "Step 0: loss = 0.0405362993478775, recon_loss = 0.03658992052078247, 0.003941071219742298, kl_loss = 0.0026537245139479637\n",
      "\n",
      "Epoch 513\n",
      "Step 0: loss = 0.038843367248773575, recon_loss = 0.0341896191239357, 0.00464949756860733, kl_loss = 0.002124503254890442\n",
      "\n",
      "Epoch 514\n",
      "Step 0: loss = 0.0375601090490818, recon_loss = 0.03362361714243889, 0.003932437859475613, kl_loss = 0.0020272862166166306\n",
      "\n",
      "Epoch 515\n",
      "Step 0: loss = 0.04211723059415817, recon_loss = 0.036165863275527954, 0.005943657830357552, kl_loss = 0.0038555925711989403\n",
      "\n",
      "Epoch 516\n",
      "Step 0: loss = 0.03765704855322838, recon_loss = 0.03381698578596115, 0.0038348098751157522, kl_loss = 0.002626522444188595\n",
      "\n",
      "Epoch 517\n",
      "Step 0: loss = 0.03776468709111214, recon_loss = 0.03329164534807205, 0.004468861036002636, kl_loss = 0.002089517191052437\n",
      "\n",
      "Epoch 518\n",
      "Step 0: loss = 0.03992994502186775, recon_loss = 0.03485472500324249, 0.00507055688649416, kl_loss = 0.002331433817744255\n",
      "\n",
      "Epoch 519\n",
      "Step 0: loss = 0.03509559482336044, recon_loss = 0.030454207211732864, 0.004635912366211414, kl_loss = 0.002738574519753456\n",
      "\n",
      "Epoch 520\n",
      "Step 0: loss = 0.03843086585402489, recon_loss = 0.0327628031373024, 0.005663642194122076, kl_loss = 0.0022114962339401245\n",
      "\n",
      "Epoch 521\n",
      "Step 0: loss = 0.03391635790467262, recon_loss = 0.029640937224030495, 0.004269179422408342, kl_loss = 0.003120589070022106\n",
      "\n",
      "Epoch 522\n",
      "Step 0: loss = 0.03463922068476677, recon_loss = 0.03008568100631237, 0.004549168050289154, kl_loss = 0.0021846992895007133\n",
      "\n",
      "Epoch 523\n",
      "Step 0: loss = 0.03416593745350838, recon_loss = 0.029900265857577324, 0.004257651045918465, kl_loss = 0.004009980708360672\n",
      "\n",
      "Epoch 524\n",
      "Step 0: loss = 0.035017769783735275, recon_loss = 0.02889896370470524, 0.006110782735049725, kl_loss = 0.004012051969766617\n",
      "\n",
      "Epoch 525\n",
      "Step 0: loss = 0.035431571304798126, recon_loss = 0.030483901500701904, 0.00493372417986393, kl_loss = 0.006973715499043465\n",
      "\n",
      "Epoch 526\n",
      "Step 0: loss = 0.034086793661117554, recon_loss = 0.03020823933184147, 0.0038518691435456276, kl_loss = 0.013341874815523624\n",
      "\n",
      "Epoch 527\n",
      "Step 0: loss = 0.03276808187365532, recon_loss = 0.02846398577094078, 0.0042882515117526054, kl_loss = 0.007921895012259483\n",
      "\n",
      "Epoch 528\n",
      "Step 0: loss = 0.03059685416519642, recon_loss = 0.025859583169221878, 0.004729872569441795, kl_loss = 0.003699607215821743\n",
      "\n",
      "Epoch 529\n",
      "Step 0: loss = 0.032054025679826736, recon_loss = 0.026560066267848015, 0.005487627815455198, kl_loss = 0.0031667836010456085\n",
      "\n",
      "Epoch 530\n",
      "Step 0: loss = 0.03239036723971367, recon_loss = 0.02748994156718254, 0.004891644231975079, kl_loss = 0.004390152171254158\n",
      "\n",
      "Epoch 531\n",
      "Step 0: loss = 0.032121967524290085, recon_loss = 0.02837919257581234, 0.0037214993499219418, kl_loss = 0.010637307539582253\n",
      "\n",
      "Epoch 532\n",
      "Step 0: loss = 0.031725890934467316, recon_loss = 0.026658574119210243, 0.005060906056314707, kl_loss = 0.0032059205695986748\n",
      "\n",
      "Epoch 533\n",
      "Step 0: loss = 0.034056153148412704, recon_loss = 0.02826114371418953, 0.005791057832539082, kl_loss = 0.0019765179604291916\n",
      "\n",
      "Epoch 534\n",
      "Step 0: loss = 0.03038308024406433, recon_loss = 0.02519439160823822, 0.005183207802474499, kl_loss = 0.002739725634455681\n",
      "\n",
      "Epoch 535\n",
      "Step 0: loss = 0.03212917596101761, recon_loss = 0.026783199980854988, 0.005329011939466, kl_loss = 0.008481641300022602\n",
      "\n",
      "Epoch 536\n",
      "Step 0: loss = 0.03325103223323822, recon_loss = 0.028221147134900093, 0.00501586589962244, kl_loss = 0.007008535787463188\n",
      "\n",
      "Epoch 537\n",
      "Step 0: loss = 0.030535126104950905, recon_loss = 0.025550685822963715, 0.004976888652890921, kl_loss = 0.003775235265493393\n",
      "\n",
      "Epoch 538\n",
      "Step 0: loss = 0.031232094392180443, recon_loss = 0.026875069364905357, 0.004344216547906399, kl_loss = 0.006404414772987366\n",
      "\n",
      "Epoch 539\n",
      "Step 0: loss = 0.0310350488871336, recon_loss = 0.025877369567751884, 0.00514219980686903, kl_loss = 0.007740527391433716\n",
      "\n",
      "Epoch 540\n",
      "Step 0: loss = 0.031118342652916908, recon_loss = 0.025932958349585533, 0.0051695057190954685, kl_loss = 0.007939068600535393\n",
      "\n",
      "Epoch 541\n",
      "Step 0: loss = 0.03188665583729744, recon_loss = 0.028129613026976585, 0.0037449889350682497, kl_loss = 0.006027857773005962\n",
      "\n",
      "Epoch 542\n",
      "Step 0: loss = 0.03396383672952652, recon_loss = 0.028504278510808945, 0.00544174388051033, kl_loss = 0.008907604031264782\n",
      "\n",
      "Epoch 543\n",
      "Step 0: loss = 0.029550909996032715, recon_loss = 0.024330049753189087, 0.005214638076722622, kl_loss = 0.0031105754896998405\n",
      "\n",
      "Epoch 544\n",
      "Step 0: loss = 0.03157205134630203, recon_loss = 0.026946822181344032, 0.0046182963997125626, kl_loss = 0.003466010093688965\n",
      "\n",
      "Epoch 545\n",
      "Step 0: loss = 0.02817670628428459, recon_loss = 0.02376159466803074, 0.004407469183206558, kl_loss = 0.0038207555189728737\n",
      "\n",
      "Epoch 546\n",
      "Step 0: loss = 0.02965470217168331, recon_loss = 0.025337377563118935, 0.004310924559831619, kl_loss = 0.003200314939022064\n",
      "\n",
      "Epoch 547\n",
      "Step 0: loss = 0.033509328961372375, recon_loss = 0.027565686032176018, 0.005938151851296425, kl_loss = 0.0027451785281300545\n",
      "\n",
      "Epoch 548\n",
      "Step 0: loss = 0.030833087861537933, recon_loss = 0.025540832430124283, 0.0052860090509057045, kl_loss = 0.003123415634036064\n",
      "\n",
      "Epoch 549\n",
      "Step 0: loss = 0.02966208942234516, recon_loss = 0.024723052978515625, 0.004931908566504717, kl_loss = 0.003564116545021534\n",
      "\n",
      "Epoch 550\n",
      "Step 0: loss = 0.028041312471032143, recon_loss = 0.024325206875801086, 0.003708174452185631, kl_loss = 0.00396539643406868\n",
      "\n",
      "Epoch 551\n",
      "Step 0: loss = 0.029790641739964485, recon_loss = 0.02513483539223671, 0.00464651919901371, kl_loss = 0.004643207415938377\n",
      "\n",
      "Epoch 552\n",
      "Step 0: loss = 0.025483835488557816, recon_loss = 0.021640446037054062, 0.0038320112507790327, kl_loss = 0.00568950641900301\n",
      "\n",
      "Epoch 553\n",
      "Step 0: loss = 0.02881169505417347, recon_loss = 0.022366775199770927, 0.006434716284275055, kl_loss = 0.005101620219647884\n",
      "\n",
      "Epoch 554\n",
      "Step 0: loss = 0.032037895172834396, recon_loss = 0.024637186899781227, 0.007391150575131178, kl_loss = 0.0047800252214074135\n",
      "\n",
      "Epoch 555\n",
      "Step 0: loss = 0.03148604929447174, recon_loss = 0.024762548506259918, 0.006701841484755278, kl_loss = 0.010830257087945938\n",
      "\n",
      "Epoch 556\n",
      "Step 0: loss = 0.02840554155409336, recon_loss = 0.02041047066450119, 0.007969269528985023, kl_loss = 0.012900730594992638\n",
      "\n",
      "Epoch 557\n",
      "Step 0: loss = 0.028527043759822845, recon_loss = 0.02285441942512989, 0.0056616151705384254, kl_loss = 0.005503826774656773\n",
      "\n",
      "Epoch 558\n",
      "Step 0: loss = 0.030166804790496826, recon_loss = 0.023400403559207916, 0.0067592160776257515, kl_loss = 0.003593248315155506\n",
      "\n",
      "Epoch 559\n",
      "Step 0: loss = 0.025753125548362732, recon_loss = 0.020619219169020653, 0.0051256464794278145, kl_loss = 0.00412963330745697\n",
      "\n",
      "Epoch 560\n",
      "Step 0: loss = 0.029232289642095566, recon_loss = 0.023732364177703857, 0.005489787086844444, kl_loss = 0.005068879574537277\n",
      "\n",
      "Epoch 561\n",
      "Step 0: loss = 0.028778469190001488, recon_loss = 0.02259228751063347, 0.006177350878715515, kl_loss = 0.004415472969412804\n",
      "\n",
      "Epoch 562\n",
      "Step 0: loss = 0.026047758758068085, recon_loss = 0.021681321784853935, 0.004359239712357521, kl_loss = 0.003599051386117935\n",
      "\n",
      "Epoch 563\n",
      "Step 0: loss = 0.02383730374276638, recon_loss = 0.018796466290950775, 0.005034071858972311, kl_loss = 0.003382987342774868\n",
      "\n",
      "Epoch 564\n",
      "Step 0: loss = 0.0276948194950819, recon_loss = 0.021826131269335747, 0.005858561955392361, kl_loss = 0.005063737742602825\n",
      "\n",
      "Epoch 565\n",
      "Step 0: loss = 0.02436603605747223, recon_loss = 0.0194492656737566, 0.004909750074148178, kl_loss = 0.003510299138724804\n",
      "\n",
      "Epoch 566\n",
      "Step 0: loss = 0.024213556200265884, recon_loss = 0.01958457939326763, 0.004620993975549936, kl_loss = 0.003992105834186077\n",
      "\n",
      "Epoch 567\n",
      "Step 0: loss = 0.023294763639569283, recon_loss = 0.019136467948555946, 0.004152154084295034, kl_loss = 0.003070124424993992\n",
      "\n",
      "Epoch 568\n",
      "Step 0: loss = 0.024708053097128868, recon_loss = 0.01968126744031906, 0.005021733231842518, kl_loss = 0.002526748925447464\n",
      "\n",
      "Epoch 569\n",
      "Step 0: loss = 0.024123694747686386, recon_loss = 0.01976281963288784, 0.004356091842055321, kl_loss = 0.0023914920166134834\n",
      "\n",
      "Epoch 570\n",
      "Step 0: loss = 0.022996321320533752, recon_loss = 0.01856822520494461, 0.004422773607075214, kl_loss = 0.0026618987321853638\n",
      "\n",
      "Epoch 571\n",
      "Step 0: loss = 0.024972258135676384, recon_loss = 0.018591543659567833, 0.006374310702085495, kl_loss = 0.003201795741915703\n",
      "\n",
      "Epoch 572\n",
      "Step 0: loss = 0.026804111897945404, recon_loss = 0.020080389454960823, 0.006714929826557636, kl_loss = 0.004396037198603153\n",
      "\n",
      "Epoch 573\n",
      "Step 0: loss = 0.025089705362915993, recon_loss = 0.01922713592648506, 0.005855566821992397, kl_loss = 0.0035005537793040276\n",
      "\n",
      "Epoch 574\n",
      "Step 0: loss = 0.025554975494742393, recon_loss = 0.020091570913791656, 0.005451996810734272, kl_loss = 0.005704355426132679\n",
      "\n",
      "Epoch 575\n",
      "Step 0: loss = 0.0236867256462574, recon_loss = 0.020025772973895073, 0.0036526876501739025, kl_loss = 0.0041318628937006\n",
      "\n",
      "Epoch 576\n",
      "Step 0: loss = 0.02512025088071823, recon_loss = 0.01848193258047104, 0.006627845577895641, kl_loss = 0.005235760472714901\n",
      "\n",
      "Epoch 577\n",
      "Step 0: loss = 0.023783771321177483, recon_loss = 0.01894928328692913, 0.004827550612390041, kl_loss = 0.003468855284154415\n",
      "\n",
      "Epoch 578\n",
      "Step 0: loss = 0.02326107583940029, recon_loss = 0.018407050520181656, 0.00484592467546463, kl_loss = 0.004050185903906822\n",
      "\n",
      "Epoch 579\n",
      "Step 0: loss = 0.023374438285827637, recon_loss = 0.017871195450425148, 0.0054951682686805725, kl_loss = 0.004037683829665184\n",
      "\n",
      "Epoch 580\n",
      "Step 0: loss = 0.023768069222569466, recon_loss = 0.01843295618891716, 0.005324378143996, kl_loss = 0.0053674448281526566\n",
      "\n",
      "Epoch 581\n",
      "Step 0: loss = 0.024628492072224617, recon_loss = 0.017671355977654457, 0.006948122289031744, kl_loss = 0.0045065004378557205\n",
      "\n",
      "Epoch 582\n",
      "Step 0: loss = 0.023977532982826233, recon_loss = 0.017753923311829567, 0.006215359549969435, kl_loss = 0.004124659113585949\n",
      "\n",
      "Epoch 583\n",
      "Step 0: loss = 0.024155691266059875, recon_loss = 0.017945267260074615, 0.006201070733368397, kl_loss = 0.004677212797105312\n",
      "\n",
      "Epoch 584\n",
      "Step 0: loss = 0.025363653898239136, recon_loss = 0.017825637012720108, 0.0075213187374174595, kl_loss = 0.00834903959184885\n",
      "\n",
      "Epoch 585\n",
      "Step 0: loss = 0.023436056450009346, recon_loss = 0.016341984272003174, 0.007069785613566637, kl_loss = 0.012143961153924465\n",
      "\n",
      "Epoch 586\n",
      "Step 0: loss = 0.024008838459849358, recon_loss = 0.017824800685048103, 0.006172487046569586, kl_loss = 0.005775135010480881\n",
      "\n",
      "Epoch 587\n",
      "Step 0: loss = 0.023518946021795273, recon_loss = 0.01817251369357109, 0.005332994274795055, kl_loss = 0.006718825548887253\n",
      "\n",
      "Epoch 588\n",
      "Step 0: loss = 0.02049899287521839, recon_loss = 0.01449069008231163, 0.0059967041015625, kl_loss = 0.005799631588160992\n",
      "\n",
      "Epoch 589\n",
      "Step 0: loss = 0.020371997728943825, recon_loss = 0.01603269763290882, 0.004329734016209841, kl_loss = 0.004783174023032188\n",
      "\n",
      "Epoch 590\n",
      "Step 0: loss = 0.02230040915310383, recon_loss = 0.017160777002573013, 0.0051340144127607346, kl_loss = 0.0028089461848139763\n",
      "\n",
      "Epoch 591\n",
      "Step 0: loss = 0.020017553120851517, recon_loss = 0.015111986547708511, 0.004900479689240456, kl_loss = 0.002543702721595764\n",
      "\n",
      "Epoch 592\n",
      "Step 0: loss = 0.021773353219032288, recon_loss = 0.016182884573936462, 0.005578190553933382, kl_loss = 0.006139577366411686\n",
      "\n",
      "Epoch 593\n",
      "Step 0: loss = 0.02233082242310047, recon_loss = 0.015272621065378189, 0.007051634602248669, kl_loss = 0.0032832277938723564\n",
      "\n",
      "Epoch 594\n",
      "Step 0: loss = 0.021866310387849808, recon_loss = 0.01587354950606823, 0.00598448421806097, kl_loss = 0.00413911696523428\n",
      "\n",
      "Epoch 595\n",
      "Step 0: loss = 0.02236904203891754, recon_loss = 0.01675330474972725, 0.005609078798443079, kl_loss = 0.0033297836780548096\n",
      "\n",
      "Epoch 596\n",
      "Step 0: loss = 0.02075185999274254, recon_loss = 0.014820732176303864, 0.005923306569457054, kl_loss = 0.003910556435585022\n",
      "\n",
      "Epoch 597\n",
      "Step 0: loss = 0.021455522626638412, recon_loss = 0.016034217551350594, 0.005411886610090733, kl_loss = 0.0047090183943510056\n",
      "\n",
      "Epoch 598\n",
      "Step 0: loss = 0.021600784733891487, recon_loss = 0.016312573105096817, 0.005279297474771738, kl_loss = 0.004457483068108559\n",
      "\n",
      "Epoch 599\n",
      "Step 0: loss = 0.02052503265440464, recon_loss = 0.014752652496099472, 0.005767263006418943, kl_loss = 0.002558770589530468\n",
      "\n",
      "Epoch 600\n",
      "Step 0: loss = 0.021679392084479332, recon_loss = 0.01588096097111702, 0.005793009418994188, kl_loss = 0.0027109142392873764\n",
      "\n",
      "Epoch 601\n",
      "Step 0: loss = 0.021324671804904938, recon_loss = 0.01492929458618164, 0.006386422086507082, kl_loss = 0.004477521404623985\n",
      "\n",
      "Epoch 602\n",
      "Step 0: loss = 0.020758789032697678, recon_loss = 0.015420306473970413, 0.005330958403646946, kl_loss = 0.003762626089155674\n",
      "\n",
      "Epoch 603\n",
      "Step 0: loss = 0.02275880239903927, recon_loss = 0.014162393286824226, 0.00858844630420208, kl_loss = 0.003980973735451698\n",
      "\n",
      "Epoch 604\n",
      "Step 0: loss = 0.020110726356506348, recon_loss = 0.01456747017800808, 0.005531533155590296, kl_loss = 0.00586149375885725\n",
      "\n",
      "Epoch 605\n",
      "Step 0: loss = 0.021070541813969612, recon_loss = 0.01438574306666851, 0.006665914319455624, kl_loss = 0.009442655369639397\n",
      "\n",
      "Epoch 606\n",
      "Step 0: loss = 0.021964669227600098, recon_loss = 0.01567942276597023, 0.006274271756410599, kl_loss = 0.0054874420166015625\n",
      "\n",
      "Epoch 607\n",
      "Step 0: loss = 0.02050614356994629, recon_loss = 0.014676019549369812, 0.00581671018153429, kl_loss = 0.006707316264510155\n",
      "\n",
      "Epoch 608\n",
      "Step 0: loss = 0.019075186923146248, recon_loss = 0.013802720233798027, 0.005264222156256437, kl_loss = 0.0041217561811208725\n",
      "\n",
      "Epoch 609\n",
      "Step 0: loss = 0.020696070045232773, recon_loss = 0.014973299577832222, 0.005714247934520245, kl_loss = 0.004261808469891548\n",
      "\n",
      "Epoch 610\n",
      "Step 0: loss = 0.019198955968022346, recon_loss = 0.013972742483019829, 0.005215152632445097, kl_loss = 0.0055305566638708115\n",
      "\n",
      "Epoch 611\n",
      "Step 0: loss = 0.018148068338632584, recon_loss = 0.012791046872735023, 0.005343368276953697, kl_loss = 0.006826695054769516\n",
      "\n",
      "Epoch 612\n",
      "Step 0: loss = 0.01896272785961628, recon_loss = 0.014339659363031387, 0.004608486779034138, kl_loss = 0.007291490212082863\n",
      "\n",
      "Epoch 613\n",
      "Step 0: loss = 0.017903350293636322, recon_loss = 0.0132016371935606, 0.004690143279731274, kl_loss = 0.00578553881496191\n",
      "\n",
      "Epoch 614\n",
      "Step 0: loss = 0.019394664093852043, recon_loss = 0.013827808201313019, 0.005559192039072514, kl_loss = 0.003832305781543255\n",
      "\n",
      "Epoch 615\n",
      "Step 0: loss = 0.019755830988287926, recon_loss = 0.01341274008154869, 0.006332259625196457, kl_loss = 0.005415456369519234\n",
      "\n",
      "Epoch 616\n",
      "Step 0: loss = 0.018524199724197388, recon_loss = 0.012994889169931412, 0.0055237943306565285, kl_loss = 0.0027590394020080566\n",
      "\n",
      "Epoch 617\n",
      "Step 0: loss = 0.02129930816590786, recon_loss = 0.014007722958922386, 0.007281661964952946, kl_loss = 0.004960930906236172\n",
      "\n",
      "Epoch 618\n",
      "Step 0: loss = 0.018199166283011436, recon_loss = 0.012858599424362183, 0.005323369987308979, kl_loss = 0.008598586544394493\n",
      "\n",
      "Epoch 619\n",
      "Step 0: loss = 0.021543817594647408, recon_loss = 0.01368780992925167, 0.007832666859030724, kl_loss = 0.011670305393636227\n",
      "\n",
      "Epoch 620\n",
      "Step 0: loss = 0.015532291494309902, recon_loss = 0.01143612153828144, 0.004080837592482567, kl_loss = 0.007665977813303471\n",
      "\n",
      "Epoch 621\n",
      "Step 0: loss = 0.0165396835654974, recon_loss = 0.012188451364636421, 0.0043428512290120125, kl_loss = 0.004190131090581417\n",
      "\n",
      "Epoch 622\n",
      "Step 0: loss = 0.018916675820946693, recon_loss = 0.012079905718564987, 0.006828435231000185, kl_loss = 0.004167890176177025\n",
      "\n",
      "Epoch 623\n",
      "Step 0: loss = 0.019454924389719963, recon_loss = 0.013382093980908394, 0.0060592093504965305, kl_loss = 0.006810793653130531\n",
      "\n",
      "Epoch 624\n",
      "Step 0: loss = 0.01991252228617668, recon_loss = 0.013476911932229996, 0.006427586078643799, kl_loss = 0.004012380726635456\n",
      "\n",
      "Epoch 625\n",
      "Step 0: loss = 0.017762592062354088, recon_loss = 0.012325409799814224, 0.005419059190899134, kl_loss = 0.009062089957296848\n",
      "\n",
      "Epoch 626\n",
      "Step 0: loss = 0.015886133536696434, recon_loss = 0.011870838701725006, 0.00398787809535861, kl_loss = 0.013707808218896389\n",
      "\n",
      "Epoch 627\n",
      "Step 0: loss = 0.01667759381234646, recon_loss = 0.011530568823218346, 0.005131793208420277, kl_loss = 0.007615131326019764\n",
      "\n",
      "Epoch 628\n",
      "Step 0: loss = 0.016772545874118805, recon_loss = 0.01154610887169838, 0.005211587529629469, kl_loss = 0.0074249207973480225\n",
      "\n",
      "Epoch 629\n",
      "Step 0: loss = 0.013777819462120533, recon_loss = 0.009938452392816544, 0.003822757862508297, kl_loss = 0.00830464344471693\n",
      "\n",
      "Epoch 630\n",
      "Step 0: loss = 0.015936123207211494, recon_loss = 0.01128784753382206, 0.0046375822275877, kl_loss = 0.005346260033547878\n",
      "\n",
      "Epoch 631\n",
      "Step 0: loss = 0.017952915281057358, recon_loss = 0.011455772444605827, 0.006477177143096924, kl_loss = 0.009982693009078503\n",
      "\n",
      "Epoch 632\n",
      "Step 0: loss = 0.01756208762526512, recon_loss = 0.012048179283738136, 0.0055001829750835896, kl_loss = 0.0068626487627625465\n",
      "\n",
      "Epoch 633\n",
      "Step 0: loss = 0.017158234491944313, recon_loss = 0.011639721691608429, 0.005500349216163158, kl_loss = 0.009081427939236164\n",
      "\n",
      "Epoch 634\n",
      "Step 0: loss = 0.014738861471414566, recon_loss = 0.010441910475492477, 0.00428523076698184, kl_loss = 0.005859974771738052\n",
      "\n",
      "Epoch 635\n",
      "Step 0: loss = 0.016172893345355988, recon_loss = 0.011542480438947678, 0.0046207495033741, kl_loss = 0.004832016304135323\n",
      "\n",
      "Epoch 636\n",
      "Step 0: loss = 0.018505174666643143, recon_loss = 0.011978693306446075, 0.006514736916869879, kl_loss = 0.005871642380952835\n",
      "\n",
      "Epoch 637\n",
      "Step 0: loss = 0.01715773344039917, recon_loss = 0.011705057695508003, 0.005435666535049677, kl_loss = 0.008504753932356834\n",
      "\n",
      "Epoch 638\n",
      "Step 0: loss = 0.01857149787247181, recon_loss = 0.013014839962124825, 0.00553713645786047, kl_loss = 0.00976089108735323\n",
      "\n",
      "Epoch 639\n",
      "Step 0: loss = 0.019355205819010735, recon_loss = 0.012744870036840439, 0.0066007571294903755, kl_loss = 0.004789786413311958\n",
      "\n",
      "Epoch 640\n",
      "Step 0: loss = 0.016908423975110054, recon_loss = 0.01228935457766056, 0.004612916149199009, kl_loss = 0.0030762804672122\n",
      "\n",
      "Epoch 641\n",
      "Step 0: loss = 0.014635556377470493, recon_loss = 0.009570110589265823, 0.005056522786617279, kl_loss = 0.004461443051695824\n",
      "\n",
      "Epoch 642\n",
      "Step 0: loss = 0.015950169414281845, recon_loss = 0.010795475915074348, 0.00514242285862565, kl_loss = 0.006135161034762859\n",
      "\n",
      "Epoch 643\n",
      "Step 0: loss = 0.015925196930766106, recon_loss = 0.010037841275334358, 0.005881353281438351, kl_loss = 0.0030014794319868088\n",
      "\n",
      "Epoch 644\n",
      "Step 0: loss = 0.016796955838799477, recon_loss = 0.009903335943818092, 0.006886918563395739, kl_loss = 0.0033508213236927986\n",
      "\n",
      "Epoch 645\n",
      "Step 0: loss = 0.018356118351221085, recon_loss = 0.011190755292773247, 0.007158956956118345, kl_loss = 0.003202461637556553\n",
      "\n",
      "Epoch 646\n",
      "Step 0: loss = 0.018270043656229973, recon_loss = 0.010890169069170952, 0.007374193519353867, kl_loss = 0.002840542234480381\n",
      "\n",
      "Epoch 647\n",
      "Step 0: loss = 0.015135634690523148, recon_loss = 0.01061207428574562, 0.004517434164881706, kl_loss = 0.003063325770199299\n",
      "\n",
      "Epoch 648\n",
      "Step 0: loss = 0.015398683026432991, recon_loss = 0.009855173528194427, 0.005539282225072384, kl_loss = 0.0021134670823812485\n",
      "\n",
      "Epoch 649\n",
      "Step 0: loss = 0.017023568972945213, recon_loss = 0.00985880009829998, 0.007159492466598749, kl_loss = 0.002638407051563263\n",
      "\n",
      "Epoch 650\n",
      "Step 0: loss = 0.014363388530910015, recon_loss = 0.00958167389035225, 0.004776692483574152, kl_loss = 0.002511119470000267\n",
      "\n",
      "Epoch 651\n",
      "Step 0: loss = 0.0141980592161417, recon_loss = 0.009321263059973717, 0.004870163276791573, kl_loss = 0.0033165058121085167\n",
      "\n",
      "Epoch 652\n",
      "Step 0: loss = 0.01407381147146225, recon_loss = 0.008430901914834976, 0.005634994246065617, kl_loss = 0.003957787528634071\n",
      "\n",
      "Epoch 653\n",
      "Step 0: loss = 0.013002167455852032, recon_loss = 0.008145855739712715, 0.004838434513658285, kl_loss = 0.008938389830291271\n",
      "\n",
      "Epoch 654\n",
      "Step 0: loss = 0.013744591735303402, recon_loss = 0.009103173390030861, 0.004629464820027351, kl_loss = 0.005976980552077293\n",
      "\n",
      "Epoch 655\n",
      "Step 0: loss = 0.013127664104104042, recon_loss = 0.008908247575163841, 0.004207257181406021, kl_loss = 0.006079851649701595\n",
      "\n",
      "Epoch 656\n",
      "Step 0: loss = 0.014812692999839783, recon_loss = 0.009642846882343292, 0.0051613450050354, kl_loss = 0.004250579513609409\n",
      "\n",
      "Epoch 657\n",
      "Step 0: loss = 0.015162600204348564, recon_loss = 0.009208593517541885, 0.0059483288787305355, kl_loss = 0.0028387000784277916\n",
      "\n",
      "Epoch 658\n",
      "Step 0: loss = 0.015704374760389328, recon_loss = 0.010204024612903595, 0.005485175177454948, kl_loss = 0.007587628439068794\n",
      "\n",
      "Epoch 659\n",
      "Step 0: loss = 0.015735618770122528, recon_loss = 0.01047075167298317, 0.005256918724626303, kl_loss = 0.0039743781089782715\n",
      "\n",
      "Epoch 660\n",
      "Step 0: loss = 0.018701838329434395, recon_loss = 0.01159285195171833, 0.007094344589859247, kl_loss = 0.00732087716460228\n",
      "\n",
      "Epoch 661\n",
      "Step 0: loss = 0.017273837700486183, recon_loss = 0.01085796020925045, 0.00638230238109827, kl_loss = 0.016787171363830566\n",
      "\n",
      "Epoch 662\n",
      "Step 0: loss = 0.014956674538552761, recon_loss = 0.010763144120573997, 0.004177821800112724, kl_loss = 0.007854294031858444\n",
      "\n",
      "Epoch 663\n",
      "Step 0: loss = 0.01470143347978592, recon_loss = 0.010515224188566208, 0.004171103239059448, kl_loss = 0.00755279790610075\n",
      "\n",
      "Epoch 664\n",
      "Step 0: loss = 0.015912003815174103, recon_loss = 0.010655749589204788, 0.005245544947683811, kl_loss = 0.005354705266654491\n",
      "\n",
      "Epoch 665\n",
      "Step 0: loss = 0.015397840179502964, recon_loss = 0.009608259424567223, 0.005780336447060108, kl_loss = 0.004622181877493858\n",
      "\n",
      "Epoch 666\n",
      "Step 0: loss = 0.013457121327519417, recon_loss = 0.009179191663861275, 0.004269702825695276, kl_loss = 0.004113540984690189\n",
      "\n",
      "Epoch 667\n",
      "Step 0: loss = 0.015242218971252441, recon_loss = 0.009386980906128883, 0.0058385166339576244, kl_loss = 0.00836064200848341\n",
      "\n",
      "Epoch 668\n",
      "Step 0: loss = 0.014983207918703556, recon_loss = 0.009592678397893906, 0.0053796907886862755, kl_loss = 0.005419475957751274\n",
      "\n",
      "Epoch 669\n",
      "Step 0: loss = 0.016263576224446297, recon_loss = 0.009039238095283508, 0.007215237244963646, kl_loss = 0.004550223238766193\n",
      "\n",
      "Epoch 670\n",
      "Step 0: loss = 0.014776703901588917, recon_loss = 0.008875448256731033, 0.005893808323889971, kl_loss = 0.003724082373082638\n",
      "\n",
      "Epoch 671\n",
      "Step 0: loss = 0.016729628667235374, recon_loss = 0.010258648544549942, 0.006450638175010681, kl_loss = 0.010170554742217064\n",
      "\n",
      "Epoch 672\n",
      "Step 0: loss = 0.014501918107271194, recon_loss = 0.00841684639453888, 0.00607699528336525, kl_loss = 0.004038295708596706\n",
      "\n",
      "Epoch 673\n",
      "Step 0: loss = 0.017395447939634323, recon_loss = 0.010567689314484596, 0.006819188594818115, kl_loss = 0.0042853038758039474\n",
      "\n",
      "Epoch 674\n",
      "Step 0: loss = 0.01450431440025568, recon_loss = 0.009926792234182358, 0.004568865522742271, kl_loss = 0.004328092560172081\n",
      "\n",
      "Epoch 675\n",
      "Step 0: loss = 0.01525054033845663, recon_loss = 0.009268656373023987, 0.005977741442620754, kl_loss = 0.0020711375400424004\n",
      "\n",
      "Epoch 676\n",
      "Step 0: loss = 0.014176823198795319, recon_loss = 0.0083211250603199, 0.005846027284860611, kl_loss = 0.004835527390241623\n",
      "\n",
      "Epoch 677\n",
      "Step 0: loss = 0.014437861740589142, recon_loss = 0.00954282097518444, 0.004883076064288616, kl_loss = 0.005982302129268646\n",
      "\n",
      "Epoch 678\n",
      "Step 0: loss = 0.012835587374866009, recon_loss = 0.008533162996172905, 0.00429378729313612, kl_loss = 0.0043184272944927216\n",
      "\n",
      "Epoch 679\n",
      "Step 0: loss = 0.013910743407905102, recon_loss = 0.008479984477162361, 0.005418837070465088, kl_loss = 0.005961157381534576\n",
      "\n",
      "Epoch 680\n",
      "Step 0: loss = 0.013906440697610378, recon_loss = 0.007988844066858292, 0.005911129526793957, kl_loss = 0.0032335296273231506\n",
      "\n",
      "Epoch 681\n",
      "Step 0: loss = 0.013580887578427792, recon_loss = 0.009374009445309639, 0.0041996147483587265, kl_loss = 0.0036318255588412285\n",
      "\n",
      "Epoch 682\n",
      "Step 0: loss = 0.014811445027589798, recon_loss = 0.009887704625725746, 0.004915990866720676, kl_loss = 0.003874874673783779\n",
      "\n",
      "Epoch 683\n",
      "Step 0: loss = 0.01560478936880827, recon_loss = 0.010814936831593513, 0.004778137896209955, kl_loss = 0.005857562646269798\n",
      "\n",
      "Epoch 684\n",
      "Step 0: loss = 0.0128270722925663, recon_loss = 0.0077376589179039, 0.005077683366835117, kl_loss = 0.005864979699254036\n",
      "\n",
      "Epoch 685\n",
      "Step 0: loss = 0.01457423996180296, recon_loss = 0.008994879201054573, 0.005567397456616163, kl_loss = 0.0059812553226947784\n",
      "\n",
      "Epoch 686\n",
      "Step 0: loss = 0.013764940202236176, recon_loss = 0.009160563349723816, 0.004594060126692057, kl_loss = 0.005158685147762299\n",
      "\n",
      "Epoch 687\n",
      "Step 0: loss = 0.01533897127956152, recon_loss = 0.009221876040101051, 0.006110639311373234, kl_loss = 0.0032280273735523224\n",
      "\n",
      "Epoch 688\n",
      "Step 0: loss = 0.01364667434245348, recon_loss = 0.009284986183047295, 0.004355689510703087, kl_loss = 0.0029991576448082924\n",
      "\n",
      "Epoch 689\n",
      "Step 0: loss = 0.013682189397513866, recon_loss = 0.008117349818348885, 0.005527111701667309, kl_loss = 0.01886408030986786\n",
      "\n",
      "Epoch 690\n",
      "Step 0: loss = 0.013157409615814686, recon_loss = 0.008484851568937302, 0.00463668629527092, kl_loss = 0.01793602481484413\n",
      "\n",
      "Epoch 691\n",
      "Step 0: loss = 0.01461757905781269, recon_loss = 0.008855549618601799, 0.0057555194944143295, kl_loss = 0.003255174495279789\n",
      "\n",
      "Epoch 692\n",
      "Step 0: loss = 0.013730809092521667, recon_loss = 0.006943393498659134, 0.006771898362785578, kl_loss = 0.007758730091154575\n",
      "\n",
      "Epoch 693\n",
      "Step 0: loss = 0.012741184793412685, recon_loss = 0.007716704159975052, 0.005013640038669109, kl_loss = 0.005420356057584286\n",
      "\n",
      "Epoch 694\n",
      "Step 0: loss = 0.0177724901586771, recon_loss = 0.010058259591460228, 0.007706589065492153, kl_loss = 0.0038214335218071938\n",
      "\n",
      "Epoch 695\n",
      "Step 0: loss = 0.013643375597894192, recon_loss = 0.008676914498209953, 0.004953217692673206, kl_loss = 0.006621764972805977\n",
      "\n",
      "Epoch 696\n",
      "Step 0: loss = 0.015435215085744858, recon_loss = 0.009049760177731514, 0.006369052454829216, kl_loss = 0.008201438933610916\n",
      "\n",
      "Epoch 697\n",
      "Step 0: loss = 0.012447912245988846, recon_loss = 0.0076438505202531815, 0.0047800117172300816, kl_loss = 0.012025143951177597\n",
      "\n",
      "Epoch 698\n",
      "Step 0: loss = 0.012186605483293533, recon_loss = 0.007787395268678665, 0.004384415224194527, kl_loss = 0.007397271692752838\n",
      "\n",
      "Epoch 699\n",
      "Step 0: loss = 0.012952843680977821, recon_loss = 0.007123230025172234, 0.005824022926390171, kl_loss = 0.002795557491481304\n",
      "\n",
      "Epoch 700\n",
      "Step 0: loss = 0.01348988339304924, recon_loss = 0.007540246471762657, 0.0059453877620399, kl_loss = 0.002124412916600704\n",
      "\n",
      "Epoch 701\n",
      "Step 0: loss = 0.013810495845973492, recon_loss = 0.006697718054056168, 0.007098769769072533, kl_loss = 0.0070040179416537285\n",
      "\n",
      "Epoch 702\n",
      "Step 0: loss = 0.01280134730041027, recon_loss = 0.006719304248690605, 0.006063262932002544, kl_loss = 0.009390211664140224\n",
      "\n",
      "Epoch 703\n",
      "Step 0: loss = 0.011299978010356426, recon_loss = 0.0071053411811590195, 0.004179312847554684, kl_loss = 0.007662130519747734\n",
      "\n",
      "Epoch 704\n",
      "Step 0: loss = 0.012843762524425983, recon_loss = 0.006979355588555336, 0.00584373576566577, kl_loss = 0.01033518835902214\n",
      "\n",
      "Epoch 705\n",
      "Step 0: loss = 0.01209827046841383, recon_loss = 0.007004017010331154, 0.00507320836186409, kl_loss = 0.010522511787712574\n",
      "\n",
      "Epoch 706\n",
      "Step 0: loss = 0.010372741147875786, recon_loss = 0.005932716652750969, 0.004424838349223137, kl_loss = 0.0075929416343569756\n",
      "\n",
      "Epoch 707\n",
      "Step 0: loss = 0.012384478002786636, recon_loss = 0.006543170660734177, 0.005829462315887213, kl_loss = 0.005922376178205013\n",
      "\n",
      "Epoch 708\n",
      "Step 0: loss = 0.012859219685196877, recon_loss = 0.006621185690164566, 0.0062249815091490746, kl_loss = 0.006526407785713673\n",
      "\n",
      "Epoch 709\n",
      "Step 0: loss = 0.011875941418111324, recon_loss = 0.007263693958520889, 0.00460243783891201, kl_loss = 0.00490495003759861\n",
      "\n",
      "Epoch 710\n",
      "Step 0: loss = 0.010094774886965752, recon_loss = 0.005773993209004402, 0.004314702935516834, kl_loss = 0.0030395835638046265\n",
      "\n",
      "Epoch 711\n",
      "Step 0: loss = 0.010832925327122211, recon_loss = 0.005879014730453491, 0.004947510082274675, kl_loss = 0.0032004714012145996\n",
      "\n",
      "Epoch 712\n",
      "Step 0: loss = 0.012289043515920639, recon_loss = 0.006628910079598427, 0.005654065869748592, kl_loss = 0.003034009598195553\n",
      "\n",
      "Epoch 713\n",
      "Step 0: loss = 0.012695866636931896, recon_loss = 0.0068400707095861435, 0.005848918575793505, kl_loss = 0.0034389235079288483\n",
      "\n",
      "Epoch 714\n",
      "Step 0: loss = 0.014584111049771309, recon_loss = 0.007624942809343338, 0.006947210058569908, kl_loss = 0.005979268811643124\n",
      "\n",
      "Epoch 715\n",
      "Step 0: loss = 0.013499433174729347, recon_loss = 0.006524333730340004, 0.0069619277492165565, kl_loss = 0.006585991941392422\n",
      "\n",
      "Epoch 716\n",
      "Step 0: loss = 0.009706832468509674, recon_loss = 0.006444118916988373, 0.003242775797843933, kl_loss = 0.009968798607587814\n",
      "\n",
      "Epoch 717\n",
      "Step 0: loss = 0.013151000253856182, recon_loss = 0.007220769301056862, 0.0058916229754686356, kl_loss = 0.01930391415953636\n",
      "\n",
      "Epoch 718\n",
      "Step 0: loss = 0.012614259496331215, recon_loss = 0.007104881107807159, 0.005493484437465668, kl_loss = 0.007946917787194252\n",
      "\n",
      "Epoch 719\n",
      "Step 0: loss = 0.010296856984496117, recon_loss = 0.006008170545101166, 0.004275240004062653, kl_loss = 0.006723150610923767\n",
      "\n",
      "Epoch 720\n",
      "Step 0: loss = 0.011889004148542881, recon_loss = 0.006622849032282829, 0.005250405054539442, kl_loss = 0.007874811999499798\n",
      "\n",
      "Epoch 721\n",
      "Step 0: loss = 0.012472908943891525, recon_loss = 0.00702960230410099, 0.005413869861513376, kl_loss = 0.014718450605869293\n",
      "\n",
      "Epoch 722\n",
      "Step 0: loss = 0.012666592374444008, recon_loss = 0.007492821663618088, 0.005146839655935764, kl_loss = 0.013465719297528267\n",
      "\n",
      "Epoch 723\n",
      "Step 0: loss = 0.012483792379498482, recon_loss = 0.00672304630279541, 0.005736172199249268, kl_loss = 0.012287063524127007\n",
      "\n",
      "Epoch 724\n",
      "Step 0: loss = 0.013365554623305798, recon_loss = 0.007148372009396553, 0.006201653741300106, kl_loss = 0.007764648646116257\n",
      "\n",
      "Epoch 725\n",
      "Step 0: loss = 0.010757562704384327, recon_loss = 0.006014054641127586, 0.004735487047582865, kl_loss = 0.004010720178484917\n",
      "\n",
      "Epoch 726\n",
      "Step 0: loss = 0.011186289601027966, recon_loss = 0.00667029432952404, 0.0045090620405972, kl_loss = 0.003467024303972721\n",
      "\n",
      "Epoch 727\n",
      "Step 0: loss = 0.0120757557451725, recon_loss = 0.007099874317646027, 0.004970338195562363, kl_loss = 0.0027715014293789864\n",
      "\n",
      "Epoch 728\n",
      "Step 0: loss = 0.010871614329516888, recon_loss = 0.0064138323068618774, 0.004452988039702177, kl_loss = 0.002396584488451481\n",
      "\n",
      "Epoch 729\n",
      "Step 0: loss = 0.01068444550037384, recon_loss = 0.005793031305074692, 0.004882678389549255, kl_loss = 0.004367786459624767\n",
      "\n",
      "Epoch 730\n",
      "Step 0: loss = 0.011755654588341713, recon_loss = 0.006068868562579155, 0.005671009421348572, kl_loss = 0.007888495922088623\n",
      "\n",
      "Epoch 731\n",
      "Step 0: loss = 0.013308181427419186, recon_loss = 0.007373649626970291, 0.005928436294198036, kl_loss = 0.0030477065593004227\n",
      "\n",
      "Epoch 732\n",
      "Step 0: loss = 0.011644740588963032, recon_loss = 0.007339617237448692, 0.0042920298874378204, kl_loss = 0.006546778604388237\n",
      "\n",
      "Epoch 733\n",
      "Step 0: loss = 0.012174845673143864, recon_loss = 0.006748581305146217, 0.005400669761002064, kl_loss = 0.0127974608913064\n",
      "\n",
      "Epoch 734\n",
      "Step 0: loss = 0.012216814793646336, recon_loss = 0.006868256255984306, 0.005327840801328421, kl_loss = 0.01035870797932148\n",
      "\n",
      "Epoch 735\n",
      "Step 0: loss = 0.010605926625430584, recon_loss = 0.005515139549970627, 0.005080109462141991, kl_loss = 0.005338859744369984\n",
      "\n",
      "Epoch 736\n",
      "Step 0: loss = 0.011374498717486858, recon_loss = 0.00608217716217041, 0.0052830674685537815, kl_loss = 0.00462721474468708\n",
      "\n",
      "Epoch 737\n",
      "Step 0: loss = 0.010890111327171326, recon_loss = 0.006364444270730019, 0.0045162891037762165, kl_loss = 0.0046891989186406136\n",
      "\n",
      "Epoch 738\n",
      "Step 0: loss = 0.012900695204734802, recon_loss = 0.007076317444443703, 0.005814283154904842, kl_loss = 0.005047373473644257\n",
      "\n",
      "Epoch 739\n",
      "Step 0: loss = 0.01292694266885519, recon_loss = 0.006883485242724419, 0.006031841970980167, kl_loss = 0.005807532928884029\n",
      "\n",
      "Epoch 740\n",
      "Step 0: loss = 0.012049573473632336, recon_loss = 0.0065394192934036255, 0.005498852580785751, kl_loss = 0.00565071776509285\n",
      "\n",
      "Epoch 741\n",
      "Step 0: loss = 0.012861551716923714, recon_loss = 0.006677962839603424, 0.006168927066028118, kl_loss = 0.0073310211300849915\n",
      "\n",
      "Epoch 742\n",
      "Step 0: loss = 0.014763419516384602, recon_loss = 0.008380115032196045, 0.006376294419169426, kl_loss = 0.0035051312297582626\n",
      "\n",
      "Epoch 743\n",
      "Step 0: loss = 0.015494003891944885, recon_loss = 0.007624125108122826, 0.007856659591197968, kl_loss = 0.006609817035496235\n",
      "\n",
      "Epoch 744\n",
      "Step 0: loss = 0.013085425831377506, recon_loss = 0.00854102149605751, 0.004528850317001343, kl_loss = 0.007776830345392227\n",
      "\n",
      "Epoch 745\n",
      "Step 0: loss = 0.01218695379793644, recon_loss = 0.0066496022045612335, 0.005528111942112446, kl_loss = 0.004619887098670006\n",
      "\n",
      "Epoch 746\n",
      "Step 0: loss = 0.010960649698972702, recon_loss = 0.006079670041799545, 0.004873447120189667, kl_loss = 0.0037664538249373436\n",
      "\n",
      "Epoch 747\n",
      "Step 0: loss = 0.013053846545517445, recon_loss = 0.0067407358437776566, 0.006305726710706949, kl_loss = 0.0036921612918376923\n",
      "\n",
      "Epoch 748\n",
      "Step 0: loss = 0.015753693878650665, recon_loss = 0.007618114352226257, 0.008111674338579178, kl_loss = 0.011952857486903667\n",
      "\n",
      "Epoch 749\n",
      "Step 0: loss = 0.01470873411744833, recon_loss = 0.008017752319574356, 0.006673491559922695, kl_loss = 0.008745047263801098\n",
      "\n",
      "Epoch 750\n",
      "Step 0: loss = 0.010851575061678886, recon_loss = 0.006236795336008072, 0.004590064287185669, kl_loss = 0.012357502244412899\n",
      "\n",
      "Epoch 751\n",
      "Step 0: loss = 0.009992983192205429, recon_loss = 0.006383199244737625, 0.0035926129203289747, kl_loss = 0.008585457690060139\n",
      "\n",
      "Epoch 752\n",
      "Step 0: loss = 0.010156067088246346, recon_loss = 0.005849799141287804, 0.00429687462747097, kl_loss = 0.004696871154010296\n",
      "\n",
      "Epoch 753\n",
      "Step 0: loss = 0.010913290083408356, recon_loss = 0.00660347193479538, 0.004301592707633972, kl_loss = 0.0041128238663077354\n",
      "\n",
      "Epoch 754\n",
      "Step 0: loss = 0.011175333522260189, recon_loss = 0.006381075829267502, 0.0047849444672465324, kl_loss = 0.0046565840020775795\n",
      "\n",
      "Epoch 755\n",
      "Step 0: loss = 0.0108136385679245, recon_loss = 0.006758235394954681, 0.0040386454202234745, kl_loss = 0.008379063569009304\n",
      "\n",
      "Epoch 756\n",
      "Step 0: loss = 0.009257370606064796, recon_loss = 0.005188675597310066, 0.004040445666760206, kl_loss = 0.014124645851552486\n",
      "\n",
      "Epoch 757\n",
      "Step 0: loss = 0.011703052558004856, recon_loss = 0.005586663261055946, 0.006093294359743595, kl_loss = 0.011547399684786797\n",
      "\n",
      "Epoch 758\n",
      "Step 0: loss = 0.010817871429026127, recon_loss = 0.005679313093423843, 0.00513361394405365, kl_loss = 0.002472265623509884\n",
      "\n",
      "Epoch 759\n",
      "Step 0: loss = 0.012423692271113396, recon_loss = 0.006094468757510185, 0.0063227759674191475, kl_loss = 0.003223912790417671\n",
      "\n",
      "Epoch 760\n",
      "Step 0: loss = 0.010997097007930279, recon_loss = 0.005299821496009827, 0.005685213953256607, kl_loss = 0.006030763499438763\n",
      "\n",
      "Epoch 761\n",
      "Step 0: loss = 0.012463719584047794, recon_loss = 0.006276678293943405, 0.006173693574965, kl_loss = 0.006673908792436123\n",
      "\n",
      "Epoch 762\n",
      "Step 0: loss = 0.00997896958142519, recon_loss = 0.005751194432377815, 0.0042114704847335815, kl_loss = 0.00815221481025219\n",
      "\n",
      "Epoch 763\n",
      "Step 0: loss = 0.012025106698274612, recon_loss = 0.006221946328878403, 0.005789638962596655, kl_loss = 0.006760469637811184\n",
      "\n",
      "Epoch 764\n",
      "Step 0: loss = 0.011567579582333565, recon_loss = 0.005698747932910919, 0.005859395954757929, kl_loss = 0.004717911593616009\n",
      "\n",
      "Epoch 765\n",
      "Step 0: loss = 0.009690248407423496, recon_loss = 0.005341170355677605, 0.0043432703241705894, kl_loss = 0.0029039625078439713\n",
      "\n",
      "Epoch 766\n",
      "Step 0: loss = 0.01004975475370884, recon_loss = 0.005443152040243149, 0.004599141888320446, kl_loss = 0.0037302514538168907\n",
      "\n",
      "Epoch 767\n",
      "Step 0: loss = 0.00913710705935955, recon_loss = 0.005417313426733017, 0.003704649629071355, kl_loss = 0.007572111673653126\n",
      "\n",
      "Epoch 768\n",
      "Step 0: loss = 0.00901012308895588, recon_loss = 0.0048493891954422, 0.004146690480411053, kl_loss = 0.007021645084023476\n",
      "\n",
      "Epoch 769\n",
      "Step 0: loss = 0.010588636621832848, recon_loss = 0.0056674908846616745, 0.004910532385110855, kl_loss = 0.00530676543712616\n",
      "\n",
      "Epoch 770\n",
      "Step 0: loss = 0.011113577522337437, recon_loss = 0.005856875330209732, 0.005238988436758518, kl_loss = 0.00885710958391428\n",
      "\n",
      "Epoch 771\n",
      "Step 0: loss = 0.01215745136141777, recon_loss = 0.006055653095245361, 0.006092410534620285, kl_loss = 0.004694071598351002\n",
      "\n",
      "Epoch 772\n",
      "Step 0: loss = 0.011135660111904144, recon_loss = 0.005582574754953384, 0.005545604974031448, kl_loss = 0.0037404196336865425\n",
      "\n",
      "Epoch 773\n",
      "Step 0: loss = 0.010579360648989677, recon_loss = 0.005609691143035889, 0.004963025450706482, kl_loss = 0.0033221570774912834\n",
      "\n",
      "Epoch 774\n",
      "Step 0: loss = 0.010586827993392944, recon_loss = 0.0059585291892290115, 0.004622524604201317, kl_loss = 0.0028872480615973473\n",
      "\n",
      "Epoch 775\n",
      "Step 0: loss = 0.009878106415271759, recon_loss = 0.006027441471815109, 0.003844717051833868, kl_loss = 0.0029738619923591614\n",
      "\n",
      "Epoch 776\n",
      "Step 0: loss = 0.010095934383571148, recon_loss = 0.00622158870100975, 0.003853489411994815, kl_loss = 0.010427888482809067\n",
      "\n",
      "Epoch 777\n",
      "Step 0: loss = 0.013348400592803955, recon_loss = 0.006421789526939392, 0.006915348116308451, kl_loss = 0.005631774663925171\n",
      "\n",
      "Epoch 778\n",
      "Step 0: loss = 0.010962331667542458, recon_loss = 0.006357632577419281, 0.004597812425345182, kl_loss = 0.003442908637225628\n",
      "\n",
      "Epoch 779\n",
      "Step 0: loss = 0.010735781863331795, recon_loss = 0.005812672898173332, 0.004917648620903492, kl_loss = 0.002730005420744419\n",
      "\n",
      "Epoch 780\n",
      "Step 0: loss = 0.01140372734516859, recon_loss = 0.006399847567081451, 0.004990641959011555, kl_loss = 0.006618955172598362\n",
      "\n",
      "Epoch 781\n",
      "Step 0: loss = 0.011528966017067432, recon_loss = 0.006222350522875786, 0.005288566928356886, kl_loss = 0.009024038910865784\n",
      "\n",
      "Epoch 782\n",
      "Step 0: loss = 0.009307562373578548, recon_loss = 0.005753077566623688, 0.00353549188002944, kl_loss = 0.009496305137872696\n",
      "\n",
      "Epoch 783\n",
      "Step 0: loss = 0.010636328719556332, recon_loss = 0.005662551149725914, 0.004965662490576506, kl_loss = 0.004057359881699085\n",
      "\n",
      "Epoch 784\n",
      "Step 0: loss = 0.011727627366781235, recon_loss = 0.005638401955366135, 0.006084592547267675, kl_loss = 0.0023163026198744774\n",
      "\n",
      "Epoch 785\n",
      "Step 0: loss = 0.010538293980062008, recon_loss = 0.005366949364542961, 0.005167498253285885, kl_loss = 0.00192327331751585\n",
      "\n",
      "Epoch 786\n",
      "Step 0: loss = 0.01179252378642559, recon_loss = 0.006292987614870071, 0.0054953088983893394, kl_loss = 0.00211360864341259\n",
      "\n",
      "Epoch 787\n",
      "Step 0: loss = 0.01036566961556673, recon_loss = 0.005553577095270157, 0.004800986498594284, kl_loss = 0.005552917718887329\n",
      "\n",
      "Epoch 788\n",
      "Step 0: loss = 0.011422783136367798, recon_loss = 0.005820833146572113, 0.005597860552370548, kl_loss = 0.0020447447896003723\n",
      "\n",
      "Epoch 789\n",
      "Step 0: loss = 0.010910249315202236, recon_loss = 0.006163137033581734, 0.004741748794913292, kl_loss = 0.0026817908510565758\n",
      "\n",
      "Epoch 790\n",
      "Step 0: loss = 0.010999111458659172, recon_loss = 0.005828512832522392, 0.00516123604029417, kl_loss = 0.004681207239627838\n",
      "\n",
      "Epoch 791\n",
      "Step 0: loss = 0.009562806226313114, recon_loss = 0.005360998213291168, 0.0041939979419112206, kl_loss = 0.003904818557202816\n",
      "\n",
      "Epoch 792\n",
      "Step 0: loss = 0.010824937373399734, recon_loss = 0.006118617951869965, 0.004700591787695885, kl_loss = 0.0028636502102017403\n",
      "\n",
      "Epoch 793\n",
      "Step 0: loss = 0.008686844259500504, recon_loss = 0.005034059286117554, 0.003646793309599161, kl_loss = 0.0029961345717310905\n",
      "\n",
      "Epoch 794\n",
      "Step 0: loss = 0.010447732172906399, recon_loss = 0.005876142531633377, 0.004559401422739029, kl_loss = 0.006094048731029034\n",
      "\n",
      "Epoch 795\n",
      "Step 0: loss = 0.01050300057977438, recon_loss = 0.0057546403259038925, 0.004737049341201782, kl_loss = 0.005655382759869099\n",
      "\n",
      "Epoch 796\n",
      "Step 0: loss = 0.01079639419913292, recon_loss = 0.005980579182505608, 0.004805584903806448, kl_loss = 0.005114859901368618\n",
      "\n",
      "Epoch 797\n",
      "Step 0: loss = 0.011088740080595016, recon_loss = 0.006157059222459793, 0.004912857431918383, kl_loss = 0.009411969222128391\n",
      "\n",
      "Epoch 798\n",
      "Step 0: loss = 0.010973099619150162, recon_loss = 0.006322681903839111, 0.0046207476407289505, kl_loss = 0.014834844507277012\n",
      "\n",
      "Epoch 799\n",
      "Step 0: loss = 0.011905073188245296, recon_loss = 0.006820380687713623, 0.005056045949459076, kl_loss = 0.01432346273213625\n",
      "\n",
      "Epoch 800\n",
      "Step 0: loss = 0.01232073549181223, recon_loss = 0.005852049216628075, 0.006453744601458311, kl_loss = 0.007470560260117054\n",
      "\n",
      "Epoch 801\n",
      "Step 0: loss = 0.010656566359102726, recon_loss = 0.006198486313223839, 0.004434683360159397, kl_loss = 0.011698568239808083\n",
      "\n",
      "Epoch 802\n",
      "Step 0: loss = 0.010629184544086456, recon_loss = 0.0060949865728616714, 0.004526033066213131, kl_loss = 0.004082574509084225\n",
      "\n",
      "Epoch 803\n",
      "Step 0: loss = 0.00911896862089634, recon_loss = 0.00505431555211544, 0.004060043953359127, kl_loss = 0.002304566092789173\n",
      "\n",
      "Epoch 804\n",
      "Step 0: loss = 0.011964485980570316, recon_loss = 0.006252288818359375, 0.005703981965780258, kl_loss = 0.004107465036213398\n",
      "\n",
      "Epoch 805\n",
      "Step 0: loss = 0.013200074434280396, recon_loss = 0.0061919596046209335, 0.006997103802859783, kl_loss = 0.005505489185452461\n",
      "\n",
      "Epoch 806\n",
      "Step 0: loss = 0.010725495405495167, recon_loss = 0.0055466871708631516, 0.005165912210941315, kl_loss = 0.006448227912187576\n",
      "\n",
      "Epoch 807\n",
      "Step 0: loss = 0.010216199792921543, recon_loss = 0.005474435165524483, 0.004732001572847366, kl_loss = 0.004881575703620911\n",
      "\n",
      "Epoch 808\n",
      "Step 0: loss = 0.010094212368130684, recon_loss = 0.005638148635625839, 0.004448920488357544, kl_loss = 0.0035716844722628593\n",
      "\n",
      "Epoch 809\n",
      "Step 0: loss = 0.00983609352260828, recon_loss = 0.0056831203401088715, 0.00414555286988616, kl_loss = 0.003710079938173294\n",
      "\n",
      "Epoch 810\n",
      "Step 0: loss = 0.011202367022633553, recon_loss = 0.005589386448264122, 0.005604770500212908, kl_loss = 0.004105329513549805\n",
      "\n",
      "Epoch 811\n",
      "Step 0: loss = 0.010269722901284695, recon_loss = 0.005165960639715195, 0.005084935575723648, kl_loss = 0.009413573890924454\n",
      "\n",
      "Epoch 812\n",
      "Step 0: loss = 0.011007724329829216, recon_loss = 0.005961589515209198, 0.005019519943743944, kl_loss = 0.0133078433573246\n",
      "\n",
      "Epoch 813\n",
      "Step 0: loss = 0.00886304210871458, recon_loss = 0.005066528916358948, 0.0037844337057322264, kl_loss = 0.006039596162736416\n",
      "\n",
      "Epoch 814\n",
      "Step 0: loss = 0.011928480118513107, recon_loss = 0.0060959067195653915, 0.005813869647681713, kl_loss = 0.00935197900980711\n",
      "\n",
      "Epoch 815\n",
      "Step 0: loss = 0.01160386297851801, recon_loss = 0.00699683278799057, 0.0045851836912333965, kl_loss = 0.010923178866505623\n",
      "\n",
      "Epoch 816\n",
      "Step 0: loss = 0.011583707295358181, recon_loss = 0.006170278415083885, 0.005392821971327066, kl_loss = 0.010303598828613758\n",
      "\n",
      "Epoch 817\n",
      "Step 0: loss = 0.012349206022918224, recon_loss = 0.00756007619202137, 0.004775801673531532, kl_loss = 0.006663983687758446\n",
      "\n",
      "Epoch 818\n",
      "Step 0: loss = 0.010691558010876179, recon_loss = 0.006075272336602211, 0.004588245414197445, kl_loss = 0.014020239934325218\n",
      "\n",
      "Epoch 819\n",
      "Step 0: loss = 0.011546635068953037, recon_loss = 0.0063432324677705765, 0.0051810964941978455, kl_loss = 0.011153042316436768\n",
      "\n",
      "Epoch 820\n",
      "Step 0: loss = 0.011369017884135246, recon_loss = 0.005969380959868431, 0.0053796060383319855, kl_loss = 0.010015412233769894\n",
      "\n",
      "Epoch 821\n",
      "Step 0: loss = 0.010907452553510666, recon_loss = 0.005981342867016792, 0.0049146790988743305, kl_loss = 0.005715597420930862\n",
      "\n",
      "Epoch 822\n",
      "Step 0: loss = 0.011574376374483109, recon_loss = 0.0057764314115047455, 0.005784299224615097, kl_loss = 0.006822805851697922\n",
      "\n",
      "Epoch 823\n",
      "Step 0: loss = 0.010006940923631191, recon_loss = 0.006232565268874168, 0.003766185138374567, kl_loss = 0.004095502197742462\n",
      "\n",
      "Epoch 824\n",
      "Step 0: loss = 0.009936085902154446, recon_loss = 0.006246576085686684, 0.0036807735450565815, kl_loss = 0.0043681832030415535\n",
      "\n",
      "Epoch 825\n",
      "Step 0: loss = 0.009830032475292683, recon_loss = 0.005740426480770111, 0.00408027321100235, kl_loss = 0.004666443914175034\n",
      "\n",
      "Epoch 826\n",
      "Step 0: loss = 0.012033650651574135, recon_loss = 0.006553981453180313, 0.00547366589307785, kl_loss = 0.003001488745212555\n",
      "\n",
      "Epoch 827\n",
      "Step 0: loss = 0.011116844601929188, recon_loss = 0.00608273409307003, 0.005024611484259367, kl_loss = 0.00474934559315443\n",
      "\n",
      "Epoch 828\n",
      "Step 0: loss = 0.012592261657118797, recon_loss = 0.006224006414413452, 0.00635603629052639, kl_loss = 0.006109248846769333\n",
      "\n",
      "Epoch 829\n",
      "Step 0: loss = 0.011902236379683018, recon_loss = 0.006564939394593239, 0.005322209093719721, kl_loss = 0.00754399411380291\n",
      "\n",
      "Epoch 830\n",
      "Step 0: loss = 0.013537226244807243, recon_loss = 0.0071103498339653015, 0.006414556875824928, kl_loss = 0.006159622222185135\n",
      "\n",
      "Epoch 831\n",
      "Step 0: loss = 0.009383716620504856, recon_loss = 0.005982879549264908, 0.003385277697816491, kl_loss = 0.007779593579471111\n",
      "\n",
      "Epoch 832\n",
      "Step 0: loss = 0.012367292307317257, recon_loss = 0.006452042609453201, 0.005892631597816944, kl_loss = 0.011309182271361351\n",
      "\n",
      "Epoch 833\n",
      "Step 0: loss = 0.01144309900701046, recon_loss = 0.00787346251308918, 0.0035542999394237995, kl_loss = 0.00766874011605978\n",
      "\n",
      "Epoch 834\n",
      "Step 0: loss = 0.011121808551251888, recon_loss = 0.005887812003493309, 0.005223282612860203, kl_loss = 0.0053568873554468155\n",
      "\n",
      "Epoch 835\n",
      "Step 0: loss = 0.012049664743244648, recon_loss = 0.006968660280108452, 0.0050661894492805, kl_loss = 0.007407447323203087\n",
      "\n",
      "Epoch 836\n",
      "Step 0: loss = 0.010415025986731052, recon_loss = 0.005741707980632782, 0.00466568861156702, kl_loss = 0.0038147130981087685\n",
      "\n",
      "Epoch 837\n",
      "Step 0: loss = 0.010466332547366619, recon_loss = 0.00548936240375042, 0.004965584725141525, kl_loss = 0.005692821927368641\n",
      "\n",
      "Epoch 838\n",
      "Step 0: loss = 0.011076279915869236, recon_loss = 0.005765439942479134, 0.005299696698784828, kl_loss = 0.0055714985355734825\n",
      "\n",
      "Epoch 839\n",
      "Step 0: loss = 0.011231786571443081, recon_loss = 0.0057908762246370316, 0.005433317739516497, kl_loss = 0.003796318545937538\n",
      "\n",
      "Epoch 840\n",
      "Step 0: loss = 0.012142037972807884, recon_loss = 0.006497679278254509, 0.005635606590658426, kl_loss = 0.004376213997602463\n",
      "\n",
      "Epoch 841\n",
      "Step 0: loss = 0.00928223691880703, recon_loss = 0.00551941990852356, 0.0037562567740678787, kl_loss = 0.0032803425565361977\n",
      "\n",
      "Epoch 842\n",
      "Step 0: loss = 0.011508749797940254, recon_loss = 0.00535132922232151, 0.006150189787149429, kl_loss = 0.003615190275013447\n",
      "\n",
      "Epoch 843\n",
      "Step 0: loss = 0.012675885111093521, recon_loss = 0.007081521674990654, 0.0055823903530836105, kl_loss = 0.005986742675304413\n",
      "\n",
      "Epoch 844\n",
      "Step 0: loss = 0.011135216802358627, recon_loss = 0.0061683617532253265, 0.00495653273537755, kl_loss = 0.005161367356777191\n",
      "\n",
      "Epoch 845\n",
      "Step 0: loss = 0.01005276944488287, recon_loss = 0.00625305250287056, 0.003789182286709547, kl_loss = 0.005267090164124966\n",
      "\n",
      "Epoch 846\n",
      "Step 0: loss = 0.012861772440373898, recon_loss = 0.006601247936487198, 0.00625113258138299, kl_loss = 0.004696425050497055\n",
      "\n",
      "Epoch 847\n",
      "Step 0: loss = 0.013186766766011715, recon_loss = 0.006111973896622658, 0.0070653813891112804, kl_loss = 0.004705309867858887\n",
      "\n",
      "Epoch 848\n",
      "Step 0: loss = 0.011084869503974915, recon_loss = 0.006149640306830406, 0.004925726912915707, kl_loss = 0.004751279018819332\n",
      "\n",
      "Epoch 849\n",
      "Step 0: loss = 0.011053514666855335, recon_loss = 0.006256315857172012, 0.004791711922734976, kl_loss = 0.002742997370660305\n",
      "\n",
      "Epoch 850\n",
      "Step 0: loss = 0.011332172900438309, recon_loss = 0.006037164479494095, 0.005285692401230335, kl_loss = 0.004657885059714317\n",
      "\n",
      "Epoch 851\n",
      "Step 0: loss = 0.011983560398221016, recon_loss = 0.00595221109688282, 0.006014701910316944, kl_loss = 0.008323588408529758\n",
      "\n",
      "Epoch 852\n",
      "Step 0: loss = 0.011512135155498981, recon_loss = 0.005961958318948746, 0.005537328775972128, kl_loss = 0.00642387755215168\n",
      "\n",
      "Epoch 853\n",
      "Step 0: loss = 0.011228534393012524, recon_loss = 0.005766235291957855, 0.005447611212730408, kl_loss = 0.007344122044742107\n",
      "\n",
      "Epoch 854\n",
      "Step 0: loss = 0.011955533176660538, recon_loss = 0.0068233683705329895, 0.005110177211463451, kl_loss = 0.010993842966854572\n",
      "\n",
      "Epoch 855\n",
      "Step 0: loss = 0.011338620446622372, recon_loss = 0.006192237138748169, 0.005127997137606144, kl_loss = 0.009193134494125843\n",
      "\n",
      "Epoch 856\n",
      "Step 0: loss = 0.01022088062018156, recon_loss = 0.0053339432924985886, 0.004874636419117451, kl_loss = 0.0061502885073423386\n",
      "\n",
      "Epoch 857\n",
      "Step 0: loss = 0.010162346996366978, recon_loss = 0.005601098760962486, 0.0045522889122366905, kl_loss = 0.004479480907320976\n",
      "\n",
      "Epoch 858\n",
      "Step 0: loss = 0.011539137922227383, recon_loss = 0.00635664165019989, 0.005167737603187561, kl_loss = 0.007379255257546902\n",
      "\n",
      "Epoch 859\n",
      "Step 0: loss = 0.012529691681265831, recon_loss = 0.00608350895345211, 0.00643353583291173, kl_loss = 0.0063235461711883545\n",
      "\n",
      "Epoch 860\n",
      "Step 0: loss = 0.013062204234302044, recon_loss = 0.006963273510336876, 0.006090843118727207, kl_loss = 0.004043786786496639\n",
      "\n",
      "Epoch 861\n",
      "Step 0: loss = 0.012569984421133995, recon_loss = 0.007602246478199959, 0.004953752271831036, kl_loss = 0.00699281319975853\n",
      "\n",
      "Epoch 862\n",
      "Step 0: loss = 0.012000099755823612, recon_loss = 0.006563825532793999, 0.005424726754426956, kl_loss = 0.005773607641458511\n",
      "\n",
      "Epoch 863\n",
      "Step 0: loss = 0.011128651909530163, recon_loss = 0.005521327257156372, 0.0055982619524002075, kl_loss = 0.004531295038759708\n",
      "\n",
      "Epoch 864\n",
      "Step 0: loss = 0.009583007544279099, recon_loss = 0.005452718585729599, 0.004119871649891138, kl_loss = 0.00520901195704937\n",
      "\n",
      "Epoch 865\n",
      "Step 0: loss = 0.011198832653462887, recon_loss = 0.005983985960483551, 0.005198712460696697, kl_loss = 0.008066990412771702\n",
      "\n",
      "Epoch 866\n",
      "Step 0: loss = 0.010336400009691715, recon_loss = 0.005905110388994217, 0.004413245245814323, kl_loss = 0.009022070094943047\n",
      "\n",
      "Epoch 867\n",
      "Step 0: loss = 0.010349452495574951, recon_loss = 0.005988845601677895, 0.004349932540208101, kl_loss = 0.005337505601346493\n",
      "\n",
      "Epoch 868\n",
      "Step 0: loss = 0.012462809681892395, recon_loss = 0.006184153258800507, 0.006268057506531477, kl_loss = 0.005299377255141735\n",
      "\n",
      "Epoch 869\n",
      "Step 0: loss = 0.010054337792098522, recon_loss = 0.006075168028473854, 0.003972075879573822, kl_loss = 0.0035469122231006622\n",
      "\n",
      "Epoch 870\n",
      "Step 0: loss = 0.010567194782197475, recon_loss = 0.005640137940645218, 0.004922517575323582, kl_loss = 0.002269837073981762\n",
      "\n",
      "Epoch 871\n",
      "Step 0: loss = 0.00960813369601965, recon_loss = 0.005007769912481308, 0.004596625454723835, kl_loss = 0.0018691662698984146\n",
      "\n",
      "Epoch 872\n",
      "Step 0: loss = 0.010234257206320763, recon_loss = 0.005236951634287834, 0.00498589314520359, kl_loss = 0.005706367082893848\n",
      "\n",
      "Epoch 873\n",
      "Step 0: loss = 0.010156330652534962, recon_loss = 0.005589623004198074, 0.004557672888040543, kl_loss = 0.004517586901783943\n",
      "\n",
      "Epoch 874\n",
      "Step 0: loss = 0.01061172503978014, recon_loss = 0.005452228710055351, 0.005150384269654751, kl_loss = 0.004555811174213886\n",
      "\n",
      "Epoch 875\n",
      "Step 0: loss = 0.010229109786450863, recon_loss = 0.005673561245203018, 0.004537894390523434, kl_loss = 0.008827118203043938\n",
      "\n",
      "Epoch 876\n",
      "Step 0: loss = 0.010170001536607742, recon_loss = 0.005277866497635841, 0.004877710714936256, kl_loss = 0.007212210446596146\n",
      "\n",
      "Epoch 877\n",
      "Step 0: loss = 0.011493404395878315, recon_loss = 0.005780575796961784, 0.005684829782694578, kl_loss = 0.013999159447848797\n",
      "\n",
      "Epoch 878\n",
      "Step 0: loss = 0.0136257978156209, recon_loss = 0.007187046110630035, 0.006403406150639057, kl_loss = 0.017672952264547348\n",
      "\n",
      "Epoch 879\n",
      "Step 0: loss = 0.012459385208785534, recon_loss = 0.0061359163373708725, 0.006309462245553732, kl_loss = 0.007003065198659897\n",
      "\n",
      "Epoch 880\n",
      "Step 0: loss = 0.008627548813819885, recon_loss = 0.005776271224021912, 0.002835084218531847, kl_loss = 0.008096737787127495\n",
      "\n",
      "Epoch 881\n",
      "Step 0: loss = 0.011117278598248959, recon_loss = 0.005840294063091278, 0.005256799980998039, kl_loss = 0.010092394426465034\n",
      "\n",
      "Epoch 882\n",
      "Step 0: loss = 0.009378492832183838, recon_loss = 0.005339434370398521, 0.004026129841804504, kl_loss = 0.006464519537985325\n",
      "\n",
      "Epoch 883\n",
      "Step 0: loss = 0.010937820188701153, recon_loss = 0.005958182737231255, 0.004963458515703678, kl_loss = 0.008089357987046242\n",
      "\n",
      "Epoch 884\n",
      "Step 0: loss = 0.011125797405838966, recon_loss = 0.005674665793776512, 0.00541342981159687, kl_loss = 0.018850738182663918\n",
      "\n",
      "Epoch 885\n",
      "Step 0: loss = 0.009533068165183067, recon_loss = 0.00475681945681572, 0.004751177038997412, kl_loss = 0.012535486370325089\n",
      "\n",
      "Epoch 886\n",
      "Step 0: loss = 0.009347629733383656, recon_loss = 0.005183162167668343, 0.0041445051319897175, kl_loss = 0.009981559589505196\n",
      "\n",
      "Epoch 887\n",
      "Step 0: loss = 0.010974508710205555, recon_loss = 0.0053041353821754456, 0.005640547722578049, kl_loss = 0.014912777580320835\n",
      "\n",
      "Epoch 888\n",
      "Step 0: loss = 0.009063181467354298, recon_loss = 0.005573568865656853, 0.0034613660536706448, kl_loss = 0.014122901484370232\n",
      "\n",
      "Epoch 889\n",
      "Step 0: loss = 0.011244365945458412, recon_loss = 0.005531426519155502, 0.005702345632016659, kl_loss = 0.0052969129756093025\n",
      "\n",
      "Epoch 890\n",
      "Step 0: loss = 0.011787115596234798, recon_loss = 0.006429297849535942, 0.005338907241821289, kl_loss = 0.00945533812046051\n",
      "\n",
      "Epoch 891\n",
      "Step 0: loss = 0.012836677022278309, recon_loss = 0.006964981555938721, 0.005853548645973206, kl_loss = 0.009073218330740929\n",
      "\n",
      "Epoch 892\n",
      "Step 0: loss = 0.012751294299960136, recon_loss = 0.0066236816346645355, 0.00610882043838501, kl_loss = 0.009396116249263287\n",
      "\n",
      "Epoch 893\n",
      "Step 0: loss = 0.011807838454842567, recon_loss = 0.00634237565100193, 0.005452424753457308, kl_loss = 0.006519458256661892\n",
      "\n",
      "Epoch 894\n",
      "Step 0: loss = 0.012688436545431614, recon_loss = 0.0063977353274822235, 0.006275760941207409, kl_loss = 0.0074701858684420586\n",
      "\n",
      "Epoch 895\n",
      "Step 0: loss = 0.01202144380658865, recon_loss = 0.006761757656931877, 0.005252516828477383, kl_loss = 0.0035846643149852753\n",
      "\n",
      "Epoch 896\n",
      "Step 0: loss = 0.011881768703460693, recon_loss = 0.006433641538023949, 0.005442225374281406, kl_loss = 0.00295084435492754\n",
      "\n",
      "Epoch 897\n",
      "Step 0: loss = 0.013478581793606281, recon_loss = 0.0065209027379751205, 0.0069519588723778725, kl_loss = 0.002860060892999172\n",
      "\n",
      "Epoch 898\n",
      "Step 0: loss = 0.011544202454388142, recon_loss = 0.006593359634280205, 0.004944515414535999, kl_loss = 0.0031638452783226967\n",
      "\n",
      "Epoch 899\n",
      "Step 0: loss = 0.012275675311684608, recon_loss = 0.006104925647377968, 0.0061586759984493256, kl_loss = 0.006036810576915741\n",
      "\n",
      "Epoch 900\n",
      "Step 0: loss = 0.012678527273237705, recon_loss = 0.006057180464267731, 0.006613760255277157, kl_loss = 0.0037933336570858955\n",
      "\n",
      "Epoch 901\n",
      "Step 0: loss = 0.010618433356285095, recon_loss = 0.005651179701089859, 0.0049583157524466515, kl_loss = 0.004468733444809914\n",
      "\n",
      "Epoch 902\n",
      "Step 0: loss = 0.00993768684566021, recon_loss = 0.006011839956045151, 0.003918459638953209, kl_loss = 0.0036934325471520424\n",
      "\n",
      "Epoch 903\n",
      "Step 0: loss = 0.009304391220211983, recon_loss = 0.005091579630970955, 0.004203246906399727, kl_loss = 0.004782509990036488\n",
      "\n",
      "Epoch 904\n",
      "Step 0: loss = 0.008791227824985981, recon_loss = 0.004941465333104134, 0.0038329269737005234, kl_loss = 0.008417719043791294\n",
      "\n",
      "Epoch 905\n",
      "Step 0: loss = 0.010057846084237099, recon_loss = 0.005311885848641396, 0.004730209708213806, kl_loss = 0.007875402458012104\n",
      "\n",
      "Epoch 906\n",
      "Step 0: loss = 0.009769822470843792, recon_loss = 0.005482034757733345, 0.004280532244592905, kl_loss = 0.003628005273640156\n",
      "\n",
      "Epoch 907\n",
      "Step 0: loss = 0.011304156854748726, recon_loss = 0.005788879469037056, 0.005510131828486919, kl_loss = 0.0025727907195687294\n",
      "\n",
      "Epoch 908\n",
      "Step 0: loss = 0.010188322514295578, recon_loss = 0.005607208237051964, 0.00457481574267149, kl_loss = 0.0031491508707404137\n",
      "\n",
      "Epoch 909\n",
      "Step 0: loss = 0.008847222663462162, recon_loss = 0.004640687257051468, 0.004200146533548832, kl_loss = 0.0031945407390594482\n",
      "\n",
      "Epoch 910\n",
      "Step 0: loss = 0.01028255932033062, recon_loss = 0.005646988749504089, 0.004628109745681286, kl_loss = 0.0037303082644939423\n",
      "\n",
      "Epoch 911\n",
      "Step 0: loss = 0.011079137213528156, recon_loss = 0.005450688302516937, 0.005618301220238209, kl_loss = 0.0050740670412778854\n",
      "\n",
      "Epoch 912\n",
      "Step 0: loss = 0.010348391719162464, recon_loss = 0.005419326946139336, 0.004920457024127245, kl_loss = 0.004304090514779091\n",
      "\n",
      "Epoch 913\n",
      "Step 0: loss = 0.012118625454604626, recon_loss = 0.006025638431310654, 0.0060815392062067986, kl_loss = 0.005723937414586544\n",
      "\n",
      "Epoch 914\n",
      "Step 0: loss = 0.01042286679148674, recon_loss = 0.005352165549993515, 0.005063720978796482, kl_loss = 0.003490205854177475\n",
      "\n",
      "Epoch 915\n",
      "Step 0: loss = 0.012485826388001442, recon_loss = 0.005795354023575783, 0.006675540003925562, kl_loss = 0.0074661895632743835\n",
      "\n",
      "Epoch 916\n",
      "Step 0: loss = 0.011750039644539356, recon_loss = 0.005588453263044357, 0.00613782461732626, kl_loss = 0.011880908161401749\n",
      "\n",
      "Epoch 917\n",
      "Step 0: loss = 0.009239270351827145, recon_loss = 0.004947710782289505, 0.004280476830899715, kl_loss = 0.005541312508285046\n",
      "\n",
      "Epoch 918\n",
      "Step 0: loss = 0.00966865848749876, recon_loss = 0.005683224648237228, 0.003980716690421104, kl_loss = 0.0023583434522151947\n",
      "\n",
      "Epoch 919\n",
      "Step 0: loss = 0.00967169739305973, recon_loss = 0.0046029407531023026, 0.005061245523393154, kl_loss = 0.003755588084459305\n",
      "\n",
      "Epoch 920\n",
      "Step 0: loss = 0.010701182298362255, recon_loss = 0.005368206650018692, 0.005321182776242495, kl_loss = 0.005896887741982937\n",
      "\n",
      "Epoch 921\n",
      "Step 0: loss = 0.010714706964790821, recon_loss = 0.004851158708333969, 0.005851126275956631, kl_loss = 0.006210844032466412\n",
      "\n",
      "Epoch 922\n",
      "Step 0: loss = 0.009043024852871895, recon_loss = 0.004248823970556259, 0.004778314381837845, kl_loss = 0.007943108677864075\n",
      "\n",
      "Epoch 923\n",
      "Step 0: loss = 0.010276085697114468, recon_loss = 0.0052541159093379974, 0.00500473752617836, kl_loss = 0.008615982718765736\n",
      "\n",
      "Epoch 924\n",
      "Step 0: loss = 0.00903461966663599, recon_loss = 0.0049820151180028915, 0.004039971623569727, kl_loss = 0.006316539831459522\n",
      "\n",
      "Epoch 925\n",
      "Step 0: loss = 0.00909938383847475, recon_loss = 0.00480029359459877, 0.004289858974516392, kl_loss = 0.004615818150341511\n",
      "\n",
      "Epoch 926\n",
      "Step 0: loss = 0.009520811960101128, recon_loss = 0.004882486537098885, 0.004632940981537104, kl_loss = 0.0026922645047307014\n",
      "\n",
      "Epoch 927\n",
      "Step 0: loss = 0.01194025669246912, recon_loss = 0.005150584504008293, 0.006782807409763336, kl_loss = 0.0034321900457143784\n",
      "\n",
      "Epoch 928\n",
      "Step 0: loss = 0.011584672145545483, recon_loss = 0.005665311589837074, 0.00590377114713192, kl_loss = 0.007794729433953762\n",
      "\n",
      "Epoch 929\n",
      "Step 0: loss = 0.010670755989849567, recon_loss = 0.005593255162239075, 0.005058766342699528, kl_loss = 0.009367072954773903\n",
      "\n",
      "Epoch 930\n",
      "Step 0: loss = 0.008773483335971832, recon_loss = 0.005109243094921112, 0.0036505148746073246, kl_loss = 0.006862764246761799\n",
      "\n",
      "Epoch 931\n",
      "Step 0: loss = 0.010884404182434082, recon_loss = 0.004713987931609154, 0.0061536747962236404, kl_loss = 0.008370866999030113\n",
      "\n",
      "Epoch 932\n",
      "Step 0: loss = 0.008906110189855099, recon_loss = 0.0047432538121938705, 0.004147812258452177, kl_loss = 0.007521837018430233\n",
      "\n",
      "Epoch 933\n",
      "Step 0: loss = 0.010292083956301212, recon_loss = 0.005567856132984161, 0.0047117238864302635, kl_loss = 0.006252124905586243\n",
      "\n",
      "Epoch 934\n",
      "Step 0: loss = 0.012350198812782764, recon_loss = 0.007206311449408531, 0.005128752440214157, kl_loss = 0.007567526772618294\n",
      "\n",
      "Epoch 935\n",
      "Step 0: loss = 0.011373321525752544, recon_loss = 0.006364123895764351, 0.0050025335513055325, kl_loss = 0.003332010470330715\n",
      "\n",
      "Epoch 936\n",
      "Step 0: loss = 0.012110911309719086, recon_loss = 0.006320498883724213, 0.005773917771875858, kl_loss = 0.008247273974120617\n",
      "\n",
      "Epoch 937\n",
      "Step 0: loss = 0.013028831221163273, recon_loss = 0.006200987845659256, 0.006818204652518034, kl_loss = 0.00481922272592783\n",
      "\n",
      "Epoch 938\n",
      "Step 0: loss = 0.011278042569756508, recon_loss = 0.006275633350014687, 0.004987470805644989, kl_loss = 0.007469003088772297\n",
      "\n",
      "Epoch 939\n",
      "Step 0: loss = 0.012110824696719646, recon_loss = 0.007054414600133896, 0.005041808355599642, kl_loss = 0.00730122160166502\n",
      "\n",
      "Epoch 940\n",
      "Step 0: loss = 0.010255412198603153, recon_loss = 0.006372556090354919, 0.0038700259756296873, kl_loss = 0.006414984352886677\n",
      "\n",
      "Epoch 941\n",
      "Step 0: loss = 0.012532029300928116, recon_loss = 0.006191013380885124, 0.006326119881123304, kl_loss = 0.007447788491845131\n",
      "\n",
      "Epoch 942\n",
      "Step 0: loss = 0.011371794156730175, recon_loss = 0.0068665724247694016, 0.004497866611927748, kl_loss = 0.003677232190966606\n",
      "\n",
      "Epoch 943\n",
      "Step 0: loss = 0.009300796315073967, recon_loss = 0.005348287522792816, 0.003946426324546337, kl_loss = 0.003041427582502365\n",
      "\n",
      "Epoch 944\n",
      "Step 0: loss = 0.01094773318618536, recon_loss = 0.0061440784484148026, 0.004792029038071632, kl_loss = 0.00581267848610878\n",
      "\n",
      "Epoch 945\n",
      "Step 0: loss = 0.010852993465960026, recon_loss = 0.005351094529032707, 0.005495831836014986, kl_loss = 0.0030335960909724236\n",
      "\n",
      "Epoch 946\n",
      "Step 0: loss = 0.009365309961140156, recon_loss = 0.004988342523574829, 0.004361873492598534, kl_loss = 0.0075470516458153725\n",
      "\n",
      "Epoch 947\n",
      "Step 0: loss = 0.011219093576073647, recon_loss = 0.005332086235284805, 0.005862230435013771, kl_loss = 0.01238846592605114\n",
      "\n",
      "Epoch 948\n",
      "Step 0: loss = 0.010514585301280022, recon_loss = 0.005462668836116791, 0.005038572940975428, kl_loss = 0.006672192364931107\n",
      "\n",
      "Epoch 949\n",
      "Step 0: loss = 0.010963624343276024, recon_loss = 0.0051659829914569855, 0.005772863514721394, kl_loss = 0.012389066629111767\n",
      "\n",
      "Epoch 950\n",
      "Step 0: loss = 0.009895025752484798, recon_loss = 0.0048697758466005325, 0.004995326045900583, kl_loss = 0.014962218701839447\n",
      "\n",
      "Epoch 951\n",
      "Step 0: loss = 0.010897193104028702, recon_loss = 0.005574168637394905, 0.005313064903020859, kl_loss = 0.004979805089533329\n",
      "\n",
      "Epoch 952\n",
      "Step 0: loss = 0.010868550278246403, recon_loss = 0.005308238789439201, 0.0055433036759495735, kl_loss = 0.008504031226038933\n",
      "\n",
      "Epoch 953\n",
      "Step 0: loss = 0.008836465887725353, recon_loss = 0.005101587623357773, 0.0037207745481282473, kl_loss = 0.0070521095767617226\n",
      "\n",
      "Epoch 954\n",
      "Step 0: loss = 0.010900229215621948, recon_loss = 0.005525076761841774, 0.005363093689084053, kl_loss = 0.006029307842254639\n",
      "\n",
      "Epoch 955\n",
      "Step 0: loss = 0.009992552921175957, recon_loss = 0.005151765421032906, 0.004832957871258259, kl_loss = 0.003914820030331612\n",
      "\n",
      "Epoch 956\n",
      "Step 0: loss = 0.00958468858152628, recon_loss = 0.004484726116061211, 0.005089248530566692, kl_loss = 0.005356925539672375\n",
      "\n",
      "Epoch 957\n",
      "Step 0: loss = 0.01235984917730093, recon_loss = 0.005959963425993919, 0.006380904000252485, kl_loss = 0.00949083175510168\n",
      "\n",
      "Epoch 958\n",
      "Step 0: loss = 0.01021658442914486, recon_loss = 0.005300039425492287, 0.00489042978733778, kl_loss = 0.013057532720267773\n",
      "\n",
      "Epoch 959\n",
      "Step 0: loss = 0.011849104426801205, recon_loss = 0.005581129342317581, 0.006239069625735283, kl_loss = 0.014452881179749966\n",
      "\n",
      "Epoch 960\n",
      "Step 0: loss = 0.012105793692171574, recon_loss = 0.005945812910795212, 0.006145151797682047, kl_loss = 0.007414955645799637\n",
      "\n",
      "Epoch 961\n",
      "Step 0: loss = 0.010627048090100288, recon_loss = 0.005887230858206749, 0.004726427607238293, kl_loss = 0.006694808602333069\n",
      "\n",
      "Epoch 962\n",
      "Step 0: loss = 0.011367295868694782, recon_loss = 0.0050635673105716705, 0.006293712183833122, kl_loss = 0.005008337087929249\n",
      "\n",
      "Epoch 963\n",
      "Step 0: loss = 0.010464378632605076, recon_loss = 0.005505906417965889, 0.0049498602747917175, kl_loss = 0.004305766895413399\n",
      "\n",
      "Epoch 964\n",
      "Step 0: loss = 0.011847124435007572, recon_loss = 0.005911948159337044, 0.005926535464823246, kl_loss = 0.004320367239415646\n",
      "\n",
      "Epoch 965\n",
      "Step 0: loss = 0.010202470235526562, recon_loss = 0.005507068708539009, 0.0046856580302119255, kl_loss = 0.0048718536272645\n",
      "\n",
      "Epoch 966\n",
      "Step 0: loss = 0.01286780834197998, recon_loss = 0.005936857312917709, 0.006895976606756449, kl_loss = 0.017487645149230957\n",
      "\n",
      "Epoch 967\n",
      "Step 0: loss = 0.008092057891190052, recon_loss = 0.004583468660712242, 0.0034899930469691753, kl_loss = 0.009297986514866352\n",
      "\n",
      "Epoch 968\n",
      "Step 0: loss = 0.008250820450484753, recon_loss = 0.0047630202025175095, 0.003474787576124072, kl_loss = 0.006506146863102913\n",
      "\n",
      "Epoch 969\n",
      "Step 0: loss = 0.009356263093650341, recon_loss = 0.005231516435742378, 0.004115923307836056, kl_loss = 0.004411665722727776\n",
      "\n",
      "Epoch 970\n",
      "Step 0: loss = 0.010321663692593575, recon_loss = 0.005112700164318085, 0.005201804451644421, kl_loss = 0.0035797404125332832\n",
      "\n",
      "Epoch 971\n",
      "Step 0: loss = 0.010277519933879375, recon_loss = 0.005477381870150566, 0.00479473639279604, kl_loss = 0.002700837329030037\n",
      "\n",
      "Epoch 972\n",
      "Step 0: loss = 0.01061634998768568, recon_loss = 0.0053228456526994705, 0.005285302642732859, kl_loss = 0.004101176746189594\n",
      "\n",
      "Epoch 973\n",
      "Step 0: loss = 0.009313084185123444, recon_loss = 0.00485403835773468, 0.00445188395678997, kl_loss = 0.0035809315741062164\n",
      "\n",
      "Epoch 974\n",
      "Step 0: loss = 0.010071483440697193, recon_loss = 0.004812669008970261, 0.005250643007457256, kl_loss = 0.004085593856871128\n",
      "\n",
      "Epoch 975\n",
      "Step 0: loss = 0.009810448624193668, recon_loss = 0.00443105585873127, 0.005372485611587763, kl_loss = 0.003453914076089859\n",
      "\n",
      "Epoch 976\n",
      "Step 0: loss = 0.010262930765748024, recon_loss = 0.005877949297428131, 0.004377749748528004, kl_loss = 0.0036156820133328438\n",
      "\n",
      "Epoch 977\n",
      "Step 0: loss = 0.007954363711178303, recon_loss = 0.0041850414127111435, 0.0037599068600684404, kl_loss = 0.004707883112132549\n",
      "\n",
      "Epoch 978\n",
      "Step 0: loss = 0.010009406134486198, recon_loss = 0.005355639383196831, 0.004640445113182068, kl_loss = 0.006660928949713707\n",
      "\n",
      "Epoch 979\n",
      "Step 0: loss = 0.009578320197761059, recon_loss = 0.0045994482934474945, 0.0049670422449707985, kl_loss = 0.005914765410125256\n",
      "\n",
      "Epoch 980\n",
      "Step 0: loss = 0.010595985688269138, recon_loss = 0.005320988595485687, 0.005262869410216808, kl_loss = 0.0060637276619672775\n",
      "\n",
      "Epoch 981\n",
      "Step 0: loss = 0.010624985210597515, recon_loss = 0.005684908479452133, 0.00492994487285614, kl_loss = 0.005065809935331345\n",
      "\n",
      "Epoch 982\n",
      "Step 0: loss = 0.011411814950406551, recon_loss = 0.005935870110988617, 0.005465509369969368, kl_loss = 0.005217843689024448\n",
      "\n",
      "Epoch 983\n",
      "Step 0: loss = 0.00885948445647955, recon_loss = 0.004799339920282364, 0.004050803370773792, kl_loss = 0.004670698195695877\n",
      "\n",
      "Epoch 984\n",
      "Step 0: loss = 0.010738667100667953, recon_loss = 0.00560334138572216, 0.005128495395183563, kl_loss = 0.003415225073695183\n",
      "\n",
      "Epoch 985\n",
      "Step 0: loss = 0.010050134733319283, recon_loss = 0.005624540150165558, 0.004420391283929348, kl_loss = 0.0026014726608991623\n",
      "\n",
      "Epoch 986\n",
      "Step 0: loss = 0.00988672487437725, recon_loss = 0.005174519494175911, 0.004704940132796764, kl_loss = 0.0036328397691249847\n",
      "\n",
      "Epoch 987\n",
      "Step 0: loss = 0.012479795143008232, recon_loss = 0.006378576159477234, 0.006096522323787212, kl_loss = 0.00234821904450655\n",
      "\n",
      "Epoch 988\n",
      "Step 0: loss = 0.012972436845302582, recon_loss = 0.007070563733577728, 0.00589089747518301, kl_loss = 0.005487949587404728\n",
      "\n",
      "Epoch 989\n",
      "Step 0: loss = 0.010316096246242523, recon_loss = 0.005365230143070221, 0.00492376834154129, kl_loss = 0.013548762537539005\n",
      "\n",
      "Epoch 990\n",
      "Step 0: loss = 0.00845978781580925, recon_loss = 0.004625577479600906, 0.0038188996259123087, kl_loss = 0.007655256427824497\n",
      "\n",
      "Epoch 991\n",
      "Step 0: loss = 0.011806895025074482, recon_loss = 0.005230998620390892, 0.006566271185874939, kl_loss = 0.004812600091099739\n",
      "\n",
      "Epoch 992\n",
      "Step 0: loss = 0.010517524555325508, recon_loss = 0.005371924489736557, 0.0051392377354204655, kl_loss = 0.003181508742272854\n",
      "\n",
      "Epoch 993\n",
      "Step 0: loss = 0.0099460044875741, recon_loss = 0.005194131284952164, 0.004745466634631157, kl_loss = 0.003203168511390686\n",
      "\n",
      "Epoch 994\n",
      "Step 0: loss = 0.0124494144693017, recon_loss = 0.006274186074733734, 0.006165186874568462, kl_loss = 0.005020652897655964\n",
      "\n",
      "Epoch 995\n",
      "Step 0: loss = 0.010410706512629986, recon_loss = 0.005296548828482628, 0.005104872863739729, kl_loss = 0.004642271436750889\n",
      "\n",
      "Epoch 996\n",
      "Step 0: loss = 0.011798894964158535, recon_loss = 0.005409717559814453, 0.0063799722120165825, kl_loss = 0.004602508619427681\n",
      "\n",
      "Epoch 997\n",
      "Step 0: loss = 0.012898063287138939, recon_loss = 0.006032899022102356, 0.006860166788101196, kl_loss = 0.002498544752597809\n",
      "\n",
      "Epoch 998\n",
      "Step 0: loss = 0.012106386944651604, recon_loss = 0.0062591154128313065, 0.005824515130370855, kl_loss = 0.011378171853721142\n",
      "\n",
      "Epoch 999\n",
      "Step 0: loss = 0.012129334732890129, recon_loss = 0.006853433325886726, 0.005262331105768681, kl_loss = 0.006785162724554539\n",
      "\n",
      "Epoch 1000\n",
      "Step 0: loss = 0.009346861392259598, recon_loss = 0.005316184833645821, 0.004002408590167761, kl_loss = 0.014133857563138008\n",
      "\n",
      "Epoch 1001\n",
      "Step 0: loss = 0.00978138204663992, recon_loss = 0.005650179460644722, 0.004112381488084793, kl_loss = 0.009410679340362549\n",
      "\n",
      "Epoch 1002\n",
      "Step 0: loss = 0.01241237111389637, recon_loss = 0.005750533193349838, 0.0066487351432442665, kl_loss = 0.0065514519810676575\n",
      "\n",
      "Epoch 1003\n",
      "Step 0: loss = 0.0095601137727499, recon_loss = 0.004840489476919174, 0.004712885245680809, kl_loss = 0.0033695725724101067\n",
      "\n",
      "Epoch 1004\n",
      "Step 0: loss = 0.008854450657963753, recon_loss = 0.005127239972352982, 0.0037209028378129005, kl_loss = 0.0031537292525172234\n",
      "\n",
      "Epoch 1005\n",
      "Step 0: loss = 0.010523397475481033, recon_loss = 0.005548030138015747, 0.004970548674464226, kl_loss = 0.0024093110114336014\n",
      "\n",
      "Epoch 1006\n",
      "Step 0: loss = 0.009582910686731339, recon_loss = 0.004753641784191132, 0.004824739880859852, kl_loss = 0.0022643981501460075\n",
      "\n",
      "Epoch 1007\n",
      "Step 0: loss = 0.010658745653927326, recon_loss = 0.004643796011805534, 0.006007365882396698, kl_loss = 0.003791952505707741\n",
      "\n",
      "Epoch 1008\n",
      "Step 0: loss = 0.010836160741746426, recon_loss = 0.005248533561825752, 0.005581307224929333, kl_loss = 0.0031599821522831917\n",
      "\n",
      "Epoch 1009\n",
      "Step 0: loss = 0.011444522999227047, recon_loss = 0.005045728757977486, 0.006392105016857386, kl_loss = 0.0033447714522480965\n",
      "\n",
      "Epoch 1010\n",
      "Step 0: loss = 0.01234473567456007, recon_loss = 0.005694640800356865, 0.006642846390604973, kl_loss = 0.003624473698437214\n",
      "\n",
      "Epoch 1011\n",
      "Step 0: loss = 0.012043465860188007, recon_loss = 0.0063131097704172134, 0.0057206181809306145, kl_loss = 0.004868924617767334\n",
      "\n",
      "Epoch 1012\n",
      "Step 0: loss = 0.011766846291720867, recon_loss = 0.006421161815524101, 0.005330014042556286, kl_loss = 0.007835174910724163\n",
      "\n",
      "Epoch 1013\n",
      "Step 0: loss = 0.010792018845677376, recon_loss = 0.005670411512255669, 0.00510957557708025, kl_loss = 0.006015809252858162\n",
      "\n",
      "Epoch 1014\n",
      "Step 0: loss = 0.011042705737054348, recon_loss = 0.006047481670975685, 0.004980033729225397, kl_loss = 0.007595375180244446\n",
      "\n",
      "Epoch 1015\n",
      "Step 0: loss = 0.01070457510650158, recon_loss = 0.0059255193918943405, 0.004769941326230764, kl_loss = 0.004556750878691673\n",
      "\n",
      "Epoch 1016\n",
      "Step 0: loss = 0.010318776592612267, recon_loss = 0.0051759276539087296, 0.005127351731061935, kl_loss = 0.007748656906187534\n",
      "\n",
      "Epoch 1017\n",
      "Step 0: loss = 0.010613640770316124, recon_loss = 0.005430521443486214, 0.0051731993444263935, kl_loss = 0.0049603842198848724\n",
      "\n",
      "Epoch 1018\n",
      "Step 0: loss = 0.00987103208899498, recon_loss = 0.0051092300564050674, 0.00475333072245121, kl_loss = 0.00423557311296463\n",
      "\n",
      "Epoch 1019\n",
      "Step 0: loss = 0.010851159691810608, recon_loss = 0.005508994683623314, 0.00532373134046793, kl_loss = 0.009216897189617157\n",
      "\n",
      "Epoch 1020\n",
      "Step 0: loss = 0.009922727011144161, recon_loss = 0.004932921379804611, 0.004970520734786987, kl_loss = 0.009642531163990498\n",
      "\n",
      "Epoch 1021\n",
      "Step 0: loss = 0.012115849182009697, recon_loss = 0.006186835467815399, 0.005896665155887604, kl_loss = 0.01617448404431343\n",
      "\n",
      "Epoch 1022\n",
      "Step 0: loss = 0.013274932280182838, recon_loss = 0.005811542272567749, 0.007448818534612656, kl_loss = 0.0072856443002820015\n",
      "\n",
      "Epoch 1023\n",
      "Step 0: loss = 0.013221155852079391, recon_loss = 0.006224626675248146, 0.006983041297644377, kl_loss = 0.006743893958628178\n",
      "\n",
      "Epoch 1024\n",
      "Step 0: loss = 0.011572811752557755, recon_loss = 0.005601394921541214, 0.005961768329143524, kl_loss = 0.004824236035346985\n",
      "\n",
      "Epoch 1025\n",
      "Step 0: loss = 0.011135679669678211, recon_loss = 0.006153391674160957, 0.004971670918166637, kl_loss = 0.005308435298502445\n",
      "\n",
      "Epoch 1026\n",
      "Step 0: loss = 0.011933907866477966, recon_loss = 0.005771074444055557, 0.006144451908767223, kl_loss = 0.00919068418443203\n",
      "\n",
      "Epoch 1027\n",
      "Step 0: loss = 0.01075743418186903, recon_loss = 0.00595146045088768, 0.004760389216244221, kl_loss = 0.022792423143982887\n",
      "\n",
      "Epoch 1028\n",
      "Step 0: loss = 0.010763995349407196, recon_loss = 0.0058412328362464905, 0.004876594990491867, kl_loss = 0.023083854466676712\n",
      "\n",
      "Epoch 1029\n",
      "Step 0: loss = 0.010940510779619217, recon_loss = 0.005965922027826309, 0.0049493880942463875, kl_loss = 0.012600245885550976\n",
      "\n",
      "Epoch 1030\n",
      "Step 0: loss = 0.011792618781328201, recon_loss = 0.006104633212089539, 0.005678403191268444, kl_loss = 0.004791079089045525\n",
      "\n",
      "Epoch 1031\n",
      "Step 0: loss = 0.009715628810226917, recon_loss = 0.005586884915828705, 0.004114978481084108, kl_loss = 0.0068827299401164055\n",
      "\n",
      "Epoch 1032\n",
      "Step 0: loss = 0.011666961945593357, recon_loss = 0.005961596965789795, 0.005699109751731157, kl_loss = 0.0031276894733309746\n",
      "\n",
      "Epoch 1033\n",
      "Step 0: loss = 0.013401033356785774, recon_loss = 0.006279654800891876, 0.007111424580216408, kl_loss = 0.00497684720903635\n",
      "\n",
      "Epoch 1034\n",
      "Step 0: loss = 0.009739759378135204, recon_loss = 0.004497366026043892, 0.005232705269008875, kl_loss = 0.004844391718506813\n",
      "\n",
      "Epoch 1035\n",
      "Step 0: loss = 0.011216486804187298, recon_loss = 0.006397176533937454, 0.004804150201380253, kl_loss = 0.007579936645925045\n",
      "\n",
      "Epoch 1036\n",
      "Step 0: loss = 0.011011704802513123, recon_loss = 0.0052350182086229324, 0.0057531967759132385, kl_loss = 0.01174484845250845\n",
      "\n",
      "Epoch 1037\n",
      "Step 0: loss = 0.009541717357933521, recon_loss = 0.004770798608660698, 0.004757829010486603, kl_loss = 0.006544820964336395\n",
      "\n",
      "Epoch 1038\n",
      "Step 0: loss = 0.010704493150115013, recon_loss = 0.0053795017302036285, 0.005309863016009331, kl_loss = 0.007564104162156582\n",
      "\n",
      "Epoch 1039\n",
      "Step 0: loss = 0.009987669065594673, recon_loss = 0.005337562412023544, 0.004635910969227552, kl_loss = 0.007097458466887474\n",
      "\n",
      "Epoch 1040\n",
      "Step 0: loss = 0.01027381606400013, recon_loss = 0.004829512909054756, 0.005426876246929169, kl_loss = 0.008713608607649803\n",
      "\n",
      "Epoch 1041\n",
      "Step 0: loss = 0.012097208760678768, recon_loss = 0.005902541801333427, 0.0061846282333135605, kl_loss = 0.005019320175051689\n",
      "\n",
      "Epoch 1042\n",
      "Step 0: loss = 0.012658309191465378, recon_loss = 0.005622169002890587, 0.007027170620858669, kl_loss = 0.004484864883124828\n",
      "\n",
      "Epoch 1043\n",
      "Step 0: loss = 0.010721663013100624, recon_loss = 0.005138756707310677, 0.005575546994805336, kl_loss = 0.003679784946143627\n",
      "\n",
      "Epoch 1044\n",
      "Step 0: loss = 0.010147369466722012, recon_loss = 0.005188914015889168, 0.004931360017508268, kl_loss = 0.013547586277127266\n",
      "\n",
      "Epoch 1045\n",
      "Step 0: loss = 0.01035305391997099, recon_loss = 0.0047601573169231415, 0.0055684056133031845, kl_loss = 0.012245693244040012\n",
      "\n",
      "Epoch 1046\n",
      "Step 0: loss = 0.010365501046180725, recon_loss = 0.004570560529828072, 0.005778464023023844, kl_loss = 0.008238527923822403\n",
      "\n",
      "Epoch 1047\n",
      "Step 0: loss = 0.01050523854792118, recon_loss = 0.005311585962772369, 0.005174308083951473, kl_loss = 0.009672350250184536\n",
      "\n",
      "Epoch 1048\n",
      "Step 0: loss = 0.009756173938512802, recon_loss = 0.004431974142789841, 0.0053098201751708984, kl_loss = 0.007189626805484295\n",
      "\n",
      "Epoch 1049\n",
      "Step 0: loss = 0.01193462684750557, recon_loss = 0.006467841565608978, 0.005455328151583672, kl_loss = 0.005728616379201412\n",
      "\n",
      "Epoch 1050\n",
      "Step 0: loss = 0.011647420935332775, recon_loss = 0.005956513807177544, 0.005685428157448769, kl_loss = 0.0027394741773605347\n",
      "\n",
      "Epoch 1051\n",
      "Step 0: loss = 0.010829230770468712, recon_loss = 0.005539190024137497, 0.005284692160785198, kl_loss = 0.0026744622737169266\n",
      "\n",
      "Epoch 1052\n",
      "Step 0: loss = 0.012513452209532261, recon_loss = 0.006509793922305107, 0.005996834021061659, kl_loss = 0.0034117745235562325\n",
      "\n",
      "Epoch 1053\n",
      "Step 0: loss = 0.010143439285457134, recon_loss = 0.005114136263728142, 0.005023643374443054, kl_loss = 0.00282998476177454\n",
      "\n",
      "Epoch 1054\n",
      "Step 0: loss = 0.010081235319375992, recon_loss = 0.0047386568039655685, 0.00533630046993494, kl_loss = 0.0031388625502586365\n",
      "\n",
      "Epoch 1055\n",
      "Step 0: loss = 0.012780606746673584, recon_loss = 0.006213601678609848, 0.006558367982506752, kl_loss = 0.00431857630610466\n",
      "\n",
      "Epoch 1056\n",
      "Step 0: loss = 0.008445276878774166, recon_loss = 0.004802379757165909, 0.0036356677301228046, kl_loss = 0.003614874556660652\n",
      "\n",
      "Epoch 1057\n",
      "Step 0: loss = 0.012107692658901215, recon_loss = 0.005971940234303474, 0.0061222584918141365, kl_loss = 0.006747023202478886\n",
      "\n",
      "Epoch 1058\n",
      "Step 0: loss = 0.012394054792821407, recon_loss = 0.006548946723341942, 0.005839417222887278, kl_loss = 0.002845732495188713\n",
      "\n",
      "Epoch 1059\n",
      "Step 0: loss = 0.009250950999557972, recon_loss = 0.0052956584841012955, 0.003945768810808659, kl_loss = 0.004761844873428345\n",
      "\n",
      "Epoch 1060\n",
      "Step 0: loss = 0.010162013582885265, recon_loss = 0.005224267020821571, 0.004928813315927982, kl_loss = 0.00446672085672617\n",
      "\n",
      "Epoch 1061\n",
      "Step 0: loss = 0.011162464506924152, recon_loss = 0.004997164011001587, 0.006154305301606655, kl_loss = 0.005497721955180168\n",
      "\n",
      "Epoch 1062\n",
      "Step 0: loss = 0.009313688613474369, recon_loss = 0.00480562262237072, 0.0045000286772847176, kl_loss = 0.004018597304821014\n",
      "\n",
      "Epoch 1063\n",
      "Step 0: loss = 0.0101477624848485, recon_loss = 0.005707943812012672, 0.004427674226462841, kl_loss = 0.006072237156331539\n",
      "\n",
      "Epoch 1064\n",
      "Step 0: loss = 0.010497367940843105, recon_loss = 0.005450114607810974, 0.005041145253926516, kl_loss = 0.0030542071908712387\n",
      "\n",
      "Epoch 1065\n",
      "Step 0: loss = 0.009225599467754364, recon_loss = 0.004808403551578522, 0.004409660119563341, kl_loss = 0.0037683630362153053\n",
      "\n",
      "Epoch 1066\n",
      "Step 0: loss = 0.010038862004876137, recon_loss = 0.005181420594453812, 0.004849394783377647, kl_loss = 0.004023279994726181\n",
      "\n",
      "Epoch 1067\n",
      "Step 0: loss = 0.00848868116736412, recon_loss = 0.0045720916241407394, 0.003911375068128109, kl_loss = 0.0026073306798934937\n",
      "\n",
      "Epoch 1068\n",
      "Step 0: loss = 0.01048990711569786, recon_loss = 0.005061827600002289, 0.005416276399046183, kl_loss = 0.0059019578620791435\n",
      "\n",
      "Epoch 1069\n",
      "Step 0: loss = 0.010510235093533993, recon_loss = 0.00488114170730114, 0.005618315190076828, kl_loss = 0.005389071069657803\n",
      "\n",
      "Epoch 1070\n",
      "Step 0: loss = 0.012183083221316338, recon_loss = 0.005622509866952896, 0.00651911785826087, kl_loss = 0.02072742208838463\n",
      "\n",
      "Epoch 1071\n",
      "Step 0: loss = 0.012487908825278282, recon_loss = 0.006717568263411522, 0.005750853568315506, kl_loss = 0.00974353589117527\n",
      "\n",
      "Epoch 1072\n",
      "Step 0: loss = 0.010536863468587399, recon_loss = 0.005743177607655525, 0.004782633390277624, kl_loss = 0.005525808781385422\n",
      "\n",
      "Epoch 1073\n",
      "Step 0: loss = 0.009099417366087437, recon_loss = 0.00528094545006752, 0.003809870919212699, kl_loss = 0.004300203174352646\n",
      "\n",
      "Epoch 1074\n",
      "Step 0: loss = 0.011124730110168457, recon_loss = 0.005384908989071846, 0.00572982057929039, kl_loss = 0.005000205710530281\n",
      "\n",
      "Epoch 1075\n",
      "Step 0: loss = 0.011443022638559341, recon_loss = 0.005288951098918915, 0.006147040985524654, kl_loss = 0.003515329211950302\n",
      "\n",
      "Epoch 1076\n",
      "Step 0: loss = 0.010142556391656399, recon_loss = 0.0044546592980623245, 0.005666757933795452, kl_loss = 0.01056951005011797\n",
      "\n",
      "Epoch 1077\n",
      "Step 0: loss = 0.01126785110682249, recon_loss = 0.0050801485776901245, 0.006169421132653952, kl_loss = 0.009140526875853539\n",
      "\n",
      "Epoch 1078\n",
      "Step 0: loss = 0.008823515847325325, recon_loss = 0.004491942003369331, 0.004318384453654289, kl_loss = 0.006594624370336533\n",
      "\n",
      "Epoch 1079\n",
      "Step 0: loss = 0.010281954891979694, recon_loss = 0.005181705579161644, 0.005091213621199131, kl_loss = 0.004517771303653717\n",
      "\n",
      "Epoch 1080\n",
      "Step 0: loss = 0.010732577182352543, recon_loss = 0.00572548434138298, 0.004995145834982395, kl_loss = 0.005973474122583866\n",
      "\n",
      "Epoch 1081\n",
      "Step 0: loss = 0.010373029857873917, recon_loss = 0.005442779511213303, 0.004918541759252548, kl_loss = 0.005854219198226929\n",
      "\n",
      "Epoch 1082\n",
      "Step 0: loss = 0.009652320295572281, recon_loss = 0.005190163850784302, 0.0044508809223771095, kl_loss = 0.005637624301016331\n",
      "\n",
      "Epoch 1083\n",
      "Step 0: loss = 0.011018029414117336, recon_loss = 0.00522003136575222, 0.005789994262158871, kl_loss = 0.004002059809863567\n",
      "\n",
      "Epoch 1084\n",
      "Step 0: loss = 0.010310597717761993, recon_loss = 0.005093634128570557, 0.00521064130589366, kl_loss = 0.003160826861858368\n",
      "\n",
      "Epoch 1085\n",
      "Step 0: loss = 0.010871291160583496, recon_loss = 0.0054315850138664246, 0.005427221767604351, kl_loss = 0.0062424186617136\n",
      "\n",
      "Epoch 1086\n",
      "Step 0: loss = 0.011192714795470238, recon_loss = 0.005320876836776733, 0.005866840947419405, kl_loss = 0.002498951740562916\n",
      "\n",
      "Epoch 1087\n",
      "Step 0: loss = 0.00940563902258873, recon_loss = 0.005013994872570038, 0.004384480882436037, kl_loss = 0.003581976518034935\n",
      "\n",
      "Epoch 1088\n",
      "Step 0: loss = 0.008658424019813538, recon_loss = 0.004564352333545685, 0.004088202957063913, kl_loss = 0.0029346104711294174\n",
      "\n",
      "Epoch 1089\n",
      "Step 0: loss = 0.009396622888743877, recon_loss = 0.004617558792233467, 0.004769447259604931, kl_loss = 0.004808340221643448\n",
      "\n",
      "Epoch 1090\n",
      "Step 0: loss = 0.01097720768302679, recon_loss = 0.005012379959225655, 0.005955105647444725, kl_loss = 0.004861162975430489\n",
      "\n",
      "Epoch 1091\n",
      "Step 0: loss = 0.011211217381060123, recon_loss = 0.005154740065336227, 0.006049554329365492, kl_loss = 0.003461092710494995\n",
      "\n",
      "Epoch 1092\n",
      "Step 0: loss = 0.011267788708209991, recon_loss = 0.0056098997592926025, 0.005650504492223263, kl_loss = 0.0036920253187417984\n",
      "\n",
      "Epoch 1093\n",
      "Step 0: loss = 0.011031529866158962, recon_loss = 0.005449090152978897, 0.005574722308665514, kl_loss = 0.0038588223978877068\n",
      "\n",
      "Epoch 1094\n",
      "Step 0: loss = 0.009948632679879665, recon_loss = 0.005212914198637009, 0.004727426450699568, kl_loss = 0.004145569168031216\n",
      "\n",
      "Epoch 1095\n",
      "Step 0: loss = 0.012145972810685635, recon_loss = 0.005855083465576172, 0.006284056231379509, kl_loss = 0.003416425548493862\n",
      "\n",
      "Epoch 1096\n",
      "Step 0: loss = 0.01030984241515398, recon_loss = 0.005578348413109779, 0.004718570504337549, kl_loss = 0.006462200544774532\n",
      "\n",
      "Epoch 1097\n",
      "Step 0: loss = 0.011532394215464592, recon_loss = 0.00610700249671936, 0.005406877025961876, kl_loss = 0.009257282130420208\n",
      "\n",
      "Epoch 1098\n",
      "Step 0: loss = 0.013532248325645924, recon_loss = 0.007201416417956352, 0.006316673941910267, kl_loss = 0.007078765891492367\n",
      "\n",
      "Epoch 1099\n",
      "Step 0: loss = 0.011377910152077675, recon_loss = 0.006331847980618477, 0.005025071557611227, kl_loss = 0.010495197959244251\n",
      "\n",
      "Epoch 1100\n",
      "Step 0: loss = 0.012035359628498554, recon_loss = 0.006150878965854645, 0.005865885876119137, kl_loss = 0.009297424927353859\n",
      "\n",
      "Epoch 1101\n",
      "Step 0: loss = 0.012898853980004787, recon_loss = 0.007025411352515221, 0.0058650909923017025, kl_loss = 0.004175616428256035\n",
      "\n",
      "Epoch 1102\n",
      "Step 0: loss = 0.010611741803586483, recon_loss = 0.006014084443449974, 0.004586520604789257, kl_loss = 0.005568372085690498\n",
      "\n",
      "Epoch 1103\n",
      "Step 0: loss = 0.009418918751180172, recon_loss = 0.0056528206914663315, 0.0037554767914116383, kl_loss = 0.005311056040227413\n",
      "\n",
      "Epoch 1104\n",
      "Step 0: loss = 0.011036652140319347, recon_loss = 0.005427101626992226, 0.005601013544946909, kl_loss = 0.004268811084330082\n",
      "\n",
      "Epoch 1105\n",
      "Step 0: loss = 0.009565002284944057, recon_loss = 0.004378624260425568, 0.005179768428206444, kl_loss = 0.0033049266785383224\n",
      "\n",
      "Epoch 1106\n",
      "Step 0: loss = 0.009646558202803135, recon_loss = 0.004935156553983688, 0.004704764112830162, kl_loss = 0.0033187270164489746\n",
      "\n",
      "Epoch 1107\n",
      "Step 0: loss = 0.009517005644738674, recon_loss = 0.00483366847038269, 0.0046700844541192055, kl_loss = 0.0066264038905501366\n",
      "\n",
      "Epoch 1108\n",
      "Step 0: loss = 0.009426684118807316, recon_loss = 0.004945756867527962, 0.00447325361892581, kl_loss = 0.00383671373128891\n",
      "\n",
      "Epoch 1109\n",
      "Step 0: loss = 0.011336282826960087, recon_loss = 0.00590050034224987, 0.005427321884781122, kl_loss = 0.0042305560782551765\n",
      "\n",
      "Epoch 1110\n",
      "Step 0: loss = 0.01064690388739109, recon_loss = 0.005413200706243515, 0.005227584391832352, kl_loss = 0.0030592279508709908\n",
      "\n",
      "Epoch 1111\n",
      "Step 0: loss = 0.00929165817797184, recon_loss = 0.005305508151650429, 0.003980189096182585, kl_loss = 0.0029802676290273666\n",
      "\n",
      "Epoch 1112\n",
      "Step 0: loss = 0.009551488794386387, recon_loss = 0.004852594807744026, 0.004688825458288193, kl_loss = 0.005034254863858223\n",
      "\n",
      "Epoch 1113\n",
      "Step 0: loss = 0.012114434503018856, recon_loss = 0.005425484851002693, 0.006681078113615513, kl_loss = 0.003935560584068298\n",
      "\n",
      "Epoch 1114\n",
      "Step 0: loss = 0.012145916000008583, recon_loss = 0.005535377189517021, 0.0065942974761128426, kl_loss = 0.008120669052004814\n",
      "\n",
      "Epoch 1115\n",
      "Step 0: loss = 0.011614079587161541, recon_loss = 0.005264338105916977, 0.006338347215205431, kl_loss = 0.005697364918887615\n",
      "\n",
      "Epoch 1116\n",
      "Step 0: loss = 0.011459478177130222, recon_loss = 0.006100127473473549, 0.00533831724897027, kl_loss = 0.010517077520489693\n",
      "\n",
      "Epoch 1117\n",
      "Step 0: loss = 0.01620335504412651, recon_loss = 0.00766373798251152, 0.008517468348145485, kl_loss = 0.011074728332459927\n",
      "\n",
      "Epoch 1118\n",
      "Step 0: loss = 0.013192960061132908, recon_loss = 0.0066695138812065125, 0.006489107385277748, kl_loss = 0.01716925948858261\n",
      "\n",
      "Epoch 1119\n",
      "Step 0: loss = 0.014474030584096909, recon_loss = 0.006342656910419464, 0.008095970377326012, kl_loss = 0.01770155504345894\n",
      "\n",
      "Epoch 1120\n",
      "Step 0: loss = 0.012276615016162395, recon_loss = 0.006085112690925598, 0.006143117323517799, kl_loss = 0.024192465469241142\n",
      "\n",
      "Epoch 1121\n",
      "Step 0: loss = 0.013412632048130035, recon_loss = 0.005907610058784485, 0.007476787082850933, kl_loss = 0.014117609709501266\n",
      "\n",
      "Epoch 1122\n",
      "Step 0: loss = 0.010917813517153263, recon_loss = 0.005667382851243019, 0.005226859822869301, kl_loss = 0.01178551185876131\n",
      "\n",
      "Epoch 1123\n",
      "Step 0: loss = 0.010329360142350197, recon_loss = 0.005802467465400696, 0.004514286294579506, kl_loss = 0.006303421221673489\n",
      "\n",
      "Epoch 1124\n",
      "Step 0: loss = 0.010894163511693478, recon_loss = 0.005451282486319542, 0.005432437174022198, kl_loss = 0.0052220458164811134\n",
      "\n",
      "Epoch 1125\n",
      "Step 0: loss = 0.01054014079272747, recon_loss = 0.005461398512125015, 0.005072551779448986, kl_loss = 0.0030953260138630867\n",
      "\n",
      "Epoch 1126\n",
      "Step 0: loss = 0.01009349524974823, recon_loss = 0.005430344492197037, 0.004653226118534803, kl_loss = 0.004962076433002949\n",
      "\n",
      "Epoch 1127\n",
      "Step 0: loss = 0.00969961192458868, recon_loss = 0.005345800891518593, 0.004342812113463879, kl_loss = 0.005499248392879963\n",
      "\n",
      "Epoch 1128\n",
      "Step 0: loss = 0.008320299908518791, recon_loss = 0.004741199314594269, 0.00357000227086246, kl_loss = 0.004549063742160797\n",
      "\n",
      "Epoch 1129\n",
      "Step 0: loss = 0.009101826697587967, recon_loss = 0.004391249269247055, 0.004693089984357357, kl_loss = 0.008743690326809883\n",
      "\n",
      "Epoch 1130\n",
      "Step 0: loss = 0.009801212698221207, recon_loss = 0.0045316182076931, 0.00526074692606926, kl_loss = 0.004423893988132477\n",
      "\n",
      "Epoch 1131\n",
      "Step 0: loss = 0.009776106104254723, recon_loss = 0.0046890005469322205, 0.005076833069324493, kl_loss = 0.00513614434748888\n",
      "\n",
      "Epoch 1132\n",
      "Step 0: loss = 0.0106329545378685, recon_loss = 0.004863778129220009, 0.005755010060966015, kl_loss = 0.007082960568368435\n",
      "\n",
      "Epoch 1133\n",
      "Step 0: loss = 0.01016022078692913, recon_loss = 0.005024800077080727, 0.005124472081661224, kl_loss = 0.005474538542330265\n",
      "\n",
      "Epoch 1134\n",
      "Step 0: loss = 0.010161890648305416, recon_loss = 0.005231037735939026, 0.004924875218421221, kl_loss = 0.0029889075085520744\n",
      "\n",
      "Epoch 1135\n",
      "Step 0: loss = 0.009992247447371483, recon_loss = 0.0047736503183841705, 0.005212542600929737, kl_loss = 0.003027227707207203\n",
      "\n",
      "Epoch 1136\n",
      "Step 0: loss = 0.009814496152102947, recon_loss = 0.005062902346253395, 0.004747036378830671, kl_loss = 0.0022782953456044197\n",
      "\n",
      "Epoch 1137\n",
      "Step 0: loss = 0.011323290877044201, recon_loss = 0.0049635861068964005, 0.0063542090356349945, kl_loss = 0.0027477815747261047\n",
      "\n",
      "Epoch 1138\n",
      "Step 0: loss = 0.010368469171226025, recon_loss = 0.005151780322194099, 0.00521162198856473, kl_loss = 0.002533676102757454\n",
      "\n",
      "Epoch 1139\n",
      "Step 0: loss = 0.010977871716022491, recon_loss = 0.005578983575105667, 0.005388135090470314, kl_loss = 0.005376379005610943\n",
      "\n",
      "Epoch 1140\n",
      "Step 0: loss = 0.009233246557414532, recon_loss = 0.005028406158089638, 0.0041955518536269665, kl_loss = 0.004643886350095272\n",
      "\n",
      "Epoch 1141\n",
      "Step 0: loss = 0.010396601632237434, recon_loss = 0.0052255503833293915, 0.005159725435078144, kl_loss = 0.005662788636982441\n",
      "\n",
      "Epoch 1142\n",
      "Step 0: loss = 0.01021571271121502, recon_loss = 0.00518307089805603, 0.005023700185120106, kl_loss = 0.004470902495086193\n",
      "\n",
      "Epoch 1143\n",
      "Step 0: loss = 0.010246817022562027, recon_loss = 0.005300477147102356, 0.0049362583085894585, kl_loss = 0.005040823481976986\n",
      "\n",
      "Epoch 1144\n",
      "Step 0: loss = 0.011566845700144768, recon_loss = 0.004850521683692932, 0.006700168363749981, kl_loss = 0.008077661506831646\n",
      "\n",
      "Epoch 1145\n",
      "Step 0: loss = 0.011475806124508381, recon_loss = 0.005316944792866707, 0.006145011633634567, kl_loss = 0.00692483875900507\n",
      "\n",
      "Epoch 1146\n",
      "Step 0: loss = 0.011319666169583797, recon_loss = 0.006389100104570389, 0.004915541969239712, kl_loss = 0.007511910982429981\n",
      "\n",
      "Epoch 1147\n",
      "Step 0: loss = 0.010724893771111965, recon_loss = 0.005860630422830582, 0.004858603700995445, kl_loss = 0.0028297817334532738\n",
      "\n",
      "Epoch 1148\n",
      "Step 0: loss = 0.011754910461604595, recon_loss = 0.005905997008085251, 0.005841986741870642, kl_loss = 0.0034631071612238884\n",
      "\n",
      "Epoch 1149\n",
      "Step 0: loss = 0.012178505770862103, recon_loss = 0.0055394601076841354, 0.006633119657635689, kl_loss = 0.002962937578558922\n",
      "\n",
      "Epoch 1150\n",
      "Step 0: loss = 0.012595122680068016, recon_loss = 0.006147854030132294, 0.006426108069717884, kl_loss = 0.010580434463918209\n",
      "\n",
      "Epoch 1151\n",
      "Step 0: loss = 0.01404531765729189, recon_loss = 0.007591152563691139, 0.006437205709517002, kl_loss = 0.008479918353259563\n",
      "\n",
      "Epoch 1152\n",
      "Step 0: loss = 0.01067746989428997, recon_loss = 0.0062752217054367065, 0.004382033832371235, kl_loss = 0.010107371024787426\n",
      "\n",
      "Epoch 1153\n",
      "Step 0: loss = 0.014646164141595364, recon_loss = 0.006982522085309029, 0.007643269374966621, kl_loss = 0.010186385363340378\n",
      "\n",
      "Epoch 1154\n",
      "Step 0: loss = 0.012398628517985344, recon_loss = 0.006473924964666367, 0.0059130387380719185, kl_loss = 0.005832526832818985\n",
      "\n",
      "Epoch 1155\n",
      "Step 0: loss = 0.01156541332602501, recon_loss = 0.005805628374218941, 0.005743141286075115, kl_loss = 0.008322026580572128\n",
      "\n",
      "Epoch 1156\n",
      "Step 0: loss = 0.012287769466638565, recon_loss = 0.006742877885699272, 0.005525343120098114, kl_loss = 0.009774451144039631\n",
      "\n",
      "Epoch 1157\n",
      "Step 0: loss = 0.012202686630189419, recon_loss = 0.0064860861748456955, 0.005701157264411449, kl_loss = 0.007721646688878536\n",
      "\n",
      "Epoch 1158\n",
      "Step 0: loss = 0.013204925693571568, recon_loss = 0.006772127002477646, 0.006417020224034786, kl_loss = 0.007889287546277046\n",
      "\n",
      "Epoch 1159\n",
      "Step 0: loss = 0.011244370602071285, recon_loss = 0.005806984379887581, 0.005423654802143574, kl_loss = 0.006865506060421467\n",
      "\n",
      "Epoch 1160\n",
      "Step 0: loss = 0.010087057948112488, recon_loss = 0.005820004269480705, 0.0042607770301401615, kl_loss = 0.0031385645270347595\n",
      "\n",
      "Epoch 1161\n",
      "Step 0: loss = 0.011008507572114468, recon_loss = 0.005695352330803871, 0.005305387079715729, kl_loss = 0.0038839252665638924\n",
      "\n",
      "Epoch 1162\n",
      "Step 0: loss = 0.012756879441440105, recon_loss = 0.006788192316889763, 0.005951107479631901, kl_loss = 0.008789745159447193\n",
      "\n",
      "Epoch 1163\n",
      "Step 0: loss = 0.009783643297851086, recon_loss = 0.005680723115801811, 0.00408271886408329, kl_loss = 0.010100549086928368\n",
      "\n",
      "Epoch 1164\n",
      "Step 0: loss = 0.010086236521601677, recon_loss = 0.004938015714287758, 0.005112562328577042, kl_loss = 0.017829082906246185\n",
      "\n",
      "Epoch 1165\n",
      "Step 0: loss = 0.012009195983409882, recon_loss = 0.005296027287840843, 0.006696125492453575, kl_loss = 0.008521589450538158\n",
      "\n",
      "Epoch 1166\n",
      "Step 0: loss = 0.009734805673360825, recon_loss = 0.004758628085255623, 0.004959298297762871, kl_loss = 0.008439700119197369\n",
      "\n",
      "Epoch 1167\n",
      "Step 0: loss = 0.01232569944113493, recon_loss = 0.00518033467233181, 0.007132993079721928, kl_loss = 0.006185691803693771\n",
      "\n",
      "Epoch 1168\n",
      "Step 0: loss = 0.009786801412701607, recon_loss = 0.0049147829413414, 0.004864423535764217, kl_loss = 0.0037972573190927505\n",
      "\n",
      "Epoch 1169\n",
      "Step 0: loss = 0.009500830434262753, recon_loss = 0.004418449476361275, 0.005075430031865835, kl_loss = 0.0034753000363707542\n",
      "\n",
      "Epoch 1170\n",
      "Step 0: loss = 0.01012711226940155, recon_loss = 0.005083924159407616, 0.0050391219556331635, kl_loss = 0.0020329291000962257\n",
      "\n",
      "Epoch 1171\n",
      "Step 0: loss = 0.01115444302558899, recon_loss = 0.005364932119846344, 0.005779830273240805, kl_loss = 0.004840206354856491\n",
      "\n",
      "Epoch 1172\n",
      "Step 0: loss = 0.009771876968443394, recon_loss = 0.004438057541847229, 0.0053275940008461475, kl_loss = 0.0031129056587815285\n",
      "\n",
      "Epoch 1173\n",
      "Step 0: loss = 0.01045803353190422, recon_loss = 0.004879623651504517, 0.005572604015469551, kl_loss = 0.0029028980061411858\n",
      "\n",
      "Epoch 1174\n",
      "Step 0: loss = 0.00935276597738266, recon_loss = 0.004328589886426926, 0.005015511065721512, kl_loss = 0.004332714714109898\n",
      "\n",
      "Epoch 1175\n",
      "Step 0: loss = 0.010032979771494865, recon_loss = 0.004495931789278984, 0.0055273729376494884, kl_loss = 0.004837296903133392\n",
      "\n",
      "Epoch 1176\n",
      "Step 0: loss = 0.009069787338376045, recon_loss = 0.004832252860069275, 0.004225569777190685, kl_loss = 0.005982484668493271\n",
      "\n",
      "Epoch 1177\n",
      "Step 0: loss = 0.009985447861254215, recon_loss = 0.004800083115696907, 0.005178501829504967, kl_loss = 0.0034315064549446106\n",
      "\n",
      "Epoch 1178\n",
      "Step 0: loss = 0.011178490705788136, recon_loss = 0.005493372678756714, 0.005671141669154167, kl_loss = 0.006987963803112507\n",
      "\n",
      "Epoch 1179\n",
      "Step 0: loss = 0.01186784915626049, recon_loss = 0.006060566753149033, 0.005801985505968332, kl_loss = 0.002648855559527874\n",
      "\n",
      "Epoch 1180\n",
      "Step 0: loss = 0.009972834959626198, recon_loss = 0.005294913426041603, 0.004671521484851837, kl_loss = 0.0032000159844756126\n",
      "\n",
      "Epoch 1181\n",
      "Step 0: loss = 0.010760384611785412, recon_loss = 0.005803996697068214, 0.004950809292495251, kl_loss = 0.0027894387021660805\n",
      "\n",
      "Epoch 1182\n",
      "Step 0: loss = 0.010133454576134682, recon_loss = 0.004787653684616089, 0.005340303294360638, kl_loss = 0.002748853527009487\n",
      "\n",
      "Epoch 1183\n",
      "Step 0: loss = 0.009642007760703564, recon_loss = 0.004913635551929474, 0.004720927681773901, kl_loss = 0.0037220316007733345\n",
      "\n",
      "Epoch 1184\n",
      "Step 0: loss = 0.011575616896152496, recon_loss = 0.005771350115537643, 0.005798243917524815, kl_loss = 0.0030112657696008682\n",
      "\n",
      "Epoch 1185\n",
      "Step 0: loss = 0.01212618313729763, recon_loss = 0.005320368334650993, 0.0067957439459860325, kl_loss = 0.005035611800849438\n",
      "\n",
      "Epoch 1186\n",
      "Step 0: loss = 0.010756172239780426, recon_loss = 0.004575405269861221, 0.006172374356538057, kl_loss = 0.004196627996861935\n",
      "\n",
      "Epoch 1187\n",
      "Step 0: loss = 0.010336083360016346, recon_loss = 0.005207037553191185, 0.005124188028275967, kl_loss = 0.0024291090667247772\n",
      "\n",
      "Epoch 1188\n",
      "Step 0: loss = 0.010760720819234848, recon_loss = 0.005452919751405716, 0.005300902295857668, kl_loss = 0.00344941858202219\n",
      "\n",
      "Epoch 1189\n",
      "Step 0: loss = 0.015651948750019073, recon_loss = 0.008460348471999168, 0.0071881720796227455, kl_loss = 0.0017140116542577744\n",
      "\n",
      "Epoch 1190\n",
      "Step 0: loss = 0.012604798190295696, recon_loss = 0.006155945360660553, 0.006443195976316929, kl_loss = 0.00282843504101038\n",
      "\n",
      "Epoch 1191\n",
      "Step 0: loss = 0.013812856748700142, recon_loss = 0.007978122681379318, 0.005829280242323875, kl_loss = 0.0027267485857009888\n",
      "\n",
      "Epoch 1192\n",
      "Step 0: loss = 0.012336612679064274, recon_loss = 0.00632273405790329, 0.006007771473377943, kl_loss = 0.0030534621328115463\n",
      "\n",
      "Epoch 1193\n",
      "Step 0: loss = 0.011531764641404152, recon_loss = 0.006118349730968475, 0.005406747572124004, kl_loss = 0.003333667293190956\n",
      "\n",
      "Epoch 1194\n",
      "Step 0: loss = 0.011024138890206814, recon_loss = 0.006162768229842186, 0.004852726589888334, kl_loss = 0.00432165339589119\n",
      "\n",
      "Epoch 1195\n",
      "Step 0: loss = 0.012281341478228569, recon_loss = 0.0057984162122011185, 0.0064758919179439545, kl_loss = 0.0035166889429092407\n",
      "\n",
      "Epoch 1196\n",
      "Step 0: loss = 0.00939902849495411, recon_loss = 0.004822840914130211, 0.004572541452944279, kl_loss = 0.0018228664994239807\n",
      "\n",
      "Epoch 1197\n",
      "Step 0: loss = 0.010200656950473785, recon_loss = 0.005028141662478447, 0.005166670307517052, kl_loss = 0.0029222723096609116\n",
      "\n",
      "Epoch 1198\n",
      "Step 0: loss = 0.010679344646632671, recon_loss = 0.005174694582819939, 0.00549598503857851, kl_loss = 0.004332386888563633\n",
      "\n",
      "Epoch 1199\n",
      "Step 0: loss = 0.00979909673333168, recon_loss = 0.004970364272594452, 0.0048215920105576515, kl_loss = 0.0035704495385289192\n",
      "\n",
      "Epoch 1200\n",
      "Step 0: loss = 0.009162478148937225, recon_loss = 0.005166839808225632, 0.003984707407653332, kl_loss = 0.005465351976454258\n",
      "\n",
      "Epoch 1201\n",
      "Step 0: loss = 0.009802489541471004, recon_loss = 0.004589315503835678, 0.005203733220696449, kl_loss = 0.004720552824437618\n",
      "\n",
      "Epoch 1202\n",
      "Step 0: loss = 0.009757312946021557, recon_loss = 0.004541587084531784, 0.005208222195506096, kl_loss = 0.0037517640739679337\n",
      "\n",
      "Epoch 1203\n",
      "Step 0: loss = 0.009780830703675747, recon_loss = 0.004441088065505028, 0.0053326645866036415, kl_loss = 0.0035390863195061684\n",
      "\n",
      "Epoch 1204\n",
      "Step 0: loss = 0.00920779723674059, recon_loss = 0.004356687888503075, 0.004845786839723587, kl_loss = 0.0026610223576426506\n",
      "\n",
      "Epoch 1205\n",
      "Step 0: loss = 0.009833503514528275, recon_loss = 0.003996443003416061, 0.00581846060231328, kl_loss = 0.009300297126173973\n",
      "\n",
      "Epoch 1206\n",
      "Step 0: loss = 0.0093308100476861, recon_loss = 0.0038500912487506866, 0.005465150810778141, kl_loss = 0.007784158922731876\n",
      "\n",
      "Epoch 1207\n",
      "Step 0: loss = 0.009517251513898373, recon_loss = 0.004441671073436737, 0.005064371041953564, kl_loss = 0.005604625679552555\n",
      "\n",
      "Epoch 1208\n",
      "Step 0: loss = 0.009527822025120258, recon_loss = 0.004371656104922295, 0.005147553514689207, kl_loss = 0.0043061841279268265\n",
      "\n",
      "Epoch 1209\n",
      "Step 0: loss = 0.010595135390758514, recon_loss = 0.004425905644893646, 0.006159086711704731, kl_loss = 0.005071483552455902\n",
      "\n",
      "Epoch 1210\n",
      "Step 0: loss = 0.011320004239678383, recon_loss = 0.005263699218630791, 0.006039154715836048, kl_loss = 0.008574938401579857\n",
      "\n",
      "Epoch 1211\n",
      "Step 0: loss = 0.009785017929971218, recon_loss = 0.004553874954581261, 0.005218565929681063, kl_loss = 0.0062886569648981094\n",
      "\n",
      "Epoch 1212\n",
      "Step 0: loss = 0.013307252898812294, recon_loss = 0.005279187113046646, 0.00801555160433054, kl_loss = 0.006256871856749058\n",
      "\n",
      "Epoch 1213\n",
      "Step 0: loss = 0.010321106761693954, recon_loss = 0.004881035536527634, 0.005411805585026741, kl_loss = 0.014132818207144737\n",
      "\n",
      "Epoch 1214\n",
      "Step 0: loss = 0.011184710077941418, recon_loss = 0.00550525076687336, 0.005667957477271557, kl_loss = 0.005750781856477261\n",
      "\n",
      "Epoch 1215\n",
      "Step 0: loss = 0.01276923343539238, recon_loss = 0.005866704508662224, 0.006891338154673576, kl_loss = 0.005595178343355656\n",
      "\n",
      "Epoch 1216\n",
      "Step 0: loss = 0.010206024162471294, recon_loss = 0.004960387945175171, 0.005236456636339426, kl_loss = 0.004590161144733429\n",
      "\n",
      "Epoch 1217\n",
      "Step 0: loss = 0.01228990126401186, recon_loss = 0.005807846784591675, 0.006471794098615646, kl_loss = 0.005130144767463207\n",
      "\n",
      "Epoch 1218\n",
      "Step 0: loss = 0.012605615891516209, recon_loss = 0.0068154409527778625, 0.005784708075225353, kl_loss = 0.002733512781560421\n",
      "\n",
      "Epoch 1219\n",
      "Step 0: loss = 0.009876041673123837, recon_loss = 0.005548132583498955, 0.004311071243137121, kl_loss = 0.00841854140162468\n",
      "\n",
      "Epoch 1220\n",
      "Step 0: loss = 0.011305630207061768, recon_loss = 0.005459589883685112, 0.005837203003466129, kl_loss = 0.004418819211423397\n",
      "\n",
      "Epoch 1221\n",
      "Step 0: loss = 0.011647619307041168, recon_loss = 0.005138633772730827, 0.006490579806268215, kl_loss = 0.009202957153320312\n",
      "\n",
      "Epoch 1222\n",
      "Step 0: loss = 0.01112238597124815, recon_loss = 0.006173860281705856, 0.00493613351136446, kl_loss = 0.006195954047143459\n",
      "\n",
      "Epoch 1223\n",
      "Step 0: loss = 0.009615832939743996, recon_loss = 0.004885666072368622, 0.00471403356641531, kl_loss = 0.008066565729677677\n",
      "\n",
      "Epoch 1224\n",
      "Step 0: loss = 0.011495959013700485, recon_loss = 0.005062643438577652, 0.006424391642212868, kl_loss = 0.004461842589080334\n",
      "\n",
      "Epoch 1225\n",
      "Step 0: loss = 0.010911486111581326, recon_loss = 0.005971385166049004, 0.004932450130581856, kl_loss = 0.003825637511909008\n",
      "\n",
      "Epoch 1226\n",
      "Step 0: loss = 0.010780703276395798, recon_loss = 0.00501193106174469, 0.005758035462349653, kl_loss = 0.00536829698830843\n",
      "\n",
      "Epoch 1227\n",
      "Step 0: loss = 0.011779085732996464, recon_loss = 0.005572931841015816, 0.006195043679326773, kl_loss = 0.005554760806262493\n",
      "\n",
      "Epoch 1228\n",
      "Step 0: loss = 0.0114769721403718, recon_loss = 0.006183156743645668, 0.005277369637042284, kl_loss = 0.008223209530115128\n",
      "\n",
      "Epoch 1229\n",
      "Step 0: loss = 0.011214998550713062, recon_loss = 0.006251571699976921, 0.004956040997058153, kl_loss = 0.003693029284477234\n",
      "\n",
      "Epoch 1230\n",
      "Step 0: loss = 0.012373951263725758, recon_loss = 0.0066907331347465515, 0.005675469990819693, kl_loss = 0.003873842768371105\n",
      "\n",
      "Epoch 1231\n",
      "Step 0: loss = 0.011916511692106724, recon_loss = 0.005506468936800957, 0.006401930935680866, kl_loss = 0.004056068137288094\n",
      "\n",
      "Epoch 1232\n",
      "Step 0: loss = 0.01552602369338274, recon_loss = 0.006970193237066269, 0.008551263250410557, kl_loss = 0.0022833850234746933\n",
      "\n",
      "Epoch 1233\n",
      "Step 0: loss = 0.016137056052684784, recon_loss = 0.00752062164247036, 0.008607370778918266, kl_loss = 0.004532174207270145\n",
      "\n",
      "Epoch 1234\n",
      "Step 0: loss = 0.01421924214810133, recon_loss = 0.00617784820497036, 0.008027217350900173, kl_loss = 0.007088135927915573\n",
      "\n",
      "Epoch 1235\n",
      "Step 0: loss = 0.014508774504065514, recon_loss = 0.006549732759594917, 0.007948262616991997, kl_loss = 0.005389620549976826\n",
      "\n",
      "Epoch 1236\n",
      "Step 0: loss = 0.009448176249861717, recon_loss = 0.005494005978107452, 0.003947325050830841, kl_loss = 0.003422538749873638\n",
      "\n",
      "Epoch 1237\n",
      "Step 0: loss = 0.01038743369281292, recon_loss = 0.005483297631144524, 0.004897026810795069, kl_loss = 0.0035547465085983276\n",
      "\n",
      "Epoch 1238\n",
      "Step 0: loss = 0.008952457457780838, recon_loss = 0.00466817244887352, 0.004273711703717709, kl_loss = 0.005286421626806259\n",
      "\n",
      "Epoch 1239\n",
      "Step 0: loss = 0.010493608191609383, recon_loss = 0.005041550844907761, 0.005441869609057903, kl_loss = 0.005093701183795929\n",
      "\n",
      "Epoch 1240\n",
      "Step 0: loss = 0.00955950003117323, recon_loss = 0.004385765641927719, 0.005161091219633818, kl_loss = 0.006321133114397526\n",
      "\n",
      "Epoch 1241\n",
      "Step 0: loss = 0.009585954248905182, recon_loss = 0.004400774836540222, 0.005162639543414116, kl_loss = 0.011270038783550262\n",
      "\n",
      "Epoch 1242\n",
      "Step 0: loss = 0.008772734552621841, recon_loss = 0.0050233639776706696, 0.003739291103556752, kl_loss = 0.005039726383984089\n",
      "\n",
      "Epoch 1243\n",
      "Step 0: loss = 0.010356573387980461, recon_loss = 0.005286991596221924, 0.005051356740295887, kl_loss = 0.009112440049648285\n",
      "\n",
      "Epoch 1244\n",
      "Step 0: loss = 0.010674851946532726, recon_loss = 0.005351349711418152, 0.0053122518584132195, kl_loss = 0.005625286139547825\n",
      "\n",
      "Epoch 1245\n",
      "Step 0: loss = 0.010139398276805878, recon_loss = 0.0049730334430933, 0.005157368257641792, kl_loss = 0.004498268477618694\n",
      "\n",
      "Epoch 1246\n",
      "Step 0: loss = 0.011845400556921959, recon_loss = 0.005434064194560051, 0.006400749087333679, kl_loss = 0.0052934931591153145\n",
      "\n",
      "Epoch 1247\n",
      "Step 0: loss = 0.009655000641942024, recon_loss = 0.005026109516620636, 0.004615001380443573, kl_loss = 0.006944642402231693\n",
      "\n",
      "Epoch 1248\n",
      "Step 0: loss = 0.010961789637804031, recon_loss = 0.00467311218380928, 0.006280818022787571, kl_loss = 0.003929709084331989\n",
      "\n",
      "Epoch 1249\n",
      "Step 0: loss = 0.009063595905900002, recon_loss = 0.004446022212505341, 0.00460608396679163, kl_loss = 0.005744872614741325\n",
      "\n",
      "Epoch 1250\n",
      "Step 0: loss = 0.010096023790538311, recon_loss = 0.004865692928433418, 0.005214863922446966, kl_loss = 0.007733459584414959\n",
      "\n",
      "Epoch 1251\n",
      "Step 0: loss = 0.009591910056769848, recon_loss = 0.004556691274046898, 0.005026676692068577, kl_loss = 0.004271245561540127\n",
      "\n",
      "Epoch 1252\n",
      "Step 0: loss = 0.009616842493414879, recon_loss = 0.004812933504581451, 0.00479603186249733, kl_loss = 0.003938610665500164\n",
      "\n",
      "Epoch 1253\n",
      "Step 0: loss = 0.009359310381114483, recon_loss = 0.00500313937664032, 0.004342290572822094, kl_loss = 0.006940183229744434\n",
      "\n",
      "Epoch 1254\n",
      "Step 0: loss = 0.010562562383711338, recon_loss = 0.004933638498187065, 0.005620867945253849, kl_loss = 0.004027854651212692\n",
      "\n",
      "Epoch 1255\n",
      "Step 0: loss = 0.011287374421954155, recon_loss = 0.004941325634717941, 0.0063337646424770355, kl_loss = 0.006142288446426392\n",
      "\n",
      "Epoch 1256\n",
      "Step 0: loss = 0.011670112609863281, recon_loss = 0.005799442529678345, 0.005857972893863916, kl_loss = 0.006348717026412487\n",
      "\n",
      "Epoch 1257\n",
      "Step 0: loss = 0.011448662728071213, recon_loss = 0.005157859995961189, 0.0062790522351861, kl_loss = 0.0058752140030264854\n",
      "\n",
      "Epoch 1258\n",
      "Step 0: loss = 0.01167965866625309, recon_loss = 0.005273276939988136, 0.006399153731763363, kl_loss = 0.0036138929426670074\n",
      "\n",
      "Epoch 1259\n",
      "Step 0: loss = 0.010548088699579239, recon_loss = 0.004601767286658287, 0.005941275041550398, kl_loss = 0.0025229649618268013\n",
      "\n",
      "Epoch 1260\n",
      "Step 0: loss = 0.009744180366396904, recon_loss = 0.004415465518832207, 0.005323650315403938, kl_loss = 0.0025322986766695976\n",
      "\n",
      "Epoch 1261\n",
      "Step 0: loss = 0.011697152629494667, recon_loss = 0.004873890429735184, 0.006817563436925411, kl_loss = 0.002849152311682701\n",
      "\n",
      "Epoch 1262\n",
      "Step 0: loss = 0.01057117898017168, recon_loss = 0.005250962451100349, 0.005306822247803211, kl_loss = 0.006697141565382481\n",
      "\n",
      "Epoch 1263\n",
      "Step 0: loss = 0.011318359524011612, recon_loss = 0.00505056232213974, 0.006261661183089018, kl_loss = 0.0030676648020744324\n",
      "\n",
      "Epoch 1264\n",
      "Step 0: loss = 0.01006582472473383, recon_loss = 0.005582207813858986, 0.004478424787521362, kl_loss = 0.0025960439816117287\n",
      "\n",
      "Epoch 1265\n",
      "Step 0: loss = 0.010816617868840694, recon_loss = 0.005601856857538223, 0.005202116444706917, kl_loss = 0.006322409026324749\n",
      "\n",
      "Epoch 1266\n",
      "Step 0: loss = 0.013937091454863548, recon_loss = 0.005238145589828491, 0.008681795559823513, kl_loss = 0.00857512652873993\n",
      "\n",
      "Epoch 1267\n",
      "Step 0: loss = 0.011228500865399837, recon_loss = 0.005487827584147453, 0.005710946395993233, kl_loss = 0.0148632125928998\n",
      "\n",
      "Epoch 1268\n",
      "Step 0: loss = 0.012297462671995163, recon_loss = 0.00576670840382576, 0.006518520414829254, kl_loss = 0.006116815842688084\n",
      "\n",
      "Epoch 1269\n",
      "Step 0: loss = 0.009638107381761074, recon_loss = 0.004833653569221497, 0.004794456996023655, kl_loss = 0.004998502321541309\n",
      "\n",
      "Epoch 1270\n",
      "Step 0: loss = 0.011574500240385532, recon_loss = 0.004920860752463341, 0.006639048457145691, kl_loss = 0.007295615971088409\n",
      "\n",
      "Epoch 1271\n",
      "Step 0: loss = 0.01187972817569971, recon_loss = 0.00611821748316288, 0.005756482481956482, kl_loss = 0.0025139646604657173\n",
      "\n",
      "Epoch 1272\n",
      "Step 0: loss = 0.010946625843644142, recon_loss = 0.004660721868276596, 0.006278151646256447, kl_loss = 0.0038759326562285423\n",
      "\n",
      "Epoch 1273\n",
      "Step 0: loss = 0.010515028610825539, recon_loss = 0.0045646801590919495, 0.00594453327357769, kl_loss = 0.00290754996240139\n",
      "\n",
      "Epoch 1274\n",
      "Step 0: loss = 0.010458461008965969, recon_loss = 0.00507662259042263, 0.005370103754103184, kl_loss = 0.005867232568562031\n",
      "\n",
      "Epoch 1275\n",
      "Step 0: loss = 0.009816553443670273, recon_loss = 0.004844021052122116, 0.00496278703212738, kl_loss = 0.0048727914690971375\n",
      "\n",
      "Epoch 1276\n",
      "Step 0: loss = 0.01130711194127798, recon_loss = 0.005710510537028313, 0.005587488412857056, kl_loss = 0.004556663334369659\n",
      "\n",
      "Epoch 1277\n",
      "Step 0: loss = 0.011494516395032406, recon_loss = 0.005640588700771332, 0.005843518301844597, kl_loss = 0.005204884335398674\n",
      "\n",
      "Epoch 1278\n",
      "Step 0: loss = 0.012046688236296177, recon_loss = 0.0056466348469257355, 0.006390917114913464, kl_loss = 0.004568333737552166\n",
      "\n",
      "Epoch 1279\n",
      "Step 0: loss = 0.011345324106514454, recon_loss = 0.006362583488225937, 0.004974076524376869, kl_loss = 0.00433206744492054\n",
      "\n",
      "Epoch 1280\n",
      "Step 0: loss = 0.009981331415474415, recon_loss = 0.005418069660663605, 0.0045536658726632595, kl_loss = 0.004797586239874363\n",
      "\n",
      "Epoch 1281\n",
      "Step 0: loss = 0.010719831101596355, recon_loss = 0.0054913051426410675, 0.005217226222157478, kl_loss = 0.00565001554787159\n",
      "\n",
      "Epoch 1282\n",
      "Step 0: loss = 0.008439376018941402, recon_loss = 0.0044617000967264175, 0.0039718132466077805, kl_loss = 0.002931315451860428\n",
      "\n",
      "Epoch 1283\n",
      "Step 0: loss = 0.009958471171557903, recon_loss = 0.004737880080938339, 0.005212906748056412, kl_loss = 0.003842337988317013\n",
      "\n",
      "Epoch 1284\n",
      "Step 0: loss = 0.010173266753554344, recon_loss = 0.005147024989128113, 0.005018581636250019, kl_loss = 0.003830273635685444\n",
      "\n",
      "Epoch 1285\n",
      "Step 0: loss = 0.01122261956334114, recon_loss = 0.005335202440619469, 0.005872026551514864, kl_loss = 0.007695329375565052\n",
      "\n",
      "Epoch 1286\n",
      "Step 0: loss = 0.01111704669892788, recon_loss = 0.005138741806149483, 0.005954345688223839, kl_loss = 0.011979768052697182\n",
      "\n",
      "Epoch 1287\n",
      "Step 0: loss = 0.010942002758383751, recon_loss = 0.006132014095783234, 0.004788607824593782, kl_loss = 0.010690463706851006\n",
      "\n",
      "Epoch 1288\n",
      "Step 0: loss = 0.014772430062294006, recon_loss = 0.0067097824066877365, 0.008052373304963112, kl_loss = 0.005137227475643158\n",
      "\n",
      "Epoch 1289\n",
      "Step 0: loss = 0.014377395622432232, recon_loss = 0.006539257243275642, 0.007826633751392365, kl_loss = 0.005752150900661945\n",
      "\n",
      "Epoch 1290\n",
      "Step 0: loss = 0.012484987266361713, recon_loss = 0.006442602723836899, 0.006027406081557274, kl_loss = 0.007489269599318504\n",
      "\n",
      "Epoch 1291\n",
      "Step 0: loss = 0.010727658867835999, recon_loss = 0.0057502370327711105, 0.004964891821146011, kl_loss = 0.006265017203986645\n",
      "\n",
      "Epoch 1292\n",
      "Step 0: loss = 0.009420732967555523, recon_loss = 0.005594909191131592, 0.0038159003015607595, kl_loss = 0.004961550235748291\n",
      "\n",
      "Epoch 1293\n",
      "Step 0: loss = 0.012244450859725475, recon_loss = 0.006230812519788742, 0.00600441312417388, kl_loss = 0.004612255841493607\n",
      "\n",
      "Epoch 1294\n",
      "Step 0: loss = 0.012401520274579525, recon_loss = 0.005343720316886902, 0.007042816374450922, kl_loss = 0.007492227479815483\n",
      "\n",
      "Epoch 1295\n",
      "Step 0: loss = 0.008513102307915688, recon_loss = 0.004607737064361572, 0.003890358842909336, kl_loss = 0.007503080181777477\n",
      "\n",
      "Epoch 1296\n",
      "Step 0: loss = 0.011516787111759186, recon_loss = 0.0060736555606126785, 0.005435879807919264, kl_loss = 0.0036255987361073494\n",
      "\n",
      "Epoch 1297\n",
      "Step 0: loss = 0.011944619007408619, recon_loss = 0.0051160696893930435, 0.006817901972681284, kl_loss = 0.005323908291757107\n",
      "\n",
      "Epoch 1298\n",
      "Step 0: loss = 0.010706876404583454, recon_loss = 0.005339613184332848, 0.005359428934752941, kl_loss = 0.0039171380922198296\n",
      "\n",
      "Epoch 1299\n",
      "Step 0: loss = 0.011145662516355515, recon_loss = 0.005473960191011429, 0.0056648049503564835, kl_loss = 0.0034486018121242523\n",
      "\n",
      "Epoch 1300\n",
      "Step 0: loss = 0.009940076619386673, recon_loss = 0.004572020843625069, 0.00536119332537055, kl_loss = 0.0034311143681406975\n",
      "\n",
      "Epoch 1301\n",
      "Step 0: loss = 0.009884839877486229, recon_loss = 0.004684070125222206, 0.0051942989230155945, kl_loss = 0.0032356102019548416\n",
      "\n",
      "Epoch 1302\n",
      "Step 0: loss = 0.00941293966025114, recon_loss = 0.004904966801404953, 0.004501683637499809, kl_loss = 0.003144507296383381\n",
      "\n",
      "Epoch 1303\n",
      "Step 0: loss = 0.010267013683915138, recon_loss = 0.005087707191705704, 0.005173017270863056, kl_loss = 0.0031446265056729317\n",
      "\n",
      "Epoch 1304\n",
      "Step 0: loss = 0.010949190706014633, recon_loss = 0.004442065954208374, 0.006494362838566303, kl_loss = 0.006380999460816383\n",
      "\n",
      "Epoch 1305\n",
      "Step 0: loss = 0.009763246402144432, recon_loss = 0.005339229479432106, 0.004418305121362209, kl_loss = 0.002856021746993065\n",
      "\n",
      "Epoch 1306\n",
      "Step 0: loss = 0.010220780037343502, recon_loss = 0.005684349685907364, 0.004530005156993866, kl_loss = 0.0032126428559422493\n",
      "\n",
      "Epoch 1307\n",
      "Step 0: loss = 0.012287790887057781, recon_loss = 0.0055383797734975815, 0.006742458790540695, kl_loss = 0.0034763794392347336\n",
      "\n",
      "Epoch 1308\n",
      "Step 0: loss = 0.0111579904332757, recon_loss = 0.005324328318238258, 0.005829439032822847, kl_loss = 0.0021118521690368652\n",
      "\n",
      "Epoch 1309\n",
      "Step 0: loss = 0.012526139616966248, recon_loss = 0.005139114335179329, 0.0073813339695334435, kl_loss = 0.0028458861634135246\n",
      "\n",
      "Epoch 1310\n",
      "Step 0: loss = 0.009829659946262836, recon_loss = 0.004728460684418678, 0.005096284672617912, kl_loss = 0.0024572042748332024\n",
      "\n",
      "Epoch 1311\n",
      "Step 0: loss = 0.008938885293900967, recon_loss = 0.004490036517381668, 0.004446008708328009, kl_loss = 0.00141975749284029\n",
      "\n",
      "Epoch 1312\n",
      "Step 0: loss = 0.008982989005744457, recon_loss = 0.004146886989474297, 0.004831728525459766, kl_loss = 0.0021867994219064713\n",
      "\n",
      "Epoch 1313\n",
      "Step 0: loss = 0.010197928175330162, recon_loss = 0.004119100049138069, 0.006074399687349796, kl_loss = 0.00221419520676136\n",
      "\n",
      "Epoch 1314\n",
      "Step 0: loss = 0.010635146871209145, recon_loss = 0.00447508692741394, 0.006151970475912094, kl_loss = 0.004044542089104652\n",
      "\n",
      "Epoch 1315\n",
      "Step 0: loss = 0.010273166932165623, recon_loss = 0.0043375007808208466, 0.0059301648288965225, kl_loss = 0.002750847488641739\n",
      "\n",
      "Epoch 1316\n",
      "Step 0: loss = 0.009187793359160423, recon_loss = 0.003926312550902367, 0.005255703814327717, kl_loss = 0.002888638526201248\n",
      "\n",
      "Epoch 1317\n",
      "Step 0: loss = 0.011145572178065777, recon_loss = 0.004845768213272095, 0.006289067678153515, kl_loss = 0.00536811538040638\n",
      "\n",
      "Epoch 1318\n",
      "Step 0: loss = 0.010433224961161613, recon_loss = 0.004534605890512466, 0.005873947869986296, kl_loss = 0.012335550040006638\n",
      "\n",
      "Epoch 1319\n",
      "Step 0: loss = 0.011096925474703312, recon_loss = 0.005132151767611504, 0.0059414878487586975, kl_loss = 0.01164295244961977\n",
      "\n",
      "Epoch 1320\n",
      "Step 0: loss = 0.009334798902273178, recon_loss = 0.004230763763189316, 0.005089027341455221, kl_loss = 0.00750380102545023\n",
      "\n",
      "Epoch 1321\n",
      "Step 0: loss = 0.01111888699233532, recon_loss = 0.0053290706127882, 0.0057809017598629, kl_loss = 0.004457453265786171\n",
      "\n",
      "Epoch 1322\n",
      "Step 0: loss = 0.010474586859345436, recon_loss = 0.005484679713845253, 0.0049814870581030846, kl_loss = 0.004210229031741619\n",
      "\n",
      "Epoch 1323\n",
      "Step 0: loss = 0.009462841786444187, recon_loss = 0.004926742985844612, 0.004526527598500252, kl_loss = 0.004785534925758839\n",
      "\n",
      "Epoch 1324\n",
      "Step 0: loss = 0.008713853545486927, recon_loss = 0.0041337572038173676, 0.004574193619191647, kl_loss = 0.00295113492757082\n",
      "\n",
      "Epoch 1325\n",
      "Step 0: loss = 0.007710056379437447, recon_loss = 0.0036828722804784775, 0.004019059706479311, kl_loss = 0.004062272608280182\n",
      "\n",
      "Epoch 1326\n",
      "Step 0: loss = 0.00961298681795597, recon_loss = 0.0037964917719364166, 0.005810572765767574, kl_loss = 0.002961152233183384\n",
      "\n",
      "Epoch 1327\n",
      "Step 0: loss = 0.00956303346902132, recon_loss = 0.004088636487722397, 0.005467606242746115, kl_loss = 0.003395242616534233\n",
      "\n",
      "Epoch 1328\n",
      "Step 0: loss = 0.01114359125494957, recon_loss = 0.005032675340771675, 0.006100987084209919, kl_loss = 0.004964509047567844\n",
      "\n",
      "Epoch 1329\n",
      "Step 0: loss = 0.011312628164887428, recon_loss = 0.004814751446247101, 0.006490393541753292, kl_loss = 0.0037414971739053726\n",
      "\n",
      "Epoch 1330\n",
      "Step 0: loss = 0.010150566697120667, recon_loss = 0.004500027745962143, 0.005641574040055275, kl_loss = 0.0044826725497841835\n",
      "\n",
      "Epoch 1331\n",
      "Step 0: loss = 0.009124798700213432, recon_loss = 0.004035919904708862, 0.0050843930803239346, kl_loss = 0.0022425567731261253\n",
      "\n",
      "Epoch 1332\n",
      "Step 0: loss = 0.00953271146863699, recon_loss = 0.004530072212219238, 0.004983927123248577, kl_loss = 0.009356115013360977\n",
      "\n",
      "Epoch 1333\n",
      "Step 0: loss = 0.008535053580999374, recon_loss = 0.004156704992055893, 0.004361907951533794, kl_loss = 0.008220129646360874\n",
      "\n",
      "Epoch 1334\n",
      "Step 0: loss = 0.009490275755524635, recon_loss = 0.004469988867640495, 0.005012115929275751, kl_loss = 0.004085695371031761\n",
      "\n",
      "Epoch 1335\n",
      "Step 0: loss = 0.011432072147727013, recon_loss = 0.004719946533441544, 0.006705659907311201, kl_loss = 0.0032324660569429398\n",
      "\n",
      "Epoch 1336\n",
      "Step 0: loss = 0.010171654634177685, recon_loss = 0.004455685615539551, 0.005710265599191189, kl_loss = 0.002851857803761959\n",
      "\n",
      "Epoch 1337\n",
      "Step 0: loss = 0.011010576039552689, recon_loss = 0.004853826016187668, 0.006139624863862991, kl_loss = 0.008562624454498291\n",
      "\n",
      "Epoch 1338\n",
      "Step 0: loss = 0.011789570562541485, recon_loss = 0.0054137371480464935, 0.006360916420817375, kl_loss = 0.007458675652742386\n",
      "\n",
      "Epoch 1339\n",
      "Step 0: loss = 0.012373767793178558, recon_loss = 0.006146300584077835, 0.006218850612640381, kl_loss = 0.004308315925300121\n",
      "\n",
      "Epoch 1340\n",
      "Step 0: loss = 0.012134554795920849, recon_loss = 0.005428427830338478, 0.006698541343212128, kl_loss = 0.0037928568199276924\n",
      "\n",
      "Epoch 1341\n",
      "Step 0: loss = 0.010476278141140938, recon_loss = 0.0057195015251636505, 0.004746703431010246, kl_loss = 0.005036499351263046\n",
      "\n",
      "Epoch 1342\n",
      "Step 0: loss = 0.01239066757261753, recon_loss = 0.005129147320985794, 0.007248895242810249, kl_loss = 0.0063123758882284164\n",
      "\n",
      "Epoch 1343\n",
      "Step 0: loss = 0.01127659436315298, recon_loss = 0.0054702311754226685, 0.005798913538455963, kl_loss = 0.0037248022854328156\n",
      "\n",
      "Epoch 1344\n",
      "Step 0: loss = 0.010886628180742264, recon_loss = 0.005185399204492569, 0.005694161169230938, kl_loss = 0.003533732146024704\n",
      "\n",
      "Epoch 1345\n",
      "Step 0: loss = 0.011742091737687588, recon_loss = 0.005321936681866646, 0.00640874495729804, kl_loss = 0.005704798735678196\n",
      "\n",
      "Epoch 1346\n",
      "Step 0: loss = 0.010203318670392036, recon_loss = 0.00502559170126915, 0.005154518876224756, kl_loss = 0.011604385450482368\n",
      "\n",
      "Epoch 1347\n",
      "Step 0: loss = 0.010686450637876987, recon_loss = 0.005066219717264175, 0.005595389753580093, kl_loss = 0.01242044847458601\n",
      "\n",
      "Epoch 1348\n",
      "Step 0: loss = 0.00879121944308281, recon_loss = 0.005048701539635658, 0.0037304444704204798, kl_loss = 0.006036656908690929\n",
      "\n",
      "Epoch 1349\n",
      "Step 0: loss = 0.012095906771719456, recon_loss = 0.005692273378372192, 0.006392939947545528, kl_loss = 0.005346493795514107\n",
      "\n",
      "Epoch 1350\n",
      "Step 0: loss = 0.011602775193750858, recon_loss = 0.005574602633714676, 0.006018627434968948, kl_loss = 0.004772603511810303\n",
      "\n",
      "Epoch 1351\n",
      "Step 0: loss = 0.010151463560760021, recon_loss = 0.0055014584213495255, 0.004644876345992088, kl_loss = 0.0025643184781074524\n",
      "\n",
      "Epoch 1352\n",
      "Step 0: loss = 0.009663965553045273, recon_loss = 0.005163630470633507, 0.004477399867027998, kl_loss = 0.011467254720628262\n",
      "\n",
      "Epoch 1353\n",
      "Step 0: loss = 0.011849585920572281, recon_loss = 0.005434788763523102, 0.00638723187148571, kl_loss = 0.013782802037894726\n",
      "\n",
      "Epoch 1354\n",
      "Step 0: loss = 0.009398081339895725, recon_loss = 0.004312163218855858, 0.005066019482910633, kl_loss = 0.009949197061359882\n",
      "\n",
      "Epoch 1355\n",
      "Step 0: loss = 0.010220509953796864, recon_loss = 0.004478653892874718, 0.0057289088144898415, kl_loss = 0.006473751738667488\n",
      "\n",
      "Epoch 1356\n",
      "Step 0: loss = 0.009079084731638432, recon_loss = 0.004702979698777199, 0.00436060968786478, kl_loss = 0.007747594267129898\n",
      "\n",
      "Epoch 1357\n",
      "Step 0: loss = 0.009163498878479004, recon_loss = 0.004572022706270218, 0.004579856991767883, kl_loss = 0.005809665657579899\n",
      "\n",
      "Epoch 1358\n",
      "Step 0: loss = 0.010081974789500237, recon_loss = 0.004116743803024292, 0.005955708213150501, kl_loss = 0.004761174321174622\n",
      "\n",
      "Epoch 1359\n",
      "Step 0: loss = 0.007996856234967709, recon_loss = 0.0038414914160966873, 0.004145388025790453, kl_loss = 0.004988634958863258\n",
      "\n",
      "Epoch 1360\n",
      "Step 0: loss = 0.007632162421941757, recon_loss = 0.0036077145487070084, 0.004014008678495884, kl_loss = 0.005219561047852039\n",
      "\n",
      "Epoch 1361\n",
      "Step 0: loss = 0.00933243241161108, recon_loss = 0.004040984436869621, 0.0052808839827775955, kl_loss = 0.005281876772642136\n",
      "\n",
      "Epoch 1362\n",
      "Step 0: loss = 0.009197061881422997, recon_loss = 0.004632174968719482, 0.004548513330519199, kl_loss = 0.008186925202608109\n",
      "\n",
      "Epoch 1363\n",
      "Step 0: loss = 0.00995966512709856, recon_loss = 0.004668302834033966, 0.005285834893584251, kl_loss = 0.002763618715107441\n",
      "\n",
      "Epoch 1364\n",
      "Step 0: loss = 0.010821444913744926, recon_loss = 0.005044197663664818, 0.005771283060312271, kl_loss = 0.0029819216579198837\n",
      "\n",
      "Epoch 1365\n",
      "Step 0: loss = 0.010032467544078827, recon_loss = 0.004442518576979637, 0.005583545193076134, kl_loss = 0.0032019559293985367\n",
      "\n",
      "Epoch 1366\n",
      "Step 0: loss = 0.009899848140776157, recon_loss = 0.004859419539570808, 0.005029977299273014, kl_loss = 0.005225462839007378\n",
      "\n",
      "Epoch 1367\n",
      "Step 0: loss = 0.008479340933263302, recon_loss = 0.003997666761279106, 0.004473026376217604, kl_loss = 0.004323703236877918\n",
      "\n",
      "Epoch 1368\n",
      "Step 0: loss = 0.00996557530015707, recon_loss = 0.004537217319011688, 0.005420059897005558, kl_loss = 0.004149242304265499\n",
      "\n",
      "Epoch 1369\n",
      "Step 0: loss = 0.011386102996766567, recon_loss = 0.005020935088396072, 0.006356685422360897, kl_loss = 0.004241215065121651\n",
      "\n",
      "Epoch 1370\n",
      "Step 0: loss = 0.010216755792498589, recon_loss = 0.004747789353132248, 0.005456968210637569, kl_loss = 0.005998951382935047\n",
      "\n",
      "Epoch 1371\n",
      "Step 0: loss = 0.010064800269901752, recon_loss = 0.004577282816171646, 0.0054696849547326565, kl_loss = 0.008915978483855724\n",
      "\n",
      "Epoch 1372\n",
      "Step 0: loss = 0.01130979135632515, recon_loss = 0.0058419667184352875, 0.005460847169160843, kl_loss = 0.0034887203946709633\n",
      "\n",
      "Epoch 1373\n",
      "Step 0: loss = 0.01078011840581894, recon_loss = 0.004834819585084915, 0.005924807861447334, kl_loss = 0.010245674289762974\n",
      "\n",
      "Epoch 1374\n",
      "Step 0: loss = 0.009249377064406872, recon_loss = 0.0047015231102705, 0.0045336405746638775, kl_loss = 0.0071065304800868034\n",
      "\n",
      "Epoch 1375\n",
      "Step 0: loss = 0.010817415080964565, recon_loss = 0.004915749654173851, 0.0058675240725278854, kl_loss = 0.017070651054382324\n",
      "\n",
      "Epoch 1376\n",
      "Step 0: loss = 0.0111573226749897, recon_loss = 0.004641437903046608, 0.006490213796496391, kl_loss = 0.012835255824029446\n",
      "\n",
      "Epoch 1377\n",
      "Step 0: loss = 0.011989968828856945, recon_loss = 0.005406783893704414, 0.006562042981386185, kl_loss = 0.010571044869720936\n",
      "\n",
      "Epoch 1378\n",
      "Step 0: loss = 0.010059613734483719, recon_loss = 0.004327006638050079, 0.005725156515836716, kl_loss = 0.003725350834429264\n",
      "\n",
      "Epoch 1379\n",
      "Step 0: loss = 0.00944144930690527, recon_loss = 0.004211794584989548, 0.005221569910645485, kl_loss = 0.004042389802634716\n",
      "\n",
      "Epoch 1380\n",
      "Step 0: loss = 0.009381669573485851, recon_loss = 0.004265589639544487, 0.0050976891070604324, kl_loss = 0.009195302613079548\n",
      "\n",
      "Epoch 1381\n",
      "Step 0: loss = 0.010819750837981701, recon_loss = 0.004327001050114632, 0.006477766204625368, kl_loss = 0.007492108270525932\n",
      "\n",
      "Epoch 1382\n",
      "Step 0: loss = 0.009519737213850021, recon_loss = 0.004261260852217674, 0.005248186178505421, kl_loss = 0.005145092494785786\n",
      "\n",
      "Epoch 1383\n",
      "Step 0: loss = 0.008849088102579117, recon_loss = 0.004325529560446739, 0.004518340807408094, kl_loss = 0.0026084082201123238\n",
      "\n",
      "Epoch 1384\n",
      "Step 0: loss = 0.00946083664894104, recon_loss = 0.004749564453959465, 0.004706243518739939, kl_loss = 0.002514573745429516\n",
      "\n",
      "Epoch 1385\n",
      "Step 0: loss = 0.009136546403169632, recon_loss = 0.004373176023364067, 0.0047575500793755054, kl_loss = 0.002910434268414974\n",
      "\n",
      "Epoch 1386\n",
      "Step 0: loss = 0.009358208626508713, recon_loss = 0.0042538829147815704, 0.005092779640108347, kl_loss = 0.005773165263235569\n",
      "\n",
      "Epoch 1387\n",
      "Step 0: loss = 0.012663481757044792, recon_loss = 0.00609385222196579, 0.006556362845003605, kl_loss = 0.006633530370891094\n",
      "\n",
      "Epoch 1388\n",
      "Step 0: loss = 0.009581450372934341, recon_loss = 0.0034705549478530884, 0.006103334948420525, kl_loss = 0.003780142404139042\n",
      "\n",
      "Epoch 1389\n",
      "Step 0: loss = 0.012528064660727978, recon_loss = 0.004735227674245834, 0.0077719977125525475, kl_loss = 0.010419589467346668\n",
      "\n",
      "Epoch 1390\n",
      "Step 0: loss = 0.010050589218735695, recon_loss = 0.0049146320670843124, 0.0051234713755548, kl_loss = 0.006242602132260799\n",
      "\n",
      "Epoch 1391\n",
      "Step 0: loss = 0.009174748323857784, recon_loss = 0.004434382542967796, 0.004732608795166016, kl_loss = 0.0038782646879553795\n",
      "\n",
      "Epoch 1392\n",
      "Step 0: loss = 0.013531742617487907, recon_loss = 0.005461929365992546, 0.008056649938225746, kl_loss = 0.006581570953130722\n",
      "\n",
      "Epoch 1393\n",
      "Step 0: loss = 0.008922534063458443, recon_loss = 0.004451049491763115, 0.004443909041583538, kl_loss = 0.01378776878118515\n",
      "\n",
      "Epoch 1394\n",
      "Step 0: loss = 0.009329431690275669, recon_loss = 0.0049591101706027985, 0.004354128614068031, kl_loss = 0.008096336387097836\n",
      "\n",
      "Epoch 1395\n",
      "Step 0: loss = 0.012711838819086552, recon_loss = 0.005359640344977379, 0.007339625619351864, kl_loss = 0.006286647170782089\n",
      "\n",
      "Epoch 1396\n",
      "Step 0: loss = 0.015001601539552212, recon_loss = 0.0066406819969415665, 0.008352048695087433, kl_loss = 0.004435652866959572\n",
      "\n",
      "Epoch 1397\n",
      "Step 0: loss = 0.011565051041543484, recon_loss = 0.004985250532627106, 0.0065709855407476425, kl_loss = 0.004407424479722977\n",
      "\n",
      "Epoch 1398\n",
      "Step 0: loss = 0.01130653452128172, recon_loss = 0.005190540105104446, 0.006109839305281639, kl_loss = 0.0030776020139455795\n",
      "\n",
      "Epoch 1399\n",
      "Step 0: loss = 0.010794511996209621, recon_loss = 0.00529857724905014, 0.00548990722745657, kl_loss = 0.003013984300196171\n",
      "\n",
      "Epoch 1400\n",
      "Step 0: loss = 0.010508609004318714, recon_loss = 0.005474476143717766, 0.00502958707511425, kl_loss = 0.0022727735340595245\n",
      "\n",
      "Epoch 1401\n",
      "Step 0: loss = 0.011673464439809322, recon_loss = 0.005810542032122612, 0.005858937278389931, kl_loss = 0.0019927313551306725\n",
      "\n",
      "Epoch 1402\n",
      "Step 0: loss = 0.01075715757906437, recon_loss = 0.0053036753088235855, 0.0054495735093951225, kl_loss = 0.0019546104595065117\n",
      "\n",
      "Epoch 1403\n",
      "Step 0: loss = 0.011803588829934597, recon_loss = 0.0052714478224515915, 0.006525754928588867, kl_loss = 0.0031931381672620773\n",
      "\n",
      "Epoch 1404\n",
      "Step 0: loss = 0.01106310449540615, recon_loss = 0.005738504230976105, 0.005314898677170277, kl_loss = 0.004850856959819794\n",
      "\n",
      "Epoch 1405\n",
      "Step 0: loss = 0.009769700467586517, recon_loss = 0.005090236663818359, 0.004660747013986111, kl_loss = 0.009358376264572144\n",
      "\n",
      "Epoch 1406\n",
      "Step 0: loss = 0.011270026676356792, recon_loss = 0.004901440814137459, 0.0063585215248167515, kl_loss = 0.00503231305629015\n",
      "\n",
      "Epoch 1407\n",
      "Step 0: loss = 0.009260269813239574, recon_loss = 0.004925129935145378, 0.004328766837716103, kl_loss = 0.0031864233314990997\n",
      "\n",
      "Epoch 1408\n",
      "Step 0: loss = 0.008717602118849754, recon_loss = 0.004750421270728111, 0.003958789631724358, kl_loss = 0.004195651039481163\n",
      "\n",
      "Epoch 1409\n",
      "Step 0: loss = 0.008885150775313377, recon_loss = 0.0039025284349918365, 0.0049781715497374535, kl_loss = 0.002225266769528389\n",
      "\n",
      "Epoch 1410\n",
      "Step 0: loss = 0.00894511491060257, recon_loss = 0.003968838602304459, 0.004968574736267328, kl_loss = 0.003850911743938923\n",
      "\n",
      "Epoch 1411\n",
      "Step 0: loss = 0.008471525274217129, recon_loss = 0.004101268947124481, 0.004364624619483948, kl_loss = 0.0028159162029623985\n",
      "\n",
      "Epoch 1412\n",
      "Step 0: loss = 0.010260331444442272, recon_loss = 0.004306646063923836, 0.005949060432612896, kl_loss = 0.002312261611223221\n",
      "\n",
      "Epoch 1413\n",
      "Step 0: loss = 0.008927068673074245, recon_loss = 0.003825373947620392, 0.005095276981592178, kl_loss = 0.003208753652870655\n",
      "\n",
      "Epoch 1414\n",
      "Step 0: loss = 0.009081880562007427, recon_loss = 0.0049881767481565475, 0.0040903836488723755, kl_loss = 0.001659969799220562\n",
      "\n",
      "Epoch 1415\n",
      "Step 0: loss = 0.009864685125648975, recon_loss = 0.004926387220621109, 0.00493464432656765, kl_loss = 0.0018267538398504257\n",
      "\n",
      "Epoch 1416\n",
      "Step 0: loss = 0.010373365134000778, recon_loss = 0.005149269476532936, 0.005219068378210068, kl_loss = 0.002513832412660122\n",
      "\n",
      "Epoch 1417\n",
      "Step 0: loss = 0.009875433519482613, recon_loss = 0.0046630967408418655, 0.005207712762057781, kl_loss = 0.0023119673132896423\n",
      "\n",
      "Epoch 1418\n",
      "Step 0: loss = 0.010135924443602562, recon_loss = 0.00443512387573719, 0.005693062674254179, kl_loss = 0.003868590109050274\n",
      "\n",
      "Epoch 1419\n",
      "Step 0: loss = 0.009424579329788685, recon_loss = 0.004902876913547516, 0.004516673274338245, kl_loss = 0.0025146817788481712\n",
      "\n",
      "Epoch 1420\n",
      "Step 0: loss = 0.011025612242519855, recon_loss = 0.0046436116099357605, 0.006379060447216034, kl_loss = 0.0014699790626764297\n",
      "\n",
      "Epoch 1421\n",
      "Step 0: loss = 0.009904839098453522, recon_loss = 0.003937527537345886, 0.005960741080343723, kl_loss = 0.003285463899374008\n",
      "\n",
      "Epoch 1422\n",
      "Step 0: loss = 0.009603026323020458, recon_loss = 0.004664063453674316, 0.004931594245135784, kl_loss = 0.0036843232810497284\n",
      "\n",
      "Epoch 1423\n",
      "Step 0: loss = 0.010254371911287308, recon_loss = 0.004789963364601135, 0.005459030158817768, kl_loss = 0.0026893150061368942\n",
      "\n",
      "Epoch 1424\n",
      "Step 0: loss = 0.008514370769262314, recon_loss = 0.00401301309466362, 0.004494846798479557, kl_loss = 0.0032552359625697136\n",
      "\n",
      "Epoch 1425\n",
      "Step 0: loss = 0.009502789936959743, recon_loss = 0.00421878881752491, 0.005278419237583876, kl_loss = 0.002790650352835655\n",
      "\n",
      "Epoch 1426\n",
      "Step 0: loss = 0.009017020463943481, recon_loss = 0.0036683976650238037, 0.005343931261450052, kl_loss = 0.0023460937663912773\n",
      "\n",
      "Epoch 1427\n",
      "Step 0: loss = 0.009169197641313076, recon_loss = 0.0038348622620105743, 0.005327976308763027, kl_loss = 0.0031795240938663483\n",
      "\n",
      "Epoch 1428\n",
      "Step 0: loss = 0.009561767801642418, recon_loss = 0.004525091499090195, 0.005031942389905453, kl_loss = 0.0023671425879001617\n",
      "\n",
      "Epoch 1429\n",
      "Step 0: loss = 0.010078114457428455, recon_loss = 0.004514869302511215, 0.005557562690228224, kl_loss = 0.002841024659574032\n",
      "\n",
      "Epoch 1430\n",
      "Step 0: loss = 0.01066052820533514, recon_loss = 0.004842080175876617, 0.005813463591039181, kl_loss = 0.0024921707808971405\n",
      "\n",
      "Epoch 1431\n",
      "Step 0: loss = 0.009843435138463974, recon_loss = 0.004959695041179657, 0.0048698256723582745, kl_loss = 0.006956998258829117\n",
      "\n",
      "Epoch 1432\n",
      "Step 0: loss = 0.010106483474373817, recon_loss = 0.004797356203198433, 0.0052873073145747185, kl_loss = 0.010910202749073505\n",
      "\n",
      "Epoch 1433\n",
      "Step 0: loss = 0.011235302314162254, recon_loss = 0.005429815500974655, 0.005794248078018427, kl_loss = 0.00561943557113409\n",
      "\n",
      "Epoch 1434\n",
      "Step 0: loss = 0.011168568395078182, recon_loss = 0.0052138324826955795, 0.005945002660155296, kl_loss = 0.004866554401814938\n",
      "\n",
      "Epoch 1435\n",
      "Step 0: loss = 0.011082759127020836, recon_loss = 0.0053972285240888596, 0.005680623464286327, kl_loss = 0.002453443594276905\n",
      "\n",
      "Epoch 1436\n",
      "Step 0: loss = 0.014345593750476837, recon_loss = 0.005712222307920456, 0.008620113134384155, kl_loss = 0.006629126146435738\n",
      "\n",
      "Epoch 1437\n",
      "Step 0: loss = 0.013518090359866619, recon_loss = 0.0071384478360414505, 0.0063673825934529305, kl_loss = 0.006129895336925983\n",
      "\n",
      "Epoch 1438\n",
      "Step 0: loss = 0.008551391772925854, recon_loss = 0.005003867670893669, 0.003539274912327528, kl_loss = 0.004124200902879238\n",
      "\n",
      "Epoch 1439\n",
      "Step 0: loss = 0.010379823856055737, recon_loss = 0.0051584262400865555, 0.005206308793276548, kl_loss = 0.007544250227510929\n",
      "\n",
      "Epoch 1440\n",
      "Step 0: loss = 0.0118679478764534, recon_loss = 0.00554274208843708, 0.006293433718383312, kl_loss = 0.015885978937149048\n",
      "\n",
      "Epoch 1441\n",
      "Step 0: loss = 0.010293949395418167, recon_loss = 0.00461023673415184, 0.005658809095621109, kl_loss = 0.012451589107513428\n",
      "\n",
      "Epoch 1442\n",
      "Step 0: loss = 0.013516180217266083, recon_loss = 0.005648501217365265, 0.007853813469409943, kl_loss = 0.006932834163308144\n",
      "\n",
      "Epoch 1443\n",
      "Step 0: loss = 0.012025534175336361, recon_loss = 0.00512436218559742, 0.006892154458910227, kl_loss = 0.004508989863097668\n",
      "\n",
      "Epoch 1444\n",
      "Step 0: loss = 0.01091995183378458, recon_loss = 0.004919193685054779, 0.00599004328250885, kl_loss = 0.005357228219509125\n",
      "\n",
      "Epoch 1445\n",
      "Step 0: loss = 0.009377424605190754, recon_loss = 0.004786917939782143, 0.0045826006680727005, kl_loss = 0.003952795639634132\n",
      "\n",
      "Epoch 1446\n",
      "Step 0: loss = 0.009287257678806782, recon_loss = 0.004410300403833389, 0.004865535534918308, kl_loss = 0.005710730329155922\n",
      "\n",
      "Epoch 1447\n",
      "Step 0: loss = 0.011028895154595375, recon_loss = 0.005856068804860115, 0.005166666582226753, kl_loss = 0.0030800867825746536\n",
      "\n",
      "Epoch 1448\n",
      "Step 0: loss = 0.010929307900369167, recon_loss = 0.004679419100284576, 0.006244189105927944, kl_loss = 0.0028499383479356766\n",
      "\n",
      "Epoch 1449\n",
      "Step 0: loss = 0.010585417971014977, recon_loss = 0.00475526787340641, 0.005824686959385872, kl_loss = 0.0027313968166708946\n",
      "\n",
      "Epoch 1450\n",
      "Step 0: loss = 0.011153874918818474, recon_loss = 0.004853039979934692, 0.006296428851783276, kl_loss = 0.0022031422704458237\n",
      "\n",
      "Epoch 1451\n",
      "Step 0: loss = 0.009727797470986843, recon_loss = 0.004265252500772476, 0.0054535288363695145, kl_loss = 0.0045078713446855545\n",
      "\n",
      "Epoch 1452\n",
      "Step 0: loss = 0.011422534473240376, recon_loss = 0.0045564863830804825, 0.006857205182313919, kl_loss = 0.004421589896082878\n",
      "\n",
      "Epoch 1453\n",
      "Step 0: loss = 0.010755804367363453, recon_loss = 0.004536865279078484, 0.006207023281604052, kl_loss = 0.005958224646747112\n",
      "\n",
      "Epoch 1454\n",
      "Step 0: loss = 0.010800675489008427, recon_loss = 0.004824923351407051, 0.005965567193925381, kl_loss = 0.005092332139611244\n",
      "\n",
      "Epoch 1455\n",
      "Step 0: loss = 0.009165111929178238, recon_loss = 0.004643987864255905, 0.0045102499425411224, kl_loss = 0.005436955951154232\n",
      "\n",
      "Epoch 1456\n",
      "Step 0: loss = 0.011241616681218147, recon_loss = 0.004826480522751808, 0.006392632611095905, kl_loss = 0.011251573450863361\n",
      "\n",
      "Epoch 1457\n",
      "Step 0: loss = 0.010275625623762608, recon_loss = 0.004533443599939346, 0.0057146623730659485, kl_loss = 0.013759739696979523\n",
      "\n",
      "Epoch 1458\n",
      "Step 0: loss = 0.010432609356939793, recon_loss = 0.004089644178748131, 0.006327488459646702, kl_loss = 0.007738511078059673\n",
      "\n",
      "Epoch 1459\n",
      "Step 0: loss = 0.009163714945316315, recon_loss = 0.0036182329058647156, 0.005538748577237129, kl_loss = 0.0033668940886855125\n",
      "\n",
      "Epoch 1460\n",
      "Step 0: loss = 0.009075136855244637, recon_loss = 0.004093196243047714, 0.004978133365511894, kl_loss = 0.0019038151949644089\n",
      "\n",
      "Epoch 1461\n",
      "Step 0: loss = 0.009494118392467499, recon_loss = 0.004395339637994766, 0.005093244835734367, kl_loss = 0.0027669966220855713\n",
      "\n",
      "Epoch 1462\n",
      "Step 0: loss = 0.008560268208384514, recon_loss = 0.003887634724378586, 0.004668249748647213, kl_loss = 0.002191665582358837\n",
      "\n",
      "Epoch 1463\n",
      "Step 0: loss = 0.009270411916077137, recon_loss = 0.004121536388993263, 0.005145789124071598, kl_loss = 0.0015429966151714325\n",
      "\n",
      "Epoch 1464\n",
      "Step 0: loss = 0.010636566206812859, recon_loss = 0.004566287621855736, 0.006065061315894127, kl_loss = 0.002608555369079113\n",
      "\n",
      "Epoch 1465\n",
      "Step 0: loss = 0.011748571880161762, recon_loss = 0.005103806033730507, 0.006639968603849411, kl_loss = 0.0023988131433725357\n",
      "\n",
      "Epoch 1466\n",
      "Step 0: loss = 0.009559170342981815, recon_loss = 0.003904925659298897, 0.005649859551340342, kl_loss = 0.0021929126232862473\n",
      "\n",
      "Epoch 1467\n",
      "Step 0: loss = 0.008811774663627148, recon_loss = 0.0050588808953762054, 0.003748958697542548, kl_loss = 0.001967516727745533\n",
      "\n",
      "Epoch 1468\n",
      "Step 0: loss = 0.01031844038516283, recon_loss = 0.00496145524084568, 0.005340904928743839, kl_loss = 0.008039900101721287\n",
      "\n",
      "Epoch 1469\n",
      "Step 0: loss = 0.010434716939926147, recon_loss = 0.004437612369656563, 0.005992596037685871, kl_loss = 0.0022540567442774773\n",
      "\n",
      "Epoch 1470\n",
      "Step 0: loss = 0.009970409795641899, recon_loss = 0.004148567095398903, 0.005815221928060055, kl_loss = 0.0033104605972766876\n",
      "\n",
      "Epoch 1471\n",
      "Step 0: loss = 0.009625141508877277, recon_loss = 0.004690900444984436, 0.004927892237901688, kl_loss = 0.003174225799739361\n",
      "\n",
      "Epoch 1472\n",
      "Step 0: loss = 0.010831777937710285, recon_loss = 0.00479518249630928, 0.006026986055076122, kl_loss = 0.004804619587957859\n",
      "\n",
      "Epoch 1473\n",
      "Step 0: loss = 0.01091621071100235, recon_loss = 0.004621291533112526, 0.006288144271820784, kl_loss = 0.003387361764907837\n",
      "\n",
      "Epoch 1474\n",
      "Step 0: loss = 0.010813852772116661, recon_loss = 0.004498401656746864, 0.0063116662204265594, kl_loss = 0.001892278902232647\n",
      "\n",
      "Epoch 1475\n",
      "Step 0: loss = 0.009851791895925999, recon_loss = 0.004358930513262749, 0.0054862708784639835, kl_loss = 0.003295457921922207\n",
      "\n",
      "Epoch 1476\n",
      "Step 0: loss = 0.009868286550045013, recon_loss = 0.00420895591378212, 0.005650814156979322, kl_loss = 0.004257862456142902\n",
      "\n",
      "Epoch 1477\n",
      "Step 0: loss = 0.009991754777729511, recon_loss = 0.004752412438392639, 0.0052350712940096855, kl_loss = 0.00213551614433527\n",
      "\n",
      "Epoch 1478\n",
      "Step 0: loss = 0.00904148630797863, recon_loss = 0.0041903965175151825, 0.004845042712986469, kl_loss = 0.003023502416908741\n",
      "\n",
      "Epoch 1479\n",
      "Step 0: loss = 0.009316222742199898, recon_loss = 0.004203850403428078, 0.0051087685860693455, kl_loss = 0.0018018977716565132\n",
      "\n",
      "Epoch 1480\n",
      "Step 0: loss = 0.00965911615639925, recon_loss = 0.00465012714266777, 0.005005634389817715, kl_loss = 0.0016774898394942284\n",
      "\n",
      "Epoch 1481\n",
      "Step 0: loss = 0.011148355901241302, recon_loss = 0.006113408133387566, 0.005028354935348034, kl_loss = 0.0032962197437882423\n",
      "\n",
      "Epoch 1482\n",
      "Step 0: loss = 0.01025888230651617, recon_loss = 0.004664108157157898, 0.005587512627243996, kl_loss = 0.003630773164331913\n",
      "\n",
      "Epoch 1483\n",
      "Step 0: loss = 0.00866754911839962, recon_loss = 0.004347991198301315, 0.0043105557560920715, kl_loss = 0.004500959068536758\n",
      "\n",
      "Epoch 1484\n",
      "Step 0: loss = 0.010659669525921345, recon_loss = 0.005102615803480148, 0.005544648040086031, kl_loss = 0.006203092634677887\n",
      "\n",
      "Epoch 1485\n",
      "Step 0: loss = 0.010483887977898121, recon_loss = 0.005173029378056526, 0.005297748371958733, kl_loss = 0.006555337458848953\n",
      "\n",
      "Epoch 1486\n",
      "Step 0: loss = 0.012009539641439915, recon_loss = 0.006159551441669464, 0.0058402204886078835, kl_loss = 0.004884075373411179\n",
      "\n",
      "Epoch 1487\n",
      "Step 0: loss = 0.010882449336349964, recon_loss = 0.005058620125055313, 0.005818673875182867, kl_loss = 0.0025774771347641945\n",
      "\n",
      "Epoch 1488\n",
      "Step 0: loss = 0.00912009458988905, recon_loss = 0.004870599135756493, 0.004245171323418617, kl_loss = 0.002162081189453602\n",
      "\n",
      "Epoch 1489\n",
      "Step 0: loss = 0.008857476525008678, recon_loss = 0.004754910245537758, 0.004093209747225046, kl_loss = 0.004678345285356045\n",
      "\n",
      "Epoch 1490\n",
      "Step 0: loss = 0.010092319920659065, recon_loss = 0.005046248435974121, 0.005038716830313206, kl_loss = 0.003677363507449627\n",
      "\n",
      "Epoch 1491\n",
      "Step 0: loss = 0.01009781751781702, recon_loss = 0.004520231857895851, 0.0055681150406599045, kl_loss = 0.00473520252853632\n",
      "\n",
      "Epoch 1492\n",
      "Step 0: loss = 0.011724192649126053, recon_loss = 0.004988197237253189, 0.006723479367792606, kl_loss = 0.006257856264710426\n",
      "\n",
      "Epoch 1493\n",
      "Step 0: loss = 0.010764777660369873, recon_loss = 0.0050667691975831985, 0.005675090476870537, kl_loss = 0.011458957567811012\n",
      "\n",
      "Epoch 1494\n",
      "Step 0: loss = 0.010036075487732887, recon_loss = 0.004911726340651512, 0.00511248130351305, kl_loss = 0.0059336889535188675\n",
      "\n",
      "Epoch 1495\n",
      "Step 0: loss = 0.012305455282330513, recon_loss = 0.005393173545598984, 0.006899887230247259, kl_loss = 0.006197189912199974\n",
      "\n",
      "Epoch 1496\n",
      "Step 0: loss = 0.009946550242602825, recon_loss = 0.00499340146780014, 0.004940683487802744, kl_loss = 0.006233080290257931\n",
      "\n",
      "Epoch 1497\n",
      "Step 0: loss = 0.010711015202105045, recon_loss = 0.005265260115265846, 0.005435426719486713, kl_loss = 0.005164233036339283\n",
      "\n",
      "Epoch 1498\n",
      "Step 0: loss = 0.010577552951872349, recon_loss = 0.0046789441257715225, 0.005891941487789154, kl_loss = 0.0033336421474814415\n",
      "\n",
      "Epoch 1499\n",
      "Step 0: loss = 0.008539583534002304, recon_loss = 0.004504090175032616, 0.0040295980870723724, kl_loss = 0.0029476387426257133\n",
      "\n",
      "Epoch 1500\n",
      "Step 0: loss = 0.009990009479224682, recon_loss = 0.004753001034259796, 0.0052300794050097466, kl_loss = 0.003464748151600361\n",
      "\n",
      "Epoch 1501\n",
      "Step 0: loss = 0.010632583871483803, recon_loss = 0.004834244027733803, 0.005789983551949263, kl_loss = 0.004177931696176529\n",
      "\n",
      "Epoch 1502\n",
      "Step 0: loss = 0.00802632886916399, recon_loss = 0.003801818937063217, 0.004220888484269381, kl_loss = 0.001810816116631031\n",
      "\n",
      "Epoch 1503\n",
      "Step 0: loss = 0.009769071824848652, recon_loss = 0.0046285055577754974, 0.0051289889961481094, kl_loss = 0.0057885171845555305\n",
      "\n",
      "Epoch 1504\n",
      "Step 0: loss = 0.009748892858624458, recon_loss = 0.00455506332218647, 0.005185391753911972, kl_loss = 0.004219026304781437\n",
      "\n",
      "Epoch 1505\n",
      "Step 0: loss = 0.010485456325113773, recon_loss = 0.0042559485882520676, 0.006219309754669666, kl_loss = 0.005098958499729633\n",
      "\n",
      "Epoch 1506\n",
      "Step 0: loss = 0.01046864502131939, recon_loss = 0.004475371912121773, 0.0059836674481630325, kl_loss = 0.0048029422760009766\n",
      "\n",
      "Epoch 1507\n",
      "Step 0: loss = 0.009624556638300419, recon_loss = 0.003943685442209244, 0.005673082545399666, kl_loss = 0.0038945432752370834\n",
      "\n",
      "Epoch 1508\n",
      "Step 0: loss = 0.009104260243475437, recon_loss = 0.003687674179673195, 0.005411106161773205, kl_loss = 0.0027398336678743362\n",
      "\n",
      "Epoch 1509\n",
      "Step 0: loss = 0.007757030893117189, recon_loss = 0.0037192292511463165, 0.004032121505588293, kl_loss = 0.0028401734307408333\n",
      "\n",
      "Epoch 1510\n",
      "Step 0: loss = 0.00927161518484354, recon_loss = 0.004148513078689575, 0.005113034974783659, kl_loss = 0.005033397115767002\n",
      "\n",
      "Epoch 1511\n",
      "Step 0: loss = 0.00871958490461111, recon_loss = 0.0035495907068252563, 0.005160829983651638, kl_loss = 0.004581933841109276\n",
      "\n",
      "Epoch 1512\n",
      "Step 0: loss = 0.010248801670968533, recon_loss = 0.004523612558841705, 0.005715532228350639, kl_loss = 0.004828520119190216\n",
      "\n",
      "Epoch 1513\n",
      "Step 0: loss = 0.011862457729876041, recon_loss = 0.004876807332038879, 0.00697830505669117, kl_loss = 0.003672441467642784\n",
      "\n",
      "Epoch 1514\n",
      "Step 0: loss = 0.010493889451026917, recon_loss = 0.004283469170331955, 0.0062019964680075645, kl_loss = 0.0042120348662137985\n",
      "\n",
      "Epoch 1515\n",
      "Step 0: loss = 0.010450245812535286, recon_loss = 0.004461966454982758, 0.00598186207935214, kl_loss = 0.0032083066180348396\n",
      "\n",
      "Epoch 1516\n",
      "Step 0: loss = 0.01099899597465992, recon_loss = 0.004277991130948067, 0.006715917959809303, kl_loss = 0.002543623559176922\n",
      "\n",
      "Epoch 1517\n",
      "Step 0: loss = 0.00876278430223465, recon_loss = 0.004775401204824448, 0.003983499947935343, kl_loss = 0.0019419826567173004\n",
      "\n",
      "Epoch 1518\n",
      "Step 0: loss = 0.011157757602632046, recon_loss = 0.005513740703463554, 0.005634787026792765, kl_loss = 0.004615047946572304\n",
      "\n",
      "Epoch 1519\n",
      "Step 0: loss = 0.011059124954044819, recon_loss = 0.005577044561505318, 0.005469792522490025, kl_loss = 0.006143773905932903\n",
      "\n",
      "Epoch 1520\n",
      "Step 0: loss = 0.011349249631166458, recon_loss = 0.004966968670487404, 0.006375066004693508, kl_loss = 0.0036075152456760406\n",
      "\n",
      "Epoch 1521\n",
      "Step 0: loss = 0.010279535315930843, recon_loss = 0.005003476515412331, 0.00526867900043726, kl_loss = 0.003690103068947792\n",
      "\n",
      "Epoch 1522\n",
      "Step 0: loss = 0.01089196652173996, recon_loss = 0.0056702811270952225, 0.005214700475335121, kl_loss = 0.0034925080835819244\n",
      "\n",
      "Epoch 1523\n",
      "Step 0: loss = 0.01216449961066246, recon_loss = 0.005233544856309891, 0.006921847350895405, kl_loss = 0.004553724080324173\n",
      "\n",
      "Epoch 1524\n",
      "Step 0: loss = 0.012470022775232792, recon_loss = 0.005893563851714134, 0.0065672919154167175, kl_loss = 0.004583393223583698\n",
      "\n",
      "Epoch 1525\n",
      "Step 0: loss = 0.013806633651256561, recon_loss = 0.007014723494648933, 0.006785139441490173, kl_loss = 0.003385518677532673\n",
      "\n",
      "Epoch 1526\n",
      "Step 0: loss = 0.011331078596413136, recon_loss = 0.005446858704090118, 0.005872202571481466, kl_loss = 0.006008647382259369\n",
      "\n",
      "Epoch 1527\n",
      "Step 0: loss = 0.010064617730677128, recon_loss = 0.004931638017296791, 0.005124960094690323, kl_loss = 0.004009678959846497\n",
      "\n",
      "Epoch 1528\n",
      "Step 0: loss = 0.00976555235683918, recon_loss = 0.004791032522916794, 0.0049676429480314255, kl_loss = 0.003438577987253666\n",
      "\n",
      "Epoch 1529\n",
      "Step 0: loss = 0.01037356723099947, recon_loss = 0.0045722536742687225, 0.0057974690571427345, kl_loss = 0.001922445371747017\n",
      "\n",
      "Epoch 1530\n",
      "Step 0: loss = 0.00887216068804264, recon_loss = 0.004542360082268715, 0.004326269030570984, kl_loss = 0.0017657028511166573\n",
      "\n",
      "Epoch 1531\n",
      "Step 0: loss = 0.012970478273928165, recon_loss = 0.005173755809664726, 0.007792641408741474, kl_loss = 0.0020404011011123657\n",
      "\n",
      "Epoch 1532\n",
      "Step 0: loss = 0.01047444622963667, recon_loss = 0.004982158541679382, 0.0054857730865478516, kl_loss = 0.0032571814954280853\n",
      "\n",
      "Epoch 1533\n",
      "Step 0: loss = 0.009194900281727314, recon_loss = 0.0044673290103673935, 0.004723250400274992, kl_loss = 0.0021602921187877655\n",
      "\n",
      "Epoch 1534\n",
      "Step 0: loss = 0.008569848723709583, recon_loss = 0.004337990656495094, 0.004226770251989365, kl_loss = 0.0025441190227866173\n",
      "\n",
      "Epoch 1535\n",
      "Step 0: loss = 0.009430039674043655, recon_loss = 0.004129806533455849, 0.0052968645468354225, kl_loss = 0.0016841962933540344\n",
      "\n",
      "Epoch 1536\n",
      "Step 0: loss = 0.009509334340691566, recon_loss = 0.004352131858468056, 0.005149414762854576, kl_loss = 0.0038940710946917534\n",
      "\n",
      "Epoch 1537\n",
      "Step 0: loss = 0.01070450060069561, recon_loss = 0.004493113607168198, 0.006205272860825062, kl_loss = 0.003057176247239113\n",
      "\n",
      "Epoch 1538\n",
      "Step 0: loss = 0.009384643286466599, recon_loss = 0.0042046550661325455, 0.005168566480278969, kl_loss = 0.005710993893444538\n",
      "\n",
      "Epoch 1539\n",
      "Step 0: loss = 0.01040596142411232, recon_loss = 0.00491437129676342, 0.005481538362801075, kl_loss = 0.005025880411267281\n",
      "\n",
      "Epoch 1540\n",
      "Step 0: loss = 0.012798713520169258, recon_loss = 0.005359821021556854, 0.007429423741996288, kl_loss = 0.004734290763735771\n",
      "\n",
      "Epoch 1541\n",
      "Step 0: loss = 0.01023048348724842, recon_loss = 0.0046829283237457275, 0.005540864542126656, kl_loss = 0.003345368430018425\n",
      "\n",
      "Epoch 1542\n",
      "Step 0: loss = 0.009065135382115841, recon_loss = 0.004267523065209389, 0.004793841857463121, kl_loss = 0.0018855789676308632\n",
      "\n",
      "Epoch 1543\n",
      "Step 0: loss = 0.007444299757480621, recon_loss = 0.0035472307354211807, 0.003892898326739669, kl_loss = 0.0020852284505963326\n",
      "\n",
      "Epoch 1544\n",
      "Step 0: loss = 0.007886623032391071, recon_loss = 0.004323827102780342, 0.0035577607341110706, kl_loss = 0.0025177020579576492\n",
      "\n",
      "Epoch 1545\n",
      "Step 0: loss = 0.00891256146132946, recon_loss = 0.004568764939904213, 0.00432986905798316, kl_loss = 0.006963363848626614\n",
      "\n",
      "Epoch 1546\n",
      "Step 0: loss = 0.00926203466951847, recon_loss = 0.004318155348300934, 0.004936165176331997, kl_loss = 0.0038569075986742973\n",
      "\n",
      "Epoch 1547\n",
      "Step 0: loss = 0.007851459085941315, recon_loss = 0.0035899076610803604, 0.004255012609064579, kl_loss = 0.0032693054527044296\n",
      "\n",
      "Epoch 1548\n",
      "Step 0: loss = 0.008962033316493034, recon_loss = 0.004053084179759026, 0.004904809407889843, kl_loss = 0.0020699873566627502\n",
      "\n",
      "Epoch 1549\n",
      "Step 0: loss = 0.008943527936935425, recon_loss = 0.0036787744611501694, 0.005256771109998226, kl_loss = 0.003991285338997841\n",
      "\n",
      "Epoch 1550\n",
      "Step 0: loss = 0.011378610506653786, recon_loss = 0.0043940190225839615, 0.0069786543026566505, kl_loss = 0.0029684975743293762\n",
      "\n",
      "Epoch 1551\n",
      "Step 0: loss = 0.010187004692852497, recon_loss = 0.004373902454972267, 0.005807807669043541, kl_loss = 0.002647159621119499\n",
      "\n",
      "Epoch 1552\n",
      "Step 0: loss = 0.01059370394796133, recon_loss = 0.004697829484939575, 0.005890242755413055, kl_loss = 0.0028157737106084824\n",
      "\n",
      "Epoch 1553\n",
      "Step 0: loss = 0.011245420202612877, recon_loss = 0.0049177054315805435, 0.006316957529634237, kl_loss = 0.005378388799726963\n",
      "\n",
      "Epoch 1554\n",
      "Step 0: loss = 0.012811016291379929, recon_loss = 0.005429306998848915, 0.007375563960522413, kl_loss = 0.0030724629759788513\n",
      "\n",
      "Epoch 1555\n",
      "Step 0: loss = 0.010610596276819706, recon_loss = 0.005030473694205284, 0.005567595362663269, kl_loss = 0.006263650022447109\n",
      "\n",
      "Epoch 1556\n",
      "Step 0: loss = 0.011316226795315742, recon_loss = 0.005357569083571434, 0.005948449019342661, kl_loss = 0.005104376003146172\n",
      "\n",
      "Epoch 1557\n",
      "Step 0: loss = 0.009006194770336151, recon_loss = 0.004288759082555771, 0.004710845649242401, kl_loss = 0.003295179456472397\n",
      "\n",
      "Epoch 1558\n",
      "Step 0: loss = 0.009234471246600151, recon_loss = 0.004964035004377365, 0.004265943542122841, kl_loss = 0.0022464701905846596\n",
      "\n",
      "Epoch 1559\n",
      "Step 0: loss = 0.007393051870167255, recon_loss = 0.0038344431668519974, 0.003544888226315379, kl_loss = 0.006860105320811272\n",
      "\n",
      "Epoch 1560\n",
      "Step 0: loss = 0.009275413118302822, recon_loss = 0.004257384687662125, 0.004999313969165087, kl_loss = 0.009357279166579247\n",
      "\n",
      "Epoch 1561\n",
      "Step 0: loss = 0.010384057648479939, recon_loss = 0.004714019596576691, 0.0056557422503829, kl_loss = 0.007147824391722679\n",
      "\n",
      "Epoch 1562\n",
      "Step 0: loss = 0.00936218723654747, recon_loss = 0.004719164222478867, 0.0046308692544698715, kl_loss = 0.0060769859701395035\n",
      "\n",
      "Epoch 1563\n",
      "Step 0: loss = 0.009441523812711239, recon_loss = 0.004045985639095306, 0.005391119047999382, kl_loss = 0.002209351398050785\n",
      "\n",
      "Epoch 1564\n",
      "Step 0: loss = 0.010121840983629227, recon_loss = 0.004005420953035355, 0.006109945476055145, kl_loss = 0.0032373182475566864\n",
      "\n",
      "Epoch 1565\n",
      "Step 0: loss = 0.009060894139111042, recon_loss = 0.004024595022201538, 0.005031290929764509, kl_loss = 0.00250395480543375\n",
      "\n",
      "Epoch 1566\n",
      "Step 0: loss = 0.010971223935484886, recon_loss = 0.003856850787997246, 0.007110372185707092, kl_loss = 0.0020004455000162125\n",
      "\n",
      "Epoch 1567\n",
      "Step 0: loss = 0.008353115990757942, recon_loss = 0.0039006248116493225, 0.004449153319001198, kl_loss = 0.0016691414639353752\n",
      "\n",
      "Epoch 1568\n",
      "Step 0: loss = 0.008671225979924202, recon_loss = 0.0038366373628377914, 0.004832079168409109, kl_loss = 0.0012544402852654457\n",
      "\n",
      "Epoch 1569\n",
      "Step 0: loss = 0.009115244261920452, recon_loss = 0.00394878163933754, 0.005158658139407635, kl_loss = 0.0039021512493491173\n",
      "\n",
      "Epoch 1570\n",
      "Step 0: loss = 0.009203051216900349, recon_loss = 0.0037863850593566895, 0.005410401150584221, kl_loss = 0.0031323647126555443\n",
      "\n",
      "Epoch 1571\n",
      "Step 0: loss = 0.009338952600955963, recon_loss = 0.0040800441056489944, 0.005253929179161787, kl_loss = 0.0024895677343010902\n",
      "\n",
      "Epoch 1572\n",
      "Step 0: loss = 0.007456914987415075, recon_loss = 0.00325818732380867, 0.004192415624856949, kl_loss = 0.0031559476628899574\n",
      "\n",
      "Epoch 1573\n",
      "Step 0: loss = 0.009360374882817268, recon_loss = 0.004227079451084137, 0.005127715412527323, kl_loss = 0.0027901260182261467\n",
      "\n",
      "Epoch 1574\n",
      "Step 0: loss = 0.00961653608828783, recon_loss = 0.004522236064076424, 0.005086926743388176, kl_loss = 0.0036867670714855194\n",
      "\n",
      "Epoch 1575\n",
      "Step 0: loss = 0.009014863520860672, recon_loss = 0.004026947543025017, 0.004976010415703058, kl_loss = 0.005953238345682621\n",
      "\n",
      "Epoch 1576\n",
      "Step 0: loss = 0.010611450299620628, recon_loss = 0.004312114790081978, 0.006292767822742462, kl_loss = 0.0032838890329003334\n",
      "\n",
      "Epoch 1577\n",
      "Step 0: loss = 0.009387221187353134, recon_loss = 0.004557520151138306, 0.004824530798941851, kl_loss = 0.002585296519100666\n",
      "\n",
      "Epoch 1578\n",
      "Step 0: loss = 0.011413803324103355, recon_loss = 0.0047052595764398575, 0.006701702252030373, kl_loss = 0.0034206323325634003\n",
      "\n",
      "Epoch 1579\n",
      "Step 0: loss = 0.011338882148265839, recon_loss = 0.0049796514213085175, 0.006352079566568136, kl_loss = 0.003575132228434086\n",
      "\n",
      "Epoch 1580\n",
      "Step 0: loss = 0.011083774268627167, recon_loss = 0.005070038139820099, 0.0060073817148804665, kl_loss = 0.003177313134074211\n",
      "\n",
      "Epoch 1581\n",
      "Step 0: loss = 0.0090487627312541, recon_loss = 0.004491154104471207, 0.004553862381726503, kl_loss = 0.0018735574558377266\n",
      "\n",
      "Epoch 1582\n",
      "Step 0: loss = 0.01124033983796835, recon_loss = 0.005162062123417854, 0.006073357537388802, kl_loss = 0.002460266463458538\n",
      "\n",
      "Epoch 1583\n",
      "Step 0: loss = 0.011309805326163769, recon_loss = 0.004942175000905991, 0.006355590187013149, kl_loss = 0.006020110100507736\n",
      "\n",
      "Epoch 1584\n",
      "Step 0: loss = 0.009182063862681389, recon_loss = 0.0045973677188158035, 0.0045743780210614204, kl_loss = 0.005158936604857445\n",
      "\n",
      "Epoch 1585\n",
      "Step 0: loss = 0.009347173385322094, recon_loss = 0.004528626799583435, 0.004813654348254204, kl_loss = 0.002446121536195278\n",
      "\n",
      "Epoch 1586\n",
      "Step 0: loss = 0.010468784719705582, recon_loss = 0.004179507493972778, 0.006284345872700214, kl_loss = 0.002465757541358471\n",
      "\n",
      "Epoch 1587\n",
      "Step 0: loss = 0.011464755982160568, recon_loss = 0.004589797928929329, 0.006871865130960941, kl_loss = 0.001546681858599186\n",
      "\n",
      "Epoch 1588\n",
      "Step 0: loss = 0.009565556421875954, recon_loss = 0.004743581637740135, 0.0048178983852267265, kl_loss = 0.0020380746573209763\n",
      "\n",
      "Epoch 1589\n",
      "Step 0: loss = 0.009117086417973042, recon_loss = 0.004434419795870781, 0.0046765487641096115, kl_loss = 0.0030589941889047623\n",
      "\n",
      "Epoch 1590\n",
      "Step 0: loss = 0.010469595901668072, recon_loss = 0.0036689303815364838, 0.00679329177364707, kl_loss = 0.0036864187568426132\n",
      "\n",
      "Epoch 1591\n",
      "Step 0: loss = 0.00864162016659975, recon_loss = 0.003927070647478104, 0.004708722233772278, kl_loss = 0.002913682721555233\n",
      "\n",
      "Epoch 1592\n",
      "Step 0: loss = 0.009857015684247017, recon_loss = 0.003941858187317848, 0.0059086596593260765, kl_loss = 0.0032489094883203506\n",
      "\n",
      "Epoch 1593\n",
      "Step 0: loss = 0.008927078917622566, recon_loss = 0.004010871052742004, 0.004907800350338221, kl_loss = 0.004204060882329941\n",
      "\n",
      "Epoch 1594\n",
      "Step 0: loss = 0.008884950540959835, recon_loss = 0.00457332469522953, 0.004306646529585123, kl_loss = 0.0024897530674934387\n",
      "\n",
      "Epoch 1595\n",
      "Step 0: loss = 0.010709282010793686, recon_loss = 0.00425783172249794, 0.0064448025077581406, kl_loss = 0.003323669545352459\n",
      "\n",
      "Epoch 1596\n",
      "Step 0: loss = 0.008214486762881279, recon_loss = 0.0034888572990894318, 0.004720269702374935, kl_loss = 0.002680058591067791\n",
      "\n",
      "Epoch 1597\n",
      "Step 0: loss = 0.010465185157954693, recon_loss = 0.004385076463222504, 0.0060727703385055065, kl_loss = 0.0036689694970846176\n",
      "\n",
      "Epoch 1598\n",
      "Step 0: loss = 0.011274334043264389, recon_loss = 0.004600167274475098, 0.006648623384535313, kl_loss = 0.012771721929311752\n",
      "\n",
      "Epoch 1599\n",
      "Step 0: loss = 0.013518097810447216, recon_loss = 0.006293531507253647, 0.007192582357674837, kl_loss = 0.01599203795194626\n",
      "\n",
      "Epoch 1600\n",
      "Step 0: loss = 0.010618261992931366, recon_loss = 0.005448037758469582, 0.005149227101355791, kl_loss = 0.010498731397092342\n",
      "\n",
      "Epoch 1601\n",
      "Step 0: loss = 0.01034725084900856, recon_loss = 0.004983533173799515, 0.00534718157723546, kl_loss = 0.008268212899565697\n",
      "\n",
      "Epoch 1602\n",
      "Step 0: loss = 0.010259607806801796, recon_loss = 0.0048720501363277435, 0.005378000903874636, kl_loss = 0.0047788359224796295\n",
      "\n",
      "Epoch 1603\n",
      "Step 0: loss = 0.012099784798920155, recon_loss = 0.005265578627586365, 0.0068273479118943214, kl_loss = 0.0034292684867978096\n",
      "\n",
      "Epoch 1604\n",
      "Step 0: loss = 0.0125607093796134, recon_loss = 0.005568878725171089, 0.006986852735280991, kl_loss = 0.002488771453499794\n",
      "\n",
      "Epoch 1605\n",
      "Step 0: loss = 0.013729637488722801, recon_loss = 0.006528856232762337, 0.007195434998720884, kl_loss = 0.0026727011427283287\n",
      "\n",
      "Epoch 1606\n",
      "Step 0: loss = 0.012205921113491058, recon_loss = 0.005551230162382126, 0.006646576337516308, kl_loss = 0.004057341255247593\n",
      "\n",
      "Epoch 1607\n",
      "Step 0: loss = 0.012525077909231186, recon_loss = 0.005915246903896332, 0.00660157622769475, kl_loss = 0.004127471707761288\n",
      "\n",
      "Epoch 1608\n",
      "Step 0: loss = 0.01191157940775156, recon_loss = 0.005641881376504898, 0.006261146627366543, kl_loss = 0.004275553859770298\n",
      "\n",
      "Epoch 1609\n",
      "Step 0: loss = 0.008901502937078476, recon_loss = 0.004725473001599312, 0.004165699705481529, kl_loss = 0.0051649101078510284\n",
      "\n",
      "Epoch 1610\n",
      "Step 0: loss = 0.009213379584252834, recon_loss = 0.004677491262555122, 0.004511759616434574, kl_loss = 0.012064176611602306\n",
      "\n",
      "Epoch 1611\n",
      "Step 0: loss = 0.011318419128656387, recon_loss = 0.005413725972175598, 0.005875364877283573, kl_loss = 0.014664296992123127\n",
      "\n",
      "Epoch 1612\n",
      "Step 0: loss = 0.012898512184619904, recon_loss = 0.006119163706898689, 0.006769794505089521, kl_loss = 0.004776645451784134\n",
      "\n",
      "Epoch 1613\n",
      "Step 0: loss = 0.010342038236558437, recon_loss = 0.005079811438918114, 0.005251935683190823, kl_loss = 0.005145669914782047\n",
      "\n",
      "Epoch 1614\n",
      "Step 0: loss = 0.012341254390776157, recon_loss = 0.005542309954762459, 0.0067883743904531, kl_loss = 0.00528464000672102\n",
      "\n",
      "Epoch 1615\n",
      "Step 0: loss = 0.012556745670735836, recon_loss = 0.0051325466483831406, 0.007410323712974787, kl_loss = 0.006937729194760323\n",
      "\n",
      "Epoch 1616\n",
      "Step 0: loss = 0.009876776486635208, recon_loss = 0.004755500704050064, 0.005109809804707766, kl_loss = 0.00573329720646143\n",
      "\n",
      "Epoch 1617\n",
      "Step 0: loss = 0.012064075097441673, recon_loss = 0.004597852006554604, 0.007451262325048447, kl_loss = 0.007480320520699024\n",
      "\n",
      "Epoch 1618\n",
      "Step 0: loss = 0.012686412781476974, recon_loss = 0.004987120628356934, 0.007681259885430336, kl_loss = 0.009016292169690132\n",
      "\n",
      "Epoch 1619\n",
      "Step 0: loss = 0.011114217340946198, recon_loss = 0.00528457947075367, 0.005816963035613298, kl_loss = 0.0063377441838383675\n",
      "\n",
      "Epoch 1620\n",
      "Step 0: loss = 0.010014072991907597, recon_loss = 0.0051884837448596954, 0.004810379818081856, kl_loss = 0.0076045868918299675\n",
      "\n",
      "Epoch 1621\n",
      "Step 0: loss = 0.011390014551579952, recon_loss = 0.004856087267398834, 0.006514837499707937, kl_loss = 0.009544522501528263\n",
      "\n",
      "Epoch 1622\n",
      "Step 0: loss = 0.011205838993191719, recon_loss = 0.005747547373175621, 0.005443830508738756, kl_loss = 0.007230854593217373\n",
      "\n",
      "Epoch 1623\n",
      "Step 0: loss = 0.010407964698970318, recon_loss = 0.005154188722372055, 0.005246845539659262, kl_loss = 0.00346513744443655\n",
      "\n",
      "Epoch 1624\n",
      "Step 0: loss = 0.010406715795397758, recon_loss = 0.0050292834639549255, 0.005372335202991962, kl_loss = 0.002548396587371826\n",
      "\n",
      "Epoch 1625\n",
      "Step 0: loss = 0.008778141811490059, recon_loss = 0.004446202889084816, 0.004326723515987396, kl_loss = 0.002607768401503563\n",
      "\n",
      "Epoch 1626\n",
      "Step 0: loss = 0.009335188195109367, recon_loss = 0.004554498940706253, 0.0047742826864123344, kl_loss = 0.0032032784074544907\n",
      "\n",
      "Epoch 1627\n",
      "Step 0: loss = 0.010459956713020802, recon_loss = 0.005064845085144043, 0.005390457343310118, kl_loss = 0.00232711061835289\n",
      "\n",
      "Epoch 1628\n",
      "Step 0: loss = 0.011299880221486092, recon_loss = 0.0046933311969041824, 0.006600826978683472, kl_loss = 0.0028610173612833023\n",
      "\n",
      "Epoch 1629\n",
      "Step 0: loss = 0.009717429056763649, recon_loss = 0.004782766103744507, 0.004929528106004, kl_loss = 0.002567627467215061\n",
      "\n",
      "Epoch 1630\n",
      "Step 0: loss = 0.010018770582973957, recon_loss = 0.004471844062209129, 0.005542918108403683, kl_loss = 0.002004164271056652\n",
      "\n",
      "Epoch 1631\n",
      "Step 0: loss = 0.00874180905520916, recon_loss = 0.003967761993408203, 0.00476288516074419, kl_loss = 0.005580968223512173\n",
      "\n",
      "Epoch 1632\n",
      "Step 0: loss = 0.011240493506193161, recon_loss = 0.005121869966387749, 0.0061113908886909485, kl_loss = 0.003616420552134514\n",
      "\n",
      "Epoch 1633\n",
      "Step 0: loss = 0.008575206622481346, recon_loss = 0.003980126231908798, 0.00458898488432169, kl_loss = 0.0030477596446871758\n",
      "\n",
      "Epoch 1634\n",
      "Step 0: loss = 0.010741877369582653, recon_loss = 0.00487012043595314, 0.005868264473974705, kl_loss = 0.0017461497336626053\n",
      "\n",
      "Epoch 1635\n",
      "Step 0: loss = 0.008657709695398808, recon_loss = 0.004173772409558296, 0.004479235038161278, kl_loss = 0.0023513026535511017\n",
      "\n",
      "Epoch 1636\n",
      "Step 0: loss = 0.01196896843612194, recon_loss = 0.005333244800567627, 0.0066308025270700455, kl_loss = 0.0024603959172964096\n",
      "\n",
      "Epoch 1637\n",
      "Step 0: loss = 0.011681589297950268, recon_loss = 0.004581786692142487, 0.0070912204682827, kl_loss = 0.004291282035410404\n",
      "\n",
      "Epoch 1638\n",
      "Step 0: loss = 0.009752357378602028, recon_loss = 0.004562152549624443, 0.005182163789868355, kl_loss = 0.004020392894744873\n",
      "\n",
      "Epoch 1639\n",
      "Step 0: loss = 0.009995635598897934, recon_loss = 0.00432451069355011, 0.005666519049555063, kl_loss = 0.0023031579330563545\n",
      "\n",
      "Epoch 1640\n",
      "Step 0: loss = 0.01041851844638586, recon_loss = 0.00441371276974678, 0.006001273635774851, kl_loss = 0.001766173169016838\n",
      "\n",
      "Epoch 1641\n",
      "Step 0: loss = 0.01023027766495943, recon_loss = 0.00418420135974884, 0.0060407184064388275, kl_loss = 0.002679011784493923\n",
      "\n",
      "Epoch 1642\n",
      "Step 0: loss = 0.007478523533791304, recon_loss = 0.004294037818908691, 0.0031808053608983755, kl_loss = 0.0018399590626358986\n",
      "\n",
      "Epoch 1643\n",
      "Step 0: loss = 0.009965360164642334, recon_loss = 0.004520762711763382, 0.0054411012679338455, kl_loss = 0.0017479006201028824\n",
      "\n",
      "Epoch 1644\n",
      "Step 0: loss = 0.013263067230582237, recon_loss = 0.005400950089097023, 0.007849026471376419, kl_loss = 0.006545242853462696\n",
      "\n",
      "Epoch 1645\n",
      "Step 0: loss = 0.011453208513557911, recon_loss = 0.005143668502569199, 0.006301962770521641, kl_loss = 0.0037884777411818504\n",
      "\n",
      "Epoch 1646\n",
      "Step 0: loss = 0.011094791814684868, recon_loss = 0.005034042522311211, 0.00605399627238512, kl_loss = 0.0033765770494937897\n",
      "\n",
      "Epoch 1647\n",
      "Step 0: loss = 0.01028243824839592, recon_loss = 0.004926128312945366, 0.005351554602384567, kl_loss = 0.0023777754977345467\n",
      "\n",
      "Epoch 1648\n",
      "Step 0: loss = 0.0105522982776165, recon_loss = 0.0041794553399086, 0.006363346241414547, kl_loss = 0.0047482335940003395\n",
      "\n",
      "Epoch 1649\n",
      "Step 0: loss = 0.007770486641675234, recon_loss = 0.0038011502474546432, 0.0039605204947292805, kl_loss = 0.004408039152622223\n",
      "\n",
      "Epoch 1650\n",
      "Step 0: loss = 0.011962191201746464, recon_loss = 0.004747621715068817, 0.007205316796898842, kl_loss = 0.004626418463885784\n",
      "\n",
      "Epoch 1651\n",
      "Step 0: loss = 0.01012676302343607, recon_loss = 0.004064351320266724, 0.006055685691535473, kl_loss = 0.00336312223225832\n",
      "\n",
      "Epoch 1652\n",
      "Step 0: loss = 0.006717091891914606, recon_loss = 0.003427429124712944, 0.003283966798335314, kl_loss = 0.0028480403125286102\n",
      "\n",
      "Epoch 1653\n",
      "Step 0: loss = 0.0075882053934037685, recon_loss = 0.0038852207362651825, 0.0036996814887970686, kl_loss = 0.0016515273600816727\n",
      "\n",
      "Epoch 1654\n",
      "Step 0: loss = 0.010038094595074654, recon_loss = 0.003932604566216469, 0.0060998499393463135, kl_loss = 0.002820011228322983\n",
      "\n",
      "Epoch 1655\n",
      "Step 0: loss = 0.007885180413722992, recon_loss = 0.0038195643573999405, 0.004062086809426546, kl_loss = 0.0017647594213485718\n",
      "\n",
      "Epoch 1656\n",
      "Step 0: loss = 0.010475775226950645, recon_loss = 0.0037977490574121475, 0.006674374453723431, kl_loss = 0.0018258728086948395\n",
      "\n",
      "Epoch 1657\n",
      "Step 0: loss = 0.010962761007249355, recon_loss = 0.004858208820223808, 0.006098703946918249, kl_loss = 0.002923690713942051\n",
      "\n",
      "Epoch 1658\n",
      "Step 0: loss = 0.009419194422662258, recon_loss = 0.004477567970752716, 0.00493440218269825, kl_loss = 0.0036121485754847527\n",
      "\n",
      "Epoch 1659\n",
      "Step 0: loss = 0.010548273101449013, recon_loss = 0.003979332745075226, 0.0065625510178506374, kl_loss = 0.003194250166416168\n",
      "\n",
      "Epoch 1660\n",
      "Step 0: loss = 0.009147433564066887, recon_loss = 0.004246041178703308, 0.004897206090390682, kl_loss = 0.0020932750776410103\n",
      "\n",
      "Epoch 1661\n",
      "Step 0: loss = 0.008837338536977768, recon_loss = 0.004233416169881821, 0.004598534666001797, kl_loss = 0.0026936614885926247\n",
      "\n",
      "Epoch 1662\n",
      "Step 0: loss = 0.009400446899235249, recon_loss = 0.00400121882557869, 0.005395982414484024, kl_loss = 0.0016230251640081406\n",
      "\n",
      "Epoch 1663\n",
      "Step 0: loss = 0.0109091280028224, recon_loss = 0.005007069557905197, 0.005891541484743357, kl_loss = 0.005258568562567234\n",
      "\n",
      "Epoch 1664\n",
      "Step 0: loss = 0.010601318441331387, recon_loss = 0.004569001495838165, 0.006019000429660082, kl_loss = 0.006658710539340973\n",
      "\n",
      "Epoch 1665\n",
      "Step 0: loss = 0.011633055284619331, recon_loss = 0.0054634809494018555, 0.006162075325846672, kl_loss = 0.0037495987489819527\n",
      "\n",
      "Epoch 1666\n",
      "Step 0: loss = 0.01049483846873045, recon_loss = 0.005074281245470047, 0.005413996055722237, kl_loss = 0.0032807234674692154\n",
      "\n",
      "Epoch 1667\n",
      "Step 0: loss = 0.01366051658987999, recon_loss = 0.005591953173279762, 0.008050746284425259, kl_loss = 0.008908546529710293\n",
      "\n",
      "Epoch 1668\n",
      "Step 0: loss = 0.009858141653239727, recon_loss = 0.0044486019760370255, 0.005395595449954271, kl_loss = 0.006972481496632099\n",
      "\n",
      "Epoch 1669\n",
      "Step 0: loss = 0.010576514527201653, recon_loss = 0.004507863894104958, 0.0060479287058115005, kl_loss = 0.010360757820308208\n",
      "\n",
      "Epoch 1670\n",
      "Step 0: loss = 0.011172271333634853, recon_loss = 0.00494999997317791, 0.0062043508514761925, kl_loss = 0.008960127830505371\n",
      "\n",
      "Epoch 1671\n",
      "Step 0: loss = 0.01091359369456768, recon_loss = 0.0049215760082006454, 0.005967685021460056, kl_loss = 0.0121662812307477\n",
      "\n",
      "Epoch 1672\n",
      "Step 0: loss = 0.009703467600047588, recon_loss = 0.004830101504921913, 0.004862302914261818, kl_loss = 0.005531812086701393\n",
      "\n",
      "Epoch 1673\n",
      "Step 0: loss = 0.013062951155006886, recon_loss = 0.005358492955565453, 0.007697016932070255, kl_loss = 0.003720449283719063\n",
      "\n",
      "Epoch 1674\n",
      "Step 0: loss = 0.010112621821463108, recon_loss = 0.005106398835778236, 0.004997876472771168, kl_loss = 0.004173158667981625\n",
      "\n",
      "Epoch 1675\n",
      "Step 0: loss = 0.009277582168579102, recon_loss = 0.004344716668128967, 0.004929773975163698, kl_loss = 0.0015460867434740067\n",
      "\n",
      "Epoch 1676\n",
      "Step 0: loss = 0.010598234832286835, recon_loss = 0.005077943205833435, 0.005514962133020163, kl_loss = 0.0026644282042980194\n",
      "\n",
      "Epoch 1677\n",
      "Step 0: loss = 0.00899076834321022, recon_loss = 0.0037018898874521255, 0.005284822545945644, kl_loss = 0.0020279474556446075\n",
      "\n",
      "Epoch 1678\n",
      "Step 0: loss = 0.008933219127357006, recon_loss = 0.0043408628553152084, 0.004586681257933378, kl_loss = 0.002837223932147026\n",
      "\n",
      "Epoch 1679\n",
      "Step 0: loss = 0.010197670198976994, recon_loss = 0.005012718960642815, 0.005172809585928917, kl_loss = 0.006070702336728573\n",
      "\n",
      "Epoch 1680\n",
      "Step 0: loss = 0.010226038284599781, recon_loss = 0.0046369768679142, 0.005577037110924721, kl_loss = 0.006012371741235256\n",
      "\n",
      "Epoch 1681\n",
      "Step 0: loss = 0.012553936801850796, recon_loss = 0.00466843880712986, 0.00786537118256092, kl_loss = 0.010063531808555126\n",
      "\n",
      "Epoch 1682\n",
      "Step 0: loss = 0.010338058695197105, recon_loss = 0.005410503596067429, 0.004919285420328379, kl_loss = 0.004135097377002239\n",
      "\n",
      "Epoch 1683\n",
      "Step 0: loss = 0.01121110562235117, recon_loss = 0.004185592755675316, 0.007019330747425556, kl_loss = 0.003091071732342243\n",
      "\n",
      "Epoch 1684\n",
      "Step 0: loss = 0.01041893195360899, recon_loss = 0.0050125084817409515, 0.005401925649493933, kl_loss = 0.0022488413378596306\n",
      "\n",
      "Epoch 1685\n",
      "Step 0: loss = 0.009313305839896202, recon_loss = 0.004361012950539589, 0.004948086570948362, kl_loss = 0.0021029626950621605\n",
      "\n",
      "Epoch 1686\n",
      "Step 0: loss = 0.009780820459127426, recon_loss = 0.004560329020023346, 0.005217059515416622, kl_loss = 0.0017158109694719315\n",
      "\n",
      "Epoch 1687\n",
      "Step 0: loss = 0.010743594728410244, recon_loss = 0.005066046491265297, 0.00567343644797802, kl_loss = 0.002055997960269451\n",
      "\n",
      "Epoch 1688\n",
      "Step 0: loss = 0.009895301423966885, recon_loss = 0.004827260971069336, 0.005059803836047649, kl_loss = 0.004118084907531738\n",
      "\n",
      "Epoch 1689\n",
      "Step 0: loss = 0.009987951256334782, recon_loss = 0.004236504435539246, 0.005745983682572842, kl_loss = 0.002731718122959137\n",
      "\n",
      "Epoch 1690\n",
      "Step 0: loss = 0.010223260149359703, recon_loss = 0.004257451742887497, 0.005958987399935722, kl_loss = 0.0034103505313396454\n",
      "\n",
      "Epoch 1691\n",
      "Step 0: loss = 0.010114475153386593, recon_loss = 0.005313549190759659, 0.004796040244400501, kl_loss = 0.002442636527121067\n",
      "\n",
      "Epoch 1692\n",
      "Step 0: loss = 0.010795045644044876, recon_loss = 0.004623739048838615, 0.006166838109493256, kl_loss = 0.0022342344745993614\n",
      "\n",
      "Epoch 1693\n",
      "Step 0: loss = 0.009561807848513126, recon_loss = 0.0040875449776649475, 0.0054677873849868774, kl_loss = 0.0032376395538449287\n",
      "\n",
      "Epoch 1694\n",
      "Step 0: loss = 0.008188927546143532, recon_loss = 0.0035490095615386963, 0.004634520970284939, kl_loss = 0.0026984522119164467\n",
      "\n",
      "Epoch 1695\n",
      "Step 0: loss = 0.00843537412583828, recon_loss = 0.0040547531098127365, 0.004375129006803036, kl_loss = 0.0027459831908345222\n",
      "\n",
      "Epoch 1696\n",
      "Step 0: loss = 0.009392082691192627, recon_loss = 0.003990551456809044, 0.005395178683102131, kl_loss = 0.0031764712184667587\n",
      "\n",
      "Epoch 1697\n",
      "Step 0: loss = 0.009589471854269505, recon_loss = 0.0038901437073946, 0.005694037303328514, kl_loss = 0.0026455530896782875\n",
      "\n",
      "Epoch 1698\n",
      "Step 0: loss = 0.00789534579962492, recon_loss = 0.0036213602870702744, 0.004270177334547043, kl_loss = 0.0019039148464798927\n",
      "\n",
      "Epoch 1699\n",
      "Step 0: loss = 0.009593871422111988, recon_loss = 0.004102054983377457, 0.005488543771207333, kl_loss = 0.0016361838206648827\n",
      "\n",
      "Epoch 1700\n",
      "Step 0: loss = 0.009541149251163006, recon_loss = 0.004077918827533722, 0.00546111399307847, kl_loss = 0.0010582441464066505\n",
      "\n",
      "Epoch 1701\n",
      "Step 0: loss = 0.009906799532473087, recon_loss = 0.004185495898127556, 0.005715011153370142, kl_loss = 0.003146447241306305\n",
      "\n",
      "Epoch 1702\n",
      "Step 0: loss = 0.00865160021930933, recon_loss = 0.004159962758421898, 0.004481717944145203, kl_loss = 0.004959982819855213\n",
      "\n",
      "Epoch 1703\n",
      "Step 0: loss = 0.008692285977303982, recon_loss = 0.004228617995977402, 0.004456937778741121, kl_loss = 0.003365378826856613\n",
      "\n",
      "Epoch 1704\n",
      "Step 0: loss = 0.011333414353430271, recon_loss = 0.004982547834515572, 0.006345349829643965, kl_loss = 0.002758339047431946\n",
      "\n",
      "Epoch 1705\n",
      "Step 0: loss = 0.011457639746367931, recon_loss = 0.00525241531431675, 0.006198202259838581, kl_loss = 0.0035110609605908394\n",
      "\n",
      "Epoch 1706\n",
      "Step 0: loss = 0.013091344386339188, recon_loss = 0.006171101704239845, 0.006913268472999334, kl_loss = 0.003487006761133671\n",
      "\n",
      "Epoch 1707\n",
      "Step 0: loss = 0.011044169776141644, recon_loss = 0.005544833838939667, 0.005494372919201851, kl_loss = 0.0024815229699015617\n",
      "\n",
      "Epoch 1708\n",
      "Step 0: loss = 0.00932769849896431, recon_loss = 0.004364319145679474, 0.004958111792802811, kl_loss = 0.00263379979878664\n",
      "\n",
      "Epoch 1709\n",
      "Step 0: loss = 0.010198863223195076, recon_loss = 0.004540428519248962, 0.005648885853588581, kl_loss = 0.004774566739797592\n",
      "\n",
      "Epoch 1710\n",
      "Step 0: loss = 0.009840298444032669, recon_loss = 0.003811744973063469, 0.006023019552230835, kl_loss = 0.0027669519186019897\n",
      "\n",
      "Epoch 1711\n",
      "Step 0: loss = 0.008127358742058277, recon_loss = 0.00376991368830204, 0.004349795635789633, kl_loss = 0.0038242805749177933\n",
      "\n",
      "Epoch 1712\n",
      "Step 0: loss = 0.008383672684431076, recon_loss = 0.004155417904257774, 0.004221245646476746, kl_loss = 0.003504503518342972\n",
      "\n",
      "Epoch 1713\n",
      "Step 0: loss = 0.011009862646460533, recon_loss = 0.0045318566262722015, 0.0064729489386081696, kl_loss = 0.0025283992290496826\n",
      "\n",
      "Epoch 1714\n",
      "Step 0: loss = 0.009959143586456776, recon_loss = 0.004508471116423607, 0.005443996749818325, kl_loss = 0.003337807022035122\n",
      "\n",
      "Epoch 1715\n",
      "Step 0: loss = 0.010804332792758942, recon_loss = 0.0048527102917432785, 0.0059456611052155495, kl_loss = 0.0029809074476361275\n",
      "\n",
      "Epoch 1716\n",
      "Step 0: loss = 0.007121010683476925, recon_loss = 0.0038220547139644623, 0.003293072571977973, kl_loss = 0.0029414845630526543\n",
      "\n",
      "Epoch 1717\n",
      "Step 0: loss = 0.00983833521604538, recon_loss = 0.003977186977863312, 0.005855130963027477, kl_loss = 0.0030085938051342964\n",
      "\n",
      "Epoch 1718\n",
      "Step 0: loss = 0.00987071543931961, recon_loss = 0.004422545433044434, 0.0054433890618383884, kl_loss = 0.0023908931761980057\n",
      "\n",
      "Epoch 1719\n",
      "Step 0: loss = 0.010241085663437843, recon_loss = 0.004379177466034889, 0.005859040189534426, kl_loss = 0.001434451900422573\n",
      "\n",
      "Epoch 1720\n",
      "Step 0: loss = 0.00824479479342699, recon_loss = 0.004061909392476082, 0.004178662784397602, kl_loss = 0.002111334353685379\n",
      "\n",
      "Epoch 1721\n",
      "Step 0: loss = 0.01035864744335413, recon_loss = 0.005193077027797699, 0.005157839506864548, kl_loss = 0.0038652997463941574\n",
      "\n",
      "Epoch 1722\n",
      "Step 0: loss = 0.011213505640625954, recon_loss = 0.004940450191497803, 0.006262592040002346, kl_loss = 0.005231560207903385\n",
      "\n",
      "Epoch 1723\n",
      "Step 0: loss = 0.008668497204780579, recon_loss = 0.00396387092769146, 0.0046998197212815285, kl_loss = 0.0024031810462474823\n",
      "\n",
      "Epoch 1724\n",
      "Step 0: loss = 0.010572507977485657, recon_loss = 0.004783380776643753, 0.005781163461506367, kl_loss = 0.003981740213930607\n",
      "\n",
      "Epoch 1725\n",
      "Step 0: loss = 0.008878231048583984, recon_loss = 0.0039513204246759415, 0.004922168329358101, kl_loss = 0.002371133305132389\n",
      "\n",
      "Epoch 1726\n",
      "Step 0: loss = 0.010018078610301018, recon_loss = 0.004410505294799805, 0.005602870136499405, kl_loss = 0.0023514274507761\n",
      "\n",
      "Epoch 1727\n",
      "Step 0: loss = 0.011157264932990074, recon_loss = 0.00458003394305706, 0.006574191153049469, kl_loss = 0.0015197070315480232\n",
      "\n",
      "Epoch 1728\n",
      "Step 0: loss = 0.012580715119838715, recon_loss = 0.004326632246375084, 0.008237030357122421, kl_loss = 0.008526028133928776\n",
      "\n",
      "Epoch 1729\n",
      "Step 0: loss = 0.011241110973060131, recon_loss = 0.005382057279348373, 0.005840659141540527, kl_loss = 0.009197277016937733\n",
      "\n",
      "Epoch 1730\n",
      "Step 0: loss = 0.011194221675395966, recon_loss = 0.0048261769115924835, 0.006356646306812763, kl_loss = 0.005699397996068001\n",
      "\n",
      "Epoch 1731\n",
      "Step 0: loss = 0.011420798487961292, recon_loss = 0.005100363865494728, 0.006314742378890514, kl_loss = 0.0028458964079618454\n",
      "\n",
      "Epoch 1732\n",
      "Step 0: loss = 0.011294696480035782, recon_loss = 0.005705840885639191, 0.005582413636147976, kl_loss = 0.0032208338379859924\n",
      "\n",
      "Epoch 1733\n",
      "Step 0: loss = 0.012490193359553814, recon_loss = 0.006079666316509247, 0.006405306980013847, kl_loss = 0.00260984618216753\n",
      "\n",
      "Epoch 1734\n",
      "Step 0: loss = 0.012654246762394905, recon_loss = 0.005321763455867767, 0.00732814148068428, kl_loss = 0.002171136438846588\n",
      "\n",
      "Epoch 1735\n",
      "Step 0: loss = 0.009808322414755821, recon_loss = 0.005169982090592384, 0.004626785404980183, kl_loss = 0.005777547135949135\n",
      "\n",
      "Epoch 1736\n",
      "Step 0: loss = 0.01182648353278637, recon_loss = 0.005508657544851303, 0.0063092513009905815, kl_loss = 0.004287257790565491\n",
      "\n",
      "Epoch 1737\n",
      "Step 0: loss = 0.009991633705794811, recon_loss = 0.005119463428854942, 0.004865073598921299, kl_loss = 0.0035482998937368393\n",
      "\n",
      "Epoch 1738\n",
      "Step 0: loss = 0.012700422666966915, recon_loss = 0.005648985505104065, 0.007046926766633987, kl_loss = 0.002255398780107498\n",
      "\n",
      "Epoch 1739\n",
      "Step 0: loss = 0.010481304489076138, recon_loss = 0.004971349611878395, 0.005495887715369463, kl_loss = 0.007033960893750191\n",
      "\n",
      "Epoch 1740\n",
      "Step 0: loss = 0.011994601227343082, recon_loss = 0.004683511331677437, 0.007304051425307989, kl_loss = 0.0035189269110560417\n",
      "\n",
      "Epoch 1741\n",
      "Step 0: loss = 0.012658016756176949, recon_loss = 0.006055956706404686, 0.006590900477021933, kl_loss = 0.0055794911459088326\n",
      "\n",
      "Epoch 1742\n",
      "Step 0: loss = 0.01165772508829832, recon_loss = 0.005247525870800018, 0.006401831284165382, kl_loss = 0.00418380182236433\n",
      "\n",
      "Epoch 1743\n",
      "Step 0: loss = 0.007752433884888887, recon_loss = 0.003996647894382477, 0.003749094670638442, kl_loss = 0.0033455314114689827\n",
      "\n",
      "Epoch 1744\n",
      "Step 0: loss = 0.01072040293365717, recon_loss = 0.005443373695015907, 0.0052726189605891705, kl_loss = 0.0022049108520150185\n",
      "\n",
      "Epoch 1745\n",
      "Step 0: loss = 0.007741324603557587, recon_loss = 0.0037096235901117325, 0.004024891182780266, kl_loss = 0.0034049861133098602\n",
      "\n",
      "Epoch 1746\n",
      "Step 0: loss = 0.008803017437458038, recon_loss = 0.003527022898197174, 0.005270914174616337, kl_loss = 0.002540323883295059\n",
      "\n",
      "Epoch 1747\n",
      "Step 0: loss = 0.01016325131058693, recon_loss = 0.004035554826259613, 0.006123934872448444, kl_loss = 0.001880795694887638\n",
      "\n",
      "Epoch 1748\n",
      "Step 0: loss = 0.009353543631732464, recon_loss = 0.004262229427695274, 0.0050858668982982635, kl_loss = 0.002723751589655876\n",
      "\n",
      "Epoch 1749\n",
      "Step 0: loss = 0.010828105732798576, recon_loss = 0.004447266459465027, 0.006356885191053152, kl_loss = 0.011976905167102814\n",
      "\n",
      "Epoch 1750\n",
      "Step 0: loss = 0.009384716860949993, recon_loss = 0.00502370111644268, 0.00433996319770813, kl_loss = 0.01052622590214014\n",
      "\n",
      "Epoch 1751\n",
      "Step 0: loss = 0.010585399344563484, recon_loss = 0.004426885396242142, 0.006136450916528702, kl_loss = 0.011031359434127808\n",
      "\n",
      "Epoch 1752\n",
      "Step 0: loss = 0.008687414228916168, recon_loss = 0.003912953659892082, 0.004753756802529097, kl_loss = 0.010351727716624737\n",
      "\n",
      "Epoch 1753\n",
      "Step 0: loss = 0.008907929062843323, recon_loss = 0.004171993583440781, 0.004726733546704054, kl_loss = 0.004600576125085354\n",
      "\n",
      "Epoch 1754\n",
      "Step 0: loss = 0.009058428928256035, recon_loss = 0.004315627738833427, 0.004735652357339859, kl_loss = 0.0035746078938245773\n",
      "\n",
      "Epoch 1755\n",
      "Step 0: loss = 0.010792962275445461, recon_loss = 0.00426180474460125, 0.006521301809698343, kl_loss = 0.004928139038383961\n",
      "\n",
      "Epoch 1756\n",
      "Step 0: loss = 0.008917666040360928, recon_loss = 0.003929119557142258, 0.00498055387288332, kl_loss = 0.003996073268353939\n",
      "\n",
      "Epoch 1757\n",
      "Step 0: loss = 0.009700486436486244, recon_loss = 0.0038711167871952057, 0.005824662744998932, kl_loss = 0.0023533357307314873\n",
      "\n",
      "Epoch 1758\n",
      "Step 0: loss = 0.009553349576890469, recon_loss = 0.004348983988165855, 0.005200947634875774, kl_loss = 0.00170914176851511\n",
      "\n",
      "Epoch 1759\n",
      "Step 0: loss = 0.009493071585893631, recon_loss = 0.004220515489578247, 0.00526305241510272, kl_loss = 0.0047516776248812675\n",
      "\n",
      "Epoch 1760\n",
      "Step 0: loss = 0.01069715153425932, recon_loss = 0.00462869368493557, 0.0060569667257368565, kl_loss = 0.005745772272348404\n",
      "\n",
      "Epoch 1761\n",
      "Step 0: loss = 0.01143838744610548, recon_loss = 0.004565941169857979, 0.00686468742787838, kl_loss = 0.0038793962448835373\n",
      "\n",
      "Epoch 1762\n",
      "Step 0: loss = 0.01103712897747755, recon_loss = 0.004586329683661461, 0.006444416008889675, kl_loss = 0.0031914906576275826\n",
      "\n",
      "Epoch 1763\n",
      "Step 0: loss = 0.008897552266716957, recon_loss = 0.004584077745676041, 0.004305090755224228, kl_loss = 0.004191751591861248\n",
      "\n",
      "Epoch 1764\n",
      "Step 0: loss = 0.010252363048493862, recon_loss = 0.005195917561650276, 0.005046125967055559, kl_loss = 0.005159887485206127\n",
      "\n",
      "Epoch 1765\n",
      "Step 0: loss = 0.01221474539488554, recon_loss = 0.004979118704795837, 0.007223083637654781, kl_loss = 0.00627137441188097\n",
      "\n",
      "Epoch 1766\n",
      "Step 0: loss = 0.011252577416598797, recon_loss = 0.004679819568991661, 0.006566144526004791, kl_loss = 0.0033064428716897964\n",
      "\n",
      "Epoch 1767\n",
      "Step 0: loss = 0.009424497373402119, recon_loss = 0.004807287827134132, 0.004612091928720474, kl_loss = 0.0025590360164642334\n",
      "\n",
      "Epoch 1768\n",
      "Step 0: loss = 0.009252357296645641, recon_loss = 0.003872327506542206, 0.005376249551773071, kl_loss = 0.001890065148472786\n",
      "\n",
      "Epoch 1769\n",
      "Step 0: loss = 0.009806632064282894, recon_loss = 0.004179375246167183, 0.0056250859051942825, kl_loss = 0.001085224561393261\n",
      "\n",
      "Epoch 1770\n",
      "Step 0: loss = 0.009560120292007923, recon_loss = 0.0036898907274007797, 0.005862812045961618, kl_loss = 0.0037088440731167793\n",
      "\n",
      "Epoch 1771\n",
      "Step 0: loss = 0.008084634318947792, recon_loss = 0.0034349020570516586, 0.0046412404626607895, kl_loss = 0.0042459387332201\n",
      "\n",
      "Epoch 1772\n",
      "Step 0: loss = 0.00869067944586277, recon_loss = 0.003619890660047531, 0.005067053716629744, kl_loss = 0.0018673175945878029\n",
      "\n",
      "Epoch 1773\n",
      "Step 0: loss = 0.009596656076610088, recon_loss = 0.004104359075427055, 0.005486559122800827, kl_loss = 0.0028690453618764877\n",
      "\n",
      "Epoch 1774\n",
      "Step 0: loss = 0.008801364339888096, recon_loss = 0.004298798739910126, 0.004497911781072617, kl_loss = 0.0023269550874829292\n",
      "\n",
      "Epoch 1775\n",
      "Step 0: loss = 0.01012940052896738, recon_loss = 0.00428459607064724, 0.005839846096932888, kl_loss = 0.0024790000170469284\n",
      "\n",
      "Epoch 1776\n",
      "Step 0: loss = 0.01002915296703577, recon_loss = 0.005048491060733795, 0.004966466687619686, kl_loss = 0.007097494788467884\n",
      "\n",
      "Epoch 1777\n",
      "Step 0: loss = 0.010198473930358887, recon_loss = 0.004634307697415352, 0.005558311939239502, kl_loss = 0.0029269689694046974\n",
      "\n",
      "Epoch 1778\n",
      "Step 0: loss = 0.010472012683749199, recon_loss = 0.004263143986463547, 0.006204758770763874, kl_loss = 0.0020550210028886795\n",
      "\n",
      "Epoch 1779\n",
      "Step 0: loss = 0.007847164757549763, recon_loss = 0.003994561731815338, 0.0038457789923995733, kl_loss = 0.0034119198098778725\n",
      "\n",
      "Epoch 1780\n",
      "Step 0: loss = 0.009783569723367691, recon_loss = 0.004338745027780533, 0.0054403613321483135, kl_loss = 0.0022312263026833534\n",
      "\n",
      "Epoch 1781\n",
      "Step 0: loss = 0.00918624922633171, recon_loss = 0.004663912579417229, 0.004516060464084148, kl_loss = 0.0031379321590065956\n",
      "\n",
      "Epoch 1782\n",
      "Step 0: loss = 0.010864202864468098, recon_loss = 0.004945080727338791, 0.005913714878261089, kl_loss = 0.002703484147787094\n",
      "\n",
      "Epoch 1783\n",
      "Step 0: loss = 0.009761925786733627, recon_loss = 0.00412161648273468, 0.0056342752650380135, kl_loss = 0.003016822040081024\n",
      "\n",
      "Epoch 1784\n",
      "Step 0: loss = 0.01123680267482996, recon_loss = 0.005551164969801903, 0.005680597387254238, kl_loss = 0.002520006150007248\n",
      "\n",
      "Epoch 1785\n",
      "Step 0: loss = 0.009618609212338924, recon_loss = 0.003906041383743286, 0.005708523094654083, kl_loss = 0.0020224442705512047\n",
      "\n",
      "Epoch 1786\n",
      "Step 0: loss = 0.009519183076918125, recon_loss = 0.004776580259203911, 0.004738454706966877, kl_loss = 0.002074196934700012\n",
      "\n",
      "Epoch 1787\n",
      "Step 0: loss = 0.008380753919482231, recon_loss = 0.004293687641620636, 0.004082619212567806, kl_loss = 0.0022233519703149796\n",
      "\n",
      "Epoch 1788\n",
      "Step 0: loss = 0.010054200887680054, recon_loss = 0.00411980040371418, 0.005930498708039522, kl_loss = 0.0019509242847561836\n",
      "\n",
      "Epoch 1789\n",
      "Step 0: loss = 0.007769655901938677, recon_loss = 0.003384910523891449, 0.004379864316433668, kl_loss = 0.0024404963478446007\n",
      "\n",
      "Epoch 1790\n",
      "Step 0: loss = 0.010800468735396862, recon_loss = 0.0044767167419195175, 0.006316736340522766, kl_loss = 0.003507941961288452\n",
      "\n",
      "Epoch 1791\n",
      "Step 0: loss = 0.009845245629549026, recon_loss = 0.005420096218585968, 0.004419009666889906, kl_loss = 0.0030694864690303802\n",
      "\n",
      "Epoch 1792\n",
      "Step 0: loss = 0.010209535248577595, recon_loss = 0.0049555059522390366, 0.005246198270469904, kl_loss = 0.003915661945939064\n",
      "\n",
      "Epoch 1793\n",
      "Step 0: loss = 0.010956132784485817, recon_loss = 0.006051242351531982, 0.0048991297371685505, kl_loss = 0.0028805723413825035\n",
      "\n",
      "Epoch 1794\n",
      "Step 0: loss = 0.01069747842848301, recon_loss = 0.005078490823507309, 0.005614202469587326, kl_loss = 0.002392536960542202\n",
      "\n",
      "Epoch 1795\n",
      "Step 0: loss = 0.010655943304300308, recon_loss = 0.004632942378520966, 0.006019805558025837, kl_loss = 0.0015977146103978157\n",
      "\n",
      "Epoch 1796\n",
      "Step 0: loss = 0.009144900366663933, recon_loss = 0.004233276471495628, 0.00490762572735548, kl_loss = 0.0019989442080259323\n",
      "\n",
      "Epoch 1797\n",
      "Step 0: loss = 0.011715848930180073, recon_loss = 0.004841916263103485, 0.006868372671306133, kl_loss = 0.0027799000963568687\n",
      "\n",
      "Epoch 1798\n",
      "Step 0: loss = 0.011080845259130001, recon_loss = 0.00518033467233181, 0.005895837675780058, kl_loss = 0.0023360317572951317\n",
      "\n",
      "Epoch 1799\n",
      "Step 0: loss = 0.011076190508902073, recon_loss = 0.004328792914748192, 0.0067431749776005745, kl_loss = 0.002111276611685753\n",
      "\n",
      "Epoch 1800\n",
      "Step 0: loss = 0.008998503908514977, recon_loss = 0.00399591401219368, 0.00499769626185298, kl_loss = 0.0024468163028359413\n",
      "\n",
      "Epoch 1801\n",
      "Step 0: loss = 0.009408353827893734, recon_loss = 0.0038347356021404266, 0.005565359257161617, kl_loss = 0.004129410721361637\n",
      "\n",
      "Epoch 1802\n",
      "Step 0: loss = 0.008683793246746063, recon_loss = 0.003876643255352974, 0.004799406509846449, kl_loss = 0.003871646709740162\n",
      "\n",
      "Epoch 1803\n",
      "Step 0: loss = 0.009007818065583706, recon_loss = 0.0041371919214725494, 0.004866149742156267, kl_loss = 0.002238490618765354\n",
      "\n",
      "Epoch 1804\n",
      "Step 0: loss = 0.009175778366625309, recon_loss = 0.0043885670602321625, 0.004775852896273136, kl_loss = 0.0056793950498104095\n",
      "\n",
      "Epoch 1805\n",
      "Step 0: loss = 0.011882470920681953, recon_loss = 0.004816753789782524, 0.007057389244437218, kl_loss = 0.004163886420428753\n",
      "\n",
      "Epoch 1806\n",
      "Step 0: loss = 0.009840989485383034, recon_loss = 0.004627911373972893, 0.005206857342272997, kl_loss = 0.003110775724053383\n",
      "\n",
      "Epoch 1807\n",
      "Step 0: loss = 0.009709788486361504, recon_loss = 0.004630859941244125, 0.005070752464234829, kl_loss = 0.004087966866791248\n",
      "\n",
      "Epoch 1808\n",
      "Step 0: loss = 0.010267629288136959, recon_loss = 0.004706652835011482, 0.005553174763917923, kl_loss = 0.0039008716121315956\n",
      "\n",
      "Epoch 1809\n",
      "Step 0: loss = 0.009930580854415894, recon_loss = 0.004034455865621567, 0.0058885375037789345, kl_loss = 0.00379363726824522\n",
      "\n",
      "Epoch 1810\n",
      "Step 0: loss = 0.009910493157804012, recon_loss = 0.004163758829236031, 0.005739395506680012, kl_loss = 0.00366929080337286\n",
      "\n",
      "Epoch 1811\n",
      "Step 0: loss = 0.008804741315543652, recon_loss = 0.0038418136537075043, 0.004954266361892223, kl_loss = 0.004330419935286045\n",
      "\n",
      "Epoch 1812\n",
      "Step 0: loss = 0.008583859540522099, recon_loss = 0.0037680473178625107, 0.004808089695870876, kl_loss = 0.003861458972096443\n",
      "\n",
      "Epoch 1813\n",
      "Step 0: loss = 0.009374123066663742, recon_loss = 0.004082120954990387, 0.00528910756111145, kl_loss = 0.0014471104368567467\n",
      "\n",
      "Epoch 1814\n",
      "Step 0: loss = 0.010077599436044693, recon_loss = 0.005193263292312622, 0.004879421554505825, kl_loss = 0.002457188442349434\n",
      "\n",
      "Epoch 1815\n",
      "Step 0: loss = 0.010518143884837627, recon_loss = 0.004306597635149956, 0.00620855949819088, kl_loss = 0.0014933915808796883\n",
      "\n",
      "Epoch 1816\n",
      "Step 0: loss = 0.00924481637775898, recon_loss = 0.004222899675369263, 0.005016110371798277, kl_loss = 0.0029031094163656235\n",
      "\n",
      "Epoch 1817\n",
      "Step 0: loss = 0.009590172208845615, recon_loss = 0.0037737153470516205, 0.0058121103793382645, kl_loss = 0.0021731480956077576\n",
      "\n",
      "Epoch 1818\n",
      "Step 0: loss = 0.009169151075184345, recon_loss = 0.0040596891194581985, 0.005102777387946844, kl_loss = 0.003342275507748127\n",
      "\n",
      "Epoch 1819\n",
      "Step 0: loss = 0.009778751991689205, recon_loss = 0.004478050395846367, 0.005292425863444805, kl_loss = 0.004137716256082058\n",
      "\n",
      "Epoch 1820\n",
      "Step 0: loss = 0.010657875798642635, recon_loss = 0.004826365038752556, 0.00582023523747921, kl_loss = 0.0056377556174993515\n",
      "\n",
      "Epoch 1821\n",
      "Step 0: loss = 0.009762878529727459, recon_loss = 0.004221312701702118, 0.005536902230232954, kl_loss = 0.002331591211259365\n",
      "\n",
      "Epoch 1822\n",
      "Step 0: loss = 0.00981566309928894, recon_loss = 0.004834070801734924, 0.004975032992660999, kl_loss = 0.0032796086743474007\n",
      "\n",
      "Epoch 1823\n",
      "Step 0: loss = 0.011330015026032925, recon_loss = 0.005734343081712723, 0.005590980872511864, kl_loss = 0.0023456234484910965\n",
      "\n",
      "Epoch 1824\n",
      "Step 0: loss = 0.009672509506344795, recon_loss = 0.004635689780116081, 0.005031790118664503, kl_loss = 0.002514575608074665\n",
      "\n",
      "Epoch 1825\n",
      "Step 0: loss = 0.009990416467189789, recon_loss = 0.005272312089800835, 0.004714632872492075, kl_loss = 0.001735825091600418\n",
      "\n",
      "Epoch 1826\n",
      "Step 0: loss = 0.014832684770226479, recon_loss = 0.006526533514261246, 0.008289458230137825, kl_loss = 0.008346376940608025\n",
      "\n",
      "Epoch 1827\n",
      "Step 0: loss = 0.010535309091210365, recon_loss = 0.00469442643225193, 0.0058318572118878365, kl_loss = 0.0045129237696528435\n",
      "\n",
      "Epoch 1828\n",
      "Step 0: loss = 0.011097352020442486, recon_loss = 0.004977822303771973, 0.006104074418544769, kl_loss = 0.00772763229906559\n",
      "\n",
      "Epoch 1829\n",
      "Step 0: loss = 0.010580658912658691, recon_loss = 0.0048334039747715, 0.005738273728638887, kl_loss = 0.004491014406085014\n",
      "\n",
      "Epoch 1830\n",
      "Step 0: loss = 0.011409576050937176, recon_loss = 0.005019519478082657, 0.0063825370743870735, kl_loss = 0.0037598079070448875\n",
      "\n",
      "Epoch 1831\n",
      "Step 0: loss = 0.009191209450364113, recon_loss = 0.004336589947342873, 0.004849015735089779, kl_loss = 0.002801908180117607\n",
      "\n",
      "Epoch 1832\n",
      "Step 0: loss = 0.009987697005271912, recon_loss = 0.004025530070066452, 0.005957775749266148, kl_loss = 0.00219572801142931\n",
      "\n",
      "Epoch 1833\n",
      "Step 0: loss = 0.008384906686842442, recon_loss = 0.004142303019762039, 0.004238940309733152, kl_loss = 0.0018316730856895447\n",
      "\n",
      "Epoch 1834\n",
      "Step 0: loss = 0.010124840773642063, recon_loss = 0.004230635240674019, 0.005887174047529697, kl_loss = 0.003515646792948246\n",
      "\n",
      "Epoch 1835\n",
      "Step 0: loss = 0.009328234009444714, recon_loss = 0.003732111304998398, 0.005590347573161125, kl_loss = 0.0028876038268208504\n",
      "\n",
      "Epoch 1836\n",
      "Step 0: loss = 0.008278051391243935, recon_loss = 0.0038897376507520676, 0.0043829623609781265, kl_loss = 0.0026758797466754913\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2129988/2564765890.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# epochs = 2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# batch_size = 32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;31m# for i in range(epochs):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#   print(f\"Epoch {i}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#   for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#     loss_vals = train_step(vae, batch_betas, batch_dirs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2129988/79618751.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, inputs, dir_inputs, epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# outputs = model.decoder([z, dir_inputs])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdir_inputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecon_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercp_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                     )\n\u001b[1;32m   1140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msingle\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \"\"\"\n\u001b[0;32m--> 511\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    664\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_input_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m                     \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# Node is not computable, try skipping.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 for x_id, y in zip(\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                     )\n\u001b[1;32m   1140\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/keras/layers/core/dense.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    237\u001b[0m                 outputs = tf.nn.embedding_lookup_sparse(\n\u001b[1;32m    238\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 )\n\u001b[1;32m    240\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;31m# Broadcast kernel to inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensordot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOpDispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOT_SUPPORTED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3710\u001b[0m         \u001b[0madjoint_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjoint_b\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3711\u001b[0m         return gen_math_ops.batch_mat_mul_v3(\n\u001b[1;32m   3712\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3713\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3714\u001b[0;31m         return gen_math_ops.mat_mul(\n\u001b[0m\u001b[1;32m   3715\u001b[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n",
      "\u001b[0;32m/playpen/debman/ood/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   6017\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6020\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6021\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6022\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6023\u001b[0m       return mat_mul_eager_fallback(\n\u001b[1;32m   6024\u001b[0m           \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtranspose_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# epochs = 2000\n",
    "# batch_size = 32\n",
    "\n",
    "# for i in range(epochs):\n",
    "#   print(f\"Epoch {i}\")\n",
    "#   for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\n",
    "#     loss_vals = train_step(vae, batch_betas, batch_dirs)\n",
    "#     if step % 100 == 0: # tmp\n",
    "#       print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon_loss = {loss_vals[1].numpy()}, kl_loss = {loss_vals[2].numpy()}\")\n",
    "#   print()\n",
    "\n",
    "epochs = 3000\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(epochs):\n",
    "  print(f\"Epoch {i}\")\n",
    "  for step, (batch_betas, batch_dirs) in enumerate(batch(betas, random_dirs, batch_size)):\n",
    "    loss_vals = train_step(vae, batch_betas, batch_dirs, i)\n",
    "    if step % 100 == 0: # tmp\n",
    "      print(f\"Step {step}: loss = {loss_vals[0].numpy()}, recon_loss = {loss_vals[1].numpy()}, {loss_vals[2].numpy()}, kl_loss = {loss_vals[3].numpy()}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Uyl_Vz7yFRxU"
   },
   "outputs": [],
   "source": [
    "# Dont really think this works, since the latent space should be conditioned on the direction\n",
    "# Just to try something\n",
    "# Likely better to just have VAE solely on betas w/o directions\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs = np.random.randn(num_samples, d)\n",
    "  random_dirs = random_dirs / np.linalg.norm(random_dirs, axis=1, keepdims=True)\n",
    "  random_dirs = tf.constant(random_dirs)\n",
    "  latent_samples = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples, random_dirs])\n",
    "\n",
    "def generate_new_betas(model, num_samples=1):\n",
    "  random_dirs1 = np.random.randn(num_samples, d)\n",
    "  random_dirs2 = np.random.randn(num_samples, d)\n",
    "  random_dirs1 = random_dirs1 / np.linalg.norm(random_dirs1, axis=1, keepdims=True)\n",
    "  random_dirs1 = tf.constant(random_dirs1)\n",
    "  random_dirs2 = random_dirs2 / np.linalg.norm(random_dirs2, axis=1, keepdims=True)\n",
    "  random_dirs2 = tf.constant(random_dirs2)\n",
    "  latent_samples1 = tf.random.normal(shape=(num_samples, latent_dim))    \n",
    "  latent_samples2 = tf.random.normal(shape=(num_samples, latent_dim))\n",
    "  return model.decoder([latent_samples1, random_dirs1]), random_dirs1, model.decoder([latent_samples2, random_dirs1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "S0XvcHOvKGNd"
   },
   "outputs": [],
   "source": [
    "drawn_betas, dir1, drawn_betas2 = generate_new_betas(vae, 50_000)\n",
    "# drawn_betas = generate_new_betas(vae, 50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "L6UwhrytprJ7"
   },
   "outputs": [],
   "source": [
    "pperm = np.random.permutation(len(external_X))\n",
    "\n",
    "external_X = tf.constant(np.array(external_X)[pperm])\n",
    "external_Y = external_Y[pperm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "7-bWmPkrKQ5a"
   },
   "outputs": [],
   "source": [
    "# with open('/content/drive/My Drive/ml-ood/chem/val_ood.csv', 'r') as f:\n",
    "#   external_X = np.float32(np.array([line.strip().split(',')[4:] for line in f])[1:])\n",
    "\n",
    "# with open('/content/drive/My Drive/ml-ood/chem/val_ood.csv', 'r') as f:\n",
    "#   external_Y = np.float32(np.array([line.strip().split(',')[1] for line in f])[1:])\n",
    "\n",
    "# external_X = (external_X-mu_x)/sigma_x\n",
    "external_randfeats_X = get_rand_feats(external_X@pca_projs, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2uoIqkjR7G_K",
    "outputId": "0bf991ee-3291-4511-829c-846c40205d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0. ... 0. 0. 0.], shape=(2048,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[-0.9500462  -0.9970952  -0.7351471   0.25615945 -0.09993453  0.05183483\n",
      " -0.5821794   0.11442215 -0.07140958  0.35098624], shape=(10,), dtype=float32)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(external_X[0])\n",
    "print(external_randfeats_X[0][:10])\n",
    "print(external_Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZF8wAtKb14p_",
    "outputId": "bfcd92a0-6374-45ae-8850-46540e1e8f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24599, 2048)\n",
      "(24599, 1000)\n",
      "(24599,)\n"
     ]
    }
   ],
   "source": [
    "print(external_X.shape)\n",
    "print(external_randfeats_X.shape)\n",
    "print(external_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35zXouXU2X2f",
    "outputId": "0a3ffc4e-705f-447c-f5fc-3185929e811e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(10, 2048), dtype=float32)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(external_X[:10])\n",
    "print(external_Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "W32O3S_gKnEp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6567, 512) (6567, 512)\n"
     ]
    }
   ],
   "source": [
    "def get_preds(randfeats, betas):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    randfeats: N x d\n",
    "    betas: M x d\n",
    "  Return:\n",
    "    preds: N x M - each beta predicts on each instance\n",
    "  \"\"\"\n",
    "  #preds = []\n",
    "  #for i in range(len(betas)):\n",
    "  #  if i % 25_000 == 0: print(f\"{i} Predictions Made\")\n",
    "  #  preds.append(np.matmul(randfeats, betas[i]))\n",
    "  #return np.array(preds)\n",
    "  sd = (1 / (1 + np.exp(-np.concatenate([np.ones((randfeats.shape[0], 1)), randfeats], axis=-1) @ betas.numpy().T)))\n",
    "  return sd[:]\n",
    "\n",
    "  # betaT = np.transpose(betas) # d x M\n",
    "  # preds = np.matmul(randfeats, betaT) # N x M\n",
    "  # return preds\n",
    "\n",
    "def aggregate_preds(preds):\n",
    "  # mean_pred = np.mean(preds, axis=-1, keepdims=False)\n",
    "  mean_pred = np.sum(preds, axis=-1, keepdims=False)\n",
    "  std_pred = np.std(preds, axis=-1, keepdims=False)\n",
    "  # Typically 0.5 threshold, just was all 0s\n",
    "  return np.float32(mean_pred), np.float32(mean_pred), np.float32(std_pred)\n",
    "\n",
    "def get_preds_and_aggregate_sorted(randfeats, eX, dirs, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  projs = np.dot(tf.linalg.normalize(eX, axis=-1)[0], tf.transpose(tf.linalg.normalize(dirs, axis=-1)[0]))\n",
    "  print(projs.shape, preds.shape)\n",
    "  thresh = np.percentile(projs, 100 - 20, axis=-1)\n",
    "  # wghts = (projs > thresh[:, None]) * projs\n",
    "  # wghts = np.ones_like(projs > thresh[:, None])\n",
    "  wghts = (projs > thresh[:, None]).astype(np.float64)\n",
    "  wghts /= np.sum(wghts, axis=-1, keepdims=True)\n",
    "  return aggregate_preds(preds * wghts)\n",
    "\n",
    "def get_preds_and_aggregate(randfeats, betas):\n",
    "  preds = get_preds(randfeats, betas)\n",
    "  return *aggregate_preds(preds), preds\n",
    "\n",
    "ext_preds, mp_rand, sp_rand = get_preds_and_aggregate_sorted(external_randfeats_X, external_X, random_dirs[:], betas[:]) # 0.622\n",
    "\n",
    "\n",
    "# ext_preds, mp_rand, sp_rand, pred = get_preds_and_aggregate(external_randfeats_X, betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8k8Ut4rSzEG",
    "outputId": "6915fcf3-4622-436b-863d-57f520e7bfe7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01005574,  0.03530712,  0.01948281, ..., -1.0524716 ,\n",
       "        -0.00512372,  0.00957312], dtype=float32),\n",
       " array([-0.0362477 ,  0.51171252,  0.23195012, ...,  0.02307167,\n",
       "         0.96696331,  0.53054202]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drawn_betas[0].numpy(), betas[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kcwk5JuaL9hK",
    "outputId": "c7853e3c-5de9-41b4-ee14-db7cb19048bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gyB9pfuaU0dn",
    "outputId": "6cb5d323-4178-435a-fd68-0c0159e117bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(ext_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Swp-RSU52GmJ",
    "outputId": "9ca75ef9-3c3a-4d03-e527-7ae816f0b812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 Predictions:  [ True False  True  True False  True  True False  True  True]\n",
      "Total Positive Preds:  2931\n",
      "Total Preds:  6567\n",
      "% Positive Preds:  0.4463225216994061\n",
      "\n",
      "First 10 Ground Truth:  [1 1 0 0 0 1 1 1 1 1]\n",
      "Total Positive Ground Truth:  2931\n",
      "Total Ground Truth:  6567\n",
      "% Positive Ground Truth:  0.4463225216994061\n",
      "\n",
      "Accuracy:  0.5370793360743109\n"
     ]
    }
   ],
   "source": [
    "ext_preds = ext_preds > 0.5\n",
    "print(\"First 10 Predictions: \", ext_preds[:10])\n",
    "print(\"Total Positive Preds: \", sum(ext_preds))\n",
    "print(\"Total Preds: \", len(ext_preds))\n",
    "print(\"% Positive Preds: \", sum(ext_preds) / len(ext_preds))\n",
    "print()\n",
    "print(\"First 10 Ground Truth: \", external_Y[:10])\n",
    "print(\"Total Positive Ground Truth: \", sum(external_Y))\n",
    "print(\"Total Ground Truth: \", len(external_Y))\n",
    "print(\"% Positive Ground Truth: \", sum(external_Y) / len(external_Y))\n",
    "print()\n",
    "print(\"Accuracy: \", sum(ext_preds == external_Y) / len(ext_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "L7kl6J3wLped",
    "outputId": "d7a361d9-892f-481e-868d-9ef8e9059946"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2129988/3493594443.py:9: RuntimeWarning: invalid value encountered in true_divide\n",
      "  precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3cd8182b80>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "threshs = sp_rand\n",
    "std_threshs = np.linspace(np.min(threshs), np.max(threshs), 20) # Diff std. dev. thresholds (20 of them in this case)\n",
    "reject_rate = [1 - np.mean((threshs<=s)) for s in std_threshs] # Portion of instances rejected @ each std threshold\n",
    "accus = [np.mean((ext_preds==external_Y)[(threshs<=s)]) for s in std_threshs] # Acc @ each std thresh.\n",
    "tps = [np.sum(((external_Y)*(ext_preds==external_Y))[(threshs<=s)]) for s in std_threshs]  # correct and positive\n",
    "fps = [np.sum(((ext_preds)*(ext_preds!=external_Y))[(threshs<=s)]) for s in std_threshs]  # incorrect and predicted positive\n",
    "pos = np.sum(external_Y)\n",
    "recall = [tp/pos for tp in tps]\n",
    "precision = [tp/(tp+fp) for tp, fp in zip(tps, fps)]\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "\n",
    "plt.plot(recall, precision, marker='+', c='orange')\n",
    "plt.xticks(np.arange(0, 1.01, step=0.1))\n",
    "plt.xticks(np.arange(0, 1.01, step=0.05), minor=True)\n",
    "plt.yticks(np.arange(.3, 1.01, step=0.05))\n",
    "plt.grid(True, which='both')\n",
    "plt.xlabel('Recall ({} Positive)'.format(int(pos)))\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall by Thresholding Ensemble Std')\n",
    "plt.legend(['Autoencoder Method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5eUQIi5uCvqd",
    "outputId": "ac851900-90b2-49e3-ef1b-55b7c3569f4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0b50194880>]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "p, r, thres = precision_recall_curve(external_Y, ext_preds)\n",
    "\n",
    "plt.plot(r, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
